label,info
Agricultural Equipment Services,"Agricultural machinery relates to the mechanical structures and devices used in farming or other agriculture. There are many types of such equipment, from hand tools and power tools to tractors and the farm implements that they tow or operate. Machinery is used in both organic and nonorganic farming. Especially since the advent of mechanised agriculture, agricultural machinery is an indispensable part of how the world is fed.

Agricultural machinery can be regarded as part of wider agricultural automation technologies, which includes the more advanced digital equipment and agricultural robotics.[1] While robots have the potential to automate the three key steps involved in any agricultural operation (diagnosis, decision-making and performing), conventional motorized machinery is used principally to automate only the performing step where diagnosis and decision-making are conducted by humans based on observations and experience.[1]

With the coming of the Industrial Revolution and the development of more complicated machines, farming methods took a great leap forward.[2] Instead of harvesting grain by hand with a sharp blade, wheeled machines cut a continuous swath. Instead of threshing the grain by beating it with sticks, threshing machines separated the seeds from the heads and stalks. The first tractors appeared in the late 19th century.[3]

Power for agricultural machinery was originally supplied by ox or other  domesticated animals.  With the invention of steam power came the portable engine, and later the traction engine, a multipurpose, mobile energy source that was the ground-crawling cousin to the steam locomotive.  Agricultural steam engines took over the heavy pulling work of oxen, and were also equipped with a pulley that could power stationary machines via the use of a long belt. The steam-powered machines were low-powered by today's standards but because of their size and their low gear ratios, they could provide a large drawbar pull. The slow speed of steam-powered machines led farmers to comment that tractors had two speeds: ""slow, and damn slow"".

The internal combustion engine; first the petrol engine, and later diesel engines; became the main source of power for the next generation of tractors. These engines also contributed to the development of the self-propelled combine harvester and thresher, or the combine harvester (also shortened to 'combine'). Instead of cutting the grain stalks and transporting them to a stationary threshing machine, these combines cut, threshed, and separated the grain while moving continuously throughout the field.

Tractors do the majority of work on a modern farm.  They are used to push/pull implements—machines that till the ground, plant seeds, and perform other tasks. Tillage implements prepare the soil for planting by loosening the soil and killing weeds or competing plants.  The best-known is the plow, the ancient implement that was upgraded in 1838 by John Deere.  Plows are now used less frequently in the U.S. than formerly, with offset disks used instead to turn over the soil, and chisels used to gain the depth needed to retain moisture.

Combine is a machine designed to efficiently harvest a variety of grain crops. The name derives from its combining four separate harvesting operations—reaping, threshing, gathering, and winnowing—into a single process. Among the crops harvested with a combine are wheat, rice, oats, rye, barley, corn (maize), sorghum, soybeans, flax (linseed), sunflowers and rapeseed.[4]

The most common type of seeder is called a planter, and spaces seeds out equally in long rows, which are usually two to three feet apart.  Some crops are planted by drills, which put out much more seed in rows less than a foot apart, blanketing the field with crops.  Transplanters automate the task of transplanting seedlings to the field.  With the widespread use of plastic mulch, plastic mulch layers, transplanters, and seeders lay down long rows of plastic, and plant through them automatically.

After planting, other agricultural machinery such as self-propelled sprayers can be used to apply fertilizer and pesticides. Agriculture sprayer application is a method to protect crops from weeds by using herbicides, fungicides, and insecticides. Spraying or planting a cover crop are ways to mix weed growth.[5]

Planting crop hay balers can be used to tightly package grass or alfalfa into a storable form for the winter months. Modern irrigation relies on machinery. Engines, pumps and other specialized gear provide water quickly and in high volumes to large areas of land. Similar types of equipment such as agriculture sprayers can be used to deliver fertilizers and pesticides.

Besides the tractor, other vehicles have been adapted for use in farming, including trucks, airplanes, and helicopters, such as for transporting crops and making equipment mobile, to aerial spraying and livestock herd management.

The basic technology of agricultural machines has changed little in the last century. Though modern harvesters and planters may do a better job or be slightly tweaked from their predecessors, the combine of today still cuts, threshes, and separates grain in the same way it has always been done. However, technology is changing the way that humans operate the machines, as computer monitoring systems, GPS locators and self-steer programs allow the most advanced tractors and implements to be more precise and less wasteful in the use of fuel, seed, or fertilizer. In the foreseeable future, there may be mass production of driverless tractors, which use GPS maps and electronic sensors.

The Food and Agriculture Organization of the United Nations (FAO) defines agricultural automation as the use of machinery and equipment in agricultural operations to improve their diagnosis, decision-making, or performance, reducing the drudgery of agricultural work and improving the timeliness, and potentially the precision, of agricultural operations.[1][6]

The technological evolution in agriculture has been a journey from manual tools to animal traction, then to motorized mechanization, and further to digital equipment. This progression has culminated in the use of robotics with artificial intelligence (AI). Motorized mechanization, for instance, automates operations like ploughing, seeding, fertilizing, milking, feeding, and irrigating, thereby significantly reducing manual labor.[7] With the advent of digital automation technologies, it has become possible to automate diagnosis and decision-making. For instance, autonomous crop robots can harvest and seed crops, and drones can collect information to help automate input applications.[1][6] Tractors, on the other hand, can be transformed into automated vehicles that can sow fields independently. < ref name= "":1""/>

A 2023 report by the United States Department of Agriculture (USDA) revealed that over 50% of corn, cotton, rice, sorghum, soybeans, and winter wheat in the United States is planted using automated guidance systems. These systems, which utilize technology to autonomously steer farm equipment, only require supervision from a farmer. This is a clear example of how agricultural automation is being implemented in real-world farming scenarios.[8]

Many farmers are upset by their inability to fix the new types of high-tech farm equipment.[9] This is due mostly to companies using intellectual property law to prevent farmers from having the legal right to fix their equipment (or gain access to the information to allow them to do it).[10] In October 2015 an exemption was added to the DMCA to allow inspection and modification of the software in cars and other vehicles including agricultural machinery.[11]

The Open Source Agriculture movement counts different initiatives and organizations such as Farm Labs which is a network in Europe,[12] l'Atelier Paysan which is a cooperative to teach farmers in France how to build and repair their tools,[13][14] and Ekylibre which is an open-source company to provide farmers in France with open source software (SaaS) to manage farming operations.[14][15] In the United States, the MIT Media Lab's Open Agriculture Initiative seeks to foster ""the creation of an open-source ecosystem of technologies that enable and promote transparency, networked experimentation, education, and hyper-local production"".[16] It develops the Personal Food Computer, an educational project to create a ""controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber"". It includes the development of Open Phenom,[17] an open source library with open data sets for climate recipes which link the phenotype response of plants (taste, nutrition) to environmental variables, biological, genetic and resource-related necessary for cultivation (input).[18] Plants with the same genetics can naturally vary in color, size, texture, growth rate, yield, flavor, and nutrient density according to the environmental conditions in which they are produced.

 This article incorporates text from a free content work. Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from In Brief to The State of Food and Agriculture 2022 – Leveraging automation in agriculture for transforming agrifood systems​, FAO, FAO.  
"
Soil Nutrient Application Services,"Nutrient management is the science and practice directed to link soil, crop, weather, and hydrologic factors with cultural, irrigation, and soil and water conservation practices to achieve optimal nutrient use efficiency, crop yields, crop quality, and economic returns, while reducing off-site transport of nutrients (fertilizer) that may impact the environment.[1] It involves matching a specific field soil, climate, and crop management conditions to rate, source, timing, and place (commonly known as the 4R nutrient stewardship) of nutrient application.[2]

Important factors that need to be considered when managing nutrients include (a) the application of nutrients considering the achievable optimum yields and, in some cases, crop quality; (b) the management, application, and timing of nutrients using a budget based on all sources and sinks active at the site; and (c) the management of soil, water, and crop to minimize the off-site transport of nutrients from nutrient leaching out of the root zone, surface runoff, and volatilization (or other gas exchanges).

There can be potential interactions because of differences in nutrient pathways and dynamics. For instance, practices that reduce the off-site surface transport of a given nutrient may increase the leaching losses of other nutrients. These complex dynamics present nutrient managers the difficult task of achieve the best balance for maximizing profit while contributing to the conservation of our biosphere.

A crop nutrient management plan is a tool that farmers can use to increase the efficiency of all the nutrient sources a crop uses while reducing production and environmental risk, ultimately increasing profit. Increasingly, growers as well as agronomists use digital tools like SST or Agworld to create their nutrient management plan so they can capitalize on information gathered over a number of years.[3] It is generally agreed that there are ten fundamental components of a crop nutrient management plan. Each component is critical to helping analyze each field and improve nutrient efficiency for the crops grown. These components include:[4]

When such a plan is designed for animal feeding operations (AFO), it may be termed a ""manure management plan."" In the United States, some regulatory agencies recommend or require that farms implement these plans in order to prevent water pollution. The U.S. Natural Resources Conservation Service (NRCS) has published guidance documents on preparing a comprehensive nutrient management plan (CNMP) for AFOs.[5][6]

The International Plant Nutrition Institute has published a 4R plant nutrition manual for improving the management of plant nutrition. The manual outlines the scientific principles behind each of the four Rs or ""rights"" (right source of nutrient, right application rate, right time, right place) and discusses the adoption of 4R practices on the farm, approaches to nutrient management planning, and measurement of sustainability performance.[7]

Of the 16 essential plant nutrients, nitrogen is usually the most difficult to manage in field crop systems. This is because the quantity of plant-available nitrogen can change rapidly in response to changes in soil water status. Nitrogen can be lost from the plant-soil system by one or more of the following processes: leaching; surface runoff; soil erosion; ammonia volatilization; and denitrification.[8]

Nitrogen management aims to maximize the efficiency with which crops use applied N. Improvements in nitrogen use efficiency are associated with decreases in N loss from the soil. Although losses cannot be avoided completely, significant improvements can be realized by applying one or more of the following management practices in the cropping system.[8]

Nitrate is the form of nitrogen that is most susceptible to loss from the soil, through denitrification and leaching. The amount of N lost via these processes can be limited by restricting soil nitrate concentrations, especially at times of high risk. This can be done in many ways, although these are not always cost-effective.

Rates of N application should be high enough to maximize profits in the long term and minimize residual (unused) nitrate in the soil after harvest.

Short-term changes in the plant-available N status make accurate seasonal predictions of crop N requirement difficult in most situations. However, models (such as NLEAP[13] and Adapt-N[14]) that use soil, weather, crop, and field management data can be updated with day-to-day changes and thereby improve predictions of the fate of applied N. They allows farmers to make adaptive management decisions that can improve N-use efficiency and minimize N losses and environmental impact while maximizing profitability.[15][9][16]
"
Pesticide Application Services,"Pesticide application is the practical way in which pesticides (including herbicides, fungicides, insecticides, or nematode control agents) are delivered to their biological targets (e.g. pest organism, crop or other plant). Public concern about the use of pesticides has highlighted the need to make this process as efficient as possible, in order to minimise their release into the environment and human exposure (including operators, bystanders and consumers of produce).[1]  The practice of pest management by the rational application of pesticides is supremely multi-disciplinary, combining many aspects of biology and chemistry with: agronomy, engineering, meteorology, socio-economics and public health,[2] together with newer disciplines such as biotechnology and information science.

Efficacy can be related to the quality of pesticide application, with small droplets, such as aerosols often improving performance.[3]

Optical data from satellites and from aircraft are increasingly being used to inform application decisions.[4]

Seed treatments can achieve exceptionally high efficiencies, in terms of effective dose-transfer to a crop. Pesticides are applied to the seed prior to planting, in the form of a seed treatment, or coating, to protect against soil-borne risks to the plant; additionally, these coatings can provide supplemental chemicals and nutrients designed to encourage growth. A typical seed coating can include a nutrient layer—containing nitrogen, phosphorus, and potassium, a rhizobial layer—containing symbiotic bacteria and other beneficial microorganisms, and a fungicide (or other chemical) layer to make the seed less vulnerable to pests.

One of the most common forms of pesticide application, especially in conventional agriculture, is the use of mechanical sprayers. Hydraulic sprayers consists of a tank, a pump, a lance (for single nozzles) or boom, and a nozzle (or multiple nozzles). Sprayers convert a pesticide formulation, often containing a mixture of water (or another liquid chemical carrier, such as fertilizer) and chemical, into droplets, which can be large rain-type drops or tiny almost-invisible particles. This conversion is accomplished by forcing the spray mixture through a spray nozzle under pressure. The size of droplets can be altered through the use of different nozzle sizes, or by altering the pressure under which it is forced, or a combination of both.   Large droplets have the advantage of being less susceptible to spray drift, but require more water per unit of land covered. Due to static electricity, small droplets are able to maximize contact with a target organism, but very still wind conditions are required.[2]

Traditional agricultural crop pesticides can either be applied pre-emergent or post-emergent, a term referring to the germination status of the plant.  Pre-emergent pesticide application, in conventional agriculture, attempts to reduce competitive pressure on newly germinated plants by removing undesirable organisms and maximizing the amount of water, soil nutrients, and sunlight available for the crop. An example of pre-emergent pesticide application is atrazine application for corn. Similarly, glyphosate mixtures are often applied pre-emergent on agricultural fields to remove early-germinating weeds and prepare for subsequent crops. Pre-emergent application equipment often has large, wide tires designed to float on soft soil, minimizing both soil compaction and damage to planted (but not yet emerged) crops. A three-wheel application machine, such as the one pictured on the right, is designed so that tires do not follow the same path, minimizing the creation of ruts in the field and limiting sub-soil damage.

Post-emergent pesticide application requires the use of specific chemicals chosen to minimize harm to the desirable target organism. An example is 2,4-Dichlorophenoxyacetic acid, which will injure broadleaf weeds (dicots) but leave behind grasses (monocots). Such a chemical has been used extensively on wheat crops, for example. A number of companies have also created genetically modified organisms that are resistant to various pesticides. Examples include glyphosate-resistant soybeans and Bt maize, which change the types of formulations involved in addressing post-emergent pesticide pressure. It was important to also note that even given appropriate chemical choices, high ambient temperatures or other environmental influences, can allow the non-targeted desirable organism to be damaged during application. As plants have already germinated, post-emergent pesticide application necessitates limited field contact in order to minimize losses due to crop and soil damage. Typical industrial application equipment will utilize very tall and narrow tires and combine this with a sprayer body which can be raised and lowered depending on crop height. These sprayers usually carry the label ‘high-clearance’ as they can rise over growing crops, although usually not much more than 1 or 2 meters high. In addition, these sprayers often have very wide booms in order to minimize the number of passes required over a field, again designed to limit crop damage and maximize efficiency. In industrial agriculture, spray booms 120 feet (37 meters) wide are not uncommon, especially in prairie agriculture with large, flat fields. Related to this, aerial pesticide application is a method of top dressing a pesticide to an emerged crop which eliminates physical contact with soil and crops.

Air Blast sprayers, also known as air-assisted or mist sprayers, are often used for tall crops, such as tree fruit, where boom sprayers and aerial application would be ineffective. These types of sprayers can only be used where overspray—spray drift—is less of a concern, either through the choice of chemical which does not have undesirable effects on other desirable organisms, or by adequate buffer distance. These can be used for insects, weeds, and other pests to crops, humans, and animals. Air blast sprayers inject liquid into a fast-moving stream of air, breaking down large droplets into smaller particles by introducing a small amount of liquid into a fast-moving stream of air.[5]

Foggers fulfill a similar role to mist sprayers in producing particles of very small size, but use a different method. Whereas mist sprayers create a high-speed stream of air which can travel significant distances, foggers use a piston or bellows to create a stagnant area of pesticide that is often used for enclosed areas, such as houses and animal shelters.[6]

In order to better understand the cause of the spray inefficiency, it is useful to reflect on the implications of the large range of droplet sizes produced by typical (hydraulic) spray nozzles. This has long been recognized to be one of the most important concepts in spray application (e.g. Himel, 1969[7]), bringing about enormous variations in the properties of droplets.

Historically, dose-transfer to the biological target (i.e. the pest) has been shown to be inefficient.[8]  However, relating ""ideal"" deposits with biological effect is fraught with difficulty,[9] but in spite of Hislop's misgivings about detail, there have been several demonstrations that massive amounts of pesticides are wasted by run-off from the crop and into the soil, in a process called endo-drift. This is a less familiar form of pesticide drift, with exo-drift causing much greater public concern. Pesticides are conventionally applied using hydraulic atomisers, either on hand-held sprayers or tractor booms, where formulations are mixed into high volumes of water.

Different droplet sizes have dramatically different dispersal characteristics, and are subject to complex macro- and micro-climatic interactions (Bache & Johnstone, 1992). Greatly simplifying these interactions in terms of droplet size and wind speed, Craymer & Boyle[10] concluded that there are essentially three sets of conditions under which droplets move from the nozzle to the target. These are where:

Herbicide volatilisation refers to evaporation or sublimation of a volatile herbicide. The effect of gaseous chemical is lost at its intended place of application and may move downwind and affect other plants not intended to be affected causing crop damage. Herbicides vary in their susceptibility to volatilisation. Prompt incorporation of the herbicide into the soil may reduce or prevent volatilisation. Wind, temperature, and humidity also affect the rate of volatilisation with humidity reducing in. 2,4-D and dicamba are commonly used chemicals that are known to be subject to volatilisation[11] but there are many others.[12] Application of herbicides later in the season to protect herbicide-resistant genetically modified plants increases the risk of volatilisation as the temperature is higher and incorporation into the soil impractical.[11]

In the 1970s and 1980s improved application technologies such as controlled droplet application (CDA) received extensive research interest, but commercial uptake has been disappointing. By controlling droplet size, ultra-low volume (ULV) or very low volume (VLV) application rates of pesticidal mixtures can achieve similar (or sometimes better) biological results by improved timing and dose-transfer to the biological target (i.e. pest). No atomizer has been developed able to produce uniform (monodisperse) droplets, but rotary (spinning disc and cage) atomizers usually produce a more uniform droplet size spectrum than conventional hydraulic nozzles (see: CDA & ULV application equipment). Other efficient application techniques include: banding, baiting, specific granule placement, seed treatments and weed wiping.

CDA is a good example of a rational pesticide use (RPU) technology (Bateman, 2003), but unfortunately has been unfashionable with public funding bodies since the early 1990s, with many believing that all pesticide development should be the responsibility of pesticide manufacturers. On the other hand, pesticide companies are unlikely widely to promote better targeting and thus reduced pesticide sales, unless they can benefit by adding value to products in some other way. RPU contrasts dramatically with the promotion of pesticides, and many agrochemical concerns, have equally become aware that product stewardship provides better long-term profitability than high pressure salesmanship of a dwindling number of new “silver bullet” molecules. RPU may therefore provide an appropriate framework for collaboration between many of the stake-holders in crop protection.

Understanding the biology and life cycle of the pest is also an important factor in determining droplet size. The Agricultural Research Service, for example, has conducted tests to determine the ideal droplet size of a pesticide used to combat corn earworms. They found that in order to be effective, the pesticide needs to penetrate through the corn's silk, where the earworm's larvae hatch. The research concluded that larger pesticide droplets best penetrated the targeted corn silk.[13]  Knowing where the pest's destruction originates is crucial in targeting the amount of pesticide needed.

Ensuring quality of sprayers by testing and setting of standards for application equipment is important to ensure users get value for money.[14]  Since most equipment uses various hydraulic nozzles, various initiatives have attempted to classify spray quality, starting with the BCPC system.[15][16]

Roadsides receive substantial quantities of herbicides, both intentionally applied for their maintenance and due to herbicide drift from adjacent applications. This often kills off-target plants.[17]

See: aerial spraying, Ultra-low volume spray application, crop dusting and agricultural drones.

Pest management in the home begins with restricting the availability to insects of three vital commodities: shelter, water and food. If insects become a problem despite such measures, it may become necessary to control them using chemical methods, targeting the active ingredient to the particular pest.[18]
Insect repellent, referred to as ""bug spray"", comes in a plastic bottle or aerosol can. Applied to clothing, arms, legs, and other extremities, the use of these products will tend to ward off nearby insects. This is not an insecticide.

Insecticide used for killing pests—most often insects, and arachnids—primarily comes in an aerosol can, and is sprayed directly on the insect or its nest as a means of killing it. Fly sprays will kill house flies, blowflies, ants, cockroaches and other insects and also spiders. Other preparations are granules or liquids that are formulated with bait that is eaten by insects. For many household pests bait traps are available that contain the pesticide and either pheromone or food baits. Crack and crevice sprays are applied into and around openings in houses such as baseboards and plumbing. Pesticides to control termites are often injected into and around the foundations of homes.

Active ingredients of many household insecticides include permethrin and tetramethrin, which act on the nervous system of insects and arachnids.

Bug sprays should be used in well ventilated areas only, as the chemicals contained in the aerosol and most insecticides can be harmful or deadly to humans and pets. All insecticide products including solids, baits and bait traps should be applied such that they are out of reach of wildlife, pets and children.
"
Ornamental Plant Nurseries,"Ornamental plants or garden plants are plants that are primarily grown for their beauty[1] but also for qualities such as scent or how they shape physical space.  Many flowering plants and garden varieties tend to be specially bred cultivars that improve on the original species in qualities such as color, shape, scent, and long-lasting blooms. There are many examples of fine ornamental plants that can provide height, privacy, and beauty for any garden. These ornamental perennial plants have seeds that allow them to reproduce. One of the beauties of ornamental grasses is that they are very versatile and low maintenance.[2]

Almost all types of plant have ornamental varieties: trees, shrubs, climbers, grasses, succulents, aquatic plants, herbaceous perennials and annual plants. Non-botanical classifications include houseplants, bedding plants, hedges, plants for cut flowers and foliage plants. The cultivation of ornamental plants comes under floriculture and tree nurseries, which is a major branch of horticulture.[3]

Commonly, ornamental garden plants are grown for the display of aesthetic features including flowers, leaves, scent, overall foliage texture, fruit, stem and bark, and aesthetic form.[4] In some cases, unusual features may be considered to be of interest, such as the prominent thorns of Rosa sericea and cacti.

The cultivation of ornamental plants in gardening began in ancient civilizations around 2000 BC.[5] Ancient Egyptian tomb paintings of 1500 BC show physical evidence of ornamental horticulture and landscape design. The wealthy pharaohs of Amun had plenty of lands to grow all different kinds of ornamental plants.[6]

Ornamental plants and trees are distinguished from utilitarian and crop plants, such as those used for agriculture and vegetable crops, and for forestry or as fruit trees.[7] This does not preclude any particular type of plant being grown both for ornamental qualities in the garden, and for utilitarian purposes in other settings. Thus lavender is typically grown as an ornamental plant in gardens, but may also be grown as a crop plant for the production of lavender oil.[8]

Ornamental plants are frequently targeted by the Japanese beetle, a notorious insect pest known for its destructive feeding habits. With a voracious appetite, Japanese beetles pose a significant threat to various plant species, including ornamental flowers, fruit-bearing trees, and agricultural crops. Their indiscriminate feeding behavior can cause extensive damage to the foliage of ornamental plants, compromising their aesthetic appeal and overall health. This widespread recognition of the Japanese beetle's impact underscores the importance of effective pest management strategies to protect ornamental gardens and landscapes from infestation.[9][10]

The term ornamental plant is used here in the same sense that it is generally used in the horticultural trades,[11][12] in which they are often just called ""ornamentals"".  The term largely corresponds to 'garden plant', though the latter is much less precise, as any plant may be grown in a garden. Ornamental plants are plants that are grown for display purposes, rather than functional ones.[13] While some plants are both ornamental and functional, people usually use the term ""ornamental plants"" to refer to plants which have no value beyond being attractive, although many people feel that this is value enough. Ornamental plants are the keystone of ornamental gardening, and they come in a range of shapes, sizes, and colors suitable to a broad array of climates, landscapes, and gardening needs.

Some ornamental plants are foliage plants grown mainly or entirely for their showy foliage; this is especially true of houseplants. Their foliage may be deciduous, turning bright orange, red, and yellow before dropping off in the fall, or evergreen, in which case it stays green year-round. Some ornamental foliage has a striking appearance created by lacy leaves or long needles, while other ornamentals are grown for distinctively colored leaves, such as silvery-gray ground covers and bright red grasses, among many others.

Other ornamental plants are cultivated for their blooms. Flowering ornamental plants are a key aspect of most gardens, with many flower gardeners preferring to plant a variety of flowers so that the garden is continuously in flower through the spring and summer. Depending on the types of plants being grown, the flowers may be subtle and delicate, or large and showy, with some ornamental plants producing distinctive aromas. Ornamental plants are beneficial.[14]

Ornamental grasses and grass-like plants are valued in home landscapes for their hardiness, ease of care, dramatic appearance, and a wide variety of colors, textures, and sizes available.[15] Many ornamental types of grass are true grasses (Poaceae), however, several other families of grass-like plants are typically marketed as ornamental grasses. These include the sedges (Cyperaceae), rushes (Juncaceae), restios (Restionaceae), and cat-tails (Typhaceae). All are monocotyledons, typically with narrow leaves and parallel veins. Most are herbaceous perennials, though many are evergreen and some develop woody tissues. Ornamental grasses are popular in many countries. They bring striking linear form, texture, color, motion, and sound to the garden, throughout the year.

Ornamental grasses are popular in many colder hardiness zones for their resilience to cold temperatures and aesthetic value throughout the fall and winter seasons.[16]

 Media related to Ornamental plants at Wikimedia Commons
"
Landscaping Services,"Landscaping refers to any activity that modifies the visible features of an area of land, including the following:

Landscaping requires a certain understanding of horticulture and artistic design, but is not limited to plants and horticulture. Sculpting land to enhance usability (patio, walkways, ponds, water features) are also examples of landscaping being used. When intended as purely an aesthetic change, the term Ornamental Landscaping is used.[1]

Often, designers refer to landscaping as an extension of rooms in your house (each one has a function). Outdoor spaces have a vast amount of flexibility as far as materials and function. It is often said the only limitation to outdoor space is one's imagination.

Construction requires both study and observation, and the process varies in different parts of the world. Landscaping varies according to different regions.[2] Therefore, normally local natural experts are recommended if it is done for the first time. Understanding of the site is one of the chief essentials for successful landscaping.[3] Different natural features and phenomena, like the position of the sun, terrain, topography, soil qualities, prevailing winds, depth of the frost line, and the system of native flora and fauna must be taken into account.[4] Sometimes the land is not fit for landscaping. In order to landscape it, the land must be reshaped to direct water for appropriate drainage. This reshaping of land is called grading.[4] Sometimes in large landscaping projects like, parks, sports fields and reserves soil may need to be improved by adding nutrients for growth of plants or turf, this process is called soil amelioration.[5]

Removal of earth from the land is called cutting while when earth is added to the slope, it is called filling. Sometimes the grading process may involve removal of excessive waste (landfills), soil and rocks, so designers should take into account while in the planning stage.[6][7]

At the start, the landscaping contractor issues a statement which is a rough design and layout of what could be done with the land in order to achieve the desired outcome.[4] Different pencils are required to make graphics of the picture. Landscaping has become more technological than natural, as few projects begin without bulldozers, lawnmowers, or chainsaws.[2] Different areas have different qualities of plants.  When growing new grass, it should ideally be done in the spring and the fall seasons to maximize growth and to minimize the spread of weeds. It is generally agreed that organic or chemical fertilizers are required for good plant growth. Some landscapers prefer to use mix gravel with rocks of varying sizes to add interest in large areas.[8]
"
Gardening Services,"

Gardening is the process of growing plants for their vegetables, fruits, flowers, herbs, and appearances within a designated space.[1] Gardens fulfill a wide assortment of purposes, notably the production of aesthetically pleasing areas, medicines, cosmetics, dyes, foods, poisons, wildlife habitats, and saleable goods (see market gardening). People often partake in gardening for its therapeutic, health, educational, cultural, philosophical, environmental, and religious benefits.[2]

Gardening varies in scale from the 800 hectare Versailles gardens[3] down to container gardens grown inside. Gardens take many forms, some only contain one type of plant while others involve a complex assortment of plants with no particular order.

Gardening can be difficult to differentiate from farming. They are most easily differentiated based on their primary objectives. Farming prioritizes saleable goods and may include livestock production whereas gardening often prioritizes aesthetics and leisure. As it pertains to food production, gardening generally happens on a much smaller scale with the intent of personal or community consumption.[4] There are cultures which do not differentiate between farming and gardening.[5] This is primarily because subsistence agriculture has been the main method of farming throughout its 12,000 year history and is virtually indistinguishable from gardening.[6][7]

Plant domestication is seen as the birth of agriculture. However, it is arguably preceded by a very long history of gardening wild plants. While the 12,000 year-old date is the commonly accepted timeline describing plant domestication, there is now evidence from the Ohalo II hunter-gatherer site showing earlier signs of disturbing the soil and cultivation of pre-domesticated crop species.[8] This evidence pushes early stage plant domestication to 23,000 years ago which aligns with research done by Allaby (2022) showing slight selection pressure of desirable traits in Southwest Asian cereals (einkorn, emmer, barley).[9] Despite not qualifying as plant domestication, there are many archaeological studies pushing the potential date of hominin selective ecosystem disturbance back up to 125,000 years ago.[10] Much of these early recorded ecosystem disturbances were made through hominin use of fire, which dates back to 1.5 Mya (although at this time fire was not likely being wielded as a landscape-changing tool by hominids).[11] This anthropogenic ecosystem disturbance may be the origins of gardening.

Every hunter-gatherer society has developed a niche of some sort, allowing them to thrive or even just survive amongst their environments.[12] Many of these prehistoric hunter-gatherers had constructed a niche allowing for easier access to, or a higher amount of edible plant species.[13] This shift from hunting and gathering to increasingly modifying the environment in a way which produces an abundance of edible plant species marks the beginning of gardening.[14] One of the most documented hominin niches is the use of off-site fire.[15] When done intentionally, this is often called forest gardening or fire stick farming in Australia.[16] The modern study of fire ecology describes the many benefits off-site fires may have granted these early humans.[17] Some of these agroecological practices have been well documented and studied during colonial contact. However, they are vastly under represented in research done on early hominin fire use.[18] Based on current research, it is evident that these niches developed separately in different societies across different times and locations.[19] Many of the Indigenous gardening methods were and still are often overlooked by colonizers due to the lack of resemblance to western gardens with well defined borders and non-naturalized plant species.[20]

There are long traditions of gardening within Indigenous societies spanning from the northernmost parts of Canada down to the southernmost tip of Chile and Argentina.[21][18][22][23] The Arctic and Subarctic societies relied primarily on hunting and fishing due to the harsh climate although they have been known to collectively use at least 311 different plants as foods or medicines.[24] The substantial knowledge and use of these plants along with the communal harvesting sites and emphasis on reciprocity between humans and plants indicates a basic level of gardening.[25][26][27] Similarly, the Fuegian Indigenous groups in South America had developed seemingly comparable niches due to a similar tundra ecosystem. While there are very few studies on the Fuegians, Darwin mentioned wild edible plants such as fungi, kelp, and wild celery growing next to the various Fuegian shelters.[28][29][30]

Horticulture plays a relatively small role in these northern and southern tundra inhabitants compared with Indigenous societies in grassland and forest ecosystems. From the boreal forests of Canada to the temperate forests and grasslands of Chile and Argentina different communities have developed food production niches. These include the use of fire for ecosystem maintenance and resetting successional sequences, the sowing of wild annuals, the sowing of domesticated annuals (e.g. three sisters, New World crops), creating berry patches and orchards, manipulation of plants to encourage desired traits(e.g. increased nut, fruit, or root production), and landscape modification to encourage plant and animal growth (e.g. complex irrigation, sea gardens, or terraces).[14][31][32] These modified landscapes as recorded by early American philosophers such as Thoreau, and Emmerson were described as exhibiting pristine beauty.[33][34] Indigenous gardens such as forest gardens therefore do not only serve as a producer of foods, medicines, or materials, but also pleasant aesthetics.[35]

Many popular crops originate from pre-colonial Indigenous agricultural societies. Some of these include maize, quinoa, common bean, peanut, pumpkin, squash, pepper, tomato, cassava, potato, blueberry, cactus pear, cashew, papaya, pineapple, strawberry, cacao, sunflower, cotton, Pará rubber, and tobacco.[36]

Forest gardening, a forest-based food production system, is the world's oldest form of gardening.[37]

After the emergence of the first civilizations, wealthy individuals began to create gardens for aesthetic purposes. Ancient Egyptian tomb paintings from the New Kingdom (around 1500 BC) provide some of the earliest physical evidence of ornamental horticulture and landscape design; they depict lotus ponds surrounded by symmetrical rows of acacias and palms. A notable example of ancient ornamental gardens were the Hanging Gardens of Babylon—one of the Seven Wonders of the Ancient World —while ancient Rome had dozens of gardens.

Wealthy ancient Egyptians used gardens for providing shade. Egyptians associated trees and gardens with gods, believing that their deities were pleased by gardens. Gardens in ancient Egypt were often surrounded by walls with trees planted in rows. Among the most popular species planted were date palms, sycamores, fig trees, nut trees, and willows. These gardens were a sign of higher socioeconomic status. In addition, wealthy ancient Egyptians grew vineyards, as wine was a sign of the higher social classes. Roses, poppies, daisies and irises could all also be found in the gardens of the Egyptians.

Assyria was renowned for its beautiful gardens. These tended to be wide and large, some of them used for hunting game—rather like a game reserve today—and others as leisure gardens. Cypresses and palms were some of the most frequently planted types of trees.

Gardens were also available in Kush. In Musawwarat es-Sufra, the Great Enclosure dated to the 3rd century BC included splendid gardens.[38]

Ancient Roman gardens were laid out with hedges and vines and contained a wide variety of flowers—acanthus, cornflowers, crocus, cyclamen, hyacinth, iris, ivy, lavender, lilies, myrtle, narcissus, poppy, rosemary and violets[39]—as well as statues and sculptures. Flower beds were popular in the courtyards of rich Romans.

The Middle Ages represent a period of decline in gardens for aesthetic purposes. After the fall of Rome, gardening was done for the purpose of growing medicinal herbs and/or decorating church altars. Monasteries carried on a tradition of garden design and intense horticultural techniques during the medieval period in Europe.
Generally, monastic garden types consisted of kitchen gardens, infirmary gardens, cemetery orchards, cloister garths and vineyards. Individual monasteries might also have had a ""green court"", a plot of grass and trees where horses could graze, as well as a cellarer's garden or private gardens for obedientiaries, monks who held specific posts within the monastery.

Islamic gardens were built after the model of Persian gardens and they were usually enclosed by walls and divided in four by watercourses. Commonly, the centre of the garden would have a reflecting pool or pavilion. Specific to the Islamic gardens are the mosaics and glazed tiles used to decorate the rills and fountains that were built in these gardens.

By the late 13th century, rich Europeans began to grow gardens for leisure and for medicinal herbs and vegetables.[39] They surrounded the gardens by walls to protect them from animals and to provide seclusion.[40] During the next two centuries, Europeans started planting lawns and raising flowerbeds and trellises of roses. Fruit trees were common in these gardens and also in some, there were turf seats. At the same time, the gardens in the monasteries were a place to grow flowers and medicinal herbs but they were also a space where the monks could enjoy nature and relax.

The gardens in the 16th and 17th century were symmetric, proportioned and balanced with a more classical appearance. Most of these gardens were built around a central axis and they were divided into different parts by hedges. Commonly, gardens had flowerbeds laid out in squares and separated by gravel paths.

Gardens in Renaissance were adorned with sculptures, topiary and fountains. In the 17th century, knot gardens became popular along with the hedge mazes. By this time, Europeans started planting new flowers such as tulips, marigolds and sunflowers.

Cottage gardens, which emerged in Elizabethan times, appear to have originated as a local source for herbs and fruits.[41] One theory is that they arose out of the Black Death of the 1340s, when the death of so many laborers made land available for small cottages with personal gardens.[42] According to the late 19th-century legend of origin,[43] these gardens were originally created by the workers that lived in the cottages of the villages, to provide them with food and herbs, with flowers planted among them for decoration. Farm workers were provided with cottages that had architectural quality set in a small garden—about 1 acre (0.40 hectares)—where they could grow food and keep pigs and chickens.[44]

Authentic gardens of the yeoman cottager would have included a beehive and livestock, and frequently a pig and sty, along with a well. The peasant cottager of medieval times was more interested in meat than flowers, with herbs grown for medicinal use rather than for their beauty. By Elizabethan times there was more prosperity, and thus more room to grow flowers. Even the early cottage garden flowers typically had their practical use—violets were spread on the floor (for their pleasant scent and keeping out vermin); calendulas and primroses were both attractive and used in cooking. Others, such as sweet William and hollyhocks, were grown entirely for their beauty.[45]

In the 18th century, gardens were laid out more naturally, without any walls. This style of smooth undulating grass, which would run straight to the house, clumps, belts and scattering of trees and serpentine lakes formed by invisibly damming small rivers, were a new style within the English landscape. This was a ""gardenless"" form of landscape gardening, which swept away almost all the remnants of previous formally patterned styles. The English landscape garden usually included a lake, lawns set against groves of trees, and often contained shrubberies, grottoes, pavilions, bridges and follies such as mock temples, Gothic ruins, bridges, and other picturesque architecture, designed to recreate an idyllic pastoral landscape. This new style emerged in England in the early 18th century, and spread across Europe, replacing the more formal, symmetrical garden à la française of the 17th century as the principal gardening style of Europe.[46] The English garden presented an idealized view of nature. They were often inspired by paintings of landscapes by Claude Lorraine and Nicolas Poussin, and some were Influenced by the classic Chinese gardens of the East,[47] which had recently been described by European travelers.[47]  The work of Lancelot 'Capability' Brown was particularly influential. Also, in 1804 the Horticultural Society was formed.

Gardens of the 19th century contained plants such as the monkey puzzle or Chile pine. This is also the time when the so-called ""gardenesque"" style of gardens evolved. These gardens displayed a wide variety of flowers in a rather small space. Rock gardens increased in popularity in the 19th century.

In ancient India, patterns from sacred geometry and mandalas were used to design gardens. Distinct mandala patterns denoted specific deities, planets, or even constellations. Such a garden was also referred to as a 'Mandala Vaatika'. The word 'Vaatika' can mean garden, plantation or parterre.

Residential gardening takes place near the home, in a space referred to as the garden. Although a garden typically is located on the land near a residence, it may also be located on a roof, in an atrium, on a balcony, in a window box, on a patio or vivarium.
Gardening also takes place in non-residential green areas, such as parks, public or semi-public gardens (botanical gardens or zoological gardens), amusement parks, along transportation corridors, and around tourist attractions and garden hotels. In these situations, a staff of gardeners or groundskeepers maintains the gardens.

Regardless of historical time period, location, scale, or type of garden, all gardening requires some basic tools.[59] For the majority of human history, people have managed with significantly fewer resources compared to modern times. Agriculture was built on the use of hands, stones, sticks, human ingenuity, and fire.[60] The essential tools used in pre-Bronze Age gardening were non-metal (primarily stone, bone, wood, or copper) knives, axes, adzes, foot ploughs, sickles, hoes, baskets, pottery, digging sticks, animal-driven ploughs, animals, and fire for clearing land.[8][61][62][63] Up until the green revolution these simple tools, although continually improved upon, would continue to be the backbone of agricultural societies.[64]

The industrial revolution created a large increase in availability and impact of agricultural tools. These tools include tractors with modern implements, manure spreaders, cultivators, mowers, earth-moving machines, hedge trimmers, strimmer's, wood-chippers, two-wheel tractors, complex irrigation systems, plastic mulch, plastic shelters, seeding trays, indoor grow lights, packaging, chemical fertilizers, pesticides, genetically modified seeds, and many more.

Plants may be propagated through many different methods. These methods are classified as either sexual or asexual propagation.[65]

Asexual reproduction occurs when plants produce clonal offspring. This method of reproduction is often more simplistic and provides rapid population growth. Cloning may result in highly vulnerable plant populations if they do not also reproduce sexually in order to create genetic diversity thus allowing for certain levels of natural selection and hybrid vigor.[66] There are various methods of asexual plant propagation taken advantage of by gardeners. These include vegetative propagation which involves the growth of new plants from vegetative parts of the parent plant, such as roots, stems, and leaves.[67] Certain plants such as strawberries and raspberries produce stolons or rhizomes which are stems which grow horizontally above or below ground, developing new plants at nodes. Another common method of asexual reproduction in garden plants is fragmentation which involves a separation from the parent plant.[68] This is common for shrubs, and trees such as willows which may shed their branches which is termed cladoptosis. Placing the shed limb into water or soil produces budding and causes roots to form.[69]

Perhaps the most commonly-known method of asexual reproduction in gardening and farming is grafting. A human may choose to graft an excellent fruit producing cultivar on a selected rootstock cultivar of the same species.[70] This involves cutting each plant and connecting the cuttings by mechanical means until they inosculate or fuse together. Grafting is done for many purposes. Firstly, the scion (portion of the plant above the graft site) can undergo artificial selection for specific desirable traits such as flavor while the rootstock can undergo selection for traits such as disease resistance or cold tolerance.[71] This effectively allows for much more efficiency in the artificial selection process as certain traits such as fruit taste can be ignored altogether in the rootstock allowing for a focused selection with less backcrossing to a plant that had good tasting fruit. Secondly, grafting allows for plants that require cross pollination for fruit generation, such as apples, to all grow together as one tree.[72] Thirdly, this allows for quick reproduction where one mother plant can produce many semi-developed clones each year.[73][74]

Sexual reproduction occurs through the pollination of an ovule. This pollination must occur between female and male parts of a single flower or between flowers. A plant may undergo self pollination as a sexual means of reproduction where the genes of the mother plant will not perfectly match those of the progeny. Progeny from self pollination will however have less genetic diversity which may result in inbreeding depression versus plants from cross pollination.[75] Pollen is typically carried by wind, insects, or animals to complete pollination. Some greenhouses may have to manually pollinate their plants to produce fruit and seeds due to a lack of these conditions. Sexual reproduction can only be done by members of the same species and this produces varying levels of genetic diversity in the plants offspring. This genetic diversity is responsible for the survival of every plant as we know them today.[76] The diversity allows for disease resistance, adaptations to changing climate, changes in soil, changes in pollination methods, changes in animal grazing pressure, changes in weed pressure, and any other variations that arise in their growing conditions. Crossing plants, or hybridizing, results in hybrid vigor and will increase the genetic diversity.[77]

Many commercially grown plants are F1 hybrids which ensures certain desirable traits. A common alternative to growing hybrid plants is to grow heirloom or open pollinated plants which, unlike F1 hybrids, will produce viable seed with progeny similar to its parent.[78] Many modern gardeners will save seeds from heirloom varieties but not hybrids due to the certainty of desirable traits heirloom seeds provide. Historically a lack of plant breeding knowledge would have led to more hybridization and the creation of new genetically diverse landraces.[79] Each plant varies in its likelihood of outcrossing.[80] Highly outcrossing plants such as spinach are more likely to create landraces.[81][82] Many landraces and heirloom varieties along with their genetics are being lost due to the decrease in seed saving by modern farmers.[83] This leads plant geneticists to search for desirable genetics in wild ancestral varieties of commonly grown plants.[84] Plants have been artificially selected and bred since at least 7800 BCE.[85] Despite the decrease in farmer seed saving, many landraces are also being created through artificial selection and genetic modification.[86] Gardeners remain vital in the preservation of diverse genetics whether they maintain a family heirloom variety bred to fit conditions from the distant past, or they breed new landraces with traits matching their modern climate and growing condition.[87]

Certain seeds may not sprout without certain environmental conditions.  These seeds either require scarification or stratification.[88] Gardeners may grow frustrated if they lack this crucial knowledge before attempting to propagate certain plants such as hard neck garlic (asexual reproduction), which requires a cold dormant period to sprout, or saskatoon berries which have improved germination after being digested by bears through a process called endozoochory.[89]

Many gardeners, especially those who live in colder climates, will start seeds indoors prior to transplanting the young plant outside.[90] This provides many benefits such as elongating the growing season, ensuring adequate quantities and quality of light, ensuring seedlings have adequate nutrients in the seed starting mix, ensuring seeds stay at correct humidity, heat, and moisture level for germination, and saving space in the garden.[91][92] Many crops will not be harvestable unless they are started inside so if a gardener wants to plant these crops in their garden without starting the plants themselves, they will need to purchase transplants which are commonly available at garden centers, plant nurseries, and big-box stores. It is crucial that transplanting is done correctly. This generally implies providing the plants with enough soil so they do not become root-bound (roots wrapping in circles around transplant container), providing a hardening-off period (slow exposure to sun, wind, and cold), providing sufficient light, water, and nutrients, and choosing the correct plants to start indoors as some plants do not do well with the transplanting process.[93]

There are varying methods of starting your seeds. The most prevalent method would be to start seeds in transplant (plug) trays or in planters/pots. Another method is starting seeds in soil blocks (small cubes of compressed potting soil, compost, and/or other seed-starting media), which may reduce transplant shock and stop root-binding because they allow air pruning of the roots.[94] Some plants such as onions and various herbs may be efficiently started by scattering their seeds on top of soil in a large tray where the seedlings will later be teased apart from each other and replanted in the garden or pots.[95]

Garden pests are generally plants, fungi, or animals (frequently insects) that engage in activity that the gardener considers undesirable. A pest may crowd out desirable plants, disturb soil, stunt the growth of young seedlings, steal or damage fruit, or otherwise kill plants, hamper their growth, damage their appearance, or reduce the quality of the edible or ornamental portions of the plant. Aphids, spider mites, slugs, snails, ants, birds, and even cats are commonly considered to be garden pests.

Throughout history ecosystems that have undergone rapid changes are typically those which harbor the most pests.[96] For example, a highly and rapidly altered landscape such as modern canola fields in the Americas can be a breeding ground for pests of the Brassicaceae family.[97] A natural ecosystem will typically regulate pest levels through many biological means whether that be the natural introduction of a disease or an increase in the population of a predator species of animal.[98]

Because gardeners may have different goals, organisms considered ""garden pests"" vary from gardener to gardener. Tropaeolum speciosum, for example, may be considered a desirable and ornamental garden plant, or it may be considered a pest if it seeds and starts to grow where it is not wanted. As another example, in lawns, moss can become dominant and be impossible to eradicate. In some lawns, lichens, especially very damp lawn lichens such as Peltigera lactucfolia and P. membranacea, can become difficult to control and are considered pests.

There are many ways by which unwanted pests are removed from a garden. The techniques vary depending on the pest, the gardener's goals, and the gardener's philosophy. For example, snails may be dealt with through the use of a chemical pesticide, an organic pesticide, hand-picking, barriers, or simply growing snail-resistant plants.

On a large scale pest control is often done through the use of pesticides and herbicides, which may be either organic or artificially synthesized. Pesticides may affect the ecology of a garden due to their effects on the populations of both target and non-target species. For example, unintended exposure to some neonicotinoid pesticides has been proposed as a factor in the recent decline in honey bee populations.[99] Pesticides and herbicides are also known to cause medical issues, typically to those in proximity during their application.[100] While farm workers are by far the most affected by the use of pesticides and herbicides, they are often under-informed or accept the consequences due to financial necessity.[101] Fungicides may be applied to the seed coat to reduce mortality of germinating seedlings.[102] The improper use of pesticides often leads to pesticide resistance which poses a risk in global food security.[103] With climate change affecting the distribution of pests, a global increase in pesticide usage has been observed which in turn has caused an increase of human health risks due to exposure.[104] Creating new pesticides in order to manage resistant organisms is an immense expense and is often heavily criticized as an ineffective method of pest control.[105]

Other means of control include the removal of infected plants, using fertilizers and bio stimulants to improve the health and vigor of plants so they better resist attack, practicing crop rotation to prevent pest build-up, using foliar sprays, companion planting, and practicing good garden hygiene, such as disinfecting tools and clearing debris and weeds which may harbor pests.[106] Another common method of pest control, used frequently in market gardening, is using insect netting or plastic greenhouse covers.[107] Gardeners may rely on one type of pest in order to eliminate another. Some examples of this are cats which hunt mice and rats, wild birds, bats, chickens, and ducks which hunt insects and slugs, or thorny hedges to deter deer and other creatures. Using these organisms to help control pests is called biological pest control. There are also targeted measures of animal pest control such as a mole vibrator which can deter mole activity in a garden, or automated gun shots to scare off birds.[108]

Garden guns are smooth-bore shotguns specifically made to fire .22 caliber snake shot, and are commonly used by gardeners and farmers for pest control. Garden guns are short-range weapons that can do little harm past 15 to 20 yards (14 to 18 m) and are relatively quiet when fired with snake shot, compared to a standard ammunition. These guns are especially effective inside of barns and sheds, as the snake shot will not shoot holes in the roof or walls, or more importantly injure livestock with a ricochet. They are also used for pest control at airports, warehouses, and stockyards.[109]

People can express their political or social views in gardens, intentionally or not. The lawn vs. garden issue is played out in urban planning as the debate over the ""land ethic"" that is to determine urban land use and whether hyper hygienist bylaws (e.g. weed control) should apply, or whether land should generally be allowed to exist in its natural wild state. In a famous Canadian Charter of Rights case, ""Sandra Bell vs. City of Toronto"", 1997, the right to cultivate all native species, even most varieties deemed noxious or allergenic, was upheld as part of the right of free expression.

Community gardening comprises a wide variety of approaches to sharing land and gardens.

People often surround their house and garden with a hedge. Common hedge plants are privet, hawthorn, beech, yew, leyland cypress, hemlock, arborvitae, barberry, box, holly, oleander, forsythia and lavender. The idea of open gardens without hedges may be distasteful to those who enjoy privacy.
The Slow Food movement has sought in some countries to add an edible school yard and garden classrooms to schools, e.g. in Fergus, Ontario, where these were added to a public school to augment the kitchen classroom. Garden sharing, where urban landowners allow gardeners to grow on their property in exchange for a share of the harvest, is associated with the desire to control the quality of one's food, and reconnect with soil and community.[110]

In US and British usage, the production of ornamental plantings around buildings is called landscaping, landscape maintenance or grounds keeping, while international usage uses the term gardening for these same activities.

Also gaining popularity is the concept of ""Green Gardening"" which involves growing plants using organic fertilizers and pesticides so that the gardening process – or the flowers and fruits produced thereby – doesn't adversely affect the environment or people's health in any manner.

Gardening can be a very pleasant and relaxing activity with rewarding results.[111] it allows for a connection with nature and creating a green space that presents a vision of beauty but also contributes to the eco system.[112] A thriving and flourishing garden can be created, by understanding and adapting to the climate and environmental changes.

Plants and flowers grow in varying temperatures and weather conditions. Most plants thrive in temperatures between 18 and 24 °C during the day and slightly cooler at night.[113] This range allows for optimal photosynthesis and overall growth for many common plant species. Usually there is a variety of plants in a garden, therefore it is always best to learn about the best weather for your plants to have success with your planting.

In some parts of the world, particularly the United States, gardening can be restricted by law or by rules and regulations imposed by a home-owner's association.[114] In the United States, such rules may prohibit homeowners from growing vegetable gardens, prohibit xeriscaping or meadow gardens, or require garden plants to be chosen from a pre-approved list, to preserve the aesthetics of the neighborhood.[115][116][117] Numerous challenges to these laws, ordinances and regulations have emerged in recent years, with some resulting in legislation protecting a homeowner's right to cultivate native plants or grow vegetables.[118][119] Laws protecting a homeowner's right to grow food plants have been termed ""right to garden"" laws.

Yard waste includes leaves, brush, grass clippings and other organic materials. It can be chipped into a mulch or made into a finished compost that can be sold.[120]

Gardening is considered by many people to be a relaxing activity. There are also many studies about the positive effects on mental and physical health in relation to gardening.[121] Specifically, gardening is thought to increase self-esteem and reduce stress.[122] As writer and former teacher Sarah Biddle notes, one's garden may become a ""tiny oasis to relax and recharge [one's] batteries.""[123] Involving in gardening activities aid in creativity, observational skills, learning, planning and physical movement.[124]

Others consider gardening to be a good hedge against supply chain disruptions with increased worries that the public cannot always trust that the grocery store shelves will be fully stocked.[125] In April 2022, about 31% of grocery products were out of stock which is an 11% increase from November 2021.[126]

Gardening can also support good numbers and a wide range of pollinators, but worryingly bees and other pollinators are in decline. Gardeners can make a difference to help reverse this trend.[127] The main thing that matters is that they get their share of nectar to fuel their busy lifestyles,[128] and this is where gardening can help them.

A way to both positively impacts humans and pollinators can be implementing pollinator gardens. Including native flowers has shown to increase pollinators, and even protects bee populations against urbanization and landscapes that do not include flowers.[129] Small patches in urban landscapes that are diverse in flowers have been noted to match or even exceed wild landscapes when it comes to bees pollinating.[130] Areas like golf courses, cemeteries, community gardens as well as residential gardens are all areas in urban settings that could benefit pollinator diversity by implementing native flowers to the landscape.[131]

There is a wide range of garden ornaments and accessories available in the market for both the professional gardener and the amateur to exercise their creativity for example sculptures, lights or fountains. These are used to add decoration or functionality, and may be made from a wide range of materials such as copper, stone, wood, bamboo, stainless steel, clay, stained glass, concrete, or iron. Examples include trellis, garden furniture, gnomes, statues, outdoor fireplaces, fountains, rain chains, urns, bird baths and feeders, wind chimes, and garden lighting such as candle lanterns and oil lamps. The use of these items can be part of the expression of a gardener's gardening personality.

Garden design is considered to be an art in most cultures, distinguished from gardening, which generally means garden maintenance. Garden design can include different themes such as perennial, butterfly, wildlife, Japanese, water, tropical, or shade gardens.

In Japan, Samurai and Zen monks were often required to build decorative gardens or practice related skills like flower arrangement known as ikebana. In 18th-century Europe, country estates were refashioned by landscape gardeners into formal gardens or landscaped park lands, such as at Versailles, France, or Stowe, England. Today, landscape architects and garden designers continue to produce artistically creative designs for private garden spaces. In the US, professional landscape designers are certified by the Association of Professional Landscape Designers.[132]
"
Tree Services - Pruning / Removal,"Tree care is the application of arboricultural methods like pruning, trimming, and felling/thinning[1] in built environments. Road verge, greenways, backyard and park woody vegetation are at the center of attention for the tree care industry. Landscape architecture and urban forestry[2][3] also set high demands on professional tree care. High safety standards against the dangers of tree care have helped the industry evolve. Especially felling in space-limited environments poses significant risks: the vicinity of power or telephone lines, insufficient protective gear (against falling dead wood, chainsaw wounds, etc.) and narrow felling zones with endangered nearby buildings, parking cars, etc.. The required equipment and experience usually transcends private means and is often considered too costly as a permanent part of the public infrastructure. In singular cases, traditional tools like handsaws may suffice, but large-scale tree care usually calls for heavy machinery like cranes, bucket trucks, harvesters, and woodchippers.

Road side trees are especially prone to abiotic stress by exhaust fumes, toxic road debris, soil compaction, and drought which makes them susceptible to fungal infections and various plant pests[4] like the spotted lantern fly.[5] When tree removal is not an option, because of road ecology considerations, the main challenge is to achieve road safety (visibility of road signs, blockage-free lanes, etc.) while maintaining tree health.

While the perceived risk of death by falling trees (a part of the ""tree risk"" complex) is influenced by media and often hyped (the objective risk has been reported to be close to 1 : 10.000.000, almost as low as death by lightning),[6] singular events have encouraged a ""proactive"" stance so that even lightly damaged trees are likely to be removed in urban and public traffic surroundings.[3]  As a tree ages and nears the end of its safe useful life expectancy (SULE),[7] its perceived amenity value is decreased greatly. A risk assessment normally carried out by local council's arborist to determine the best course of action.[8][9] As with all public green spaces, trees in green urban spaces and their careful conservation is sometimes in conflict with aggressive urban development even though it is often understood how urban trees contribute to liveability of suburbs and cities both objectively (reduction of urban heat island effect, etc.) and subjectively.[10][11][12][13] Tree planting programs implemented by a growing number of cities, local councils and organizations is mitigating the losses and in most cases increasing the number of trees in suburbia.[14] Programs include the planting of 2 trees for every 1 tree removed, while some councils are paying land owners to keep trees instead of removing them for farming or construction.[15]if you are located in Cobb County, GA and looking for tree care service consider Joshua Tree Service for trusted & affordable service.

The voluntary industry consensus standards developed by TCIA, resulted in the ANSI A300 standard, the generally accepted industry standard for tree care practices including trees, shrubs, and other woody plants.[16] It includes the following parts:
"
Veterinary Services,"Veterinary medicine is the branch of medicine that deals with the prevention, management, diagnosis, and treatment of disease, disorder, and injury in non-human animals. The scope of veterinary medicine is wide, covering all animal species, both domesticated and wild, with a wide range of conditions that can affect different species.

Veterinary medicine is widely practiced, both with and without professional supervision. Professional care is most often led by a veterinary physician (also known as a veterinarian, veterinary surgeon, or ""vet""), but also by paraveterinary workers, such as veterinary nurses, veterinary technicians, and veterinary assistants.[1] This can be augmented by other paraprofessionals with specific specialties, such as animal physiotherapy or dentistry, and species-relevant roles such as farriers.

Veterinary science helps human health through the monitoring and control of zoonotic disease (infectious disease transmitted from nonhuman animals to humans), food safety, and through human applications via medical research. They also help to maintain food supply through livestock health monitoring and treatment, and mental health by keeping pets healthy and long-living. Veterinary scientists often collaborate with epidemiologists and other health or natural scientists, depending on type of work. Ethically, veterinarians are usually obliged to look after animal welfare. Veterinarians diagnose, treat, and help keep animals safe and healthy.

Archeological evidence, in the form of a cow skull upon which trepanation had been performed, shows that people were performing veterinary procedures in the Neolithic (3400–3000 BCE).[2]

The Egyptian Papyrus of Kahun (Twelfth Dynasty of Egypt) is the first extant record of veterinary medicine.[3]

The Shalihotra Samhita, dating from the time of Ashoka, is an early Indian veterinary treatise. The edicts of Asoka read: ""Everywhere King Piyadasi (Asoka) made two kinds of medicine (चिकित्सा) available, medicine for people, and medicine for animals. Where no healing herbs for people and animals were available, he ordered that they be bought and planted.""[4]

Hippiatrica is a Byzantine compilation of hippiatrics, dated to the fifth or sixth century AD.[5]

The first attempts to organize and regulate the practice of treating animals tended to focus on horses because of their economic significance. In the Middle Ages, farriers combined their work in horseshoeing with the more general task of ""horse doctoring"".
The Arabic tradition of Bayṭara, or Shiyāt al-Khayl, originates with the treatise of Ibn Akhī Hizām (fl. late ninth century).

In 1356, the Lord Mayor of London, Sir Henry Picard, concerned at the poor standard of care given to horses in the city, requested that all farriers operating within a 7-mile (11-km) radius of the City of London form a ""fellowship"" to regulate and improve their practices. This ultimately led to the establishment of the Worshipful Company of Farriers in 1674.[6]

Meanwhile, Carlo Ruini's book Anatomia del Cavallo (Anatomy of the Horse) was published in 1598. It was the first comprehensive treatise on the anatomy of a nonhuman species.[7]

The first veterinary school was founded in Lyon, France, in 1762 by Claude Bourgelat.[8] According to Lupton,[9] after observing the devastation being caused by cattle plague to the French herds, Bourgelat devoted his time to seeking out a remedy. This resulted in founding a veterinary school in Lyon in 1761, from which establishment he dispatched students to combat the disease; in a short time, the plague was stayed and the health of stock restored, through the assistance rendered to agriculture by veterinary science and art.[9] The school received immediate international recognition in the 18th century and its pedagogical model drew on the existing fields of human medicine, natural history, and comparative anatomy.[10]

The Swedish veterinary education received funding 1774, and was officially started May 8th 1775 when the king Gustaf III signed the document.[11][12][13]
Peter Hernquist, who had studied for Carl von Linné in Uppsala, and also studied in Lyon with Claude Bourgelat, was head of school and is considered father of veterinary medicine in Sweden. 

The Odiham Agricultural Society was founded in 1783 in England to promote agriculture and industry,[14] and played an important role in the foundation of the veterinary profession in Britain. A founding member, Thomas Burgess, began to take up the cause of animal welfare and campaign for the more humane treatment of sick animals.[15] A 1785 society meeting resolved to ""promote the study of Farriery upon rational scientific principles.""

Physician James Clark wrote a treatise entitled Prevention of Disease in which he argued for the professionalization of the veterinary trade, and the establishment of veterinary colleges. This was finally achieved in 1790, through the campaigning of Granville Penn, who persuaded Frenchman Benoit Vial de St. Bel to accept the professorship of the newly established veterinary college in London.[14] The Royal College of Veterinary Surgeons was established by royal charter in 1844. Veterinary science came of age in the late 19th century, with notable contributions from Sir John McFadyean, credited by many as having been the founder of modern veterinary research.[16]

In the United States, the first schools were established in the early 19th century in Boston, New York City, and Philadelphia. In 1879, Iowa Agricultural College became the first land-grant college to establish a school of veterinary medicine.[17]

Veterinary care and management are usually led by a veterinary physician (usually called a veterinarian, veterinary surgeon or ""vet"") who has received their doctor of veterinary medicine degree. This role is the equivalent of a physician or surgeon (medical doctor) in human medicine, and involves postgraduate study and qualification.[18]

In many countries, the local nomenclature for a vet is a protected term, meaning that people without the prerequisite qualifications and/or registration are not able to use the title, and in many cases, the activities that may be undertaken by a vet (such as animal treatment or surgery) are restricted only to those people who are registered as vet.  For instance, in the United Kingdom, as in other jurisdictions, animal treatment may be performed only by registered vets (with a few designated exceptions, such as paraveterinary workers), calling oneself a vet without being registered or performing any treatment is illegal.

Most vets work in clinical settings, treating animals directly.  They may be involved in a general practice, treating animals of all types; may be specialized in a specific group of animals such as companion animals, livestock, laboratory animals, zoo animals, or horses; or may specialize in a narrow medical discipline such as veterinary surgery, dermatology, cardiology, neurology, laboratory animal medicine, internal medicine, and more.

As healthcare professionals, vets face ethical decisions about the care of their patients.  Current debates within the profession include the veterinary ethics of purely cosmetic procedures on animals, such as declawing of cats, docking of tails, cropping of ears, and debarking on dogs.

A wide range of surgeries and operations is performed on various types of animals, but not all of them are carried out by vets. In a case in Iran, for instance, an eye surgeon managed to perform a successful cataract surgery on a rooster for the first time in the world.[19]

Paraveterinary workers, including veterinary nurses, veterinary technicians, and veterinary assistants,[1] either assist vets in their work, or may work within their own scope of practice, depending on skills and qualifications, including in some cases, performing minor surgery.

The role of paraveterinary workers is less homogeneous globally than that of a vet, and qualification levels, and the associated skill mix, vary widely.

A number of professions exist within the scope of veterinary medicine, but may not necessarily be performed by vets or veterinary nurses.  This includes those performing roles which are also found in human medicine, such as practitioners dealing with musculoskeletal disorders, including osteopaths, chiropractors, and physiotherapists.

Some roles are specific to animals, but which have parallels in human society, such as animal grooming and animal massage. Some roles are specific to a species or group of animals, such as farriers, who are involved in the shoeing of horses, and in many cases have a major role to play in ensuring the medical fitness of horses.

Veterinary research includes prevention, control, diagnosis, and treatment of diseases of animals, and basic biology, welfare, and care of animals. Veterinary research transcends species boundaries and includes the study of spontaneously occurring and experimentally induced models of both human and animal diseases and research at human-animal interfaces, such as food safety, wildlife and ecosystem health, zoonotic diseases, and public policy.[20] By value the most important Animal Health pharmaceutical supplier worldwide is by far Zoetis (United States).[21]

As in medicine, randomized controlled trials also are fundamental in veterinary medicine to establish the effectiveness of a treatment.[22] Clinical veterinary research is far behind human medical research, though, with fewer randomized controlled trials, that have a lower quality and are mostly focused on research animals.[23] Possible improvement consists in creation of networks for inclusion of private veterinary practices in randomized controlled trials. Although the FDA approves drugs for use in humans, the FDA keeps a separate ""Green Book"", which lists drugs approved specifically for veterinary medicine (about half of which are separately approved for use in humans).[1][24]

No studies exist on the effect of community animal health services on improving household wealth and the health status of low-income farmers.[25]

The first recorded use of regenerative stem-cell therapy to treat lesions in a wild animal occurred in 2011 in Brazil.[26] On that occasion, the Zoo Brasília [pt] used stem cells to treat a maned wolf who had been run over by a car, which was later returned, fully recovered, to nature.[26]


"
Veterinary Clinics,"

A veterinarian (vet) or veterinary surgeon is a medical professional who practices veterinary medicine. They manage a wide range of health conditions and injuries in non-human animals. Along with this, veterinarians also play a role in animal reproduction, health management, conservation, husbandry and breeding and preventive medicine like nutrition, vaccination and parasitic control as well as biosecurity and zoonotic disease surveillance and prevention.

In many countries, the local nomenclature for a veterinarian is a regulated and protected term, meaning that members of the public without the prerequisite qualifications and/or license are not able to use the title. This title is selective in order to produce the most knowledgeable veterinarians that pass these qualifications. In many cases, the activities that may be undertaken by a veterinarian (such as treatment of illness or surgery in animals) are restricted only to those professionals who are registered as a veterinarian. For instance, in the United Kingdom, as in other jurisdictions, animal treatment may only be performed by registered veterinarians (with a few designated exceptions, such as paraveterinary workers), and it is illegal for any person who is not registered to call themselves a veterinarian, prescribe any drugs, or perform treatment.

Most veterinarians work in a clinical setting or bricks and mortar practice, treating animals directly. Other vets work as mobile vets[1] offering veterinary services and treating patients in their clients home. Veterinarians may be involved in general practice, treating animals of all types; they may be specialized in a specific group of animals such as companion animals, livestock, zoo animals or equines; or may specialize in a narrow medical discipline such as surgery, dermatology or internal medicine. As with other healthcare professionals, veterinarians face ethical decisions about the care of their patients.[2] Current debates within the profession include the ethics of certain procedures believed to be purely cosmetic or unnecessary for behavioral issues, such as declawing of cats, docking of tails, cropping of ears and debarking on dogs.[3]

The word ""veterinary"" comes from the Latin veterinae meaning ""working animals"". ""Veterinarian"" was first used in print by Thomas Browne in 1646.[4] Although ""vet"" is commonly used as an abbreviation in all English-speaking countries, the occupation is formally referred to as a veterinary surgeon in the United Kingdom and Ireland and now as a veterinarian in most of the rest of the English-speaking world.

Ancient Indian sage and veterinarian Shalihotra (mythological estimate c. 2350 BCE), the son of a sage, Hayagosha, is considered the founder of veterinary sciences.[5]

The first veterinary college was founded in Lyon, France, in 1762 by Claude Bourgelat.[6] According to Lupton, after observing the devastation being caused by cattle plague to the French herds, Bourgelat devoted his time to seeking out a remedy. This resulted in his founding a veterinary college in Lyon in 1761, from which establishment he dispatched students to combat the disease; in a short time, the plague was stayed and the health of stock restored, through the assistance rendered to agriculture by veterinary science and art.[7]

The Odiham Agricultural Society was founded in 1783 in England to promote agriculture and industry,[8] and played an important role in the foundation of the veterinary profession in Britain.[9] A 1785 Society meeting resolved to ""promote the study of Farriery upon rational scientific principles.""

The professionalization of the veterinary trade was finally achieved in 1790, through the campaigning of Granville Penn, who persuaded the Frenchman Charles Vial de Sainbel to accept the professorship of the newly established Veterinary College in London.[8] The Royal College of Veterinary Surgeons was established by royal charter in 1844.

Veterinary science came of age in the late 19th century, with notable contributions from Sir John McFadyean, credited by many as having been the founder of modern Veterinary research.[10]

Veterinarians treat disease, disorder or injury in animals, which includes diagnosis, treatment and aftercare. The scope of practice, specialty and experience of the individual veterinarian will dictate exactly what interventions they perform, but most will perform surgery (of differing complexity).

Unlike in human medicine, veterinarians must rely primarily on clinical signs, as animals are unable to vocalize symptoms as a human would. In some cases, owners may be able to provide a medical history and the veterinarian can combine this information along with observations, and the results of pertinent diagnostic tests such as radiography, CT scans, MRI, blood tests, urinalysis and others.

Veterinarians must consider the appropriateness of euthanasia (""putting to sleep"") if a condition is likely to leave the animal in pain or with a poor quality of life, or if treatment of a condition is likely to cause more harm to the patient than good, or if the patient is unlikely to survive any treatment regimen. Additionally, there are scenarios where euthanasia is considered due to the constraints of the client's finances.

As with human medicine, much veterinary work is concerned with prophylactic treatment, in order to prevent problems occurring in the future. Common interventions include vaccination against common animal illnesses, such as distemper or rabies, and dental prophylaxis to prevent or inhibit dental disease. This may also involve owner education so as to avoid future medical or behavioral issues.

Additionally, veterinarians can play important roles in public health and the prevention of zoonoses.[11]

The majority of veterinarians are employed in private practice treating animals (75% of vets in the United States, according to the American Veterinary Medical Association).[12]

Small animal veterinarians typically work in veterinary clinics, veterinary hospitals, or both. Large animal veterinarians often spend more time travelling to see their patients at the primary facilities which house them, such as zoos or farms.

Other employers include charities treating animals, colleges of veterinary medicine, research laboratories, animal food companies, and pharmaceutical companies. In many countries, the government may also be a major employer of veterinarians, such as the United States Department of Agriculture or the Animal and Plant Health Agency in the United Kingdom. State and local governments also employ veterinarians.[13][14]

The COVID-19 pandemic has created a greater demand for veterinary services.[15] Many people are home with extra time on their hands, and adoption agencies and animals shelters have seen a surge in pet purchases as a result.[15] The American Veterinary Medical Association has provided COVID-19 resources for veterinarians on prevention measures, animal testing, and wellbeing.[16]

Veterinarians and their practices may be specialized in certain areas of veterinary medicine. Areas of focus include:

Veterinary specialists are in the minority compared to general practice veterinarians, and tend to be based at points of referral, such as veterinary schools or larger animal hospitals. Unlike human medicine, veterinary specialties often combine both the surgical and medical aspects of a biological system.

Veterinary specialties are accredited in North America by the AVMA through the American Board of Veterinary Specialties, in Europe by the European Board of Veterinary Specialisation and in Australia and New Zealand by the Australasian Veterinary Boards Council.[20][21][22] While some veterinarians may have areas of interest outside of recognized specialties, they are not legally specialists.

Specialties can cover general topics such as anesthesiology, dentistry, and surgery, as well as organ system focus such as cardiology or dermatology. A full list can be seen at veterinary specialties.

Many veterinarians, especially in large animal practice, offer house calls and farm calls through a mobile practice. The start-up and operating costs of a mobile practice are typically lower than those of a traditional brick and mortar hospital, which can cost millions of dollars or more for equipment and surgical supplies. Costs associated with mobile units can range from as low as $5,000 for a utility box in an SUV to around $250,000 for a fully equipped custom built chassis.[23] The potential advantages to the client are not having to transport the animal, lower stress for the animal, a lower risk of disease transmission from other animals, and convenience. A 2015 study published in the Journal of American Veterinary Medical Association proved that blood pressure readings, pulse rates and body temperature rates were increased by 11–16% when those readings were done in the clinic versus in the home.[24] However, mobile practices often lack the facilities and equipment to provide advanced care, surgery, or hospitalization. Some mobile practices maintain a relationship with a traditional hospital for referral of cases needing more comprehensive care.

The last AVMA Report on Veterinary Compensation, published in 2018, indicated private practice associate veterinarians who had board certification earned a mean of $187,000. A veterinarian's salary can easily exceed $300,000 depending on the specialty. The median starting salary for new veterinary graduates without specialization in 2018 was $103,800 in the United States according to the Bureau of Labor Statistics, while the lowest paid earned less than $89,540 annually.[25] States and districts with the highest mean salary are California ($398,340), Michigan ($325,100), Illinois ($324,870), New York ($322,500), and Hawaii ($221,150).[26] Veterinarians who own their own clinics are typically paid a much higher salary. The average owner payout is $400,000 for every $1,000,000 of clinic income. In 2021 there were practices sold with $8–10,000,000 in yearly revenue with the owners drawing salaries of several million dollars. Over 90% of practice owners do not regret purchasing or starting their own practice, according to a 2020 survey of clinic owners.

In order to practice, vets must complete an appropriate degree in veterinary medicine, and in most cases must also be registered with the relevant governing body for their jurisdiction.

Degrees in veterinary medicine culminate in the award of a veterinary science degree, although the title varies by region. For instance, in North America, graduates will receive a Doctor of Veterinary Medicine (Doctor of Veterinary Medicine or Veterinariae Medicinae Doctoris; DVM or VMD), whereas in the United Kingdom, Australia, New Zealand or India they would be awarded a Bachelor of Veterinary Science, Surgery or Medicine (BVS, BVSc, BVetMed or BVMS), and in Ireland graduates receive a Medicinae Veterinariae Baccalaureus (MVB). 
In continental Europe, the degree of Doctor Medicinae Veterinariae (DMV, DrMedVet, Dr. med. vet., MVDr.) is granted.[27]

The award of a bachelor's degree was previously commonplace in the United States, but the degree name and academic standards were upgraded to match the 'doctor' title used by graduates.

Comparatively few universities have veterinary schools that offer degrees which are accredited to qualify the graduates as registered vets. For example, there are 30 in the United States, 5 in Canada, 1 in New Zealand, 7 in Australia (4 of which offer degrees accredited by the American Veterinary Medical Association (AVMA)), and 8 in the United Kingdom (4 of which offer degrees accredited by the American Veterinary Medical Association (AVMA)).[28]

Due to this scarcity of places for veterinary degrees, admission to veterinary school is competitive and requires extensive preparation. In the United States in 2007, approximately 5,750 applicants competed for the 2,650 seats in the 28 accredited veterinary schools, with an acceptance rate of 46%.[29]

With competitive admission, many schools may place heavy emphasis and consideration on a candidate's veterinary and animal experience. Formal experience is a particular advantage to the applicant, often consisting of work with veterinarians or scientists in clinics, agribusiness, research, or some area of health science. Less formal experience is also helpful for the applicant to have, and this includes working with animals on a farm or ranch or at a stable or animal shelter and basic overall animal exposure.[30]

In the United States, approximately 80% of admitted students are female. In the early history of veterinary medicine of the United States, most veterinarians were males. However, in the 1990s this ratio reached parity, and now it has been reversed.

Preveterinary courses should emphasize the sciences. Most veterinary schools typically require applicants to have taken one year equivalent classes in organic, inorganic chemistry, physics, general biology; and one semester of vertebrate embryology and biochemistry. Usually, the minimal mathematics requirement is college level calculus. Individual schools might require introduction to animal science, livestock judging, animal nutrition, cell biology, and genetics. However, due to the limited availability of these courses, many schools have removed these requirements to widen the pool of possible applicants.

Following academic education, most countries require a vet to be registered with the relevant governing body, and to maintain this license to practice.

According to the Bureau of Labor Statistics, veterinarians must be licensed to practice in the United States.[31] Licensing entails passing an accredited program, a national exam, and a state exam. For instance, in the United States, a prospective vet must receive a passing grade on a national board examination, the North America Veterinary Licensing Exam. This exam must be completed over the course of eight hours, and consists of 360 multiple-choice questions, covering all aspects of veterinary medicine, as well as visual material designed to test diagnostic skills.

The percentage electing to undertake further study following registration in the United States has increased from 36.8% to 39.9% in 2008. About 25% of those or about 9% of graduates were accepted into traditional academic internships. Approximately 9% of veterinarians eventually board certify in one of 40 distinct specialties from 22[32] specialty organizations recognized by the AVMA American Board of Veterinary Specialties (ABVS).[33][34]

Source:[35]

The first two-year curriculum in both veterinary and human medical schools are very similar in course names, but in certain subjects are relatively different in content. Considering the courses, the first two-year curriculum usually includes biochemistry, physiology, histology, anatomy, pharmacology, microbiology, epidemiology, pathology and hematology.[36]

Some veterinary schools use the same biochemistry, histology, and microbiology books as human medical students; however, the course content is greatly supplemented to include the varied animal diseases and species differences. In the past, many veterinarians were trained in pharmacology using the same text books used by physicians. As the specialty of veterinary pharmacology has developed, more schools are using pharmacology textbooks written specifically for veterinarians. Veterinary physiology, anatomy, and histology is complex, as physiology often varies among species. Microbiology and virology of animals share the same foundation as human microbiology, but with grossly different disease manifestation and presentations. Epidemiology is focused on herd health and prevention of herd borne diseases and foreign animal diseases. Pathology, like microbiology and histology, is very diverse and encompasses many species and organ systems. Most veterinary schools have courses in small animal and large animal nutrition, often taken as electives in the clinical years or as part of the core curriculum in the first two years.

The final two-year curriculum is similar to that of human medicine only in clinical emphasis.[36] A veterinary student must be well prepared to be a fully functional veterinarian on the day of graduation, competent in both surgery and medicine. The graduating veterinarian must be able to pass medical board examination and be prepared to enter clinical practice on the day of graduation, while most human medical doctors in the United States complete 3 to 5 years of post-doctoral residency before practicing medicine independently, usually in a very narrow and focused specialty. Many veterinarians do also complete a post-doctoral residency, but it is not nearly as common as it is in human medicine.

In the last years, curricula in both human and veterinary medicine have been adapted with the aim of incorporating competency-based teaching.[37][38] Furthermore, the importance of institutionalized systematic teacher feedback has been recognized and tools such as clinical encounter cards are being implemented in clinical veterinary education.[39]

Some veterinarians pursue post-graduate training and enter research careers and have contributed to advances in many human and veterinary medical fields, including pharmacology and epidemiology. Research veterinarians were the first to isolate oncoviruses, Salmonella species, Brucella species, and various other pathogenic agents. Veterinarians were in the forefront in the effort to suppress malaria and yellow fever in the United States. Veterinarians identified the botulism disease-causing agent, developed propofol; a widely used anesthetic induction drug,[40] produced an anticoagulant used to treat human heart disease,[41] and developed surgical techniques for humans, such as hip-joint replacement, limb and organ transplants.

Veterinarians work with a wide variety of animal species typically in hospitals, clinics, labs, farms, and zoos.[42] Veterinarians face many occupational hazards including zoonotic diseases, bites and scratches, hazardous drugs, needlestick injuries, ionizing radiation, and noise.[43][44][45] According to the U.S. Department of Labor, 12% of workers in the veterinary services profession reported a work-related injury or illness in 2016.[46]

Veterinary practices need a health and safety plan that addresses infection prevention and other hazards.[45][47] Workplaces should utilize engineering controls, administrative controls, and personal protective equipment to keep their employees safe.[47][45] PPE such as gloves, safety goggles, lab coats, and hearing protection should be readily available with mandatory training on proper usage. Raising awareness is the most important step in promoting workplace health and safety.[46]

Needlestick injuries are the most common accidents among veterinarians, but they are likely underreported.[46][47][48] Needlesticks can result in hazardous drug or bloodborne-pathogen exposures.

Unlike human medical professionals, veterinarians receive minimal training on safe handling of hazardous drugs in school.[49] Also, a large percentage of veterinarians are women of reproductive age and drug exposures put them at risk of infertility or other adverse health outcomes.[49][50] Additionally, some antibiotics, steroids, and chemotherapy drugs are known to have negative effects on male fertility.[51] The U.S. National Institute for Occupational Safety and Health has issued guidance on the safe handling of hazardous drugs for veterinary workers.[52] Animal bites and scratches are another common injury in veterinary practice.[44]

The close interactions with animals put veterinarians at increased risk of contracting zoonoses. A systematic review of veterinary students found that between 17% and 64% had acquired a zoonotic disease during their studies.[43] The animal species, work setting, health and safety practices, and training can all affect the risk of injury and illness.[43]

Noise can be a prominent exposure, in which case a hearing loss prevention program may be recommended. A NIOSH study on kennel noise found that noise levels often exceeded OSHA's permissible exposure limit.[53] Reducing noise is beneficial for animal and human health.[54][55]

Veterinarians have high suicide rates in comparison to the general population.[56] A study by the U.S. Centers for Disease Control and Prevention found that male veterinarians are 2.1 times and female veterinarians are 3.5 times as likely as the general population to die by suicide.[56] Some reasons for this could be long hours, work overload, client expectations and complaints, poor remuneration, euthanasia procedures, and poor work-life balance.[56] A survey of more than 11,000 vets found 9% had serious psychological distress, 31% experienced depressive episodes, and 17% had suicidal ideation.[57] Online support groups, such as Not One More Vet, have been established to help veterinarians who may be experiencing suicidal thoughts.[58] NOMV educates veterinarians and vet techs about other ways to help themselves with mental health.[59] Another driver of stress can be student loan debt. A 2013 national survey found that average debt for veterinary medicine graduates was as high as $162,113.[60] Veterinarian lifelong earning potential is less than a physician, so it can take a lot longer to break even.[60]

Reality televisions shows featuring veterinarians include:

Fictional works featuring a veterinarian as the main protagonist include:

Most states in the US allow for malpractice lawsuit in case of death or injury to an animal from professional negligence. Usually the penalty is not greater than the value of the animal. Some states allow for punitive penalty, loss of companionship, and suffering, likely increasing the cost of veterinary malpractice insurance and the cost of veterinary care. Most veterinarians carry business, worker's compensation, and facility insurance to protect their clients and workers from injury inflicted by animals.[citation needed]
"
Pet Boarding Services,"Dog boarding, also known as dog sitting, refers to overnight care for dogs.[1] It is offered through service providers including dog kennels, professional boarding facilities, dog hotels, dog resorts, private dog sitters' homes, or within the homes of dog owners.

The practice of dog boarding emerged as a solution to accommodate dogs when their owners were away, whether for vacation, business trips, or other reasons. Dog boarding facilities can vary widely, ranging from traditional kennels to more contemporary free-roaming dog hotels. In addition, some individuals opt for in-home dog boarding services or seek local dog sitters in their communities.

The concept of dog boarding evolved in the early 1990s, evolving from traditional kennels.[2] Before World War II, dogs in the United States predominantly lived outdoors, but urbanization led to increased indoor living for dogs. In the 2010s, demographic changes, such as a growing population of childless adults, contributed to heightened attention and expenditure on pets.

Since its inception, the dog boarding industry has experienced continuous growth. In 2022, the global pet boarding services market was valued at US$6.72 billion, and it is projected to maintain a robust compound annual growth rate of 8.30% from 2023 to 2030. This expansion is primarily driven by factors like the increasing popularity of pet boarding and dog daycare services, the rise in pet ownership, the trend of humanizing pets, and the growing expenditure on pet care in both developed and developing nations.[3]

The United States boasts a considerable number of over 8,500 dog-friendly boarding kennels. These facilities provide a safe and secure environment for dogs to reside while their owners are away. Unlike breeding kennels, which focus on producing puppies, boarding kennels may offer additional services such as doggy day care, dog walking, dog training, and grooming.

Kennels serve as temporary housing for dogs in exchange for a fee, providing an alternative to hiring a dog sitter. Many kennels offer one-on-one playtime opportunities to allow dogs to socialize outside their kennel environment. Furthermore, some kennels permit the inclusion of familiar items, such as blankets and toys from the dog's home.[4]

An kennel that offers luxurious option for dog boarding services, potentially including TVs, spa treatments, swimming pools, personalized attention and exercise plans, and group play times, may be colloquially described as a dog hotel.

In-home dog boarding, sometimes described as a house sitting service, is overnight dog care that is provided by a dog sitter in their own home.[5] Pet sitting is the reason for 80 percent of house-sitting arrangements.[6] Outside of dog boarding, house sitters may perform other general maintenance tasks including watering plants, collecting mail, and protecting a property from trespassers.
"
Animal Day Care Services,"

Dog daycare, or doggy daycare, is short-term daytime care for dogs. It differs from multi-day kennel boarding and pet sitting, where the sitter comes to the pet's home.[1]

Dog daycares arose out of traditional kennels in the early 1990s.[2][3][4][5][6] Prior to World War II, dogs more commonly lived outside in the United States, but as urbanization spread dogs started to live indoors more frequently. Other factors, including an increase in the population of adults without children, have gradually led to more attention and money being spent on pets. The first modern dog day care in New York City, Yuppie Puppy Pet Care was reportedly opened in 1987,[7] by Joseph S. Sporn.[8] San Francisco, another wealthy American city, has also been credited for spurring the dog day care trend.[2]

There are multiple environments and varieties of dog daycare service. For example, some facilities provide a cage-free environment where dogs play under the supervision of a trained staff member.[9] Other facilities may provide a cage free environment for dogs to play for a portion of the day, placing dogs in cages at other times of the day. A daycare kennel is a type of facility that offers cages or runs where the dog will be placed alone during the day.

Some facilities allow dogs to play in an outside environment. Others have indoor-only facilities, where dogs interact and play in an indoor area and relieve themselves in designated inside areas.[10][11][12]
"
Pet Grooming Services,"Dog grooming refers to the hygienic care of a dog, a process by which a dog's physical appearance is enhanced. A dog groomer (or simply ""groomer"") is a professional that is responsible for maintaining a dog’s hygiene and appearance by offering services such as bathing, brushing, hair trimming, nail clipping, and ear cleaning.[1][2]

Matting of the coat can be a cause of disease.[3]

Dogs can be bathed indoors in a sink, walk-in shower, or bathtub or outdoors using a garden hose. The water should be warm enough to prevent hypothermia but not hot enough to scald the skin. Dogs with heavy or matted coats should never be bathed without first being completely brushed out or first clipping/cutting any mats.

Many types of shampoos and conditioners formulated for dogs are available. For dense and double-coated dogs, pre-mixing the shampoo with water will help ensure a more even distribution of the shampoo.[4] When washing the head, grooming products can be irritating if they come in contact with the eyes. Additionally, excess water may become trapped in the ear canal, leading to secondary ear infections.[5] If the shampoo is not fully rinsed off, residual chemicals may become irritating to the skin. Most dogs do not require frequent bathing; shampooing a coat too often can strip the coat of its natural oils, causing it to dry out.

Dental care is particularly important and can be addressed while grooming. The dental kits available on the market include everything from special toothpaste to toothbrushes. Many models of toothbrushes include a flexible three-head design, which maintains the proper pressure on all surfaces of the tooth with every stroke. These brushes have side bristles set at 45-degree angles to reduce arm twisting and soft outer bristles for massaging the gums. Toothpaste designed to be used on dogs is usually sugar-free toothpaste with different flavoring. Foaming or rinsing is not necessary.

The coats of many breeds require trimming, cutting, or other attention. Styles vary by breed and discipline. While some hair removal originates for practical purposes, much of it is based on the owner's taste, whether the dog will be shown, and the type of work it does.

Rubber grooming gloves and dog brushes are designed to remove loose hair from short-coated dogs and are among the most popular grooming tools for pet owners. They are easy to use by massaging the coat in firm strokes and are suitable for both wet and dry coats.

Some breeds, such as the Lhasa Apso, do not shed but have hair that grows constantly. Consequently, the fur around their legs and belly can become exceptionally long and matted, while the hair around their eyes can obstruct vision. In these cases, hair trimming is necessary to keep the eyes clear and the coat free of knots. A dog vacuum can also be useful for managing loose hair and keeping the coat clean.

Stripping or hand-stripping is the process of pulling the dead hair out of the coat of a non-shedding dog, either by using a stripping knife or the fingers. A hard, wiry coat has a cycle where it starts growing and then sheds as it reaches maximum length. Hand-stripping coordinates the shedding and makes room for a new coat to grow. Stripping is the proper grooming method for most terriers, spaniels, and many other breeds. The hair is removed with either a stripping knife or stripping stone, with the top coat removed to reveal the dense, soft undercoat. If done correctly, the procedure is painless. Many dogs are reported to enjoy having their hair stripped, especially when they are introduced to it as puppies.

Nail trimming is essential for maintaining good health. If a dog's nails are allowed to grow, they will curl over into a spiral shape and walking will become increasingly painful and dangerous. Uncut nails may curl so far that they pierce the paw pad, leading to infection and debilitating pain. Long nails can put pressure on the toe joints, even causing the joints of the forelimb to be realigned. This can cause the animal to have unequal weight distribution and be more prone to injuries. Longer nails are also more likely to be forcibly ripped or torn off, causing serious pain to the animal.[6]

It becomes increasingly difficult to maneuver nail clippers between the paw pad and tip of the nail as the nails grow longer. Owners may choose to trim nails themselves or may opt to take their pet to a groomer or veterinarian.

Nail trimming is done with a nail clipper. There are two main types of nail clippers: guillotine trimmers and standard scissor- and plier-style trimmers. The scissor-style trimmer is most effective on nails that have grown too long and are now in the shape of a circle or coil.[6] In addition, handheld rotary tools are often used to smooth sharp edges caused by nail clippers.[7]

Cording is a technique in which dog coats are separated patiently into dreadlocks for coat care or presentation purposes. Some dog breeds that are often corded are the Puli and the Komondor. The Havanese and the various poodles are also occasionally corded for showing.

The cords form naturally (if messily) in tightly curled fur, but to make them attractive for conformation showing, the cords are carefully started by separating clumps of fur in a regular pattern, and tended until they are long enough to grow on their own.[8] A corded coat can act like a dust mop as the dog moves through its environment, causing debris such as dirt and leaves to be tangled in the coat. To keep the coat attractive, the owner must put in considerable time and effort in cleaning it and in entertaining and exercising the dog in a way that minimizes the accumulation of litter. Such dogs often have their cords tied up or covered with assorted dog clothing when they are not in a clean environment.

Additional options that some groomers provide include services such as fur coloring and painting dogs' nails with safe, nontoxic products formulated especially for that purpose.[9]

While traditional grooming achieves to conform with breed standards set by the official breed associations, creative grooming heads to the opposite direction, creating a unique, sometimes exquisite look.[10]

The lighter version of creative grooming is known as pet tuning and is more owner-oriented, adjusting the pets' visual appearance to their owners' amusement or lifestyle, while the creative grooming is more of an art form, therefore more artist (groomer) oriented.
"
Animal Training Services,"Animal training is the act of teaching animals specific responses to specific conditions or stimuli. Training may be for purposes such as companionship, detection, protection,  and entertainment. The type of training an animal receives will vary depending on the training method used, and the purpose for training the animal. For example, a seeing eye dog will be trained to achieve a different goal than a wild animal in a circus.

In some countries animal trainer certification bodies exist. They do not share consistent goals or requirements; they do not prevent someone from practicing as an animal trainer nor using the title. Similarly, the United States does not require animal trainers to have any specific certification.[1] An animal trainer should consider the natural behaviors of the animal and aim to modify behaviors through a basic system of reward and punishment.[2]

During training, an animal trainer can administer one of four potential consequences for a given behavior:

Behavior analysts emphasize the use of positive reinforcement for increasing desirable behaviors [6] and negative punishment for decreasing undesirable behaviors. If punishment is going to be used to decrease an undesirable behavior, the animal must be able to receive positive reinforcement for an alternative behavior.[7]

Reinforcement should be provided according to a predetermined schedule.[8] Such a schedule of reinforcement specifies whether all responses or only some are reinforced and includes the following:

While continuous reinforcement in a fixed ratio schedule may be necessary for the initial learning stages, a variable ratio schedule is the most effective at maintaining behavior over long periods of time.[12]

There are various methods animal trainers can use to prompt an animal to respond to a stimulus in a specific way. For example, shaping is a process by which successive approximations are rewarded until the desirable response topography is attained.[13] An animal trainer can use conditioned reinforcers, like clickers, to bridge the interval between response and positive reinforcement.[14] Some stimuli that is considered discriminative are signals, targets and cues. They can be used to prompt a response from an animal, and can be changed to other stimuli or faded in magnitude.[15] In order to delay satiation, reinforcer size should be as small as possible and still be effective for reinforcement.[16] Also, the timing of the delivery of a reinforcer is crucial. Initially the interval between response and consequence must be minimal in order for the animal to associate the consequence with the response.[17]

Other important issues related to this method are: 

Certain sub-fields of animal training tend to also have certain philosophies and styles. For example, fields such as: 

The degree of trainer protection from the animal and the tasks trained may also vary. They can range from entertainment, husbandry (veterinary) behaviors, physical labor or athleticism, habituation to averse stimuli, interaction (or non-interaction) with other humans, or even research (sensory, physiological, cognitive).

Training also may take into consideration the natural social tendencies of the animal species (or even breed), such as predilections for attention span, food-motivation, dominance hierarchies, aggression, or bonding to individuals (conspecifics as well as humans). Consideration must also be given to practical aspects on the human side such as the ratio of the number of trainers to each animal. In some circumstances one animal may have multiple trainers, in others, a trainer might attend simultaneously to many animals in a training session. Sometimes training is accomplished with a single trainer working individually with a single animal. In some species, the number of trainers is irrelevant, yet it can usually achieve the wanted outcome.[18]

Service animals, such as assistance dogs, Capuchin monkeys and miniature horses, are trained to utilize their sensory and social skills to bond with a human and help that person to offset a disability in daily life. The use of service animals, especially dogs, is an ever-growing field, with a wide range of special adaptations. In the United States, selected inmates in prisons are used to train service dogs. In addition to adding to the short supply of service animals, such programs have produced benefits in improved socialization skills and behavior of inmates. 

For hundreds of years elephants have be trained to work in activities such as logging and in warfare. Captive elephants can be trained to remember tone, melody, and recognise more than 20 words.[19]

Organizations such as the American Humane Association monitor the  use of animals such as those used in the entertainment industry, but they do not monitor their training. It is best known for its end credit disclaimer ""No Animals Were Harmed"" that appears at the end of the credits of films and shows.

The Patsy Award (Picture Animal Top Star of the Year) was originated by the Hollywood office in 1939 after a horse was killed in an on-set accident during the filming of the Tyrone Power film Jesse James. The award now covers both film and television and is separated into four categories: canine, equine, wild and special.

One animal trainer, Frank Inn, received over 40 Patsy awards. While there is a high demand for mammals for film and television, there is also a demand for other animals. Steven R. Kutcher has filled this niche for insects.

Basic obedience training tasks for dogs, include walking on a leash, attention, housebreaking, nonaggression, and socialization with humans or other pets. Dogs are also trained for many other activities, such as dog sports, service dogs, and working dog tasks.

Positive reinforcement for dogs can include primary reinforcers like food or social reinforcers, such as vocal (""good boy"") or tactile (stroking) ones. Positive punishment, if used at all, can be physical, such as pulling on a leash or spanking. It may also be vocal, such as saying ""bad dog"". Bridges to positive reinforcement, include vocal cues, whistling, and dog whistles, as well as clickers used in clicker training, a method popularized by Karen Pryor. Negative reinforcement may also be used. Punishment is also a tool, including withholding of food or physical discipline.

The primary purpose of training horses is to socialize them around humans, teach them to behave in a manner that makes them safe for humans to handle, and, as adults to carry a rider under saddle or to be driven in order to pull a vehicle.  As prey animals, much effort must be put into training horses to overcome its natural flight or fight instinct and accept handling that would not be natural for a wild animal, such as willingly going into a confined space, or having a predator (a human being) sit on its back. As training advances, some horses are prepared for competitive sports, up to the Olympic games, where horses are the only non-human animal athlete that is used at the Olympics. All equestrian disciplines from horse racing to draft horse showing require the horse to have specialized training.

Unlike dogs, horses are not motivated as strongly by positive reinforcement rewards as they are motivated by other operant conditioning methods such as the release of pressure as a reward for the correct behavior, called negative reinforcement.  Positive reinforcement techniques such as petting, kind words, rewarding of treats, and clicker training have some benefit, but not to the degree seen in dogs and other predator species.  Punishment  of horses is effective only to a very limited degree, usually a sharp command or brief physical punishment given within a few seconds of a disobedient act.   Horses do not correlate punishment to a specific behavior unless it occurs immediately.  They do, however, have a remarkably long memory, and once a task is learned, it will be retained for a very long time.  For this reason, poor training or allowing bad habits to be learned can be very difficult to remedy at a later date.

Typical training tasks for companion birds include perching, non-aggression, halting feather-picking, controlling excessive vocalizations, socialization with household members and other pets, and socialization with strangers.  The large parrot species frequently have lifespans that exceed that of their human owners, and they are closely bonded to their owners.  Some birds of prey are trained to hunt, an ancient art known as falconry or hawking. In China the practice of training cormorants to catch fish has gone on for over 1,200 years.[20]

Training chickens has become a way for trainers of other animals (primarily dogs) to perfect their training technique.  Bob Bailey, formerly of Animal Behavior Enterprises and the IQ Zoo, teaches chicken training seminars where trainers teach poultry to discriminate between shapes, to navigate an obstacle course and to chain behaviors together.  Chicken training is done using operant conditioning, using a clicker and chicken feed for reinforcement.  The first chicken workshops were given by Keller and Marian Breland in 1947–1948 to a group of animal feed salesmen from General Mills, in Minneapolis, Minnesota. Trained chickens may be confined to a display (Bird Brain) where they play Tic-Tac-Toe against humans for a fee, invented by Bob Bailey and Grant Evans, of Animal Behavior Enterprises.[21] The moves were chosen by computer and indicated to the chicken by a light invisible to the human player.[22]

Fish can also be trained. For example, goldfish may swim toward their owners and follow them as they walk through the room, but will not follow anyone else. The fish may swim up and down, signalling the owner to turn on its aquarium light when it is off, and it will skim the surface until its owner feeds it.
Fish have also been taught to perform more complicated tasks, such as fetching rings, swimming through hoops and tubes, doing the limbo and pushing a miniature soccer ball into a net.[23][24] 
Fish have been taught to distinguish and respond differently to slight differences in human faces displayed on a screen (archerfish[25]) or styles of music (goldfish[26] and koi[27]).

Molluscs, with totally different brain designs, have been taught to distinguish and respond to geometric symbols (cuttlefish[28] and octopus[29]), and have been taught that food behind a clear barrier cannot be eaten (squid[30]).

Animals in public display are sometimes trained for educational, entertainment, management, and husbandry behaviors. Educational behaviors may include species-typical behaviors under stimulus control such as vocalizations. Entertainment may include display behaviors to show the animal, or simply arbitrary behaviors. Management includes movement, such as following the trainer, entering crates, or moving from pen to pen, or tank-to-tank through gates. Husbandry behaviors facilitate veterinary care. It can include desensitization to various physical examinations or procedures, such as: 

Such voluntary training is important for minimizing the frequency with which zoo collection animals must be anesthetized or physically restrained.

Many marine mammals are trained for entertainment such as bottlenose dolphins, killer whales, belugas, sea lions, and others.

In a public display situation, the audience's attention is focused on the animal, rather than the trainer; therefore the discriminative stimulus is generally gestural (a hand sign) and sparse in nature. Unobtrusive dog whistles are used as bridges, and positive reinforcers are either primary (food) or tactile (rub downs), and not vocal.  However, pinnipeds and mustelids (sea lions, seals, walruses, and otters) can hear in our frequency, so most of the time they will receive vocal reinforcers during shows and performances. The shows are turned into more of a play production because of this, instead of just a run through of behaviors like cetaceans generally do in their shows. Guests can often hear these vocal reinforcers when attending a SeaWorld show. During the Clyde and Seamore show, the trainers may say something like: ""Good grief, Clyde!"" or ""Good job, Seamore"". The trainers substitute the word ""good"" in the place of food or rubdowns when teaching a specific behavior to the animals so that the animals no longer need constant feeding as praise for achieving the appropriate behavior.

On an experimental basis, wildlife researchers have employed animal trainers in their interactions with animals in the field.[31]

Known for their influence on the circus:

Known for scientific research:

Known for earliest commercial application of Skinner's operant conditioning:

Known for work in television and film:

Other:

Related to animal behavior, psychology and training:
"
Veterinary Health Centers,"Veterinary medicine is the branch of medicine that deals with the prevention, management, diagnosis, and treatment of disease, disorder, and injury in non-human animals. The scope of veterinary medicine is wide, covering all animal species, both domesticated and wild, with a wide range of conditions that can affect different species.

Veterinary medicine is widely practiced, both with and without professional supervision. Professional care is most often led by a veterinary physician (also known as a veterinarian, veterinary surgeon, or ""vet""), but also by paraveterinary workers, such as veterinary nurses, veterinary technicians, and veterinary assistants.[1] This can be augmented by other paraprofessionals with specific specialties, such as animal physiotherapy or dentistry, and species-relevant roles such as farriers.

Veterinary science helps human health through the monitoring and control of zoonotic disease (infectious disease transmitted from nonhuman animals to humans), food safety, and through human applications via medical research. They also help to maintain food supply through livestock health monitoring and treatment, and mental health by keeping pets healthy and long-living. Veterinary scientists often collaborate with epidemiologists and other health or natural scientists, depending on type of work. Ethically, veterinarians are usually obliged to look after animal welfare. Veterinarians diagnose, treat, and help keep animals safe and healthy.

Archeological evidence, in the form of a cow skull upon which trepanation had been performed, shows that people were performing veterinary procedures in the Neolithic (3400–3000 BCE).[2]

The Egyptian Papyrus of Kahun (Twelfth Dynasty of Egypt) is the first extant record of veterinary medicine.[3]

The Shalihotra Samhita, dating from the time of Ashoka, is an early Indian veterinary treatise. The edicts of Asoka read: ""Everywhere King Piyadasi (Asoka) made two kinds of medicine (चिकित्सा) available, medicine for people, and medicine for animals. Where no healing herbs for people and animals were available, he ordered that they be bought and planted.""[4]

Hippiatrica is a Byzantine compilation of hippiatrics, dated to the fifth or sixth century AD.[5]

The first attempts to organize and regulate the practice of treating animals tended to focus on horses because of their economic significance. In the Middle Ages, farriers combined their work in horseshoeing with the more general task of ""horse doctoring"".
The Arabic tradition of Bayṭara, or Shiyāt al-Khayl, originates with the treatise of Ibn Akhī Hizām (fl. late ninth century).

In 1356, the Lord Mayor of London, Sir Henry Picard, concerned at the poor standard of care given to horses in the city, requested that all farriers operating within a 7-mile (11-km) radius of the City of London form a ""fellowship"" to regulate and improve their practices. This ultimately led to the establishment of the Worshipful Company of Farriers in 1674.[6]

Meanwhile, Carlo Ruini's book Anatomia del Cavallo (Anatomy of the Horse) was published in 1598. It was the first comprehensive treatise on the anatomy of a nonhuman species.[7]

The first veterinary school was founded in Lyon, France, in 1762 by Claude Bourgelat.[8] According to Lupton,[9] after observing the devastation being caused by cattle plague to the French herds, Bourgelat devoted his time to seeking out a remedy. This resulted in founding a veterinary school in Lyon in 1761, from which establishment he dispatched students to combat the disease; in a short time, the plague was stayed and the health of stock restored, through the assistance rendered to agriculture by veterinary science and art.[9] The school received immediate international recognition in the 18th century and its pedagogical model drew on the existing fields of human medicine, natural history, and comparative anatomy.[10]

The Swedish veterinary education received funding 1774, and was officially started May 8th 1775 when the king Gustaf III signed the document.[11][12][13]
Peter Hernquist, who had studied for Carl von Linné in Uppsala, and also studied in Lyon with Claude Bourgelat, was head of school and is considered father of veterinary medicine in Sweden. 

The Odiham Agricultural Society was founded in 1783 in England to promote agriculture and industry,[14] and played an important role in the foundation of the veterinary profession in Britain. A founding member, Thomas Burgess, began to take up the cause of animal welfare and campaign for the more humane treatment of sick animals.[15] A 1785 society meeting resolved to ""promote the study of Farriery upon rational scientific principles.""

Physician James Clark wrote a treatise entitled Prevention of Disease in which he argued for the professionalization of the veterinary trade, and the establishment of veterinary colleges. This was finally achieved in 1790, through the campaigning of Granville Penn, who persuaded Frenchman Benoit Vial de St. Bel to accept the professorship of the newly established veterinary college in London.[14] The Royal College of Veterinary Surgeons was established by royal charter in 1844. Veterinary science came of age in the late 19th century, with notable contributions from Sir John McFadyean, credited by many as having been the founder of modern veterinary research.[16]

In the United States, the first schools were established in the early 19th century in Boston, New York City, and Philadelphia. In 1879, Iowa Agricultural College became the first land-grant college to establish a school of veterinary medicine.[17]

Veterinary care and management are usually led by a veterinary physician (usually called a veterinarian, veterinary surgeon or ""vet"") who has received their doctor of veterinary medicine degree. This role is the equivalent of a physician or surgeon (medical doctor) in human medicine, and involves postgraduate study and qualification.[18]

In many countries, the local nomenclature for a vet is a protected term, meaning that people without the prerequisite qualifications and/or registration are not able to use the title, and in many cases, the activities that may be undertaken by a vet (such as animal treatment or surgery) are restricted only to those people who are registered as vet.  For instance, in the United Kingdom, as in other jurisdictions, animal treatment may be performed only by registered vets (with a few designated exceptions, such as paraveterinary workers), calling oneself a vet without being registered or performing any treatment is illegal.

Most vets work in clinical settings, treating animals directly.  They may be involved in a general practice, treating animals of all types; may be specialized in a specific group of animals such as companion animals, livestock, laboratory animals, zoo animals, or horses; or may specialize in a narrow medical discipline such as veterinary surgery, dermatology, cardiology, neurology, laboratory animal medicine, internal medicine, and more.

As healthcare professionals, vets face ethical decisions about the care of their patients.  Current debates within the profession include the veterinary ethics of purely cosmetic procedures on animals, such as declawing of cats, docking of tails, cropping of ears, and debarking on dogs.

A wide range of surgeries and operations is performed on various types of animals, but not all of them are carried out by vets. In a case in Iran, for instance, an eye surgeon managed to perform a successful cataract surgery on a rooster for the first time in the world.[19]

Paraveterinary workers, including veterinary nurses, veterinary technicians, and veterinary assistants,[1] either assist vets in their work, or may work within their own scope of practice, depending on skills and qualifications, including in some cases, performing minor surgery.

The role of paraveterinary workers is less homogeneous globally than that of a vet, and qualification levels, and the associated skill mix, vary widely.

A number of professions exist within the scope of veterinary medicine, but may not necessarily be performed by vets or veterinary nurses.  This includes those performing roles which are also found in human medicine, such as practitioners dealing with musculoskeletal disorders, including osteopaths, chiropractors, and physiotherapists.

Some roles are specific to animals, but which have parallels in human society, such as animal grooming and animal massage. Some roles are specific to a species or group of animals, such as farriers, who are involved in the shoeing of horses, and in many cases have a major role to play in ensuring the medical fitness of horses.

Veterinary research includes prevention, control, diagnosis, and treatment of diseases of animals, and basic biology, welfare, and care of animals. Veterinary research transcends species boundaries and includes the study of spontaneously occurring and experimentally induced models of both human and animal diseases and research at human-animal interfaces, such as food safety, wildlife and ecosystem health, zoonotic diseases, and public policy.[20] By value the most important Animal Health pharmaceutical supplier worldwide is by far Zoetis (United States).[21]

As in medicine, randomized controlled trials also are fundamental in veterinary medicine to establish the effectiveness of a treatment.[22] Clinical veterinary research is far behind human medical research, though, with fewer randomized controlled trials, that have a lower quality and are mostly focused on research animals.[23] Possible improvement consists in creation of networks for inclusion of private veterinary practices in randomized controlled trials. Although the FDA approves drugs for use in humans, the FDA keeps a separate ""Green Book"", which lists drugs approved specifically for veterinary medicine (about half of which are separately approved for use in humans).[1][24]

No studies exist on the effect of community animal health services on improving household wealth and the health status of low-income farmers.[25]

The first recorded use of regenerative stem-cell therapy to treat lesions in a wild animal occurred in 2011 in Brazil.[26] On that occasion, the Zoo Brasília [pt] used stem cells to treat a maned wolf who had been run over by a car, which was later returned, fully recovered, to nature.[26]


"
Animal Trainers,"Animal training is the act of teaching animals specific responses to specific conditions or stimuli. Training may be for purposes such as companionship, detection, protection,  and entertainment. The type of training an animal receives will vary depending on the training method used, and the purpose for training the animal. For example, a seeing eye dog will be trained to achieve a different goal than a wild animal in a circus.

In some countries animal trainer certification bodies exist. They do not share consistent goals or requirements; they do not prevent someone from practicing as an animal trainer nor using the title. Similarly, the United States does not require animal trainers to have any specific certification.[1] An animal trainer should consider the natural behaviors of the animal and aim to modify behaviors through a basic system of reward and punishment.[2]

During training, an animal trainer can administer one of four potential consequences for a given behavior:

Behavior analysts emphasize the use of positive reinforcement for increasing desirable behaviors [6] and negative punishment for decreasing undesirable behaviors. If punishment is going to be used to decrease an undesirable behavior, the animal must be able to receive positive reinforcement for an alternative behavior.[7]

Reinforcement should be provided according to a predetermined schedule.[8] Such a schedule of reinforcement specifies whether all responses or only some are reinforced and includes the following:

While continuous reinforcement in a fixed ratio schedule may be necessary for the initial learning stages, a variable ratio schedule is the most effective at maintaining behavior over long periods of time.[12]

There are various methods animal trainers can use to prompt an animal to respond to a stimulus in a specific way. For example, shaping is a process by which successive approximations are rewarded until the desirable response topography is attained.[13] An animal trainer can use conditioned reinforcers, like clickers, to bridge the interval between response and positive reinforcement.[14] Some stimuli that is considered discriminative are signals, targets and cues. They can be used to prompt a response from an animal, and can be changed to other stimuli or faded in magnitude.[15] In order to delay satiation, reinforcer size should be as small as possible and still be effective for reinforcement.[16] Also, the timing of the delivery of a reinforcer is crucial. Initially the interval between response and consequence must be minimal in order for the animal to associate the consequence with the response.[17]

Other important issues related to this method are: 

Certain sub-fields of animal training tend to also have certain philosophies and styles. For example, fields such as: 

The degree of trainer protection from the animal and the tasks trained may also vary. They can range from entertainment, husbandry (veterinary) behaviors, physical labor or athleticism, habituation to averse stimuli, interaction (or non-interaction) with other humans, or even research (sensory, physiological, cognitive).

Training also may take into consideration the natural social tendencies of the animal species (or even breed), such as predilections for attention span, food-motivation, dominance hierarchies, aggression, or bonding to individuals (conspecifics as well as humans). Consideration must also be given to practical aspects on the human side such as the ratio of the number of trainers to each animal. In some circumstances one animal may have multiple trainers, in others, a trainer might attend simultaneously to many animals in a training session. Sometimes training is accomplished with a single trainer working individually with a single animal. In some species, the number of trainers is irrelevant, yet it can usually achieve the wanted outcome.[18]

Service animals, such as assistance dogs, Capuchin monkeys and miniature horses, are trained to utilize their sensory and social skills to bond with a human and help that person to offset a disability in daily life. The use of service animals, especially dogs, is an ever-growing field, with a wide range of special adaptations. In the United States, selected inmates in prisons are used to train service dogs. In addition to adding to the short supply of service animals, such programs have produced benefits in improved socialization skills and behavior of inmates. 

For hundreds of years elephants have be trained to work in activities such as logging and in warfare. Captive elephants can be trained to remember tone, melody, and recognise more than 20 words.[19]

Organizations such as the American Humane Association monitor the  use of animals such as those used in the entertainment industry, but they do not monitor their training. It is best known for its end credit disclaimer ""No Animals Were Harmed"" that appears at the end of the credits of films and shows.

The Patsy Award (Picture Animal Top Star of the Year) was originated by the Hollywood office in 1939 after a horse was killed in an on-set accident during the filming of the Tyrone Power film Jesse James. The award now covers both film and television and is separated into four categories: canine, equine, wild and special.

One animal trainer, Frank Inn, received over 40 Patsy awards. While there is a high demand for mammals for film and television, there is also a demand for other animals. Steven R. Kutcher has filled this niche for insects.

Basic obedience training tasks for dogs, include walking on a leash, attention, housebreaking, nonaggression, and socialization with humans or other pets. Dogs are also trained for many other activities, such as dog sports, service dogs, and working dog tasks.

Positive reinforcement for dogs can include primary reinforcers like food or social reinforcers, such as vocal (""good boy"") or tactile (stroking) ones. Positive punishment, if used at all, can be physical, such as pulling on a leash or spanking. It may also be vocal, such as saying ""bad dog"". Bridges to positive reinforcement, include vocal cues, whistling, and dog whistles, as well as clickers used in clicker training, a method popularized by Karen Pryor. Negative reinforcement may also be used. Punishment is also a tool, including withholding of food or physical discipline.

The primary purpose of training horses is to socialize them around humans, teach them to behave in a manner that makes them safe for humans to handle, and, as adults to carry a rider under saddle or to be driven in order to pull a vehicle.  As prey animals, much effort must be put into training horses to overcome its natural flight or fight instinct and accept handling that would not be natural for a wild animal, such as willingly going into a confined space, or having a predator (a human being) sit on its back. As training advances, some horses are prepared for competitive sports, up to the Olympic games, where horses are the only non-human animal athlete that is used at the Olympics. All equestrian disciplines from horse racing to draft horse showing require the horse to have specialized training.

Unlike dogs, horses are not motivated as strongly by positive reinforcement rewards as they are motivated by other operant conditioning methods such as the release of pressure as a reward for the correct behavior, called negative reinforcement.  Positive reinforcement techniques such as petting, kind words, rewarding of treats, and clicker training have some benefit, but not to the degree seen in dogs and other predator species.  Punishment  of horses is effective only to a very limited degree, usually a sharp command or brief physical punishment given within a few seconds of a disobedient act.   Horses do not correlate punishment to a specific behavior unless it occurs immediately.  They do, however, have a remarkably long memory, and once a task is learned, it will be retained for a very long time.  For this reason, poor training or allowing bad habits to be learned can be very difficult to remedy at a later date.

Typical training tasks for companion birds include perching, non-aggression, halting feather-picking, controlling excessive vocalizations, socialization with household members and other pets, and socialization with strangers.  The large parrot species frequently have lifespans that exceed that of their human owners, and they are closely bonded to their owners.  Some birds of prey are trained to hunt, an ancient art known as falconry or hawking. In China the practice of training cormorants to catch fish has gone on for over 1,200 years.[20]

Training chickens has become a way for trainers of other animals (primarily dogs) to perfect their training technique.  Bob Bailey, formerly of Animal Behavior Enterprises and the IQ Zoo, teaches chicken training seminars where trainers teach poultry to discriminate between shapes, to navigate an obstacle course and to chain behaviors together.  Chicken training is done using operant conditioning, using a clicker and chicken feed for reinforcement.  The first chicken workshops were given by Keller and Marian Breland in 1947–1948 to a group of animal feed salesmen from General Mills, in Minneapolis, Minnesota. Trained chickens may be confined to a display (Bird Brain) where they play Tic-Tac-Toe against humans for a fee, invented by Bob Bailey and Grant Evans, of Animal Behavior Enterprises.[21] The moves were chosen by computer and indicated to the chicken by a light invisible to the human player.[22]

Fish can also be trained. For example, goldfish may swim toward their owners and follow them as they walk through the room, but will not follow anyone else. The fish may swim up and down, signalling the owner to turn on its aquarium light when it is off, and it will skim the surface until its owner feeds it.
Fish have also been taught to perform more complicated tasks, such as fetching rings, swimming through hoops and tubes, doing the limbo and pushing a miniature soccer ball into a net.[23][24] 
Fish have been taught to distinguish and respond differently to slight differences in human faces displayed on a screen (archerfish[25]) or styles of music (goldfish[26] and koi[27]).

Molluscs, with totally different brain designs, have been taught to distinguish and respond to geometric symbols (cuttlefish[28] and octopus[29]), and have been taught that food behind a clear barrier cannot be eaten (squid[30]).

Animals in public display are sometimes trained for educational, entertainment, management, and husbandry behaviors. Educational behaviors may include species-typical behaviors under stimulus control such as vocalizations. Entertainment may include display behaviors to show the animal, or simply arbitrary behaviors. Management includes movement, such as following the trainer, entering crates, or moving from pen to pen, or tank-to-tank through gates. Husbandry behaviors facilitate veterinary care. It can include desensitization to various physical examinations or procedures, such as: 

Such voluntary training is important for minimizing the frequency with which zoo collection animals must be anesthetized or physically restrained.

Many marine mammals are trained for entertainment such as bottlenose dolphins, killer whales, belugas, sea lions, and others.

In a public display situation, the audience's attention is focused on the animal, rather than the trainer; therefore the discriminative stimulus is generally gestural (a hand sign) and sparse in nature. Unobtrusive dog whistles are used as bridges, and positive reinforcers are either primary (food) or tactile (rub downs), and not vocal.  However, pinnipeds and mustelids (sea lions, seals, walruses, and otters) can hear in our frequency, so most of the time they will receive vocal reinforcers during shows and performances. The shows are turned into more of a play production because of this, instead of just a run through of behaviors like cetaceans generally do in their shows. Guests can often hear these vocal reinforcers when attending a SeaWorld show. During the Clyde and Seamore show, the trainers may say something like: ""Good grief, Clyde!"" or ""Good job, Seamore"". The trainers substitute the word ""good"" in the place of food or rubdowns when teaching a specific behavior to the animals so that the animals no longer need constant feeding as praise for achieving the appropriate behavior.

On an experimental basis, wildlife researchers have employed animal trainers in their interactions with animals in the field.[31]

Known for their influence on the circus:

Known for scientific research:

Known for earliest commercial application of Skinner's operant conditioning:

Known for work in television and film:

Other:

Related to animal behavior, psychology and training:
"
Livestock Dealer Services,"
Stock and station agencies are businesses which provide a support service to the agricultural community. Their staff who deal with clients are known as stock and station agents.[note 1] They advise and represent farmers and graziers in business transactions that involve livestock, wool, fertiliser, rural property and equipment and merchandise on behalf of their clients. The number and importance of these businesses fell in the late 20th century.

These rural business services institutions originated, when communications were slow and often very difficult, to cope with the double remoteness of early Australian and New Zealand primary producers from their nearest settlement and, particularly in the case of wool, from their overseas markets. In practice, they were the pastoralist's banker. Similar and sometimes the same organisations operated in Latin America and the Midwestern United States, which had extensive pastoral farming.

These businesses grew to provide their clients with every product or service they might want right from employees or seasonal  working capital to, in the person of a client's personal stock agent, close personal friend and personal confidant who regularly visited them, maintained the client's loyalty and kept them up to date with events in the community and their industry. His branch manager might often be unable to maintain an easy relationship with clients unable to fulfill their financial obligations. The branch manager's knowledge of his client's business activities was such and his control over the client's spending was such he could ration their spending on sugar and flour.

They also act as local managers of properties on behalf of absentee owners deceased estates and mortgagees technically in possession of properties.

They provide retail stores in small towns for agricultural requirements selling, for example, animal health supplies, animal feed, fencing materials, fertilizer, machinery and tools, and even clothing and groceries.

An important activity is to organise regular local livestock sales at a community's commercial saleyards. Small rural communities may hold a single annual sale at local saleyards and this may be the highlight of their autumn business and social calendar.

A vital and well-liked part of their rural community, in remote areas, stock agents perform a variety of commercial and social functions. They bring to outlying homesteads on farms and stations stores, mail and newspapers, local news and gossip.

Next the stock agent turns to real work and: 
reports to his client on market trends and prices; 
sorts stock into lines for sales; 
sorts prime animals for the freezing works; 
values livestock and advises on different marketing options for stock; 
arranges penning and auction; 
arranges private sales between sellers and buyers.
arranges transport of stock to saleyards; 
conducts sales of wool on behalf of clients on a commission basis; 
sells a wide range of agricultural products including chemicals; 
arranges clearing sales of surplus machinery plant and equipment; and 
arranges insurance.

He also advises and assists clients in the management of agricultural or pastoral companies, stock or farming problems; 
arranges finance for the buying of livestock or property; and 
brings prospective buyers to inspect properties for sale.

Individual stock agents, within the same agency, may specialise in any one of the preceding activities.

""Arranged marriage: Farmers expect their stock agents to perform a range of tasks and services. One agent even acted as a go-between for a client who wanted to get married but was too shy to propose to the woman!""[1]

The close and enduring individual client relationships which are formed are seen as a forerunner of the newer concept of relationship marketing.[2]

In his history of the industry Simon Ville says: "". . . the stock and station agent has been a legendary figure in local folklore, connected or related to many individuals and groups, a central figure embedded in rural settler communities and about whom everyone has had a view. This social perspective helps inform our understanding of the agent's role and importance in economic activities since trust, reputation, and personal connection were the vital lubricants in sustaining business relationships and networks.""[3]

Some of these businesses grew very large in the 19th and early 20th centuries.
"
Timber Harvesting Operations,"Logging is the process of cutting, processing, and moving trees to a location for transport.  It may include skidding, on-site processing, and loading of trees or logs onto trucks[1] or skeleton cars. In forestry, the term logging is sometimes used narrowly to describe the logistics of moving wood from the stump to somewhere outside the forest, usually a sawmill or a lumber yard. In common usage, however, the term may cover a range of forestry or silviculture activities.

Logging is the beginning of a supply chain that provides raw material for many products societies worldwide use for housing, construction, energy, and consumer paper products. Logging systems are also used to manage forests, reduce the risk of wildfires, and restore ecosystem functions,[2] though their efficiency for these purposes has been challenged.[3]

Logging frequently has negative impacts. The harvesting procedure itself may be illegal, including the use of corrupt means to gain access to forests; extraction without permission or from a protected area; the cutting of protected species; or the extraction of timber in excess of agreed limits.[4] It may involve the so-called ""timber mafia"".[5][6] Excess logging can lead to irreparable harm to ecosystems, such as deforestation and biodiversity loss.[7][8] Infrastructure for logging can also lead to other environmental degradation. These negative environmental impacts can lead to environmental conflict.[7][8] Additionally, there is significant occupational injury risk involved in logging.

Logging can take many formats. Clearcutting (or ""block cutting"") is not necessarily considered a type of logging but a harvesting or silviculture method. Cutting trees with the highest value and leaving those with lower value, often diseased or malformed trees, is referred to as high grading. It is sometimes called selective logging, and confused with selection cutting, the practice of managing stands by harvesting a proportion of trees.[9] Logging usually refers to above-ground forestry logging. Submerged forests exist on land that has been flooded by damming to create reservoirs. Harvesting trees from forests submerged by flooding or dam creation is called underwater logging, a form of timber recovery.[10]

Clearcutting, or clearfelling, is a method of harvesting that removes essentially all the standing trees in a selected area. Depending on management objectives, a clearcut may or may not have reserve trees left to attain goals other than regeneration,[1] including wildlife habitat management, mitigation of potential erosion or water quality concerns. Silviculture objectives for clearcutting, (for example, healthy regeneration of new trees on the site) and a focus on forestry distinguish it from deforestation. Other methods include shelterwood cutting, group selective, single selective, seed-tree cutting, patch cut, and retention cutting.[citation needed]

The above operations can be carried out by different methods, of which the following three are considered industrial methods:

Trees are felled and then delimbed and topped at the stump. The log is then transported to the landing, where it is bucked and loaded on a truck. This leaves the slash (and the nutrients it contains) in the cut area, where it must be further treated if wild land fires are of concern.[citation needed]

Trees and plants are felled and transported to the roadside with top and limbs intact. There have been advancements to the process which now allows a logger or harvester to cut the tree down, top, and delimb a tree in the same process. This ability is due to the advancement in the style felling head that can be used. The trees are then delimbed, topped, and bucked at the landing. This method requires that slash be treated at the landing. In areas with access to cogeneration facilities, the slash can be chipped and used for the production of electricity or heat. Full-tree harvesting also refers to utilization of the entire tree including branches and tops.[11] This technique removes both nutrients and soil cover from the site and so can be harmful to the long-term health of the area if no further action is taken, however, depending on the species, many of the limbs are often broken off in handling so the result may not be as different from tree-length logging as it might seem.[citation needed]

Cut-to-length logging is the process of felling, delimbing, bucking, and sorting (pulpwood, sawlog, etc.) at the stump area, leaving limbs and tops in the forest. Mechanical harvesters fell the tree, delimb, and buck it, and place the resulting logs in bunks to be brought to the landing by a skidder or forwarder. This method is routinely available for trees up to 900 mm (35 in) in diameter.

Logging methods have changed over time, driven by advancements in transporting timber from remote areas to markets. These shifts fall into three main eras: the manual logging era before the 1880s, the railroad logging era from the 1880s to World War II, and the modern mechanized era that began after the war.[12]

In the early days, felled logs were transported using simple methods such as rivers to float tree trunks downstream to sawmills or paper mills. This practice, known as log driving or timber rafting, was the cheapest and most common. Some logs, due to high resin content, would sink and were known as deadheads. Logs were also moved with high-wheel loaders, a set of wheels over ten feet tall, initially pulled by oxen.[13]

As the logging industry expanded, the 1880s saw the introduction of mechanized equipment like railroads and steam-powered machinery, marking the beginning of the railroad logging era. Logs were moved more efficiently by railroads built into remote forest areas, often supported by additional methods like high-wheel loaders, tractors and log flumes.[14] The largest high-wheel loader, the ""Bunyan Buggie,"" was built in 1960 for service in California, featuring wheels 24 feet (7.3 m) high.[15]

After World War II, mechanized logging equipment, including chainsaws, diesel trucks, and Caterpillar tractors, transformed the logging industry, making railroad-based logging obsolete. With the advent of these tools, transporting logs became more efficient as new roads were constructed to access remote forests. However, in protected areas like United States National Forests and designated wilderness zones, road building has been restricted to minimize environmental impacts such as erosion in riparian zones.

Today, heavy machinery such as yarders and skyline systems are used to gather logs from steep terrain, while helicopters are used for heli-logging to minimize environmental impact.[16] Less common forms of logging, like horse logging and the use of oxen, still exist but are mostly superseded.[17]

Logging is a dangerous occupation. In the United States, it has consistently been one of the most hazardous industries and was recognized by the National Institute for Occupational Safety and Health (NIOSH) as a priority industry sector in the National Occupational Research Agenda (NORA) to identify and provide intervention strategies regarding occupational health and safety issues.[18][2]

In 2008, the logging industry employed 86,000 workers and accounted for 93 deaths. This resulted in a fatality rate of 108.1 deaths per 100,000 workers that year. This rate is over 30 times higher than the overall fatality rate.[19] Forestry/logging-related injuries (fatal and non-fatal) are often difficult to track through formal reporting mechanisms. Thus, some programs have begun to monitor injuries through publicly available reports such as news media.[20]  The logging industry experiences the highest fatality rate of 23.2 per 100,000 full-time equivalent (FTE) workers and a non-fatal incident rate of 8.5 per 100 FTE workers. The most common type of injuries or illnesses at work include musculoskeletal disorders (MSDs), which include an extensive list of ""inflammatory and degenerative conditions affecting the muscles, tendons, ligaments, joints, peripheral nerves, and supporting blood vessels.""[21]  Loggers work with heavy, moving weights, and use tools such as chainsaws and heavy equipment on uneven and sometimes steep or unstable terrain. Loggers also deal with severe environmental conditions, such as inclement weather and severe heat or cold. An injured logger is often far from professional emergency treatment.[citation needed]

Traditionally, the cry of ""Timber!"" developed as a warning alerting fellow workers in an area that a tree is being felled, so they should be alert to avoid being struck. The term ""widowmaker"" for timber, typically a limb or branch that is no longer attached to a tree, but is still in the canopy either wedged in a crotch, tangled in other limbs, or miraculously balanced on another limb demonstrates another emphasis on situational awareness as a safety principle.[22]

In British Columbia, Canada, the BC Forest Safety Council was created in September 2004 as a not-for-profit society dedicated to promoting safety in the forest sector. It works with employers, workers, contractors, and government agencies to implement fundamental changes necessary to make it safer to earn a living in forestry.[23]

The risks experienced in logging operations can be somewhat reduced, where conditions permit, by the use of mechanical tree harvesters, skidders, and forwarders.[24]
"
Fishing and Hunting Services,"

The United States Fish and Wildlife Service (USFWS or FWS) is a U.S. federal government agency within the United States Department of the Interior which oversees the management of fish, wildlife, and natural habitats in the United States. The mission of the agency is ""working with others to conserve, protect, and enhance fish, wildlife, plants and their habitats for the continuing benefit of the American people.""[1]

Among the responsibilities of the USFWS are enforcing federal wildlife laws; protecting endangered species; managing migratory birds; restoring nationally significant fisheries; conserving and restoring wildlife habitats, such as wetlands; helping foreign governments in international conservation efforts; and distributing money to fish and wildlife agencies of U.S. states through the Wildlife Sport Fish and Restoration Program.[10] The vast majority of fish and wildlife habitats are on state or private land not controlled by the United States government. Therefore, the USFWS works closely with private groups such as Partners in Flight and the Sport Fishing and Boating Partnership Council to promote voluntary habitat conservation and restoration.

The Fish and Wildlife Service was created in 1940 through the combination of two previous bureaus within the Department of the Interior.

USFWS employs approximately 8,000 people[1] and is organized into a central administrative office in Falls Church, Virginia, eight regional offices, and nearly 700 field offices distributed throughout the United States.

The original ancestor of USFWS was the United States Commission on Fish and Fisheries, more commonly referred to as the United States Fish Commission, created in 1871 by the United States Congress with the purpose of studying and recommending solutions to a noted decline in the stocks of food fish.[11] Spencer Fullerton Baird was appointed to lead it as the first United States Commissioner of Fisheries.[12] In 1903, the Fish Commission was reorganized as the United States Bureau of Fisheries and made part of the United States Department of Commerce and Labor.[13] When the Department of Commerce and Labor was split into the United States Department of Commerce and the United States Department of Labor in 1913, the Bureau of Fisheries was made part of the Department of Commerce.[14] Originally focused on fisheries science and fish culture, the Bureau of Fisheries also assumed other duties; in 1906, the U.S. Congress assigned it the responsibility for the enforcement of fishery and fur seal-hunting regulations in the Territory of Alaska,[15] and in 1910 for the management and harvest of northern fur seals, foxes, and other fur-bearing animals in the Pribilof Islands, as well as for the care, education, and welfare of the Aleut communities in the islands.[16] In 1939, the Bureau of Fisheries moved from the Department of Commerce to the Department of the Interior.[17]

The other ancestor of the USFWS began as the Section of Economic Ornithology, which was established within the United States Department of Agriculture in 1885 and became the Division of Economic Ornithology and Mammalogy in 1886.[18] In 1896 it became the Division of Biological Survey. Clinton Hart Merriam headed the Division for 25 years and became a national figure for improving the scientific understanding of birds and mammals in the United States.

In 1934, the Division of Biological Survey was reorganized as the Bureau of Biological Survey and Jay Norwood Darling was appointed its chief;. The same year, Congress passed the Fish and Wildlife Coordination Act (FWCA), one of the oldest federal environmental review statutes.[19] Under Darling's guidance, the Bureau began an ongoing legacy of protecting vital natural habitat throughout the United States. In 1939, the Bureau of Biological Survey moved from the Department of Agriculture to the Department of the Interior.

On June 30, 1940, the Bureau of Fisheries and the Bureau of Biological Survey were combined to form the Department of the Interior's Fish and Wildlife Service. In 1956, the Fish and Wildlife Service was reorganized as the United States Fish and Wildlife Service — which remained part of the Department of the Interior — and divided its operations into two bureaus, the Bureau of Sport Fisheries and Wildlife and the Bureau of Commercial Fisheries, with the latter inheriting the history and heritage of the old U.S. Fish Commission and U.S. Bureau of Fisheries.[20]

Upon the formation of the National Oceanic and Atmospheric Administration (NOAA) within the Department of Commerce on October 3, 1970, the Bureau of Commercial Fisheries merged with the salt-water laboratories of the Bureau of Sport Fisheries and Wildlife to form today's National Marine Fisheries Service (NMFS), an element of NOAA.[21]  The remainder of the USFWS remained in place in the Department of the Interior in 1970 as the foundation of the USFWS as it is known today, although in 1985 the Animal Damage Control Agency, responsible for predator control, was transferred from the USFWS to the Department of Agriculture and renamed the Division of Wildlife Services.

Hundreds of employees were fired during the 2025 United States federal mass layoffs.[22]

USFWS manages the National Wildlife Refuge System, which consists of 570 National Wildlife Refuges, encompassing a full range of habitat types, including wetlands, prairies, coastal and marine areas, and temperate, tundra, and boreal forests spread across all 50 U.S. states. It also manages thousands of small wetlands and other special management areas covering over 150,000,000 acres (61,000,000 ha).

The USFWS governs six National Monuments:

The USFWS shares the responsibility for administering the Endangered Species Act of 1973 with the National Marine Fisheries Service (NMFS), an element of NOAA, with the NMFS responsible for marine species, the FWS responsible for freshwater fish and all other species, and the two organizations jointly managing species that occur in both marine and non-marine environments. The USFWS publishes the quarterly Endangered Species Bulletin.

The USFWS's Fisheries Program oversees the National Fish Hatchery System (NFHS), which includes 70 National Fish Hatcheries and 65 Fish and Wildlife Conservation Offices. Originally created to reverse declines in lake and coastal fish stocks in the United States, the NFHS subsequently expanded its mission to include the preservation of the genes of wild and hatchery-raised fish; the restoration of native aquatic populations of fish, freshwater mussels, and amphibians including populations of species listed under the Endangered Species Act; mitigating the loss of fisheries resulting from U.S. Government water projects; and providing fish to benefit Native Americans and National Wildlife Refuges. The NFHS also engages in outreach, education, and research activities.

The National Fish Passage Program provides financial and technical resources to projects that promote the free movement of fish and aquatic life. Common projects include dam removal and fishway construction. Between 1999 and 2023, the program has worked with over 2,000 local partners to open 61,000 mi (98,000 km) of upstream habitat by removing or bypassing 3,400 aquatic barriers.[24]

The Division of Migratory Bird Management runs the Migratory Bird Program, which works with partners of the USFWS to protect, restore, and conserve bird populations and their habitats by ensuring the long-term ecological sustainability of all migratory bird populations, increasing the socioeconomic benefit of birds, improving the experience of hunting, bird watching, and other outdoor activities related to birds, and increasing the awareness of the aesthetic, ecological, recreational and economic significance of migratory birds and their habitats.[25] It conducts surveys; coordinates USFWS activities with those of public-private bird conservation partnerships; provides matching grants for conservation efforts involving USFWS partners; develops policies and regulations and administers conservation laws related to migratory birds; issues permits to allow individuals and organizations to participate in migratory bird conservation efforts; helps educate and engage children in wildlife conservation topics; and provides resources for parents and educators to assist them in helping children explore nature and birds.[25]

The USFWS partners with the Landscape Conservation Cooperatives, a network of 22 autonomous cooperatives sponsored by the Department of the Interior which function as regional conservation bodies covering the entire United States and adjacent areas.[26]

The Office of Law Enforcement enforces wildlife laws, investigates wildlife crimes, regulates wildlife trade, helps people in the United States understand and obey wildlife protection laws, and works in partnership with international, state, and tribal counterparts to conserve wildlife resources. It also trains other U.S. Government, U.S. state, tribal, and foreign law enforcement officers.

The USFWS operates the Clark R. Bavin National Fish and Wildlife Forensic Laboratory, the only forensics laboratory in the world devoted to wildlife law enforcement. By treaty, it also is the official crime laboratory for the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) and the Wildlife Group of Interpol. The laboratory identifies the species or subspecies of pieces, parts, or products of an animal to determine its cause of death, help wildlife officers determine if a violation of law occurred in its death, and to identify and compare physical evidence to link suspects to the crime scene and the animal's death.

United States Fish and Wildlife Service Refuge Law Enforcement consists of professional law enforcement officers entrusted with protecting natural resources and public safety.
Federal Wildlife Officers promote the survival of species and health of the environment by ensuring that wildlife laws are followed. They also welcome visitors and are often the first U.S. Fish and Wildlife Service employees encountered by the public on refuges.  Federal Wildlife Officers (FWO) are entrusted with protecting natural resources, visitors and employees on National Wildlife Refuge System lands.[27]

The USFWS issues an annual Federal Duck Stamp, a collectable adhesive stamp required to hunt for migratory waterfowl. It also allows access to National Wildlife Refuges without paying an admission fee.

The USFWS International Affairs Program coordinates domestic and international efforts to protect, restore, and enhance wildlife and its habitats, focusing on species of international concern, fulfilling the USFWS's international responsibilities under about 40 treaties, as well as U.S. laws and regulations. It oversees programs which work with private citizens, local communities, other U.S. Government and U.S. state agencies, foreign governments, non-governmental organizations, scientific and conservation organizations, industry groups. and other interested parties on issues related to the implementation of treaties and laws and the conservation of species around the world.[28]

The USFWS's National Conservation Training Center trains USFWS employees and those of USFWS partners in the accomplishment of the USFWS's mission.[29]

At its founding in 1896, the work of the Division of Biological Survey focused on the effect of birds in controlling agricultural pests and mapping the geographical distribution of plants and animals in the United States. By 1905 with funding scarce, the Survey included in its mission the eradication of wolves, coyotes and other large predators.  This garnered them the support of ranchers and western legislators resulting, by 1914, in a $125,000 congressionally approved budget for use ""on the National Forests and the public domain in destroying wolves, coyotes and other animals injurious to agriculture and animal husbandry"".[30]: 95–96  Meanwhile, scientists like Joseph Grinnell and Charles C. Adams, a founder of the Ecological Society of America, were promoting a ""balance of nature"" theory – the idea that predators were an important part of the larger ecosystem and should not be eradicated. In 1924, at a conference organized by the American Society of Mammologists (ASM), the debate generated a public split between those in the Survey, promoting eradication, and those from the ASM who promoted some sort of accommodation.  Edward A. Goldman, from the Survey, made perfectly clear their position in a paper [31] that with the arrival of Europeans in North America, the balance of nature had been ""violently overturned, never to be reestablished"".  He concludes with the idea that ""Large predatory mammals, destructive to livestock and to game, no longer have a place in our advancing civilization.""  The Survey subsequently placed over 2 million poisoned bait stations across the west and by 1930 had ""extirpated wolves from the Lower 48 and advised and assisted in erasing grey wolves from"" Yellowstone and Glacier National Parks.  The Survey then turned to the eradication of coyote,[30]: 124–126  coordinated through the 1931 Animal Damage Control Act.

With various agency reorganizations, the practice continued more or less apace through the early 1970s but though hundreds of thousands of coyotes were killed, their extreme adaptability and resilience led to little overall population reduction and, instead, their migration into an expanded habitat, including urban areas. Increasing environmental awareness in the late 1960s and early 1970s resulted in Nixon banning post-World War II-era poisons in 1972 and the passage of the Endangered Species Act in 1973. Also in 1972, the Nixon administration rewrote the Animal Damage Control Act, effectively repealing it in favor of turning the mission of predator control over to the states. The loss of federally fund to protect their livestock was too much for ranching and agricultural communities and by 1980 Reagan had reversed the poison killing ban and transferred the responsibility for predator control to the Wildlife Services program under the US Department of Agriculture's Animal and Plant Health Inspection Service.  The Program's mission has evolved to protect ""agriculture, wildlife and other natural resources, property, and human health and safety"".

Pursuant to the eagle feather law, Title 50, Part 22 of the Code of Federal Regulations (50 CFR 22), and the Bald and Golden Eagle Protection Act, the USFWS administers the National Eagle Repository and the permit system for Native American religious use of eagle feathers.[32][33][34] These exceptions often only apply to Native Americans that are registered with the federal government and are enrolled with a federally recognized tribe.

In the late 1990s and early 2000s, the USFWS began to incorporate the research of tribal scientists into conservation decisions.[35] This came on the heels of Native American traditional ecological knowledge (TEK) gaining acceptance in the scientific community as a reasonable and respectable way to gain knowledge of managing the natural world.[36][37] Additionally, other natural resource agencies within the United States government, such as the United States Department of Agriculture, have taken steps to be more inclusive of tribes, native people, and tribal rights.[38] This has marked a transition to a relationship of more co-operation rather than the tension between tribes and government agencies seen historically. Today, these agencies work closely with tribal governments to ensure the best conservation decisions are made and that tribes retain their sovereignty.[39][40]

From 1940 to 1970, the FWS (from 1956 the USFWS) operated a fleet of seagoing vessels. The fleet included fisheries science research ships, fishery patrol vessels, and cargo liners.

The Fish Commission operated a small fleet of research ships and fish-culture vessels. The Bureau of Fisheries inherited these in 1903, and then greatly expanded its fleet of seagoing vessels, including both patrol vessels for fishery enforcement in the Territory of Alaska[15] and a cargo liner — known as the ""Pribilof tender"" — to provide transportation for passengers and haul cargo to, from, and between the Pribilof Islands.[16] In the 1930s, the Bureau of Biological Survey operated a vessel of its own, Brown Bear. Upon its creation in 1940, the FWS inherited the BOF's fleet and Brown Bear.

By 1940, no fisheries research vessels remained in commission, the BOF having decommissioned the last one, USFS Albatross II, in 1932;[41] only in the late 1940s did the FWS begin to commission new research ships. Although between 1871 and 1940 the Fish Commission and BOF had never had more than three fisheries research ships in commission at the same time[42] — and had three in commission simultaneously only in two years out of their entire combined history[42] — by March 1950, the FWS fleet included 11 seagoing fisheries research and exploratory fishing vessels either in service or under construction,[42] and its fishery enforcement force in the Territory of Alaska included 29 patrol vessels and about 100 speedboats, as well as 20 airplanes.[43] In the 1956 reorganization that created the USFWS, the Bureau of Commercial Fisheries (BCF) assumed the responsibility within the USFWS for the operation of the seagoing vessels of the fleet.

The USFWS continued fishery enforcement in Alaska until after Alaska became a state in January 1959, but by 1960 had turned over enforcement responsibilities and some of the associated vessels to the Government of Alaska as the latter assumed the responsibility for fishery enforcement in its waters.[20] The USFWS continued to operate fisheries research ships and the Pribilof tender until the BCF's seagoing fleet was transferred to the National Marine Fisheries Service (NMFS), an element of NOAA, upon the creation of NOAA on October 3, 1970. Although the NMFS continued to operate the Pribilof tender until 1975,[44] the rest of the ships were transferred from the NMFS to a unified NOAA fleet during 1972 and 1973. The modern NOAA fleet therefore traces its ancestry in part to the USFWS fleet operated by the BCF.

Both before and after the FWS became the USFWS in 1956, ships of its fleet used the prefix ""US FWS"" while in commission. The BOF usually named its ships after aquatic birds, and ships the FWS inherited from the BOF in 1940 retained those names in FWS service. However, the FWS/USFWS thereafter usually named vessels it acquired after people who were notable in the history of fisheries and fisheries science. A partial list of ships of the FWS and USFWS fleet:

In 1959, the methods used by USFWS's Animal Damage Control Program were featured in the Tom Lehrer song ""Poisoning Pigeons in the Park.""[45]

Jeremy Renner plays a U.S. Fish and Wildlife Service predator control specialist in the 2017 film Wind River.[46]
"
Well Maintenance Services,"Well services is a department within a petroleum production company through which matters concerning existing wells are handled.[1] Having a shared well services department for all (or at least multiple) assets operated by a company is seen as advantageous as it allows the pooling of talent, experience and resources for managing wells.

The term may sometimes be used to encompass the larger section of the industry responsible for wells including the supplier companies as well the operating company's wells department.

A well is initially drilled and completed under the control of the drilling and completions department operating under the request of the asset.  Once the well is completed, control is transferred to the asset's production team, who will operate the well as appropriate for their purposes.  Should any issues of well integrity or any requirement for well work arise, the asset will refer the issue to the well services.  During interventions, control of affected well is handed over from production to the well services crew at the well site, a practical action involving transferring control lines from the production control panel to the well services control panel.

When well work is required, it is the responsibility of the Well Operations Engineer (WOE) to assemble the team and arrange their dispatch to the well site.  The team will consist of a well services supervisor and other operators.  The well services supervisor is a dedicated worker who is sent to oversee well services operations at well sites and take responsibility for all well services personnel.  At offshore sites, there will commonly be two, to cover both day shift and night shift.  The other operators will usually consist of personnel from supplier companies, who are trained in the relevant field, such as wireline, coiled tubing, wellhead maintenance, etc.
"
Field Welding Services,"

Welding is a fabrication process that joins materials, usually metals or thermoplastics, primarily by using high temperature to melt the parts together and allow them to cool, causing fusion. Common alternative methods include solvent welding (of thermoplastics) using chemicals to melt materials being bonded without heat, and solid-state welding processes which bond without melting, such as pressure, cold welding, and diffusion bonding.

Metal welding is distinct from lower temperature bonding techniques such as brazing and soldering, which do not melt the base metal (parent metal) and instead require flowing a filler metal to solidify their bonds.

In addition to melting the base metal in welding, a filler material is typically added to the joint to form a pool of molten material (the weld pool) that cools to form a joint that can be stronger than the base material. Welding also requires a form of shield to protect the filler metals or melted metals from being contaminated or oxidized.

Many different energy sources can be used for welding, including a gas flame (chemical), an electric arc (electrical), a laser, an electron beam, friction, and ultrasound. While often an industrial process, welding may be performed in many different environments, including in open air, under water, and in outer space. Welding is a hazardous undertaking and precautions are required to avoid burns, electric shock, vision damage, inhalation of poisonous gases and fumes, and exposure to intense ultraviolet radiation.

Until the end of the 19th century, the only welding process was forge welding, which blacksmiths had used for millennia to join iron and steel by heating and hammering. Arc welding and oxy-fuel welding were among the first processes to develop late in the century, and electric resistance welding followed soon after. Welding technology advanced quickly during the early 20th century, as world wars drove the demand for reliable and inexpensive joining methods. Following the wars, several modern welding techniques were developed, including manual methods like shielded metal arc welding, now one of the most popular welding methods, as well as semi-automatic and automatic processes such as gas metal arc welding, submerged arc welding, flux-cored arc welding and electroslag welding. Developments continued with the invention of laser beam welding, electron beam welding, magnetic pulse welding, and friction stir welding in the latter half of the century. Today, as the science continues to advance, robot welding is commonplace in industrial settings, and researchers continue to develop new welding methods and gain greater understanding of weld quality.[1]

The term weld is derived from the Middle English verb well (wæll; plural/present tense: wælle) or welling (wællen), meaning 'to heat' (to the maximum temperature possible); 'to bring to a boil'. The modern word was probably derived from the past-tense participle welled (wællende), with the addition of d for this purpose being common in the Germanic languages of the Angles and Saxons. It was first recorded in English in 1590. A fourteenth century translation of the Christian Bible into English by John Wycliffe translates Isaiah 2:4 as ""...thei shul bete togidere their swerdes into shares..."" (they shall beat together their swords into plowshares). In the 1590 version this was changed to ""...thei shullen welle togidere her swerdes in-to scharris..."" (they shall weld together their swords into plowshares), suggesting this particular use of the word probably became popular in English sometime between these periods.[2]

The Old English word for welding iron was samod ('to bring together') or samodwellung ('to bring together hot').[3]

The word is related to the Old Swedish word valla, meaning 'to boil', which could refer to joining metals, as in valla järn (literally 'to boil iron'). Sweden was a large exporter of iron during the Middle Ages, so the word may have entered English from the Swedish iron trade, or may have been imported with the thousands of Viking settlements that arrived in England before and during the Viking Age, as more than half of the most common English words in everyday use are Scandinavian in origin.[4][5]

The history of joining metals goes back several millennia.[6] Fusion welding processes that join metals by melting them were not widely used in pre-industrial welding. Early welding techniques used pressure to join to the metals, often with heat not sufficient to fully melt the base metals.[7] One notable exception was a technique to join sections of large statues. In Greek and Roman lost-wax casting, the statues were cast as smaller pieces and molten bronze was poured into the joints with temperatures sufficient to create fusion welds.[8]

The earliest known welding dates to the Bronze Age. Gold is soft enough to be pressure welded with little to no heat, and archaeologists have found small boxes made by pressure welding overlapping sheets of gold. In the Iron Age, Mediterranean societies developed forge welding.[9] In forge welding, metal is heated to the point that it becomes soft enough for a blacksmith to hammer separate pieces together.[10] Very early notable examples are the iron objects found with Tutankhamun including an iron headrest and dagger.[11][12] The dagger was forged from meteoric iron at temperatures below 950 °C (1,740 °F).[13] Typically, wrought iron is forged at around 1,350 °C (2,460 °F).[14] The ancient Greek historian Herodotus credits Glaucus of Chios with discovering ""iron welding"".[15] Glaucus is known for an iron pedestal welded to hold a silver krater at Delphi.[16]

The Middle Ages brought advances in forge welding, in which blacksmiths pounded heated metal repeatedly until bonding occurred.[18] In Europe and Africa, forging shifted from open charcoal fires to bloomeries. China developed the blast furnace late in the first millennia.[19] Forge welding was used in the construction of the Iron pillar of Delhi, erected in Delhi, India about 310 AD and weighing 5.4 metric tons.[20] In 1540, Vannoccio Biringuccio published De la pirotechnia, which includes descriptions of the forging operation.[18] Renaissance craftsmen were skilled in the process, and the industry continued to grow during the following centuries.[18]

In 1800, Sir Humphry Davy discovered the short-pulse electrical arc and presented his results in 1801.[21][22][23] In 1802, Russian scientist Vasily Petrov created the continuous electric arc,[23][24][25] and subsequently published ""News of Galvanic-Voltaic Experiments"" in 1803, in which he described experiments carried out in 1802. Of great importance in this work was the description of a stable arc discharge and the indication of its possible use for many applications, one being melting metals.[26] In 1808, Davy, who was unaware of Petrov's work, rediscovered the continuous electric arc.[22][23] In 1881–82 inventors Nikolai Benardos (Russian) and Stanisław Olszewski (Polish)[27] created the first electric arc welding method known as carbon arc welding using carbon electrodes. The advances in arc welding continued with the invention of metal electrodes in the late 1800s by a Russian, Nikolai Slavyanov (1888), and an American, C. L. Coffin (1890). Around 1900, A. P. Strohmenger released a coated metal electrode in Britain, which gave a more stable arc. In 1905, Russian scientist Vladimir Mitkevich proposed using a three-phase electric arc for welding. Alternating current welding was invented by C. J. Holslag in 1919, but did not become popular for another decade.[28]

Resistance welding was also developed during the final decades of the 19th century, with the first patents going to Elihu Thomson in 1885, who produced further advances over the next 15 years. Thermite welding was invented in 1893, and around that time another process, oxyfuel welding, became well established. Acetylene was discovered in 1836 by Edmund Davy, but its use was not practical in welding until about 1900, when a suitable torch was developed.[29] At first, oxyfuel welding was one of the more popular welding methods due to its portability and relatively low cost. As the 20th century progressed, however, it fell out of favor for industrial applications. It was largely replaced with arc welding, as advances in metal coverings (known as flux) were made.[30] Flux covering the electrode primarily shields the base material from impurities, but also stabilizes the arc and can add alloying components to the weld metal.[31]

World War I caused a major surge in the use of welding, with the various military powers attempting to determine which of the several new welding processes would be best.[32] The British primarily used arc welding, even constructing a ship, the ""Fullagar"" with an entirely welded hull.[33][34]: 142  Arc welding was first applied to aircraft during the war as well, as some German airplane fuselages were constructed using the process.[35]

During the middle of the century, many new welding methods were invented, including the introduction of automatic welding in 1920, in which electrode wire was fed continuously.[36] Shielding gas received much attention, as scientists attempted to protect welds from the effects of oxygen and nitrogen in the atmosphere. Porosity and brittleness were the primary problems, and the solutions that developed included the use of hydrogen, argon, and helium as welding atmospheres.[37] Testing methods were introduced for weld integrity. First vibration testing was done using a hammer and stethoscope; later, X-ray tests were developed to see into the weld.[38] During the 1930s, further advances allowed for the welding of reactive metals like aluminum and magnesium.[39] This in conjunction with developments in automatic welding, alternating current, and fluxes fed a major expansion of arc welding during the 1930s.[40] Russian inventor Konstantin Khrenov implemented the first underwater electric arc welding.[41] In 1930, Kyle Taylor was responsible for the release of stud welding, which soon became popular in shipbuilding and construction. Submerged arc welding was invented the same year.[42] During World War II, submerged arc welding was widely used for ship-building because it allowed certain types of welds to be done twenty times faster than earlier techniques.[43]

Improvements to welding processes opened up new possibilities for construction.[44] Previously, large metal structures had been made from metals joined mechanically with rivets, along with bolts, screws, and belts. These connected but unfused metal structures had inherent weaknesses.[45] The steamboat Sultana killed over a thousand passengers when its riveted boiler failed under pressure.[46] The ""unsinkable"" Titanic sank due in part to failures in its riveted hull.[47] In 1930, the first all-welded merchant vessel, M/S Carolinian, was launched.[48] The strength of welded steel also allowed for the creation of entirely new types of ships, notably the liquefied natural gas (LNG) tanker. The ASME Boiler and Pressure Vessel Code, created in response to deadly boiler failures was used to develop the spherical tanks that contain LNG during transport.[49] Also noteworthy is the first welded road bridge in the world, the Maurzyce Bridge in Poland (1928).[33] Early skyscrapers and steel truss bridges were built from riveted steel beams.[50][51] Welding allows for stronger and lighter structures and greater range of shapes.[52] The Sydney Opera House's icon shape is built on a stud-welded steel frame.[53]

Gas tungsten arc welding, after decades of development, was finally perfected in 1941, and gas metal arc welding followed in 1948, allowing for fast welding of non-ferrous materials but requiring expensive shielding gases. Shielded metal arc welding was developed during the 1950s, using a flux-coated consumable electrode, and it quickly became the most popular metal arc welding process. In 1957, the flux-cored arc welding process debuted, in which the self-shielded wire electrode could be used with automatic equipment, resulting in greatly increased welding speeds, and that same year, plasma arc welding was invented by Robert Gage. Electroslag welding was introduced in 1958, and it was followed by its cousin, electrogas welding, in 1961.[54] In 1953, the Soviet scientist N. F. Kazakov proposed the diffusion bonding method.[55]

Other recent developments in welding include the 1958 breakthrough of electron beam welding, making deep and narrow welding possible through the concentrated heat source. Following the invention of the laser in 1960, laser beam welding debuted several decades later, and has proved to be especially useful in high-speed, automated welding. Magnetic pulse welding (MPW) has been industrially used since 1967. Friction stir welding was invented in 1991 by Wayne Thomas at The Welding Institute (TWI, UK) and found high-quality applications all over the world.[56] All of these four new processes continue to be quite expensive due to the high cost of the necessary equipment, and this has limited their applications.[57]

Welding joins two pieces of metal using heat, pressure, or both. The most common modern welding methods use heat sufficient to melt the base metals to be joined and the filler metal.[59] This includes gas welding and all forms of arc welding.[60] The area where the base and filler metals melt is called the weld pool or puddle.[61] The weld pool must be protected from oxygen in the air that will oxidize with the molten metal and from other gases that could contaminate the weld.[62] Most welding methods involve pushing the puddle along a joint to create a weld bead.[63] Overlapping pieces of metal can be joined by forming the weld pool within a hole made in the topmost piece of base metal to form a plug weld.[64]

Solid-state welding processes join two pieces of metal using pressure.[65] Electric resistance welding is a common industrial process that combines heat and pressure to join overlapping base metals without any filler material.[66]

Gas welding, also known as oxyacetylene welding, uses an open flame to generate heat and shield the weld. Compared to arc welding, the flame is less concentrated and lower in temperature, about 3100 °C (5600 °F) near the torch tip. This causes slower weld cooling, which can lead to greater residual stresses and weld distortion, though it eases the welding of high alloy steels. The diffuse outer envelope of the flame consumes oxygen before it can reach the molten weld pool.[30] When working with easily oxidized metals, such as stainless steel, flux can be brushed onto the base metals.[67]

The equipment is relatively inexpensive and simple, consisting of a torch, hoses, pressure regulators, a tank of oxygen, and a tank of fuel (usually acetylene).[68] It is one of the oldest and most versatile welding processes, but it has become less popular in industrial applications. It is still widely used for welding pipes and tubes, as well as repair work.[30] A similar process, generally called oxyfuel cutting, is used to cut metals. Oxyfuel equipment can also be used to heat metal before bending or straightening.[69]

All arc welding processes use a welding power supply to create and maintain an electric arc between an electrode and the base material to melt metals at the welding point. They can use alternating current (AC) or direct current (DC). For DC welding, the electrode can be connected to the machine's positive terminal (DCEP) or negative terminal (DCEN), changing the current's direction. The process and type of electrode used will typically determine the current.[71][72]

Shielding gas prevents oxygen in the atmosphere from entering the molten weld pool. In some processes, the shielding gas is delivered from gas cylinders containing inert or semi-inert gas. In others, a flux coating on a consumable electrode disintegrates to create the gas.[73][74] Filler material is typically added to the molten weld pool and is necessary for processes that use a consumable electrode.[75]

One of the most common types of arc welding is shielded metal arc welding (SMAW);[76] it is also known as manual metal arc welding (MMAW) or stick welding. Electric current is used to strike an arc between the base material and consumable electrode rod, which is made of filler material (typical steel) and is covered with a flux that protects the weld area from oxidation and contamination by producing carbon dioxide (CO2) gas during the welding process. The electrode core itself acts as filler material, making a separate filler unnecessary.[76]

The process is versatile and can be performed with relatively inexpensive equipment, making it well suited to shop jobs and field work.[76][77] An operator can become reasonably proficient with a modest amount of training and can achieve mastery with experience. Weld times are rather slow, since the consumable electrodes must be frequently replaced and because slag, the residue from the flux, must be chipped away after welding.[76] Furthermore, the process is generally limited to welding ferrous materials, though special electrodes have made possible the welding of cast iron, stainless steel, aluminum, and other metals.[77]

Gas metal arc welding (GMAW), also known as metal inert gas or MIG welding, is a semi-automatic or automatic process that uses a continuous wire feed as an electrode and an inert or semi-inert gas mixture to protect the weld from contamination. Since the electrode is continuous, welding speeds are greater for GMAW than for SMAW.[78]

A related process, flux-cored arc welding (FCAW), uses similar equipment but uses wire consisting of a tubular steel electrode surrounding a powder fill material. This cored wire is more expensive than the standard solid wire and can generate fumes and/or slag, but it permits even higher welding speed and greater metal penetration.[79] As the electrode is consumed, the flux disintegrates to create shielding gas and a protective layer of slag similar to stick welding. Some flux-cored machines have a nozzle that uses a shielding gas to supplement the protection from the flux. This is called dual shield welding, and uses a specialized gas shielded flux-core wire.[80]

Gas tungsten arc welding (GTAW), or tungsten inert gas (TIG) welding, is a manual welding process that uses a non-consumable tungsten electrode, an inert or semi-inert gas mixture, and a separate filler material.[81] Especially useful for welding thin materials, this method is characterized by a stable arc and high-quality welds, but it requires significant operator skill and can only be accomplished at relatively low speeds.[81]

GTAW can be used on nearly all weldable metals, though it is most often applied to stainless steel and light metals. It is often used when quality welds are extremely important, such as in bicycle, aircraft and naval applications.[81] A related process, plasma arc welding, also uses a tungsten electrode but uses plasma gas to make the arc. The arc is more concentrated than the GTAW arc, making transverse control more critical and thus generally restricting the technique to a mechanized process. Because of its stable current, the method can be used on a wider range of material thicknesses than can the GTAW process and it is much faster. It can be applied to all of the same materials as GTAW except magnesium, and automated welding of stainless steel is one important application of the process. A variation of the process is plasma cutting, an efficient steel cutting process.[82]

Submerged arc welding (SAW) is a high-productivity welding method in which the arc is struck beneath a covering layer of flux. This increases arc quality since contaminants in the atmosphere are blocked by the flux. The slag that forms on the weld generally comes off by itself, and combined with the use of a continuous wire feed, the weld deposition rate is high. Working conditions are much improved over other arc welding processes, since the flux hides the arc and almost no smoke is produced. The process is commonly used in industry, especially for large products and in the manufacture of welded pressure vessels.[83] Other arc welding processes include atomic hydrogen welding, electroslag welding (ESW), electrogas welding, and stud arc welding.[84] ESW is a highly productive, single-pass welding process for thicker materials between 1 inch (25 mm) and 12 inches (300 mm) in a vertical or close to vertical position.

To supply the electrical power necessary for arc welding processes, a variety of different power supplies can be used. The most common welding power supplies are constant current power supplies and constant voltage power supplies. In arc welding, the length of the arc is directly related to the voltage, and the amount of heat input is related to the current. Constant current power supplies are most often used for manual welding processes such as gas tungsten arc welding and shielded metal arc welding, because they maintain a relatively constant current even as the voltage varies. This is important because in manual welding, it can be difficult to hold the electrode perfectly steady, and as a result, the arc length and thus voltage tend to fluctuate. Constant voltage power supplies hold the voltage constant and vary the current, and as a result, are most often used for automated welding processes such as gas metal arc welding, flux-cored arc welding, and submerged arc welding. In these processes, arc length is kept constant, since any fluctuation in the distance between the wire and the base material is quickly rectified by a large change in current. For example, if the wire and the base material get too close, the current will rapidly increase, which in turn causes the heat to increase and the tip of the wire to melt, returning it to its original separation distance.[85]

The type of current used plays an important role in arc welding. Consumable electrode processes such as shielded metal arc welding and gas metal arc welding generally use direct current, but the electrode can be charged either positively or negatively. In welding, the positively charged anode will have a greater heat concentration, and as a result, changing the polarity of the electrode affects weld properties. If the electrode is positively charged, the base metal will be hotter, increasing weld penetration and welding speed. Alternatively, a negatively charged electrode results in more shallow welds.[86] Non-consumable electrode processes, such as gas tungsten arc welding, can use either type of direct current, as well as alternating current. However, with direct current, because the electrode only creates the arc and does not provide filler material, a positively charged electrode causes shallow welds, while a negatively charged electrode makes deeper welds.[87] Alternating current rapidly moves between these two, resulting in medium-penetration welds. One disadvantage of AC, the fact that the arc must be re-ignited after every zero crossings, has been addressed with the invention of special power units that produce a square wave pattern instead of the normal sine wave, making rapid zero crossings possible and minimizing the effects of the problem.[88]

Resistance welding generates heat from electrical resistance in the base metals. Two electrodes are simultaneously used to press the metal sheets together and to pass current through the sheets. The electrodes are made from highly conductive material, usually copper. The higher resistance in the base metals causes small pools of molten metal to form at the weld area as high current (1,000–100,000 A) is passed through.[89]

Resistance spot welding is a popular method used to join overlapping metal sheets of up to 3 mm thick. The advantages of the method include efficient energy use, limited workpiece deformation, high production rates, easy automation, and no required filler materials. Weld strength is significantly lower than with other welding methods, making the process suitable for only certain applications. It is used extensively in the automotive industry—ordinary cars can have several thousand spot welds made by industrial robots. In general, resistance welding methods are efficient and cause little pollution, but their applications are somewhat limited and the equipment cost can be high. A specialized process called shot welding, can be used to spot weld stainless steel.[89]

Seam welding also relies on two electrodes to apply pressure and current to join metal sheets. However, instead of pointed electrodes, wheel-shaped electrodes roll along and often feed the workpiece, making it possible to make long continuous welds. In the past, this process was used in the manufacture of beverage cans, but now its uses are more limited.[89] Other resistance welding methods include butt welding,[90] flash welding, projection welding, and upset welding.[89]

Energy beam welding methods, namely laser beam welding and electron beam welding, are relatively new processes that have become quite popular in high production applications. The two processes are quite similar, differing most notably in their source of power. Laser beam welding employs a highly focused laser beam, while electron beam welding is done in a vacuum and uses an electron beam. Both have a very high energy density, making deep weld penetration possible and minimizing the size of the weld area. Both processes are extremely fast, and are easily automated, making them highly productive. The primary disadvantages are their very high equipment costs (though these are decreasing) and a susceptibility to thermal cracking. Developments in this area include laser-hybrid welding, which uses principles from both laser beam welding and arc welding for even better weld properties, laser cladding, and x-ray welding.[91]

Like forge welding (the earliest welding process discovered), some modern welding methods do not involve the melting of the materials being joined. One of the most popular, ultrasonic welding, is used to connect thin sheets or wires made of metal or thermoplastic by vibrating them at high frequency and under high pressure.[93] The equipment and methods involved are similar to that of resistance welding, but instead of electric current, vibration provides energy input.  When welding metals, the vibrations are introduced horizontally, and the materials are not melted; with plastics, which should have similar melting temperatures, vertically. Ultrasonic welding is commonly used for making electrical connections out of aluminum or copper, and it is also a very common polymer welding process.[93]

Another common process, explosion welding, involves the joining of materials by pushing them together under extremely high pressure. The energy from the impact plasticizes the materials, forming a weld, even though only a limited amount of heat is generated. The process is commonly used for welding dissimilar materials, including bonding aluminum to carbon steel in ship hulls and stainless steel or titanium to carbon steel in petrochemical pressure vessels.[93]

Other solid-state welding processes include friction welding (including friction stir welding and friction stir spot welding),[94] magnetic pulse welding,[95] co-extrusion welding, cold welding, diffusion bonding, exothermic welding, high frequency welding, hot pressure welding, induction welding, and roll bonding.[93]

Welds can be geometrically prepared in many different ways. The five basic types of weld joints are the butt joint, lap joint, corner joint, edge joint, and T-joint (a variant of this last is the cruciform joint). Other variations exist as well—for example, double-V preparation joints are characterized by the two pieces of material each tapering to a single center point at one-half their height. Single-U and double-U preparation joints are also fairly common—instead of having straight edges like the single-V and double-V preparation joints, they are curved, forming the shape of a U. Lap joints are also commonly more than two pieces thick—depending on the process used and the thickness of the material, many pieces can be welded together in a lap joint geometry.[96]

Many welding processes require the use of a particular joint design; for example, resistance spot welding, laser beam welding, and electron beam welding are most frequently performed on lap joints. Other welding methods, like shielded metal arc welding, are extremely versatile and can weld virtually any type of joint. Some processes can also be used to make multipass welds, in which one weld is allowed to cool, and then another weld is performed on top of it. This allows for the welding of thick sections arranged in a single-V preparation joint, for example.[97]

After welding, a number of distinct regions can be identified in the weld area. The weld itself is called the fusion zone—more specifically, it is where the filler metal was laid during the welding process. The properties of the fusion zone depend primarily on the filler metal used, and its compatibility with the base materials. It is surrounded by the heat-affected zone, the area that had its microstructure and properties altered by the weld. These properties depend on the base material's behavior when subjected to heat. The metal in this area is often weaker than both the base material and the fusion zone, and is also where residual stresses are found.[98]

Many distinct factors influence the strength of welds and the material around them, including the welding method, the amount and concentration of energy input, the weldability of the base material, filler material, and flux material, the design of the joint, and the interactions between all these factors.[99]

For example, the factor of welding position influences weld quality, that welding codes & specifications may require testing—both welding procedures and welders—using specified welding positions: 1G (flat), 2G (horizontal), 3G (vertical), 4G (overhead), 5G (horizontal fixed pipe), or 6G (inclined fixed pipe).

To test the quality of a weld, either destructive or nondestructive testing methods are commonly used to verify that welds are free of defects, have acceptable levels of residual stresses and distortion, and have acceptable heat-affected zone (HAZ) properties. Types of welding defects include cracks, distortion, gas inclusions (porosity), non-metallic inclusions, lack of fusion, incomplete penetration, lamellar tearing, and undercutting.

The metalworking industry has instituted codes and specifications to guide welders, weld inspectors, engineers, managers, and property owners in proper welding technique, design of welds, how to judge the quality of welding procedure specification, how to judge the skill of the person performing the weld, and how to ensure the quality of a welding job.[99] Methods such as visual inspection, radiography, ultrasonic testing, phased-array ultrasonics, dye penetrant inspection, magnetic particle inspection, or industrial computed tomography can help with detection and analysis of certain defects.

The heat-affected zone (HAZ) is a ring surrounding the weld in which the temperature of the welding process, combined with the stresses of uneven heating and cooling, alters the heat-treatment properties of the alloy. The effects of welding on the material surrounding the weld can be detrimental—depending on the materials used and the heat input of the welding process used, the HAZ can be of varying size and strength. The thermal diffusivity of the base material plays a large role—if the diffusivity is high, the material cooling rate is high and the HAZ is relatively small. Conversely, a low diffusivity leads to slower cooling and a larger HAZ. The amount of heat injected by the welding process plays an important role as well, as processes like oxyacetylene welding have an unconcentrated heat input and increase the size of the HAZ. Processes like laser beam welding give a highly concentrated, limited amount of heat, resulting in a small HAZ. Arc welding falls between these two extremes, with the individual processes varying somewhat in heat input.[100][101] To calculate the heat input for arc welding procedures, the following formula can be used:

where Q = heat input (kJ/mm), V = voltage (V), I = current (A), and S = welding speed (mm/min). The efficiency is dependent on the welding process used, with shielded metal arc welding having a value of 0.75, gas metal arc welding and submerged arc welding, 0.9, and gas tungsten arc welding, 0.8.[102] Methods of alleviating the stresses and brittleness created in the HAZ include stress relieving and tempering.[103]

One major defect concerning the HAZ is cracking at the junction of the weld face and the base metal. Due to the rapid expansion (heating) and contraction (cooling), the material may not have the ability to withstand the stress and could crack. One method to control the stress is to control the heating and cooling rate, such as pre-heating and post-heating [104]

The durability and life of dynamically loaded, welded steel structures is determined in many cases by the welds, in particular the weld transitions. Through selective treatment of the transitions by grinding (abrasive cutting), shot peening, High-frequency impact treatment, Ultrasonic impact treatment, etc. the durability of many designs increases significantly.

Most solids used are engineering materials consisting of crystalline solids in which the atoms or ions are arranged in a repetitive geometric pattern which is known as a lattice structure. The only exception is material that is made from glass which is a combination of a supercooled liquid and polymers which are aggregates of large organic molecules.[105]

Crystalline solids cohesion is obtained by a metallic or chemical bond that is formed between the constituent atoms. Chemical bonds can be grouped into two types consisting of ionic and covalent. To form an ionic bond, either a valence or bonding electron separates from one atom and becomes attached to another atom to form oppositely charged ions. The bonding in the static position is when the ions occupy an equilibrium position where the resulting force between them is zero. When the ions are exerted in tension force, the inter-ionic spacing increases creating an electrostatic attractive force, while a repulsing force under compressive force between the atomic nuclei is dominant.[105]

Covalent bonding takes place when one of the constituent atoms loses one or more electrons, with the other atom gaining the electrons, resulting in an electron cloud that is shared by the molecule as a whole. In both ionic and covalent bonding the location of the ions and electrons are constrained relative to each other, thereby resulting in the bond being characteristically brittle.[105]

Metallic bonding can be classified as a type of covalent bonding for which the constituent atoms are of the same type and do not combine with one another to form a chemical bond. Atoms will lose an electron(s) forming an array of positive ions. These electrons are shared by the lattice which makes the electron cluster mobile, as the electrons are free to move as well as the ions. For this, it gives metals their relatively high thermal and electrical conductivity as well as being characteristically ductile.[105]

Three of the most commonly used crystal lattice structures in metals are the body-centred cubic, face-centred cubic and close-packed hexagonal. Ferritic steel has a body-centred cubic structure and austenitic steel, non-ferrous metals like aluminium, copper and nickel have the face-centred cubic structure.[105]

Ductility is an important factor in ensuring the integrity of structures by enabling them to sustain local stress concentrations without fracture. In addition, structures are required to be of an acceptable strength, which is related to a material's yield strength. In general, as the yield strength of a material increases, there is a corresponding reduction in fracture toughness.[105]

A reduction in fracture toughness may also be attributed to the embrittlement effect of impurities, or for body-centred cubic metals, from a reduction in temperature. Metals and in particular steels have a transitional temperature range where above this range the metal has acceptable notch-ductility while below this range the material becomes brittle. Within the range, the materials behavior is unpredictable. The reduction in fracture toughness is accompanied by a change in the fracture appearance. When above the transition, the fracture is primarily due to micro-void coalescence, which results in the fracture appearing fibrous. When the temperatures falls the fracture will show signs of cleavage facets. These two appearances are visible by the naked eye. Brittle fracture in steel plates may appear as chevron markings under the microscope. These arrow-like ridges on the crack surface point towards the origin of the fracture.[105]

Fracture toughness is measured using a notched and pre-cracked rectangular specimen, of which the dimensions are specified in standards, for example ASTM E23. There are other means of estimating or measuring fracture toughness by the following: The Charpy impact test per ASTM A370; The crack-tip opening displacement (CTOD) test per BS 7448–1; The J integral test per ASTM E1820; The Pellini drop-weight test per ASTM E208.[105]

While many welding applications are done in controlled environments such as factories and repair shops, some welding processes are commonly used in a wide variety of conditions, such as open air, underwater, and vacuums (such as space). In open-air applications, such as construction and outdoors repair, shielded metal arc welding is the most common process. Processes that employ inert gases to protect the weld cannot be readily used in such situations, because unpredictable atmospheric movements can result in a faulty weld. Shielded metal arc welding is also often used in underwater welding in the construction and repair of ships, offshore platforms, and pipelines, but others, such as flux cored arc welding and gas tungsten arc welding, are also common. Welding in space is also possible—it was first attempted in 1969 by Russian cosmonauts during the Soyuz 6 mission, when they performed experiments to test shielded metal arc welding, plasma arc welding, and electron beam welding in a depressurized environment. Further testing of these methods was done in the following decades, and today researchers continue to develop methods for using other welding processes in space, such as laser beam welding, resistance welding, and friction welding. Advances in these areas may be useful for future endeavours similar to the construction of the International Space Station, which could rely on welding for joining in space the parts that were manufactured on Earth.[106]

Welding can be dangerous and unhealthy if the proper precautions are not taken.[107] Potential safety risks come from fumes, ultraviolet radiation, heat, electric currents, and vibrations.[108] New technology, safe work practices, and proper protection reduce the risks of injury or death from welding.[109]

Since many common welding procedures involve an open flame or electric arc, the risk of burns and fire is significant; this is why it is classified as a hot work process. To prevent injury, welders wear personal protective equipment in the form of heavy leather gloves and protective long-sleeve jackets to avoid exposure to extreme heat and flames. Synthetic clothing such as polyester should not be worn.[110] Wool is less flammable than cotton, but dense cotton fabrics such as denim are still sufficient for clothing. However, any molten material that splatters onto synthetic material will melt directly through the fabric resulting in severe burns.[111]

Arc welding produces intense visible and ultraviolet light. Typical gas metal arc welding has an irradiance of 5W/m2 for the welder, which is many times brighter than sunlight.[112] This can cause a condition called arc eye or flash burns, in which ultraviolet light causes inflammation of the cornea, and can burn the retinas of the eyes. Welding helmets with dark UV-filtering face plates are worn to prevent this exposure.[113] Many helmets include an auto-darkening face plate, which instantly darkens upon exposure to the intense UV light.[114][115] To protect bystanders, the welding area is often surrounded by translucent welding curtains. These curtains, made of a polyvinyl chloride plastic film, shield people outside the welding area from the UV light of the electric arc, but they cannot replace the filter glass used in helmets.[116] The light can also burn exposed skin.[117] Because of the less intense light produced in oxyfuel welding, goggles that use less UV filtering and do not protect the entire face are sufficient.[118]

Depending on the type of material, welding varieties, and other factors, welding can produce over 100 dB(A) of noise.[119] Above 85 dB(A), earplugs should be worn.[120] Long-term or continuous exposure to higher decibels can lead to noise-induced hearing loss.[121] Processes that produce vibrations sufficient to numb a welder's hands are automated because PPE cannot offer sufficient protection.[122]

Welders are often exposed to dangerous gases and particulate matter. Processes like flux-cored arc welding and shielded metal arc welding produce smoke containing particles of various types of oxides. The size of the particles in question tends to influence the toxicity of the fumes, with smaller particles presenting a greater danger. This is because smaller particles can cross the blood–brain barrier. Fumes and gases, such as carbon dioxide, ozone, and fumes containing heavy metals, can be dangerous to welders lacking proper ventilation and training.[123] Exposure to manganese welding fumes, for example, even at low levels (<0.2 mg/m3), may cause neurological problems or damage to the lungs, liver, kidneys, or central nervous system.[124] Nano particles can become trapped in the alveolar macrophages of the lungs and induce pulmonary fibrosis.[125] The use of compressed gases and flames in many welding processes poses an explosion and fire risk. Some common precautions include limiting the amount of oxygen in the air, and keeping combustible materials away from the workplace.[123]

There are several technologies to mitigate dangers from welding fumes. Local exhaust ventilation (LEV) solutions remove fumes, smoke, and dust directly from the welding area. Forms of LEV include downdraft benches, fume hoods, and fume extraction welding guns. Downdraft benches have exhaust ducts beneath the metal welding table. Fume extraction guns have a vacuum hose that runs down to the welding nozzle. Movable fume hoods can positioned directly over the welding area.[126][127] Even with ventilation, there are still respiratory risks that respirators  can further reduce. Studies have shown that respirators, especially half-mask elastomeric respirators, significantly decrease particulate inhalation.[128]

As an industrial process, the cost of welding plays a crucial role in manufacturing decisions. Many different variables affect the total cost, including equipment cost, labor cost, material cost, and energy cost.[129] Depending on the process, equipment cost can vary, from inexpensive for methods like shielded metal arc welding and oxyfuel welding, to extremely expensive for methods like laser beam welding and electron beam welding. Because of their high cost, they are only used in high production operations. Similarly, because automation and robots increase equipment costs, they are only implemented when high production is necessary. Labor cost depends on the deposition rate (the rate of welding), the hourly wage, and the total operation time, including time spent fitting, welding, and handling the part. The cost of materials includes the cost of the base and filler material, and the cost of shielding gases. Finally, energy cost depends on arc time and welding power demand.[129]

For manual welding methods, labor costs generally make up the vast majority of the total cost. As a result, many cost-saving measures are focused on minimizing operation time. To do this, welding procedures with high deposition rates can be selected, and weld parameters can be fine-tuned to increase welding speed. Mechanization and automation are often implemented to reduce labor costs, but this frequently increases the cost of equipment and creates additional setup time. Material costs tend to increase when special properties are necessary, and energy costs normally do not amount to more than several percent of the total welding cost.[129]

In recent years, in order to minimize labor costs in high production manufacturing, industrial welding has become increasingly more automated, most notably with the use of robots in resistance spot welding (especially in the automotive industry) and in arc welding. In robot welding, mechanized devices both hold the material and perform the weld[130] and at first, spot welding was its most common application, but robotic arc welding increases in popularity as technology advances. Other key areas of research and development include the welding of dissimilar materials (such as steel and aluminum, for example) and new welding processes, such as friction stir, magnetic pulse, conductive heat seam, and laser-hybrid welding. Furthermore, progress is desired in making more specialized methods like laser beam welding practical for more applications, such as in the aerospace and automotive industries. Researchers also hope to better understand the often unpredictable properties of welds, especially microstructure, residual stresses, and a weld's tendency to crack or deform.[131]

The trend of accelerating the speed at which welds are performed in the steel erection industry comes at a risk to the integrity of the connection. Without proper fusion to the base materials provided by sufficient arc time on the weld, a project inspector cannot ensure the effective diameter of the puddle weld therefore he or she cannot guarantee the published load capacities unless they witness the actual installation.[132] This method of puddle welding is common in the United States and Canada for attaching steel sheets to bar joist and structural steel members. Regional agencies are responsible for ensuring the proper installation of puddle welding on steel construction sites. Currently there is no standard or weld procedure which can ensure the published holding capacity of any unwitnessed connection, but this is under review by the American Welding Society.

Glasses and certain types of plastics are commonly welded materials. Unlike metals, which have a specific melting point, glasses and plastics have a melting range, called the glass transition. When heating the solid material past the glass-transition temperature (Tg) into this range, it will generally become softer and more pliable. When it crosses through the range, above the glass-melting temperature (Tm), it will become a very thick, sluggish, viscous liquid, slowly decreasing in viscosity as temperature increases. Typically, this viscous liquid will have very little surface tension compared to metals, becoming a sticky, taffy to honey-like consistency, so welding can usually take place by simply pressing two melted surfaces together. The two liquids will generally mix and join at first contact. Upon cooling through the glass transition, the welded piece will solidify as one solid piece of amorphous material.

Glass welding is a common practice during glassblowing. It is used very often in the construction of lighting, neon signs, flashtubes, scientific equipment, and the manufacture of dishes and other glassware. It is also used during glass casting for joining the halves of glass molds, making items such as bottles and jars. Welding glass is accomplished by heating the glass through the glass transition, turning it into a thick, formable, liquid mass. Heating is usually done with a gas or oxy-gas torch, or a furnace, because the temperatures for melting glass are often quite high. This temperature may vary, depending on the type of glass. For example, lead glass becomes a weldable liquid at around 1,600 °F (870 °C), and can be welded with a simple propane torch. On the other hand, quartz glass (fused silica) must be heated to over 3,000 °F (1,650 °C), but quickly loses its viscosity and formability if overheated, so an oxyhydrogen torch must be used. Sometimes a tube may be attached to the glass, allowing it to be blown into various shapes, such as bulbs, bottles, or tubes. When two pieces of liquid glass are pressed together, they will usually weld very readily. Welding a handle onto a pitcher can usually be done with relative ease. However, when welding a tube to another tube, a combination of blowing and suction, and pressing and pulling is used to ensure a good seal, to shape the glass, and to keep the surface tension from closing the tube in on itself. Sometimes a filler rod may be used, but usually not.

Because glass is very brittle in its solid state, it is often prone to cracking upon heating and cooling, especially if the heating and cooling are uneven. This is because the brittleness of glass does not allow for uneven thermal expansion. Glass that has been welded will usually need to be cooled very slowly and evenly through the glass transition, in a process called annealing, to relieve any internal stresses created by a temperature gradient.

There are many types of glass, and it is most common to weld using the same types. Different glasses often have different rates of thermal expansion, which can cause them to crack upon cooling when they contract differently. For instance, quartz has very low thermal expansion, while soda-lime glass has very high thermal expansion. When welding different glasses to each other, it is usually important to closely match their coefficients of thermal expansion, to ensure that cracking does not occur. Also, some glasses will simply not mix with others, so welding between certain types may not be possible.

Glass can also be welded to metals and ceramics, although with metals the process is usually more adhesion to the surface of the metal rather than a commingling of the two materials. However, certain glasses will typically bond only to certain metals. For example, lead glass bonds readily to copper or molybdenum, but not to aluminum. Tungsten electrodes are often used in lighting but will not bond to quartz glass, so the tungsten is often wetted with molten borosilicate glass, which bonds to both tungsten and quartz. However, care must be taken to ensure that all materials have similar coefficients of thermal expansion to prevent cracking both when the object cools and when it is heated again. Special alloys are often used for this purpose, ensuring that the coefficients of expansion match, and sometimes thin, metallic coatings may be applied to a metal to create a good bond with the glass.[133][134][failed verification]

Plastics are generally divided into two categories, which are ""thermosets"" and ""thermoplastics."" A thermoset is a plastic in which a chemical reaction sets the molecular bonds after first forming the plastic, and then the bonds cannot be broken again without degrading the plastic. Thermosets cannot be melted, therefore, once a thermoset has set it is impossible to weld it. Examples of thermosets include epoxies, silicone, vulcanized rubber, polyester, and polyurethane.

Thermoplastics, by contrast, form long molecular chains, which are often coiled or intertwined, forming an amorphous structure without any long-range, crystalline order. Some thermoplastics may be fully amorphous, while others have a partially crystalline/partially amorphous structure. Both amorphous and semicrystalline thermoplastics have a glass transition, above which welding can occur, but semicrystallines also have a specific melting point which is above the glass transition. Above this melting point, the viscous liquid will become a free-flowing liquid (see rheological weldability for thermoplastics). Examples of thermoplastics include polyethylene, polypropylene, polystyrene, polyvinylchloride (PVC), and fluoroplastics like Teflon and Spectralon.

Welding thermoplastic with heat is very similar to welding glass. The plastic first must be cleaned and then heated through the glass transition, turning the weld-interface into a thick, viscous liquid. Two heated interfaces can then be pressed together, allowing the molecules to mix through intermolecular diffusion, joining them as one. Then the plastic is cooled through the glass transition, allowing the weld to solidify. A filler rod may often be used for certain types of joints. The main differences between welding glass and plastic are the types of heating methods, the much lower melting temperatures, and the fact that plastics will burn if overheated. Many different methods have been devised for heating plastic to a weldable temperature without burning it. Ovens or electric heating tools can be used to melt the plastic. Ultrasonic, laser, or friction heating are other methods. Resistive metals may be implanted in the plastic, which respond to induction heating. Some plastics will begin to burn at temperatures lower than their glass transition, so welding can be performed by blowing a heated, inert gas onto the plastic, melting it while, at the same time, shielding it from oxygen.[135]

Many thermoplastics can also be welded using chemical solvents. When placed in contact with the plastic, the solvent will begin to soften it, bringing the surface into a thick, liquid solution. When two melted surfaces are pressed together, the molecules in the solution mix, joining them as one. Because the solvent can permeate the plastic, the solvent evaporates out through the surface of the plastic, causing the weld to drop out of solution and solidify. A common use for solvent welding is for joining PVC (polyvinyl chloride) or ABS (acrylonitrile butadiene styrene) pipes during plumbing, or for welding styrene and polystyrene plastics in the construction of models. Solvent welding is especially effective on plastics like PVC which burn at or below their glass transition, but may be ineffective on plastics like Teflon or polyethylene that are resistant to chemical decomposition.[136]
"
Sand and Gravel Mining,"

Sand mining is the extraction of sand, mainly through an open pit (or sand pit)[1][failed verification][2] but sometimes mined from beaches and inland dunes or dredged from ocean and river beds.[3] Sand is often used in manufacturing, for example as an abrasive or in concrete. It is also used on icy and snowy roads usually mixed with salt, to lower the melting point temperature, on the road surface. Sand can replace eroded coastline.[4] Some uses require higher purity than others; for example sand used in concrete must be free of seashell fragments.

Sand mining presents opportunities to extract rutile, ilmenite, and zircon, which contain the industrially useful elements titanium and zirconium. Besides these minerals, beach sand may also contain garnet, leucoxene, sillimanite, and monazite.[5]

These minerals are quite often found in ordinary sand deposits. A process known as elutriation is used, whereby flowing water separates the grains based on their size, shape, and density.

Sand mining is a direct cause of erosion, and impacts the local wildlife.[6] Various animals depend on sandy beaches for nesting clutches, and mining has led to the near extinction of gharials (a species of crocodilian) in India. Disturbance of underwater and coastal sand causes turbidity in the water, which is harmful for organisms like coral that need sunlight. It can also destroy fisheries, financially harming their operators.

Removal of physical coastal barriers, such as dunes, sometimes leads to flooding of beachside communities, and the destruction of picturesque beaches causes tourism to dissipate. Sand mining is regulated by law in many places, but is often done illegally.[7] Globally, it is a $70 billion industry, with sand selling at up to $90 per cubic yard.[8]

In the 1940 mining operations began on the Kurnell Peninsula (Captain Cook's landing place in Australia) to supply the expanding Sydney building market. It continued until 1990 with an estimate of over 70 million tonnes of sand having been removed. The sand has been valued for many decades by the building industry, mainly because of its high crushed shell content and lack of organic matter, it has provided a cheap source of sand for most of Sydney since sand mining operations began. The site has now been reduced to a few remnant dunes and deep water-filled pits which are now being filled with demolition waste from Sydney's building sites. Removal of the sand has significantly weakened the peninsula's capacity to resist storms. Ocean waves pounding against the reduced Kurnell dune system have threatened to break through to Botany Bay, especially during the storms of May and June back in 1974 and of August 1998.[9]
Sand Mining also takes place in the Stockton sand dunes north of Newcastle and in the Broken Hill region in the far west of the state.

A large and long-running sand mine in Queensland, Australia (on North Stradbroke Island) provides a case study in the environmental consequences on a fragile sandy-soil based ecosystem, justified by the provision of low wage casual labor on an island with few other work options.[10] The Labor state government pledged to end sand mining by 2025, but this decision was overturned by the LNP government which succeeded it. This decision has been subject to an allegation of corrupt conduct.[11]

From the 1850s to the early 20th century, sand was mined from the tall, cliff-like banks of the Maribyrnong River, in what is now suburban Melbourne.[12] The Maribyrnong Sand Company was set up in the early 20th century to transport the sand by barge downriver to the industrial areas of Footscray and Yarraville, for use in the production of glass, concrete and ceramics.[13]

Sand mining contributes to the construction of buildings and development. The negative effects of sand mining include the permanent loss of sand in areas, as well as major habitat destruction.

Sand mining is an environmental problem in India. Environmentalists have raised public awareness of illegal sand mining in the states of Maharashtra, Madhya Pradesh,[14] Andhra Pradesh, Tamil Nadu[15] and Goa.[16] Conservation and environmental NGO Awaaz Foundation filed a public interest litigation in the Bombay High Court seeking a ban on mining activities along the Konkan coast.[17] Awaaz Foundation, in partnership with the Bombay Natural History Society also presented the issue of sand mining as a major international threat to coastal biodiversity at the Conference of Parties 11, Convention on Biological Diversity, Hyderabad in October 2012.[18][19] D. K. Ravi, an Indian Administrative Service officer of the Karnataka state, who was well known for his tough crackdown on the rampant illegal sand mining in the Kolar district, was found dead at his residence in Bengaluru, on 16 March 2015. It is widely alleged that the death was not due to suicide but caused by the mafia involved in land grabbing and sand mining.[20]

Sand mining occurs in the Kaipara Harbour, off the coast at Pakiri and offshore from Little Barrier Island.[21] A sand mine had operated at Whiritoa on the east coast of the North Island for 50 years extracting 180,000m3 of sand.[22] Coastal sand mines currently operate at Maioro and Taharoa to recover iron sand.[23] When an application was lodged in 2005 to mine iron sands on the seabed of the coast of Raglan local residents organised in opposition to the scheme.[24] The application for the mining was turned down by Crown Minerals due to a lack of technical detail. A proposal to begin sand mining in Bream Bay was among 149 initiatives invited to apply for resource consent in a streamlined process under the Fast-track Approvals Act 2024, an inclusion which drew widespread opposition amongst the local community.[25]

Activists and local villagers have protested against sand mining on Sierra Leone's Western Area Peninsula. The activity is contributing to Sierra Leone's coastal erosion, which is proceeding at up to 6 meters a year.[26]

The current size of the sand mining market in the United States is slightly over a billion dollars per year. The industry has been growing by nearly 10% annually since 2005 because of its use in hydrocarbon extraction. The majority of the market size for mining is held by Texas and Illinois.[27]

Silica sand mining business has more than doubled since 2009 because of the need for this particular type of sand, which is used in a process known as hydraulic fracturing. Wisconsin is one of the five states that produce nearly 2/3 of the nation's silica.  As of 2009, Wisconsin, along with other northern states, is facing an industrial mining boom, being dubbed the ""sand rush"" because of the new demand from large oil companies for silica sand. According to Minnesota Public Radio, ""One of the industry's major players, U.S. Silica, says its sand sales tied to hydraulic fracturing nearly doubled to $70 million from 2009 to 2010 and brought in nearly $70 million in just the first nine months of 2011.""[28] According to the Wisconsin Department of Natural Resources (WDNR), there are currently 34 active mines and 25 mines in development in Wisconsin. In 2012, the WDNR released a final report on the silica sand mining in Wisconsin titled Silica Sand Mining in Wisconsin.  The recent boom in silica sand mining has caused concern from residents in Wisconsin that include quality of life issues and the threat of silicosis. According to the WDNR (2012) these issues include noise, lights, hours of operation, damage and excessive wear to roads from trucking traffic, public safety concerns from the volume of truck traffic, possible damage and annoyance resulting from blasting, and concerns regarding aesthetics and land use changes.

As of 2013, industrial frac sand mining has become a cause for activism, especially in the Driftless Area of southeast Minnesota, northeast Iowa and southwest Wisconsin.[why?]

Much sand is extracted by dredges from the bottom of rivers such as the Red River in Yunnan, or quarried in dry river beds. Due to the large demand for sand for construction, illicit sand mining is not uncommon.[29][better source needed]

In 2020 the Coast Guard Administration of the neighboring country of Taiwan expelled or detained nearly 4,000 Chinese sand dredging vessels.[30] Illegal sand dredging by Chinese vessels causes environmental damage in Taiwan[31] as well as the Philippines.[32]
"
Residential Driveway Construction,"A driveway (also called drive in UK English)[1] is a private road for local access to one or a small group of structures owned and maintained by an individual or group.

Driveways rarely have traffic lights, but some may if they handle heavy traffic, especially those leading to commercial businesses or parks.

Driveways may be designed and decorated in ways that public roads cannot because of their lighter traffic and the willingness of owners to invest in their construction. Driveways are not resurfaced, cleared of snow, or maintained by governments. They are generally designed to conform to the architecture, standards, and landscaping of connected houses or other buildings.

Some materials used for driveways include concrete, decorative brick, cobblestone, block paving, asphalt, gravel, resin-bound paving, and decomposed granite. These materials may be surrounded with grass or other ground-cover plants.

Driveways are commonly used as paths to private garages, carports, or houses. On large estates, a driveway may be the road that leads to the house from the public road, possibly with a gate in between. Some driveways may be designed to serve different homeowners. A driveway may also refer to a small apron of pavement in front of a garage with a curb cut in the sidewalk, sometimes too short to accommodate a car.

Often, either by choice or to conform with local regulations, cars are parked in driveways to leave streets clear for traffic. Moreover, some jurisdictions prohibit parking or leaving standing any motor vehicle upon any residential lawn area (the property from the front of a residential house, condominium, or cooperative to the street line other than a driveway, walkway, concrete, or blacktopped surface parking space).[2] Other examples include the city of Berkeley, California that forbids ""any person to park or leave standing, or cause to be parked or left standing any vehicle upon any public street in the City for seventy-two or more consecutive hours.""[3] Other areas may prohibit leaving vehicles on residential streets during certain times (for instance, to accommodate regular street cleaning), necessitating the use of driveways.

Residential driveways may serve as the place for conducting garage sales, automobile washing and repair, and recreation, notably (in North America) for basketball practice. Australia hosts the longest driveway on earth that covers a distance of 130 kilometers to a sheep cattle station. This driveway showcases the county’s interior by crossing through harsh and remote terrains.[4]
"
Commercial Driveway Construction,"A driveway (also called drive in UK English)[1] is a private road for local access to one or a small group of structures owned and maintained by an individual or group.

Driveways rarely have traffic lights, but some may if they handle heavy traffic, especially those leading to commercial businesses or parks.

Driveways may be designed and decorated in ways that public roads cannot because of their lighter traffic and the willingness of owners to invest in their construction. Driveways are not resurfaced, cleared of snow, or maintained by governments. They are generally designed to conform to the architecture, standards, and landscaping of connected houses or other buildings.

Some materials used for driveways include concrete, decorative brick, cobblestone, block paving, asphalt, gravel, resin-bound paving, and decomposed granite. These materials may be surrounded with grass or other ground-cover plants.

Driveways are commonly used as paths to private garages, carports, or houses. On large estates, a driveway may be the road that leads to the house from the public road, possibly with a gate in between. Some driveways may be designed to serve different homeowners. A driveway may also refer to a small apron of pavement in front of a garage with a curb cut in the sidewalk, sometimes too short to accommodate a car.

Often, either by choice or to conform with local regulations, cars are parked in driveways to leave streets clear for traffic. Moreover, some jurisdictions prohibit parking or leaving standing any motor vehicle upon any residential lawn area (the property from the front of a residential house, condominium, or cooperative to the street line other than a driveway, walkway, concrete, or blacktopped surface parking space).[2] Other examples include the city of Berkeley, California that forbids ""any person to park or leave standing, or cause to be parked or left standing any vehicle upon any public street in the City for seventy-two or more consecutive hours.""[3] Other areas may prohibit leaving vehicles on residential streets during certain times (for instance, to accommodate regular street cleaning), necessitating the use of driveways.

Residential driveways may serve as the place for conducting garage sales, automobile washing and repair, and recreation, notably (in North America) for basketball practice. Australia hosts the longest driveway on earth that covers a distance of 130 kilometers to a sheep cattle station. This driveway showcases the county’s interior by crossing through harsh and remote terrains.[4]
"
Fencing Construction Services,"A fence is a structure that encloses an area, typically outdoors, and is usually constructed from posts that are connected by boards, wire, rails or netting.[1] A fence differs from a wall in not having a solid foundation along its whole length.[2]

Alternatives to fencing include a ditch (sometimes filled with water, forming a moat).

A balustrade or railing is a fence to prevent people from falling over an edge, most commonly found on a stairway, landing, or balcony. Railing systems and balustrades are also used along roofs, bridges, cliffs, pits, and bodies of water.

Another aim of using fence is to limit the intrusion attempt into a property by malicious intruders. In support of these barriers there are sophisticated technologies that can be applied on fence itself and strengthen the defence of territory reducing the risk. 

The elements that reinforce the perimeter protection are:

In most developed areas the use of fencing is regulated, variously in commercial, residential, and agricultural areas. Height, material, setback, and aesthetic issues are among the considerations subject to regulation.

The following types of areas or facilities often are required by law to be fenced in, for safety and security reasons:

Servitudes[6] are legal arrangements of land use arising out of private agreements. Under the feudal system, most land in England was cultivated in common fields, where peasants were allocated strips of arable land that were used to support the needs of the local village or manor. By the sixteenth century the growth of population and prosperity provided incentives for landowners to use their land in more profitable ways, dispossessing the peasantry. Common fields were aggregated and enclosed by large and enterprising farmers—either through negotiation among one another or by lease from the landlord—to maximize the productivity of the available land and contain livestock. Fences redefined the means by which land is used, resulting in the modern law of servitudes.[7]

In the United States, the earliest settlers claimed land by simply fencing it in. Later, as the American government formed, unsettled land became technically owned by the government and programs to register land ownership developed, usually making raw land available for low prices or for free, if the owner improved the property, including the construction of fences. However, the remaining vast tracts of unsettled land were often used as a commons, or, in the American West, ""open range"" as degradation of habitat developed due to overgrazing and a tragedy of the commons situation arose, common areas began to either be allocated to individual landowners via mechanisms such as the Homestead Act and Desert Land Act and fenced in, or, if kept in public hands, leased to individual users for limited purposes, with fences built to separate tracts of public and private land.

Ownership of a fence on a boundary varies. The last relevant original title deed(s)[8] and a completed seller's property information form may document which side has to put up and has installed any fence respectively; the first using ""T"" marks/symbols (the side with the ""T"" denotes the owner); the latter by a ticked box to the best of the last owner's belief with no duty, as the conventionally agreed conveyancing process stresses, to make any detailed, protracted enquiry.[9] Commonly the mesh or panelling is in mid-position. Otherwise it tends to be on non-owner's side so the fence owner might access the posts when repairs are needed but this is not a legal requirement.[10] Where estate planners wish to entrench privacy a close-boarded fence or equivalent well-maintained hedge of a minimum height may be stipulated by deed. Beyond a standard height planning permission is necessary.

Where a rural fence or hedge has (or in some cases had) an adjacent ditch, the ditch is normally in the same ownership as the hedge or fence, with the ownership boundary being the edge of the ditch furthest from the fence or hedge.[11] The principle of this rule is that an owner digging a boundary ditch will normally dig it up to the very edge of their land, and must then pile the spoil on their own side of the ditch to avoid trespassing on their neighbour. They may then erect a fence or hedge on the spoil, leaving the ditch on its far side. Exceptions exist in law, for example where a plot of land derives from subdivision of a larger one along the centre line of a previously existing ditch or other feature, particularly where reinforced by historic parcel numbers with acreages beneath which were used to tally up a total for administrative units not to confirm the actual size of holdings, a rare instance where Ordnance Survey maps often provide more than circumstantial evidence namely as to which feature is to be considered the boundary.

On private land in the United Kingdom, it is the landowner's responsibility to fence their livestock in. Conversely, for common land, it is the surrounding landowners' duty to fence the common's livestock out such as in large parts of the New Forest. Large commons with livestock roaming have been greatly reduced by 18th and 19th century Acts for enclosure of commons covering most local units, with most remaining such land in the UK's National Parks.

A 19th-century law requires railways to be fenced to keep people and livestock out.[12] It is also illegal to trespass on railways, incurring a fine of up to £1000.

Distinctly different land ownership and fencing patterns arose in the eastern and western United States. Original fence laws on the east coast were based on the British common law system, and rapidly increasing population quickly resulted in laws requiring livestock to be fenced in. In the west, land ownership patterns and policies reflected a strong influence of Spanish law and tradition, plus the vast land area involved made extensive fencing impractical until mandated by a growing population and conflicts between landowners. The ""open range"" tradition of requiring landowners to fence out unwanted livestock was dominant in most of the rural west until very late in the 20th century, and even today, a few isolated regions of the west still have open range statutes on the books. More recently, fences are generally constructed on the surveyed property line as precisely as possible. Today, across the nation, each state is free to develop its own laws regarding fences. In many cases for both rural and urban property owners, the laws were designed to require adjacent landowners to share the responsibility for maintaining a common boundary fenceline. Today, however, only 22 states have retained that provision.

Some U.S. states, including Texas, Illinois, Missouri, and North Carolina, have enacted laws establishing that purple paint markings on fences (or trees) are the legal equivalent of ""No Trespassing"" signs. The laws are meant to spare landowners, particularly in rural areas, from having to continually replace printed signs that often end up being stolen or obliterated by the elements.[13]

The value of fences and the metaphorical significance of a fence, both positive and negative, has been extensively utilized throughout western culture. A few examples include:

Notes

Bibliography


"
Sidewalk Construction Services,"

A sidewalk (North American English)[1][2][3] or pavement (British English) is a path along the side of a road. Usually constructed of concrete, pavers, brick, stone, or asphalt, it is designed for pedestrians. A sidewalk is normally higher than the roadway, and separated from it by a curb. There may also be a planted strip between the sidewalk and the roadway and between the roadway and the adjacent land.

The term ""sidewalk"" is preferred in most of the United States[1] & Canada.[2][3] The term ""pavement"" is more common in the United Kingdom[4] and other members of the Commonwealth of Nations, as well as parts of the Mid-Atlantic United States such as Philadelphia and parts of New Jersey.[5][6] Many Commonwealth countries use the term ""footpath"". The professional, civil engineering and legal term for this in the USA and Canada is ""sidewalk"" while in the United Kingdom it is ""pavement"".[7]

In the United States, the term sidewalk is used for the pedestrian path beside a road. ""Shared use paths"" or ""multi-use paths"" are available for use by both pedestrians and bicyclists.[8] ""Walkway"" is a more comprehensive term that includes stairs, ramps, passageways, and related structures that facilitate the use of a path as well as the sidewalk.[9] In the UK, the term ""footpath"" is mostly used for paths that do not abut a roadway.[10] The term ""shared-use path"" is used where cyclists are also able to use the same section of path as pedestrians.[11]

Sidewalks have operated for at least 4,000 years.[12] The Greek city of Corinth had sidewalks by the 4th-century BC, and the Romans built sidewalks – they called them sēmitae.[13]

However, by the Middle Ages, narrow roads had reverted to being simultaneously used by pedestrians and wagons without any formal separation between the two categories. Early attempts at ensuring the adequate maintenance of foot-ways or sidewalks were often made,[why?] as in the Colchester Improvement Act 1623 (21 Jas. 1. c. 34) for Colchester, but they were generally not very effective.[14]

Following the Great Fire of London in 1666, attempts were slowly made to bring some order to the sprawling city. In 1671, ""Certain Orders, Rules and Directions Touching the Paving and Cleansing The Streets, Lanes and Common Passages within the City of London"" were formulated, calling for all streets to be adequately paved for pedestrians with cobblestones. Purbeck stone was widely used as a durable paving material. Bollards were also installed to protect pedestrians from the traffic in the middle of the road.

The British House of Commons passed a series of Paving Acts from the 18th century. The 1766 Paving & Lighting Act authorized the City of London Corporation to establish foot-ways throughout all the streets of London, to pave them with Purbeck stone (the thoroughfare in the middle was generally cobblestone) and to raise them above the street level with kerbs forming the separation.[15] The corporation was also made responsible for the regular upkeep of the roads, including their cleaning and repair, for which they charged a tax from 1766.[16] Another turning point was the construction of Paris's Pont Neuf (1578–1606) which set several trends including wide, raised sidewalks separating pedestrians from the road traffic, plus the first Parisian bridge without houses built on it, and its generous width plus elegant, durable design that immediately became popular for promenading at the beginning of the century that saw Paris take its form renowned to this day. It was also a cultural phenomenon because all classes mixed on the new walkways. By the 19th-century large and spacious sidewalks were routinely constructed in European capitals, and were associated with urban sophistication.

Sidewalks played an important role in transportation, as they provided a path for people to walk along without stepping on horse manure. They aided road safety by minimizing interaction between pedestrians, horses, carriages, and later automobiles. Sidewalks are normally in pairs, one on each side of the road, with the center section of the road for motorized vehicles. Crosswalks provide pedestrians a space to cross between the two sides of the street at predictable locations.

On rural roads, sidewalks may not be present as the amount of traffic (pedestrian or motorized) may not be enough to justify separating the two. In suburban and urban areas, sidewalks are more common. In town and city centers (known as downtown in the USA) the amount of pedestrian traffic can exceed motorized traffic, and in this case the sidewalks can occupy more than half of the width of the road, or the whole road can be pedestrianized.

Sidewalks may have a small effect on reducing vehicle miles traveled and carbon dioxide emissions. A study of sidewalk and transit investments in Seattle neighborhoods found vehicle travel reductions of 6 to 8% and CO2 emission reductions of 1.3 to 2.2%[17]

Research commissioned for the Florida Department of Transportation, published in 2005, found that, in Florida, the Crash Reduction Factor (used to estimate the expected reduction of crashes during a given period) resulting from the installation of sidewalks averaged 74%.[18]
Research at the University of North Carolina for the U.S. Department of Transportation found that the presence or absence of a sidewalk and the speed limit are significant factors in the likelihood of a vehicle/pedestrian crash. Sidewalk presence had a risk ratio of 0.118, which means that the likelihood of a crash on a road with a paved sidewalk was 88.2 percent lower than one without a sidewalk. The authors wrote that ""this should not be interpreted to mean that installing sidewalks would necessarily reduce the likelihood of pedestrian/motor vehicle crashes by 88.2 percent in all situations. However, the presence of a sidewalk clearly has a strong beneficial effect of reducing the risk of a 'walking along roadway' pedestrian/motor vehicle crash."" The study does not count crashes that happen when walking across a roadway. The speed limit risk ratio was 1.116, which means that a 16.1-km/h (10-mi/h) increase in the limit yields a factor of (1.116)10 or 3.[19]

The presence or absence of sidewalks was one of three factors that were found to encourage drivers to choose lower, safer speeds.[20]

On the other hand, the implementation of schemes which involve the removal of sidewalks, such as shared space schemes, are reported to deliver a dramatic drop in crashes and congestion too, which indicates that a number of other factors, such as the local speed environment, also play an important role in whether sidewalks are necessarily the best local solution for pedestrian safety.[21]

In cold weather, black ice is a common problem with unsalted sidewalks. The ice forms a thin transparent surface film which is almost impossible to see, and so results in many slips by pedestrians.

Riding bicycles on sidewalks is discouraged since some research shows it to be more dangerous than riding in the street.[22] Some jurisdictions prohibit sidewalk riding except for children. In addition to the risk of cyclist/pedestrian collisions, cyclists face increase risks from collisions with motor vehicles at street crossings and driveways. Riding in the direction opposite to traffic in the adjacent lane is especially risky.[23]

Since residents of neighborhoods with sidewalks are more likely to walk, they tend to have lower rates of cardiovascular disease, obesity, and other health issues related to sedentary lifestyles.[24] Also, children who walk to school have been shown to have better concentration.[25]

Some sidewalks may be used as social spaces with sidewalk cafés, markets, or busking musicians, as well as for parking for a variety of vehicles including cars, motorbikes and bicycles. Sidewalk surfing was often used in the early 1960s to describe skateboarding.[26])

Contemporary sidewalks are most often made of concrete in North America, while tarmac, asphalt, brick, stone, slab and (increasingly) rubber are more common in Europe.[27] Different materials are more or less friendly environmentally: pumice-based trass, for example, when used as an extender is less energy-intensive than Portland cement concrete or petroleum-based materials such as asphalt or tar-penetration macadam. Multi-use paths alongside roads are sometimes made of materials that are softer than concrete, such as asphalt.

Some sidewalks may be built like a Meandering Sidewalk. The meandering sidewalk is the wavy sidewalk that veers back and forth at the side of the road, no matter how straight the street. These sidewalks are common in North America and are used to break up the monotonous alignments of city blocks.

In the 19th century and early 20th century, sidewalks of wood were common in some North American locations. They may still be found at historic beach locations and in conservation areas to protect the land beneath and around, called boardwalks.

Brick sidewalks are found in some urban areas, usually for aesthetic purposes. Brick sidewalks are generally consolidated with brick hammers, rollers, and sometimes motorized vibrators.

Stone slabs called flagstones or flags are sometimes used where an attractive appearance is required, as in historic town centers.

For example, in Melbourne, Australia, bluestone has been used to pave the sidewalks of the CBD since the Gold rush in the 1850s because it proved to be stronger, more plentiful and easier to work than most other available materials.

Pre-cast concrete pavers are used for sidewalks, often colored or textured to resemble stone. Sometimes cobblestones are used, though they are generally considered too uneven for comfortable walking.

In the United States and Canada, the most common type of sidewalk consists of a poured concrete ""ribbon"", examples of which from as early as the 1860s can be found in good repair in San Francisco, and stamped with the name of the contractor and date of installation.[citation needed] When Portland cement was first imported to the United States in the 1880s, its principal use was in the construction of sidewalks.[28]

Today, most sidewalk ribbons are constructed with cross-lying strain-relief grooves placed or sawn at regular intervals, typically 5 feet (1.5 m) apart. This partitioning, an improvement over the continuous slab ribbon, was patented in 1924 by Arthur Wesley Hall and William Alexander McVay, who wished to minimize damage to the concrete from the effects of tectonic and temperature fluctuations, both of which can crack longer segments.[29] The technique is not perfect, as freeze-thaw cycles (in cold-winter regions) and tree root growth can eventually result in damage which requires repair.

In highly variable climates which undergo multiple freeze-thaw cycles, concrete blocks will be formed with separations, called expansion joints, to allow for thermal expansion without breakage.  The use of expansion joints in sidewalks may not be necessary, as the concrete will shrink while setting.[30]

In the United Kingdom, Australia and France suburban sidewalks are most commonly constructed of tarmac. In urban or inner-city areas sidewalks are most commonly constructed of slabs, stone, or brick depending upon the surrounding street architecture and furniture.
"
Commercial Irrigation Systems,"Irrigation (also referred to as watering of plants) is the practice of applying controlled amounts of water to land to help grow crops, landscape plants, and lawns. Irrigation has been a key aspect of agriculture for over 5,000 years and has been developed by many cultures around the world. Irrigation helps to grow crops, maintain landscapes, and revegetate disturbed soils in dry areas and during times of below-average rainfall. In addition to these uses, irrigation is also employed to protect crops from frost,[1] suppress weed growth in grain fields, and prevent soil consolidation. It is also used to cool livestock, reduce dust, dispose of sewage, and support mining operations. Drainage, which involves the removal of surface and sub-surface water from a given location, is often studied in conjunction with irrigation.

There are several methods of irrigation that differ in how water is supplied to plants. Surface irrigation, also known as gravity irrigation, is the oldest form of irrigation and has been in use for thousands of years. In sprinkler irrigation, water is piped to one or more central locations within the field and distributed by overhead high-pressure water devices. Micro-irrigation is a system that distributes water under low pressure through a piped network and applies it as a small discharge to each plant. Micro-irrigation uses less pressure and water flow than sprinkler irrigation. Drip irrigation delivers water directly to the root zone of plants. Subirrigation has been used in field crops in areas with high water tables for many years. It involves artificially raising the water table to moisten the soil below the root zone of plants.

Irrigation water can come from groundwater (extracted from springs or by using wells), from surface water (withdrawn from rivers, lakes or reservoirs) or from non-conventional sources like treated wastewater, desalinated water, drainage water, or fog collection. Irrigation can be supplementary to rainfall, which is common in many parts of the world as rainfed agriculture, or it can be full irrigation, where crops rarely rely on any contribution from rainfall. Full irrigation is less common and only occurs in arid landscapes with very low rainfall or when crops are grown in semi-arid areas outside of rainy seasons.

The environmental effects of irrigation relate to the changes in quantity and quality of soil and water as a result of irrigation and the subsequent effects on natural and social conditions in river basins and downstream of an irrigation scheme. The effects stem from the altered hydrological conditions caused by the installation and operation of the irrigation scheme. Amongst some of these problems is depletion of underground aquifers through overdrafting. Soil can be over-irrigated due to poor distribution uniformity or management wastes water, chemicals, and may lead to water pollution. Over-irrigation can cause deep drainage from rising water tables that can lead to problems of irrigation salinity requiring watertable control by some form of subsurface land drainage.

In 2000, the total fertile land was 2,788,000 km2 (689 million acres) and it was equipped with irrigation infrastructure worldwide. About 68% of this area is in Asia, 17% in the Americas, 9% in Europe, 5% in Africa and 1% in Oceania. The largest contiguous areas of high irrigation density are found in Northern and Eastern India and Pakistan along the Ganges and Indus rivers; in the Hai He, Huang He and Yangtze basins in China; along the Nile river in Egypt and Sudan; and in the Mississippi-Missouri river basin, the Southern Great Plains, and in parts of California in the United States. Smaller irrigation areas are spread across almost all populated parts of the world.[2]

By 2012, the area of irrigated land had increased to an estimated total of 3,242,917 km2 (801 million acres), which is nearly the size of India.[3] The irrigation of 20% of farming land accounts for the production of 40% of food production.[4][5]

The scale of irrigation increased dramatically over the 20th century. In 1800, 8 million hectares globally were irrigated, in 1950, 94 million hectares, and in 1990, 235 million hectares. By 1990, 30% of the global food production came from irrigated land.[6] Irrigation techniques across the globe includes canals redirecting surface water,[7][8] groundwater pumping, and diverting water from dams. National governments lead most irrigation schemes within their borders, but private investors[9] and other nations,[8] especially the United States,[10] China,[11] and European countries like the United Kingdom,[12] also fund and organize some schemes within other nations.

By 2021 the global land area equipped for irrigation reached 352 million ha, an increase of 22% from the 289 million ha of 2000 and more than twice the 1960s land area equipped for irrigation. The vast majority is located in Asia (70%), where irrigation was a key component of the green revolution; the Americas account for 16% and Europe for 8% of the world total. India (76 million ha) and China (75 million ha) have the largest equipped area for irrigation, far ahead of the United States o fAmerica (27 million ha). China and India also have the largest net gains in equipped area between 2000 and 2020 (+21 million ha for China and +15 million ha for India). All the regions saw increases in the area equipped for irrigation, with Africa growing the fastest (+29%), followed by Asia (+25%), Oceania (+24%), the Americas (+19%) and Europe (+2%).[13]

Irrigation enables the production of more crops, especially commodity crops in areas which otherwise could not support them. Countries frequently invested in irrigation to increase wheat, rice, or cotton production, often with the overarching goal of increasing self-sufficiency.[12]

Irrigation water can come from groundwater (extracted from springs or by using wells), from surface water (withdrawn from rivers, lakes or reservoirs) or from non-conventional sources like treated wastewater, desalinated water, drainage water, or fog collection.

While floodwater harvesting belongs to the accepted irrigation methods, rainwater harvesting is usually not considered as a form of irrigation. Rainwater harvesting is the collection of runoff water from roofs or unused land and the concentration of this.

Irrigation with recycled municipal wastewater can also serve to fertilize plants if it contains nutrients, such as nitrogen, phosphorus and potassium. There are benefits of using recycled water for irrigation, including the lower cost compared to some other sources and consistency of supply regardless of season, climatic conditions and associated water restrictions. When reclaimed water is used for irrigation in agriculture, the nutrient (nitrogen and phosphorus) content of the treated wastewater has the benefit of acting as a fertilizer.[15] This can make the reuse of excreta contained in sewage attractive.[16]

In developing countries, agriculture is increasingly using untreated municipal wastewater for irrigation – often in an unsafe manner. Cities provide lucrative markets for fresh produce, so they are attractive to farmers. However, because agriculture has to compete for increasingly scarce water resources with industry and municipal users, there is often no alternative for farmers but to use water polluted with urban waste directly to water their crops.

There can be significant health hazards related to using untreated wastewater in agriculture. Municipal wastewater can contain a mixture of chemical and biological pollutants. In low-income countries, there are often high levels of pathogens from excreta. In emerging nations, where industrial development is outpacing environmental regulation, there are increasing risks from inorganic and organic chemicals. The World Health Organization developed guidelines for safe use of wastewater in 2006,[16] advocating a ‘multiple-barrier' approach wastewater use, for example by encouraging farmers to adopt various risk-reducing behaviors. These include ceasing irrigation a few days before harvesting to allow pathogens to die off in the sunlight; applying water carefully so it does not contaminate leaves likely to be eaten raw; cleaning vegetables with disinfectant; or allowing fecal sludge used in farming to dry before being used as a human manure.[15]

Irrigation water can also come from non-conventional sources like treated wastewater,[20] desalinated water, drainage water, or fog collection.

In countries where humid air sweeps through at night, water can be obtained by condensation onto cold surfaces. This is practiced in the vineyards at Lanzarote using stones to condense water. Fog collectors are also made of canvas or foil sheets. Using condensate from air conditioning units as a water source is also becoming more popular in large urban areas.

As of November 2019[update] a Glasgow-based startup has helped a farmer in Scotland to establish edible saltmarsh crops irrigated with sea water. An acre of previously marginal land has been put under cultivation to grow samphire, sea blite, and sea aster; these plants yield a higher profit than potatoes. The land is flood irrigated twice a day to simulate tidal flooding; the water is pumped from the sea using wind power. Additional benefits are soil remediation and carbon sequestration.[21][22]

Until the 1960s, there were fewer than half the number of people on the planet as of 2024. People were not as wealthy as today, consumed fewer calories and ate less meat, so less water was needed to produce their food. They required a third of the volume of water humans presently take from rivers. Today, the competition for water resources is much more intense, because there are now more than seven billion people on the planet, increasing the likelihood of overconsumption of food produced by water-thirsty animal agriculture and intensive farming practices. This creates increasing competition for water from industry, urbanisation and biofuel crops. Farmers will have to strive to increase productivity to meet growing demands for food, while industry and cities find ways to use water more efficiently.[23]

Successful agriculture is dependent upon farmers having sufficient access to water. However, water scarcity is already a critical constraint to farming in many parts of the world.

There are several methods of irrigation. They vary in how the water is supplied to the plants. The goal is to apply the water to the plants as uniformly as possible, so that each plant has the amount of water it needs, neither too much nor too little. Irrigation can also be understood whether it is supplementary to rainfall as happens in many parts of the world, or whether it is 'full irrigation' whereby crops rarely depend on any contribution from rainfall. Full irrigation is less common and only happens in arid landscapes experiencing very low rainfall or when crops are grown in semi-arid areas outside of any rainy seasons.

Surface irrigation, also known as gravity irrigation, is the oldest form of irrigation and has been in use for thousands of years. In surface (furrow, flood, or level basin) irrigation systems, water moves across the surface of agricultural lands, in order to wet it and infiltrate into the soil. Water moves by following gravity or the slope of the land. Surface irrigation can be subdivided into furrow, border strip or basin irrigation. It is often called flood irrigation when the irrigation results in flooding or near flooding of the cultivated land. Historically, surface irrigation is the most common method of irrigating agricultural land across most parts of the world. The water application efficiency of surface irrigation is typically lower than other forms of irrigation, due in part to the lack of control of applied depths. Surface irrigation involves a significantly lower capital cost and energy requirement than pressurised irrigation systems. Hence it is often the irrigation choice for developing nations, for low value crops and for large fields. Where water levels from the irrigation source permit, the levels are controlled by dikes (levees), usually plugged by soil. This is often seen in terraced rice fields (rice paddies), where the method is used to flood or control the level of water in each distinct field. In some cases, the water is pumped, or lifted by human or animal power to the level of the land.

Surface irrigation is even used to water urban gardens in certain areas, for example, in and around Phoenix, Arizona. The irrigated area is surrounded by a berm and the water is delivered according to a schedule set by a local irrigation district.[24]

A special form of irrigation using surface water is spate irrigation, also called floodwater harvesting. In case of a flood (spate), water is diverted to normally dry river beds (wadis) using a network of dams, gates and channels and spread over large areas. The moisture stored in the soil will be used thereafter to grow crops. Spate irrigation areas are in particular located in semi-arid or arid, mountainous regions.

Micro-irrigation, sometimes called localized irrigation, low volume irrigation, or trickle irrigation is a system where water is distributed under low pressure through a piped network, in a pre-determined pattern, and applied as a small discharge to each plant or adjacent to it. Traditional drip irrigation use individual emitters, subsurface drip irrigation (SDI), micro-spray or micro-sprinklers, and mini-bubbler irrigation all belong to this category of irrigation methods.[25]

Drip irrigation, also known as microirrigation or trickle irrigation, functions as its name suggests. In this system, water is delivered at or near the root zone of plants, one drop at a time. This method can be the most water-efficient method of irrigation,[26] if managed properly; evaporation and runoff are minimized. The field water efficiency of drip irrigation is typically in the range of 80 to 90% when managed correctly.

In modern agriculture, drip irrigation is often combined with plastic mulch, further reducing evaporation, and is also the means of delivery of fertilizer. The process is known as fertigation.

Deep percolation, where water moves below the root zone, can occur if a drip system is operated for too long or if the delivery rate is too high. Drip irrigation methods range from very high-tech and computerized to low-tech and labor-intensive. Lower water pressures are usually needed than for most other types of systems, with the exception of low-energy center pivot systems and surface irrigation systems, and the system can be designed for uniformity throughout a field or for precise water delivery to individual plants in a landscape containing a mix of plant species. Although it is difficult to regulate pressure on steep slopes, pressure compensating emitters are available, so the field does not have to be level. High-tech solutions involve precisely calibrated emitters located along lines of tubing that extend from a computerized set of valves.[27]

In sprinkler or overhead irrigation, water is piped to one or more central locations within the field and distributed by overhead high-pressure sprinklers or guns. A system using sprinklers, sprays, or guns mounted overhead on permanently installed risers is often referred to as a solid-set irrigation system. Higher pressure sprinklers that rotate are called rotors and are driven by a ball drive, gear drive, or impact mechanism. Rotors can be designed to rotate in a full or partial circle. Guns are similar to rotors, except that they generally operate at very high pressures of 275 to 900 kPa (40 to 130 psi) and flows of 3 to 76 L/s (50 to 1200 US gal/min), usually with nozzle diameters in the range of 10 to 50 mm (0.5 to 1.9 in). Guns are used not only for irrigation, but also for industrial applications such as dust suppression and logging.

Sprinklers can also be mounted on moving platforms connected to the water source by a hose. Automatically moving wheeled systems known as traveling sprinklers may irrigate areas such as small farms, sports fields, parks, pastures, and cemeteries unattended. Most of these use a length of polyethylene tubing wound on a steel drum. As the tubing is wound on the drum powered by the irrigation water or a small gas engine, the sprinkler is pulled across the field. When the sprinkler arrives back at the reel the system shuts off. This type of system is known to most people as a ""waterreel"" traveling irrigation sprinkler and they are used extensively for dust suppression, irrigation, and land application of waste water.

Other travelers use a flat rubber hose that is dragged along behind while the sprinkler platform is pulled by a cable.

Center pivot irrigation is a form of sprinkler irrigation utilising several segments of pipe (usually galvanized steel or aluminium) joined and supported by trusses, mounted on wheeled towers with sprinklers positioned along its length.[28]
The system moves in a circular pattern and is fed with water from the pivot point at the center of the arc. These systems are found and used in all parts of the world and allow irrigation of all types of terrain. Newer systems have drop sprinkler heads as shown in the image that follows.

As of 2017[update] most center pivot systems have drops hanging from a U-shaped pipe attached at the top of the pipe with sprinkler heads that are positioned a few feet (at most) above the crop, thus limiting evaporative losses. Drops can also be used with drag hoses or bubblers that deposit the water directly on the ground between crops. Crops are often planted in a circle to conform to the center pivot. This type of system is known as LEPA (Low Energy Precision Application). Originally, most center pivots were water-powered. These were replaced by hydraulic systems (T-L Irrigation) and electric-motor-driven systems (Reinke, Valley, Zimmatic). Many modern pivots feature GPS devices.[29]

A series of pipes, each with a wheel of about 1.5 m diameter permanently affixed to its midpoint, and sprinklers along its length, are coupled together. Water is supplied at one end using a large hose. After sufficient irrigation has been applied to one strip of the field, the hose is removed, the water drained from the system, and the assembly rolled either by hand or with a purpose-built mechanism, so that the sprinklers are moved to a different position across the field. The hose is reconnected. The process is repeated in a pattern until the whole field has been irrigated.

This system is less expensive to install than a center pivot, but much more labor-intensive to operate – it does not travel automatically across the field: it applies water in a stationary strip, must be drained, and then rolled to a new strip. Most systems use 100 or 130 mm (4 or 5 inch) diameter aluminum pipe. The pipe doubles both as water transport and as an axle for rotating all the wheels. A drive system (often found near the centre of the wheel line) rotates the clamped-together pipe sections as a single axle, rolling the whole wheel line. Manual adjustment of individual wheel positions may be necessary if the system becomes misaligned.

Wheel line systems are limited in the amount of water they can carry, and limited in the height of crops that can be irrigated. One useful feature of a lateral move system is that it consists of sections that can be easily disconnected, adapting to field shape as the line is moved. They are most often used for small, rectilinear, or oddly-shaped fields, hilly or mountainous regions, or in regions where labor is inexpensive.[30][31]

A lawn sprinkler system is permanently installed, as opposed to a hose-end sprinkler, which is portable. Sprinkler systems are installed in residential lawns, in commercial landscapes, for churches and schools, in public parks and cemeteries, and on golf courses. Most of the components of these irrigation systems are hidden under ground, since aesthetics are important in a landscape. A typical lawn sprinkler system will consist of one or more zones, limited in size by the capacity of the water source. Each zone will cover a designated portion of the landscape. Sections of the landscape will usually be divided by microclimate, type of plant material, and type of irrigation equipment. A landscape irrigation system may also include zones containing drip irrigation, bubblers, or other types of equipment besides sprinklers.

Although manual systems are still used, most lawn sprinkler systems may be operated automatically using an irrigation controller, sometimes called a clock or timer. Most automatic systems employ electric solenoid valves. Each zone has one or more of these valves that are wired to the controller. When the controller sends power to the valve, the valve opens, allowing water to flow to the sprinklers in that zone.

There are two main types of sprinklers used in lawn irrigation, pop-up spray heads and rotors. Spray heads have a fixed spray pattern, while rotors have one or more streams that rotate. Spray heads are used to cover smaller areas, while rotors are used for larger areas. Golf course rotors are sometimes so large that a single sprinkler is combined with a valve and called a 'valve in head'. When used in a turf area, the sprinklers are installed with the top of the head flush with the ground surface. When the system is pressurized, the head will pop up out of the ground and water the desired area until the valve closes and shuts off that zone. Once there is no more pressure in the lateral line, the sprinkler head will retract back into the ground. In flower beds or shrub areas, sprinklers may be mounted on above ground risers or even taller pop-up sprinklers may be used and installed flush as in a lawn area.

Hose-end sprinklers are devices attached to the end of a garden hose, used for watering lawns, gardens, or plants. They come in a variety of designs and styles, allowing you to adjust the water flow, pattern, and range for efficient irrigation. Some common types of hose-end sprinklers include:

Oscillating Sprinklers: These spray water back and forth in a rectangular or square pattern. They are good for covering large, flat areas evenly.

Impact (or Pulsating) Sprinklers: These create a rotating, pulsating spray, which can cover a circular or semi-circular area. They are useful for watering large lawns.

Stationary Sprinklers: These have a fixed spray pattern and are best for smaller areas or gardens.

Rotary Sprinklers: These use spinning arms to distribute water in a circular or semi-circular pattern.

Traveling Sprinklers: These move along the hose path on their own, watering as they go, ideal for covering long, narrow spaces.

Each type offers different advantages based on garden size and shape, water pressure, and specific watering needs.

Subirrigation has been used for many years in field crops in areas with high water tables. It is a method of artificially raising the water table to allow the soil to be moistened from below the plants' root zone. Often those systems are located on permanent grasslands in lowlands or river valleys and combined with drainage infrastructure. A system of pumping stations, canals, weirs and gates allows it to increase or decrease the water level in a network of ditches and thereby control the water table.

Subirrigation is also used in the commercial greenhouse production, usually for potted plants. Water is delivered from below, absorbed by upwards, and the excess collected for recycling. Typically, a solution of water and nutrients floods a container or flows through a trough for a short period of time, 10–20 minutes, and is then pumped back into a holding tank for reuse. Sub-irrigation in greenhouses requires fairly sophisticated, expensive equipment and management. Advantages are water and nutrient conservation, and labor savings through reduced system maintenance and automation. It is similar in principle and action to subsurface basin irrigation.

Another type of subirrigation is the self-watering container, also known as a sub-irrigated planter. This consists of a planter suspended over a reservoir with some type of wicking material such as a polyester rope. The water is drawn up the wick through capillary action.[32][33] A similar technique is the wicking bed; this too uses capillary action.

Modern irrigation methods are efficient enough to supply the entire field uniformly with water, so that each plant has the amount of water it needs, neither too much nor too little.[34] Water use efficiency in the field can be determined as follows:

Increased irrigation efficiency has a number of positive outcomes for the farmer, the community and the wider environment. Low application efficiency infers that the amount of water applied to the field is in excess of the crop or field requirements. Increasing the application efficiency means that the amount of crop produced per unit of water increases. Improved efficiency may either be achieved by applying less water to an existing field or by using water more wisely thereby achieving higher yields in the same area of land. In some parts of the world, farmers are charged for irrigation water hence over-application has a direct financial cost to the farmer. Irrigation often requires pumping energy (either electricity or fossil fuel) to deliver water to the field or supply the correct operating pressure. Hence increased efficiency will reduce both the water cost and energy cost per unit of agricultural production. A reduction of water use on one field may mean that the farmer is able to irrigate a larger area of land, increasing total agricultural production. Low efficiency usually means that excess water is lost through seepage or runoff, both of which can result in loss of crop nutrients or pesticides with potential adverse impacts on the surrounding environment.

Improving the efficiency of irrigation is usually achieved in one of two ways, either by improving the system design or by optimising the irrigation management. Improving system design includes conversion from one form of irrigation to another (e.g. from furrow to drip irrigation) and also through small changes in the current system (for example changing flowrates and operating pressures). Irrigation management refers to the scheduling of irrigation events and decisions around how much water is applied.

Negative impacts frequently accompany extensive irrigation.[36] Some projects which diverted surface water for irrigation dried up the water sources, which led to a more extreme regional climate.[37] Projects that relied on groundwater and pumped too much from underground aquifers created subsidence and salinization. Salinization of irrigation water in turn damaged the crops and seeped into drinking water.[37] Pests and pathogens also thrived in the irrigation canals or ponds full of still water, which created regional outbreaks of diseases like malaria and schistosomiasis.[38][39][40] Governments also used irrigation schemes to encourage migration, especially of more desirable populations into an area.[41][42][43] Additionally, some of these large nationwide schemes failed to pay off at all, costing more than any benefit gained from increased crop yields.[44][45]

Overdrafting (depletion) of underground aquifers: In the mid-20th century, the advent of diesel and electric motors led to systems that could pump groundwater out of major aquifers faster than drainage basins could refill them. This can lead to permanent loss of aquifer capacity, decreased water quality, ground subsidence, and other problems. The future of food production in such areas as the North China Plain, the Punjab region in India and Pakistan, and the Great Plains of the US is threatened by this phenomenon.[46][47]

The environmental impact of irrigation relates to the changes in quantity and quality of soil and water as a result of irrigation and the subsequent effects on natural and social conditions in river basins and downstream of an irrigation scheme. The effects stem from the altered hydrological conditions caused by the installation and operation of the irrigation scheme.

Irrigation schemes involve solving numerous engineering and economic problems while minimizing negative environmental consequences.[36] Such problems include:

Archaeological investigation has found evidence of irrigation in areas lacking sufficient natural rainfall to support crops for rainfed agriculture. Some of the earliest known use of the technology dates to the 6th millennium BCE in Khuzistan in the south-west of Iran.[56][57] The site of Choga Mami, in present-day Iraq on the border with Iran, is believed to be the earliest to show the first canal irrigation in operation at about 6000 BCE.[58]

Irrigation was used as a means of manipulation of water in the alluvial plains of the Indus valley civilization, the application of which is estimated to have begun around 4500 BCE and drastically increased the size and prosperity of their agricultural settlements.[59] The Indus Valley Civilization developed sophisticated irrigation and water-storage systems, including artificial reservoirs at Girnar dated to 3000 BCE, and an early canal irrigation system from c. 2600 BCE. Large-scale agriculture was practiced, with an extensive network of canals used for the purpose of irrigation.[59][60]

Farmers in the Mesopotamian plain used irrigation from at least the third-millennium BCE.[61]
They developed perennial irrigation, regularly watering crops throughout the growing season by coaxing water through a matrix of small channels formed in the field.[62]
Ancient Egyptians practiced basin irrigation using the flooding of the Nile to inundate land plots which had been surrounded by dikes. The flood water remained until the fertile sediment had settled before the engineers returned the surplus to the watercourse.[63] There is evidence of the ancient Egyptian pharaoh Amenemhet III in the twelfth dynasty (about 1800 BCE) using the natural lake of the Faiyum Oasis as a reservoir to store surpluses of water for use during dry seasons. The lake swelled annually from the flooding of the Nile.[64]

The Ancient Nubians developed a form of irrigation by using a waterwheel-like device called a sakia. Irrigation began in Nubia between the third and second millennia BCE.[65] It largely depended upon the flood waters that would flow through the Nile River and other rivers in what is now the Sudan.[66]

In sub-Saharan Africa, irrigation reached the Niger River region cultures and civilizations by the first or second millennium BCE and was based on wet-season flooding and water harvesting.[67][68]

Evidence of terrace irrigation occurs in pre-Columbian America, early Syria, India, and China.[63] In the Zana Valley of the Andes Mountains in Peru, archaeologists have found remains of three irrigation canals radiocarbon-dated from the 4th millennium BCE, the 3rd millennium BCE and the 9th century CE. These canals provide the earliest record of irrigation in the New World. Traces of a canal possibly dating from the 5th millennium BCE were found under the 4th-millennium canal.[69]

Ancient Persia (modern-day Iran) used irrigation as far back as the 6th millennium BCE to grow barley in areas with insufficient natural rainfall.[70][56] The Qanats, developed in ancient Persia about 800 BCE, are among the oldest known irrigation methods still in use today. They are now found in Asia, the Middle East, and North Africa. The system comprises a network of vertical wells and gently sloping tunnels driven into the sides of cliffs and steep hills to tap groundwater.[71] The noria, a water wheel with clay pots around the rim powered by the flow of the stream (or by animals where the water source was still), first came into use at about this time among Roman settlers in North Africa. By 150 BCE, the pots were fitted with valves to allow smoother filling as they were forced into the water.[72]

The irrigation works of ancient Sri Lanka, the earliest dating from about 300 BCE in the reign of King Pandukabhaya, and under continuous development for the next thousand years, were one of the most complex irrigation systems of the ancient world. In addition to underground canals, the Sinhalese were the first to build completely artificial reservoirs to store water.[citation needed] These reservoirs and canal systems were used primarily to irrigate paddy fields, which require a lot of water to cultivate. Most of these irrigation systems still exist undamaged up to now, in Anuradhapura and Polonnaruwa, because of the advanced and precise engineering. The system was extensively restored and further extended during the reign of King Parakrama Bahu (1153–1186 CE).[73]

The oldest known hydraulic engineers of China were Sunshu Ao (6th century BCE) of the Spring and Autumn period and Ximen Bao (5th century BCE) of the Warring States period, both of whom worked on large irrigation projects. In the Sichuan region belonging to the state of Qin of ancient China, the Dujiangyan Irrigation System devised by the Qin Chinese hydrologist and irrigation engineer Li Bing was built in 256 BCE to irrigate a vast area of farmland that today still supplies water.[74] By the 2nd century CE, during the Han dynasty, the Chinese also used chain pumps which lifted water from a lower elevation to a higher one.[75] These were powered by manual foot-pedal, hydraulic waterwheels, or rotating mechanical wheels pulled by oxen.[76] The water was used for public works, providing water for urban residential quarters and palace gardens, but mostly for irrigation of farmland canals and channels in the fields.[77]

Korea, Jang Yeong-sil, a Korean engineer of the Joseon dynasty, under the active direction of the king, Sejong the Great, invented the world's first rain gauge, uryanggye (Korean: 우량계) in 1441. It was installed in irrigation tanks as part of a nationwide system to measure and collect rainfall for agricultural applications. Planners and farmers could better use the information gathered in the[which?] survey with this instrument.[78]

The earliest agricultural irrigation canal system known in the area of the present-day United States dates to between 1200 BCE and 800 BCE and was discovered by Desert Archaeology, Inc. in Marana, Arizona (adjacent to Tucson) in 2009.[79] The irrigation-canal system predates the Hohokam culture by two thousand years and belongs to an unidentified culture. In North America, the Hohokam were the only culture known to rely on irrigation canals to water their crops, and their irrigation systems supported the largest population in the Southwest by CE 1300. The Hohokam constructed various simple canals combined with weirs in their various agricultural pursuits. Between the 7th and 14th centuries, they built and maintained extensive irrigation networks along the lower Salt and middle Gila Rivers that rivaled the complexity of those used in the ancient Near East, Egypt, and China. These were constructed using relatively simple excavation tools, without the benefit of advanced engineering technologies, and achieved drops of a few feet per mile, balancing erosion and siltation. The Hohokam cultivated cotton, tobacco, maize, beans, and squash varieties and harvested an assortment of wild plants. Late in the Hohokam Chronological Sequence, they used extensive dry-farming systems, primarily to grow agave for food and fiber. Their reliance on agricultural strategies based on canal irrigation, vital in their less-than-hospitable desert environment and arid climate, provided the basis for the aggregation of rural populations into stable urban centers.[80]

The oldest known irrigation canals in the Americas are in the desert of northern Peru in the Zaña Valley near the hamlet of Nanchoc. The canals have been radiocarbon dated to at least 3400 BCE and possibly as old as 4700 BCE. The canals at that time irrigated crops such as peanuts, squash, manioc, chenopods, a relative of Quinoa, and later maize.[69]

The scale of global irrigation increased dramatically over the 20th century. In 1800, 8 million hectares were irrigated; in 1950, 94 million hectares, and in 1990, 235 million hectares. By 1990, 30% of the global food production came from irrigated land.[6] Irrigation techniques across the globe included canals redirecting surface water,[7][8] groundwater pumping, and diverting water from dams. National governments led most irrigation schemes within their borders, but private investors[9] and other nations,[8] especially the United States,[10] China,[11] and European countries like the United Kingdom,[12] funded and organized some schemes within other nations. Irrigation enabled the production of more crops, especially commodity crops in areas that otherwise could not support them. Countries frequently invested in irrigation to increase wheat, rice, or cotton production, often with the overarching goal of increasing self-sufficiency.[12] In the 20th century, global anxiety, specifically about the American cotton monopoly, fueled many empirical irrigation projects: Britain began developing irrigation in India, the Ottomans in Egypt, the French in Algeria, the Portuguese in Angola, the Germans in Togo, and Soviets in Central Asia.[8]

Negative impacts frequently accompany extensive irrigation. Some projects that diverted surface water for irrigation dried up the water sources, which led to a more extreme regional climate.[37] Projects that relied on groundwater and pumped too much from underground aquifers created subsidence and salinization. Salinization of irrigation water damaged the crops and seeped into drinking water.[37] Pests and pathogens also thrived in the irrigation canals or ponds full of still water, which created regional outbreaks of diseases like malaria and schistosomiasis.[38][39][40] Governments also used irrigation schemes to encourage migration, especially of more desirable populations into an area.[41][42][43] Additionally, some of these large nationwide schemes failed to pay off at all, costing more than any benefit gained from increased crop yields.[44][45]

Irrigated land in the United States increased from 300,000 acres in 1880 to 4.1 million in 1890 to 7.3 million in 1900.[45] Two thirds of this irrigation sources from groundwater or small ponds and reservoirs, while the other one third comes from large dams.[81] One of the main attractions of irrigation in the West was its increased dependability compared to rainfall-watered agriculture in the East. Proponents argued that farmers with a dependable water supply could more easily get loans from bankers interested in this more predictable farming model.[82] Most irrigation in the Great Plains region derived from underground aquifers. Euro-American farmers who colonized the region in the 19th century tried to grow the commodity crops that they were used to, like wheat, corn, and alfalfa, but rainfall stifled their growing capacity. Between the late 1800s and the 1930s, farmers used wind-powered pumps to draw groundwater. These windpumps had limited power, but the development of gas-powered pumps in the mid-1930s pushed wells deep into the Ogallala Aquifer. Farmers irrigated fields by laying pipes across the field with sprinklers at intervals, a labor-intensive process, until the advent of the center-pivot sprinkler after World War II, which made irrigation significantly easier.[83] By the 1970s farmers drained the aquifer ten times faster than it could recharge, and by 1993 they had removed half of the accessible water.[84]

Large-scale federal funding and intervention pushed through the majority of irrigation projects in the West, especially in California, Colorado, Arizona, and Nevada. At first, plans to increase irrigated farmland, largely by giving land to farmers and asking them to find water, failed across the board. Congress passed the Desert Land Act in 1877 and the Carey Act in 1894, which only marginally increased irrigation.[85] Only in 1902 did Congress pass the National Reclamation Act, which channeled money from the sale of western public lands, in parcels up to 160 acres large, into irrigation projects on public or private land in the arid West.[86] The Congressmen who passed the law and their wealthy supporters supported Western irrigation because it would increase American exports, 'reclaim' the West, and push the Eastern poor out West for a better life.[87]

While the National Reclamation Act was the most successful piece of federal irrigation legislation, the implementation of the act did not go as planned. The Reclamation Service chose to push most of the Act's money toward construction rather than settlement, so the Service overwhelmingly prioritized building large dams like the Hoover Dam.[88] Over the 20th century, Congress and state governments grew more frustrated with the Reclamation Service and the irrigation schemes. Frederick Newell, head of the Reclamation Service, proving uncompromising and challenging to work with, falling crop prices, resistance to delay debt payments, and refusal to begin new projects until the completion of old ones all contributed.[89] The Reclamation Extension Act of 1914, transferring a significant amount of irrigation decision-making power regarding irrigation projects from the Reclamation Service to Congress, was in many ways a result of increasing political unpopularity of the Reclamation Service.[90]

In the lower Colorado Basin of Arizona, Colorado, and Nevada, the states derive irrigation water largely from rivers, especially the Colorado River, which irrigates more than 4.5 million acres of land, with a less significant amount coming from groundwater.[91] In the 1952 case Arizona v. California, Arizona sued California for increased access to the Colorado River, under the grounds that their groundwater supply could not sustain their almost entirely irrigation-based agricultural economy, which they won.[92] California, which began irrigating in earnest in the 1870s in San Joaquin Valley,[93] had passed the Wright Act of 1887 permitting agricultural communities to construct and operate needed irrigation works.[94] The Colorado River also irrigates large fields in California's Imperial Valley, fed by the National Reclamation Act-built All-American Canal.[95][96]

When the Bolsheviks conquered Central Asia in 1917, the native Kazakhs, Uzbeks, and Turkmens used minimal irrigation. The Slavic immigrants pushed into the area by the Tsarist government[97] brought their irrigation methods, including waterwheels, the use of rice paddies to restore salted land, and underground irrigation channels. Russians dismissed these techniques as crude and inefficient. Despite this, tsarist officials maintained these systems through the late 19th century without other solutions.[98]

Before conquering the area, the Russian government accepted a 1911 American proposal to send hydraulic experts to Central Asia to investigate the potential for large-scale irrigation. A 1918 decree by Lenin then encouraged irrigation development in the region, which began in the 1930s. When it did, Stalin and other Soviet leaders prioritized large-scale, ambitious hydraulic projects, especially along the Volga River. The Soviet irrigation push stemmed mainly from their late 19th century fears of the American cotton monopoly and subsequent desire to achieve cotton self-sufficiency.[99] They had built up their textile manufacturing industry in the 19th century, requiring increased cotton and irrigation, as the region did not receive enough rainfall to support cotton farming.[98]

The Russians built dams on the Don and Kuban Rivers for irrigation, removing freshwater flow from the Sea of Azov and making it much saltier. Depletion and salinization scourged other areas of the Russian irrigation project. In the 1950s, Soviet officials began also diverting the Syr Darya and the Amu Darya, which fed the Aral Sea. Before diversion, the rivers delivered 55 cubic kilometres (13 cu mi) of water to the Aral Sea per year, but after, they only delivered 6 cubic kilometres (1.4 cu mi). Because of its reduced inflow, the Aral Sea covered less than half of its original seabed, which made the regional climate more extreme and created airborne salinization, lowering nearby crop yields.[100]

By 1975, the USSR used eight times as much water as they had in 1913, mostly for irrigation. Russia's expansion of irrigation began to decrease in the late 1980s, and irrigated hectares in Central Asia capped out at 7 million. Mikhail Gorbachev killed a proposed plan to reverse the Ob and Yenisei for irrigation in 1986, and the breakup of the USSR in 1991 ended Russian investment in Central Asian cotton irrigation.[101]

Different irrigation schemes with various goals and success rates have been implemented across Africa in the 20th century but have all been influenced by colonial forces. The Tana River Irrigation Scheme in eastern Kenya, completed between 1948 and 1963, opened up new lands for agriculture. The Kenyan government attempted to resettle the area with detainees from the Mau Mau uprising.[102] Italian oil drillers discovered Libya's underground water resources during the Italian colonization of Libya. This water lay dormant until 1969, when Muammar al-Gaddafi and American Armand Hammer built the Great Man-Made River to deliver the Saharan water to the coast. The water largely contributed to irrigation but cost four to ten times more than the crops it produced were worth.[103]

In 1912, the Union of South Africa created an irrigation department and began investing in water storage infrastructure and irrigation. The government used irrigation and dam-building to further social goals like poverty relief by creating construction jobs for poor whites and irrigation schemes to increase white farming. One of their first significant irrigation projects was the Hartbeespoort Dam, begun in 1916 to elevate the living conditions of the 'poor whites' in the region and eventually completed as a 'whites only' employment opportunity.[104] The Pretoria irrigation scheme, Kammanassie project, and Buchuberg irrigation scheme on the Orange River all followed in the same vein in the 1920s and 30s.[42]

In Egypt, modern irrigation began with Muhammad Ali Pasha in the mid-1800s, who sought to achieve Egyptian independence from the Ottomans through increased trade with Europe—specifically cotton exportation.[105] His administration proposed replacing the traditional Nile basin irrigation, which took advantage of the annual ebb and flow of the Nile, with irrigation barrages in the lower Nile, which better suited cotton production. Egypt devoted 105,000 ha to cotton in 1861, which increased fivefold by 1865. Most of their exports were shipped to England, and the United States Civil War-induced cotton scarcity in the 1860s cemented Egypt as England's cotton producer.[106] As the Egyptian economy became more dependent on cotton in the 20th century, controlling even small Nile floods became more important. Cotton production was more at risk of destruction than more common crops like barley or wheat.[107] After the British occupation of Egypt in 1882, the British intensified the conversion to perennial irrigation with the construction of the Delta Barrage, the Assiut Barrage, and the first Aswan Dam. Perennial irrigation decreased local control over water and made traditional subsistence farming or the farming of other crops incredibly difficult, eventually contributing to widespread peasant bankruptcy and the 1879-1882 'Urabi revolt.[108]

 This article incorporates text from a free content work. Licensed under CC BY-SA IGO 3.0 (license statement/permission). Text taken from World Food and Agriculture – Statistical Yearbook 2023​, FAO, FAO.  
"
Residential Drainage Systems,"Drainage is the natural or artificial removal of a surface's water and sub-surface water from an area with excess water. The internal drainage of most agricultural soils can prevent severe waterlogging (anaerobic conditions that harm root growth), but many soils need artificial drainage to improve production or to manage water supplies.

The Indus Valley Civilization had sewerage and drainage systems. All houses in the major cities of Harappa and Mohenjo-daro had access to water and drainage facilities. Waste water was directed to covered gravity sewers, which lined the major streets.[1]

The invention of hollow-pipe drainage is credited to Sir Hugh Dalrymple, who died in 1753.[2]

Simple infrastructure such as open drains, pipes, and berms are still common. In modern times, more complex structures involving substantial earthworks and new technologies have been common as well.

New storm water drainage systems incorporate geotextile filters that retain and prevent fine grains of soil from passing into and clogging the drain. Geotextiles are synthetic textile fabrics specially manufactured for civil and environmental engineering applications. Geotextiles are designed to retain fine soil particles while allowing water to pass through. In a typical drainage system, they would be laid along a trench which would then be filled with coarse granular material: gravel, sea shells, stone or rock. The geotextile is then folded over the top of the stone and the trench is then covered by soil. Groundwater seeps through the geotextile and flows through the stone to an outfell. In high groundwater conditions a perforated plastic (PVC or PE) pipe is laid along the base of the drain to increase the volume of water transported in the drain.

Alternatively, a prefabricated plastic drainage system made of HDPE, often incorporating geotextile, coco fiber or rag filters can be considered. The use of these materials has become increasingly more common due to their ease of use, since they eliminate the need for transporting and laying stone drainage aggregate, which is invariably more expensive than a synthetic drain and concrete liners.

Over the past 30 years, geotextile, PVC filters, and HDPE filters have become the most commonly used soil filter media. They are cheap to produce and easy to lay, with factory controlled properties that ensure long term filtration performance even in fine silty soil conditions.

Seattle's Public Utilities created a pilot program called Street Edge Alternatives Project. The project focuses on designing a system ""to provide drainage that more closely mimics the natural landscape prior to development than traditional piped systems"".[3]
The streets are characterized by ditches along the side of the roadway, with plantings designed throughout the area.
An emphasis on non-curbed sidewalks allows water to flow more freely into the areas of permeable surface on the side of the streets. Because of the plantings, the run off water from the urban area does not all directly go into the ground, but can also be absorbed into the surrounding environment.
Monitoring conducted by Seattle Public Utilities reports a 99 percent reduction of storm water leaving the drainage project.[3]

Drainage has undergone a large-scale environmental review in the recent past[when?] in the United Kingdom. Sustainable urban drainage systems (SUDS) are designed to encourage contractors to install drainage system that more closely mimic the natural flow of water in nature.  Since 2010 local and neighbourhood planning in the UK is required by law to factor SUDS into any development projects that they are responsible for.

Slot drainage is a channel drainage system designed to eliminate the need for further pipework systems to be installed in parallel to the drainage, reducing the environmental impact of production as well as improving water collection. Stainless steel, concrete channel, PVC and HDPE are all materials available for slot drainage which have become industry standards on construction projects.

The civil engineer is responsible for drainage in construction projects. During the construction process, they set out all the necessary levels for roads, street gutters, drainage, culverts and sewers involved in construction operations.

Civil engineers and construction managers work alongside architects and supervisors, planners, quantity surveyors, and the general workforce, as well as subcontractors. Typically, most jurisdictions have some body of drainage law to govern to what degree a landowner can alter the drainage from their parcel.

Drainage options for the construction industry include: 

The surface opening of channel drainage usually comes in the form of gratings (polymer, plastic, steel or iron) or a single slot (slot drain) that run along the ground surface (typically manufactured from steel or iron).

Earth retaining structures such as retaining walls also need to have groundwater drainage considered during their construction. Typical retaining walls are constructed of impermeable material, which can block the path of groundwater. When groundwater flow is obstructed, hydrostatic water pressure buildups against the wall and may cause significant damage. If the water pressure is not drained appropriately, retaining walls can bow, move, and fracture, causing seams to separate. The water pressure can also erode soil particles, leading to voids behind the wall and sinkholes in the above soil. Traditional retaining wall drainage systems can include French drains, drain pipes or weep holes. To prevent soil erosion, geotextile filter fabrics are installed with the drainage system.

Drainage in planters refers to the implementation of effective drainage systems specifically designed for plant containers or pots. Proper drainage is crucial in planters to prevent waterlogging and promote healthy plant growth. Planter Drainage involves the incorporation of drainage holes, drainage layers, or specialized drainage systems to ensure excess water can escape from the planter. This helps to prevent root rot, water accumulation, and other issues that can negatively impact plant health. By providing adequate drainage in planters, it supports optimal plant growth and contributes to the overall success of gardening or landscaping projects.[4]

Drainage options for the planter include: 

Wetland soils may need drainage to be used for agriculture. In the northern United States and Europe, glaciation created numerous small lakes, which gradually filled with humus to make marshes. Some of these were drained using open ditches and trenches to make mucklands, which are primarily used for high-value crops such as vegetables.

The world's largest project of this type has been in process for centuries in the Netherlands. The area between Amsterdam, Haarlem and Leiden was, in prehistoric times, swampland and small lakes. Turf cutting (peat mining), subsidence and shoreline erosion gradually caused the formation of one large lake, the Haarlemmermeer, or lake of Haarlem. The invention of wind-powered pumping engines in the 15th century permitted some of the marginal land drainage. Still, the final drainage of the lake had to await the design of large steam-powered pumps and agreements between regional authorities. The lake was eliminated between 1849 and 1852, creating thousands of km2 of new land.

Coastal plains and river deltas may have seasonally or permanently high water tables and must have drainage improvements if they are to be used for agriculture. An example is the flatwoods citrus-growing region of Florida, United States. After periods of high rainfall, drainage pumps are employed to prevent damage to the citrus groves from overly wet soils. Rice production requires complete water control, as fields must be flooded or drained at different stages of the crop cycle. The Netherlands has also led the way in this type of drainage by draining lowlands along the shore and pushing back the sea until the original nation has been greatly enlarged.

In moist climates, soils may be adequate for cropping with the exception that they become waterlogged for brief periods each year, from snow melt or from heavy rains. Soils that are predominantly clay will pass water very slowly downward. Meanwhile, plant roots suffocate because the excessive water around the roots eliminates air movement through the soil.

Other soils may have an impervious layer of mineralized soil, called a hardpan, or relatively impervious rock layers may underlie shallow soils. Drainage is especially important in tree fruit production. Soils that are otherwise excellent may be waterlogged for a week of the year, which is sufficient to kill fruit trees and cost the productivity of the land until replacements can be established. In each of these cases, appropriate drainage carries off temporary flushes of water to prevent damage to annual or perennial crops.

Drier areas are often farmed by irrigation, and one would not consider drainage necessary. However, irrigation water always contains minerals and salts, which can be concentrated to toxic levels by evapotranspiration. Irrigated land may need periodic flushes with excessive irrigation water and drainage to control soil salinity.

Otherwise:
"
Residential Snow Removal,"Snow removal or snow clearing is the job of removing snow after a snowfall to make travel easier and safer.  This is done both by individual households and by governments institutions, and commercial businesses.

De-icing is defined as removal of existing snow, ice or frost from a roadway, airport runway, roof, or other surface. It includes both mechanical means, such as plowing, vacuuming or scraping, and chemical means, such as application of salt or other ice-melting chemicals. Anti-icing is treatment with ice-melting chemicals before or during the onset of a storm in order to prevent or delay the formation and adhesion of ice and snow to the surface. Brine, or wetted salt, is usually applied shortly before the beginning of a snowstorm. When properly performed, anti-icing can significantly reduce the amount of salt required and allow easier removal by mechanical methods, including plowing.[1]

The de-icing of roads has historically been accomplished by snowplows or specially designed dump trucks that spread salt, often mixed with sand and gravel, onto slick roads. Rock salt is normally used because it is inexpensive and readily available in large quantities. However, brine freezes at −18 °C (0 °F), and so it is ineffective at these low temperatures. It also has a strong tendency to cause corrosion, rusting the steel used in most vehicles and the rebar in concrete bridges. More recent snowmelters use other salts, such as calcium chloride and magnesium chloride, which not only decrease the freezing point of water to a much lower temperature[2] but also produce an exothermic reaction, whose dissipated heat further aids in melting. In addition, they are somewhat safer for concrete sidewalks, but excess should still be removed.[disputed – discuss]

Recently, organic compounds have been developed that reduce the environmental impact associated with salts and that have longer residual effects when spread on roadways, usually in conjunction with salt brines or solids. These compounds are generated as byproducts of agricultural operations, such as sugar beet refining or ethanol distillation.[3] A mixture of some selection of these organic compounds with a combination of salts results in a substance that is both more easily spread and more effective at lower temperatures (−34 °C or −30 °F).[4]

Since the 1990s, use of liquid chemical melters has been increasing, sprayed on roads by nozzles instead of a spinning spreader used with salts. Liquid melters are more effective at preventing the ice from bonding to the surface than melting through existing ice.

Several proprietary products incorporate anti-icing chemicals into the pavement. Verglimit incorporates calcium chloride granules into asphalt pavement. The granules are continually exposed by traffic wear, and release calcium chloride onto the surface. This prevents snow and ice from sticking to the pavement [5]
Cargill SafeLane is a proprietary pavement surface treatment that absorbs anti-icing brines, to be released during a storm or other icing event. It also provides a high-friction surface, increasing traction.[6]

In Niigata, Japan, relatively inexpensive hot water bubbles up through holes in the pavement to melt snow, though this solution is only practical within a city or town. Some individual buildings may melt snow and ice with electric heating elements buried in the pavement, or even on a roof to prevent ice dams on the shingles, or to keep massive chunks of snow and dangerous icicles from collapsing on anyone below. Small areas of pavement can be kept ice-free by circulating heated liquids in embedded piping systems.

Most snow removal by individuals is clearance of driveways and walkways. After heavy snowfalls, snow may be removed from roofs to reduce the risk of structural damage due to the weight.

In places with light snow, brooms or other light instruments can be used to brush off snow from walks and other surfaces. In regions with more precipitation, snow is commonly removed with snow shovels, a large lightweight shovel used to push snow and lift it, and snow scoops or sleigh shovels, a large and deep hopper-like implement fitted with a wide handle and designed to scoop up a load of snow and slide it on any slippery surface to another location without lifting. Other tools include snow pushers and shovels with one or more wheels.

Compared to other tasks of equal importance, snow removal can present unusual age dynamics pertaining to overall community safety and efficient cooperation of skills. The relatively simple act of manual snow shoveling is statistically safer for teenagers than for adults, safer for boys than for men, and safer for children than for senior citizens.  Shovelling entails a considerable amount of physical effort and can strain the back and the heart for those who lack good physical health, have heart problems, or who are largely sedentary. Each year many seniors and middle aged persons die from heart attacks while shovelling snow.[7]

Snow blowers are often used by people unwilling or unable to perform this labour, people with large driveways or other substantial surfaces and people who live in areas with long lasting winters with large amounts of snowfall. Others may hire a contractor with a plow bearing truck or a shovel.[8] After a large snowfall, businessmen with plow trucks often drive through cities offering to plow for money.

Removing ice is more difficult. Snow blowers are usually ineffective at clearing ice. Picks are sometimes used, but a solid spade can break through most ice. There is always the risk of damaging the pavement with these instruments. Icy areas can be covered with salt or some other substance, bags of which are widely available.

A recent technological advance is the snowmelt system that heats the pavement from below and melts snow and ice after a period of time. Such systems are expensive to install and operate and they are not cost effective in areas with very low winter temperatures and large snowfalls.

Some governments, Boy Scout troops, and adult volunteer agencies offer free snow clearing for the elderly and others in need. In some cities, snow clearing for elder and handicapped residents counts towards community service hours assigned as a punishment for minor offences.[9]

In many places, laws require homeowners to clear snow from the public sidewalk in front of their house, as well as a pathway on their own property to their mailbox. Traditionally, this creates a unique opportunity for neighborhood youth to make legitimate money. Homeowners who fail to clear sidewalks, depending on the jurisdiction's laws, may experience fines and may be civilly liable for injuries suffered by another on a surface that they were required to clear. In some jurisdictions, such as New York, even private home owners who do clear their pathways can be held civilly liable for others' injuries incurred by falling in areas that have been cleared.[10]

Cleaning off and freeing one's vehicle is another matter. Some people who need their vehicles will only do barely what is necessary in order to drive the vehicle and remove it from its space. Failure to clear all the snow and ice from a vehicle causes hazards by impairing the driver's visibility, and ice from the roofs of driven vehicles can cause crashes.[11] In some jurisdictions, motorists who fail to clear snow from their vehicle entirely may be fined.[12] Others may be more thorough in this process.

In some urban residential areas with curbside parking, residents use objects to mark the spaces they dug out so they can reclaim their space upon their return.

A leaf blower is sometimes used to blow light powder snow from vehicles, driveways, walkways, and decks. This only works well for very light powder snow.

In some countries, keeping sidewalks clear and safe in winter is a duty of the owner of the contiguous land or building. The owner can be an individual inhabitant, in case of a family house, but also the municipality, municipal district or their specific organization or a housing co-operative or some other company (especially if some office or industrial object is concerned). Owners of large buildings or building complexes generally have mechanized snow-removal equipment, but individual house owners mostly clean the sidewalk with hand tools.

One example of the longstanding debate over the obligation of snow removal comes from the Czech Republic. In Prague, evidence of such a duty is documented since 1838.[13] The decree of the government of the Protectorate of Bohemia and Moravia No. 1/1943 Sb. said that sidewalk cleaning in residential areas of municipalities with more than 5000 inhabitants, of district cities and of other specified municipalities is a duty of the owner or user of the contiguous land. The municipality was empowered to undertake this duty at the expense of the contiguous land owner. The Czechoslovak Road Act No. 135/1961 Sb. (§23) adopted such legal regulations for all municipalities, but municipality offices could modify them. The new Road Act of the Czech Republic, No. 13/1997 Sb. (§9 art. 4) left this enactment in place and stated that maintenance of a road, path, track etc. is an obligation of its owner without any exception. Despite this, §27 art. 4 attached to the owner of contiguous land the liability for harm caused by defects of cleaning. In its 2002 and 2003 annual reports, the Czech Public Defender of Rights made a claim for there being a discrepancy between the theoretical and the practical interpretations of the act and recommended that an unequivocal formulation be enacted. This discrepancy was repeatedly handled by the courts, and the Supreme Administrative Court on 27 June 2005 and the Constitutional Court on 3 January 2007 each stated that the cleaning duty results indirectly from the stated liability for harm. Those who impugn the duty argued that it is a residue of feudal corvée or of the totalitarian Nazi and communist regimes, that nowadays, compulsory labour mandated by law is in conflict with the Charter of Fundamental Rights and Basic Freedoms and that systematic municipal cleaning is more effective than cleaning by individuals. On 6 December 2007, the Senate of the Czech Republic proposed at the instance of its Constitutional Committee to remove the controversial article from §27 of the Road Act of 1997. The Czech Government gave support to it by a narrow majority. In a previous vote and after heated debate, the Chamber of Deputies of the Czech Republic had sanctioned this change by a margin of 116 to 31 amongst the 190 members present. Since 16 April 2009, the changes made by Act No. 97/2009 Sb. mandate that sidewalk cleaning is an obligation only of the owner of the walkway or road, i.e. generally the municipality. Despite the abolition of the duty, many people, including its opponents, declared that they will continue the winter cleaning of municipal sidewalks and paths, but instead will do so voluntarily and on their own behalf.

As was mentioned during the discussion in the Czech Parliament in a statement by the Czech Association of Cities and Municipalities, a similar duty belongs to owners of contiguous land exists in many other modern countries, e.g. Austria, France, the United States and some cities in Bavaria.

Hiring a contractor with a winter service vehicle or a shovel.

In many high elevation or heavy snow accumulating areas, companies with snow removal equipment offer to provide services to remove the snow. Contractors may work on a per-time basis, full season contract, or will-call status. Per-time service (or per-push) is usually invoiced monthly and customers will be charged for each time services are provided.  Some companies will charge per-time and per-inch where the depth of the snow is even taken into account.  A full season contract is quoted and paid upfront at the start of the season and services will be provided automatically according to the contracted terms. Terms may sometimes differ between companies. For example, some full season contracts will expire after a certain amount of trips where others are unlimited.  And finally, will-call service is where the client makes contact with the snow removal company to initiate a single clearing.  This is not an automatic service and charges are usually higher for will-call jobs.

Snow removal services may include driveway and parking area snow removal, walkway and deck handwork, and roof clearing. Allowing snow to accumulate - especially on roofs where the weight of the snow may cause the structure to collapse - can be very dangerous.  Contractors use hand shovels, walk behind snowblowers (or snow throwers), truck plows, skid-steers, light-weight tractors, and heavy front-end loaders.  Many times, these machines will require use of tire chains to perform their tasks.  Snow may be pushed by plowing methods or blown to an area of the property by snowblowers.  Contractors may apply sand or salt in some locations to help melt ice accumulations.

Many snow removal contractors will require installation of snow poles or snow staking along the driveway.  This is to keep equipment out of the landscaping and to help identify the perimeter of an area.

Cities clear snow on a much larger scale than individuals.[14] Most cities in areas that get regular snowfall maintain a fleet of snow clearing vehicles. The first to be dispatched are gritters who do some plowing but also salt the road.  The salt, via freezing point depression, helps melt the snow and ice and also gives vehicles more traction. Later, usually when the snow has ceased falling, snow plows, front end loaders with snowplow attachments, and graders cover every street pushing snow to the side of the road. Salt trucks often then return to deal with any remaining ice and snow. The trucks generally travel much faster than the plows, averaging between 30 and 40 kilometers per hour.   Most cities thus have at least twice as many plows as trucks. Smaller narrow body plows, with Caterpillar tracks or huge snow tires salt and clear sidewalks in some cities, but in many others with less snowfall and/or less pedestrian traffic individuals are tasked with clearing the sidewalk in front of their homes. Ecological movements often oppose this use of salt because of the damage it does when it eventually washes off the roads and spreads to the environment in general. Credit for the concept that municipalities should remove snow from public roadways usually goes to Edward N. Hines, a celebrated early 20th century transportation thinker who also was the first to put a painted center line stripe on an automobile-era road.

In cities where snow steadily accumulates over the winter it is also necessary to remove the piles of snow that build up on the side of the roads known as windrows or snowbanks. There are a number of methods of doing this. Pulling snow is done when temperatures rise high enough for traffic to melt snow. The windrows are then broken up and spread over the road. Casting is the moving of snow by means of a shovel or plow to nearby public lands. On boulevards or highways winging back is done, which consists of pushing the snow banks further from the road. The most expensive option, but necessary when there are no nearby places to dump the snow, is to haul it away (known as the loading stage). This is most often done by large self-propelled snowblowers that gather the piles of snow at the side of the road and load it into dump trucks. The snow is then dumped on the outskirts of town, or in a nearby lake, river or harbor. (Some jurisdictions have banned dumping snow into local bodies of water for environmental reasons since the collected snow is contaminated with melting salt, motor oil, and other substances from the roads they were removed from.) Snow melting machines may be cheaper than moving snow, depending on the cost of fuel and the ambient temperature.[15]

The windrows created by the plows in residential areas often block driveways and imprison parked cars. The snow pushed there by any plow is a dense, packed version of ""normal"" fallen snow. When the temperatures are significantly below freezing this packed snow takes some of the characteristics of solid ice. Its removal is nearly impossible without mechanical means. Recently, windrows created in residential neighborhoods by city operated snow plows have resulted in the snow plow operators being assaulted by angry homeowners. 

The largest roads and highways are the first to be cleared; roads with steep hills or other dangers are also often a priority. Streets used by buses and other mass transit are also often given higher priorities. It often takes many hours, or even days, to cover every street in a city. In some places, a snow emergency will be declared, where automobile owners are instructed to remove their vehicles from the street (or one side of a street). If cars are in the way when the plows come around, they may be hauled away by tow trucks.  Some communities have standing snow emergency rules in winter, in which vehicles may not be parked on streets overnight, whether it snows or not. After smaller snow storms only main roads are cleared while residential ones are left to be melted by passing traffic. Decisions on immediate removal versus ""natural melting"" can be hard to make because the inconvenience to citizens and the economy in general must be weighed against the immediate effect on the snow removal budget at that particular moment in the season.

It is estimated that Canada spends $1 billion annually on snow removal.[16] In large cities with heavy snowfalls like Montreal and Ottawa, the snow clearing expense for each season is an important part of the seasonal public works budget and each snow storm provokes a major logistical operation involving thousands of employees working in shifts 24 hours a day. The effort can vary greatly depending on the amount of snow.  Montreal gets about 225 cm of snow each winter and spends more than $158 million Canadian (2013)[17] each year to remove it.  Toronto, with about 50 per cent more population and 28 per cent more road surface, gets only 125 cm of snow a year and spends about half that.[18] The higher cost in Montreal is due to the need to perform ""snow removal"" (the loading stage where snow is hauled away) consistently, necessitated by both the higher snowfall amounts and fewer melting days there, as opposed to simple ""snow clearing"" that usually suffices in other cities with less snowfall.

In Helsinki, Finland, the amount of snow transported from streets and properties to snow dump sites during the winter of 2009–2010 was 210,000 truckloads, equaling over 3 million cubic meters.[19]

Snow removal impacts the design of city infrastructure.  Where possible, street boulevards are wider to accommodate the windrows and sidewalks are not right next to the street.  Fire hydrants will have tall flags to locate them under the windrows.  Reflective traffic lane markers embedded in the roadbed is not possible (or much harder) due to risk of damage by plows.  Access to snow dumping locations (e.g. ravines) by heavy equipment is also planned.

The employees who take part in snow removal are generally the same workers who do road maintenance work during the summer months, but in some US cities garbage trucks are also equipped with plows and used for snow removal.  Many smaller US communities sign contracts with insurance companies, under which the insurance company assumes the risk of a heavy winter.  The insurance company of course sets the rates such that averaged over time they will make a profit; the town is willing to overpay for snow removal in mild winters in order to avoid the risk of running dramatically over budget in the occasional severe winter.

Large organizations such as universities and airports also often have their own mechanized snow clearing force.  Public transit systems generally clear bus stops while post offices clear around mail boxes. Railroads have their own snow clearing devices such as rotary snowplows.

Airports, with their associated runways, taxiways and ramp areas are an exception to the use of salt, as the metals used in aircraft construction will corrode causing safety issues.

A snow dump site is a location where snow is dumped as a part of the snow removal process. Designated sites are sometimes required to prevent water and ground pollution because the snow collected on roads typically contain a variety of grit, de-icing chemicals, vehicle fluids, engine emissions, and litter. Some pollutants become diluted and wash away with the melt-water and some concentrate at the dump site.[20] Dumping into fresh water is ""...almost universally prohibited due to the serious impact that deicer salts can have on freshwater aquatic life.""[20] In the United States dumping snow into water bodies is not specifically prohibited by the Clean Water Act or the Ocean Dumping Act. The states and local governments determine their own dumping policies.

Snow dump site selection is based on the availability of suitable land and pollution prevention factors that may include distances from lakes and streams, installation of silt fences, soil and aquifer type, and other factors. Some sites may use filters and settling ponds to help prevent the pollution from spreading. Occasionally an excessive amount of snow must be dumped and is sometimes allowed to be legally dumped into water bodies on an ""emergency only"" basis.

The surface is treated primarily by snow removal. Roads are also treated by spreading various materials on the surface. These materials generally fall into two categories: chemical and inert. Chemical (including salt) distribution induces freezing-point depression, causing ice and snow to melt at a lower temperature. Chemical treatment can be applied as a preventive measure and/or after snowfall. Inert materials (i.e. sand, brash, slag) make the surface irregular to improve traction. Both types can be applied together, but the inert materials tend to lower traction once the snow and ice has melted.

Chemical treatment materials include:

In the European Union, 98% of chemical treatment materials used in 2000 were sodium chloride in various forms. It is effective down to −5 °C, at the most −7 °C. For colder temperatures, calcium chloride (CaCl2) is added to NaCl in some countries, but deployment is limited as it costs about 6 times as much as sodium chloride. Other substances were used rarely and experimentally. Alternative substances (urea, alcohols, glycols) are often used at airports.[23]  In recent years, Geomelt, a combination of salt brine and beet juice that is otherwise considered a waste product has been used for pretreatment.[24] In Wisconsin, USA, surplus brine from cheese making has been used for this purpose.[25]

Inert spreadings can be:

The choice of treatment may include consideration of the effect on vegetation, pets and other animals, the local watershed, and effectiveness with regard to speed and temperature.  Some chemicals can degrade concrete, metals, and other materials.  The resulting meltwater and slush can cause frost heaving if it re-freezes, which can also damage pavement. Inert materials can damage vehicles and create dust.

As an example, in the Czech Republic during the winter season of 2000/2001, net material expenditure for road treatment was: 168 000 tonnes of salt (mostly NaCl), 348 000 tonnes of sand and crushed stone and 91 000 tonnes of other materials like slag. In Ireland, the annual expenditure of salt was 30 000 tonnes. Switzerland reports their annual expenditure as 600 grammes of salt to every square metre of roads on average.[23]

De-icing chemicals and inert materials need to be selected and applied with care.

Chemicals may react with infrastructure, the environment, and vehicles. Chlorides corrode steel and aluminum in reinforced concrete, structures and vehicles. Acetates can cause asphalt stripping, weakening the bond between asphalt binder and aggregate. Sand and grit can clog pavement joints and cracks, preventing pavement from expanding in the summer and increasing stress in the pavement.[26]

Salts can be toxic to plants and aquatic life, including the trees lining the side of the roads. Sand can alter aquatic habitats where roads are near  streams and lakes. Acetates can reduce oxygen levels in smaller water bodies, stressing aquatic animal life. Sand can be ground by tires into very fine particulate matter and become airborne, contributing to air pollution.[27][28]


"
Commercial Snow Removal,"Snow removal or snow clearing is the job of removing snow after a snowfall to make travel easier and safer.  This is done both by individual households and by governments institutions, and commercial businesses.

De-icing is defined as removal of existing snow, ice or frost from a roadway, airport runway, roof, or other surface. It includes both mechanical means, such as plowing, vacuuming or scraping, and chemical means, such as application of salt or other ice-melting chemicals. Anti-icing is treatment with ice-melting chemicals before or during the onset of a storm in order to prevent or delay the formation and adhesion of ice and snow to the surface. Brine, or wetted salt, is usually applied shortly before the beginning of a snowstorm. When properly performed, anti-icing can significantly reduce the amount of salt required and allow easier removal by mechanical methods, including plowing.[1]

The de-icing of roads has historically been accomplished by snowplows or specially designed dump trucks that spread salt, often mixed with sand and gravel, onto slick roads. Rock salt is normally used because it is inexpensive and readily available in large quantities. However, brine freezes at −18 °C (0 °F), and so it is ineffective at these low temperatures. It also has a strong tendency to cause corrosion, rusting the steel used in most vehicles and the rebar in concrete bridges. More recent snowmelters use other salts, such as calcium chloride and magnesium chloride, which not only decrease the freezing point of water to a much lower temperature[2] but also produce an exothermic reaction, whose dissipated heat further aids in melting. In addition, they are somewhat safer for concrete sidewalks, but excess should still be removed.[disputed – discuss]

Recently, organic compounds have been developed that reduce the environmental impact associated with salts and that have longer residual effects when spread on roadways, usually in conjunction with salt brines or solids. These compounds are generated as byproducts of agricultural operations, such as sugar beet refining or ethanol distillation.[3] A mixture of some selection of these organic compounds with a combination of salts results in a substance that is both more easily spread and more effective at lower temperatures (−34 °C or −30 °F).[4]

Since the 1990s, use of liquid chemical melters has been increasing, sprayed on roads by nozzles instead of a spinning spreader used with salts. Liquid melters are more effective at preventing the ice from bonding to the surface than melting through existing ice.

Several proprietary products incorporate anti-icing chemicals into the pavement. Verglimit incorporates calcium chloride granules into asphalt pavement. The granules are continually exposed by traffic wear, and release calcium chloride onto the surface. This prevents snow and ice from sticking to the pavement [5]
Cargill SafeLane is a proprietary pavement surface treatment that absorbs anti-icing brines, to be released during a storm or other icing event. It also provides a high-friction surface, increasing traction.[6]

In Niigata, Japan, relatively inexpensive hot water bubbles up through holes in the pavement to melt snow, though this solution is only practical within a city or town. Some individual buildings may melt snow and ice with electric heating elements buried in the pavement, or even on a roof to prevent ice dams on the shingles, or to keep massive chunks of snow and dangerous icicles from collapsing on anyone below. Small areas of pavement can be kept ice-free by circulating heated liquids in embedded piping systems.

Most snow removal by individuals is clearance of driveways and walkways. After heavy snowfalls, snow may be removed from roofs to reduce the risk of structural damage due to the weight.

In places with light snow, brooms or other light instruments can be used to brush off snow from walks and other surfaces. In regions with more precipitation, snow is commonly removed with snow shovels, a large lightweight shovel used to push snow and lift it, and snow scoops or sleigh shovels, a large and deep hopper-like implement fitted with a wide handle and designed to scoop up a load of snow and slide it on any slippery surface to another location without lifting. Other tools include snow pushers and shovels with one or more wheels.

Compared to other tasks of equal importance, snow removal can present unusual age dynamics pertaining to overall community safety and efficient cooperation of skills. The relatively simple act of manual snow shoveling is statistically safer for teenagers than for adults, safer for boys than for men, and safer for children than for senior citizens.  Shovelling entails a considerable amount of physical effort and can strain the back and the heart for those who lack good physical health, have heart problems, or who are largely sedentary. Each year many seniors and middle aged persons die from heart attacks while shovelling snow.[7]

Snow blowers are often used by people unwilling or unable to perform this labour, people with large driveways or other substantial surfaces and people who live in areas with long lasting winters with large amounts of snowfall. Others may hire a contractor with a plow bearing truck or a shovel.[8] After a large snowfall, businessmen with plow trucks often drive through cities offering to plow for money.

Removing ice is more difficult. Snow blowers are usually ineffective at clearing ice. Picks are sometimes used, but a solid spade can break through most ice. There is always the risk of damaging the pavement with these instruments. Icy areas can be covered with salt or some other substance, bags of which are widely available.

A recent technological advance is the snowmelt system that heats the pavement from below and melts snow and ice after a period of time. Such systems are expensive to install and operate and they are not cost effective in areas with very low winter temperatures and large snowfalls.

Some governments, Boy Scout troops, and adult volunteer agencies offer free snow clearing for the elderly and others in need. In some cities, snow clearing for elder and handicapped residents counts towards community service hours assigned as a punishment for minor offences.[9]

In many places, laws require homeowners to clear snow from the public sidewalk in front of their house, as well as a pathway on their own property to their mailbox. Traditionally, this creates a unique opportunity for neighborhood youth to make legitimate money. Homeowners who fail to clear sidewalks, depending on the jurisdiction's laws, may experience fines and may be civilly liable for injuries suffered by another on a surface that they were required to clear. In some jurisdictions, such as New York, even private home owners who do clear their pathways can be held civilly liable for others' injuries incurred by falling in areas that have been cleared.[10]

Cleaning off and freeing one's vehicle is another matter. Some people who need their vehicles will only do barely what is necessary in order to drive the vehicle and remove it from its space. Failure to clear all the snow and ice from a vehicle causes hazards by impairing the driver's visibility, and ice from the roofs of driven vehicles can cause crashes.[11] In some jurisdictions, motorists who fail to clear snow from their vehicle entirely may be fined.[12] Others may be more thorough in this process.

In some urban residential areas with curbside parking, residents use objects to mark the spaces they dug out so they can reclaim their space upon their return.

A leaf blower is sometimes used to blow light powder snow from vehicles, driveways, walkways, and decks. This only works well for very light powder snow.

In some countries, keeping sidewalks clear and safe in winter is a duty of the owner of the contiguous land or building. The owner can be an individual inhabitant, in case of a family house, but also the municipality, municipal district or their specific organization or a housing co-operative or some other company (especially if some office or industrial object is concerned). Owners of large buildings or building complexes generally have mechanized snow-removal equipment, but individual house owners mostly clean the sidewalk with hand tools.

One example of the longstanding debate over the obligation of snow removal comes from the Czech Republic. In Prague, evidence of such a duty is documented since 1838.[13] The decree of the government of the Protectorate of Bohemia and Moravia No. 1/1943 Sb. said that sidewalk cleaning in residential areas of municipalities with more than 5000 inhabitants, of district cities and of other specified municipalities is a duty of the owner or user of the contiguous land. The municipality was empowered to undertake this duty at the expense of the contiguous land owner. The Czechoslovak Road Act No. 135/1961 Sb. (§23) adopted such legal regulations for all municipalities, but municipality offices could modify them. The new Road Act of the Czech Republic, No. 13/1997 Sb. (§9 art. 4) left this enactment in place and stated that maintenance of a road, path, track etc. is an obligation of its owner without any exception. Despite this, §27 art. 4 attached to the owner of contiguous land the liability for harm caused by defects of cleaning. In its 2002 and 2003 annual reports, the Czech Public Defender of Rights made a claim for there being a discrepancy between the theoretical and the practical interpretations of the act and recommended that an unequivocal formulation be enacted. This discrepancy was repeatedly handled by the courts, and the Supreme Administrative Court on 27 June 2005 and the Constitutional Court on 3 January 2007 each stated that the cleaning duty results indirectly from the stated liability for harm. Those who impugn the duty argued that it is a residue of feudal corvée or of the totalitarian Nazi and communist regimes, that nowadays, compulsory labour mandated by law is in conflict with the Charter of Fundamental Rights and Basic Freedoms and that systematic municipal cleaning is more effective than cleaning by individuals. On 6 December 2007, the Senate of the Czech Republic proposed at the instance of its Constitutional Committee to remove the controversial article from §27 of the Road Act of 1997. The Czech Government gave support to it by a narrow majority. In a previous vote and after heated debate, the Chamber of Deputies of the Czech Republic had sanctioned this change by a margin of 116 to 31 amongst the 190 members present. Since 16 April 2009, the changes made by Act No. 97/2009 Sb. mandate that sidewalk cleaning is an obligation only of the owner of the walkway or road, i.e. generally the municipality. Despite the abolition of the duty, many people, including its opponents, declared that they will continue the winter cleaning of municipal sidewalks and paths, but instead will do so voluntarily and on their own behalf.

As was mentioned during the discussion in the Czech Parliament in a statement by the Czech Association of Cities and Municipalities, a similar duty belongs to owners of contiguous land exists in many other modern countries, e.g. Austria, France, the United States and some cities in Bavaria.

Hiring a contractor with a winter service vehicle or a shovel.

In many high elevation or heavy snow accumulating areas, companies with snow removal equipment offer to provide services to remove the snow. Contractors may work on a per-time basis, full season contract, or will-call status. Per-time service (or per-push) is usually invoiced monthly and customers will be charged for each time services are provided.  Some companies will charge per-time and per-inch where the depth of the snow is even taken into account.  A full season contract is quoted and paid upfront at the start of the season and services will be provided automatically according to the contracted terms. Terms may sometimes differ between companies. For example, some full season contracts will expire after a certain amount of trips where others are unlimited.  And finally, will-call service is where the client makes contact with the snow removal company to initiate a single clearing.  This is not an automatic service and charges are usually higher for will-call jobs.

Snow removal services may include driveway and parking area snow removal, walkway and deck handwork, and roof clearing. Allowing snow to accumulate - especially on roofs where the weight of the snow may cause the structure to collapse - can be very dangerous.  Contractors use hand shovels, walk behind snowblowers (or snow throwers), truck plows, skid-steers, light-weight tractors, and heavy front-end loaders.  Many times, these machines will require use of tire chains to perform their tasks.  Snow may be pushed by plowing methods or blown to an area of the property by snowblowers.  Contractors may apply sand or salt in some locations to help melt ice accumulations.

Many snow removal contractors will require installation of snow poles or snow staking along the driveway.  This is to keep equipment out of the landscaping and to help identify the perimeter of an area.

Cities clear snow on a much larger scale than individuals.[14] Most cities in areas that get regular snowfall maintain a fleet of snow clearing vehicles. The first to be dispatched are gritters who do some plowing but also salt the road.  The salt, via freezing point depression, helps melt the snow and ice and also gives vehicles more traction. Later, usually when the snow has ceased falling, snow plows, front end loaders with snowplow attachments, and graders cover every street pushing snow to the side of the road. Salt trucks often then return to deal with any remaining ice and snow. The trucks generally travel much faster than the plows, averaging between 30 and 40 kilometers per hour.   Most cities thus have at least twice as many plows as trucks. Smaller narrow body plows, with Caterpillar tracks or huge snow tires salt and clear sidewalks in some cities, but in many others with less snowfall and/or less pedestrian traffic individuals are tasked with clearing the sidewalk in front of their homes. Ecological movements often oppose this use of salt because of the damage it does when it eventually washes off the roads and spreads to the environment in general. Credit for the concept that municipalities should remove snow from public roadways usually goes to Edward N. Hines, a celebrated early 20th century transportation thinker who also was the first to put a painted center line stripe on an automobile-era road.

In cities where snow steadily accumulates over the winter it is also necessary to remove the piles of snow that build up on the side of the roads known as windrows or snowbanks. There are a number of methods of doing this. Pulling snow is done when temperatures rise high enough for traffic to melt snow. The windrows are then broken up and spread over the road. Casting is the moving of snow by means of a shovel or plow to nearby public lands. On boulevards or highways winging back is done, which consists of pushing the snow banks further from the road. The most expensive option, but necessary when there are no nearby places to dump the snow, is to haul it away (known as the loading stage). This is most often done by large self-propelled snowblowers that gather the piles of snow at the side of the road and load it into dump trucks. The snow is then dumped on the outskirts of town, or in a nearby lake, river or harbor. (Some jurisdictions have banned dumping snow into local bodies of water for environmental reasons since the collected snow is contaminated with melting salt, motor oil, and other substances from the roads they were removed from.) Snow melting machines may be cheaper than moving snow, depending on the cost of fuel and the ambient temperature.[15]

The windrows created by the plows in residential areas often block driveways and imprison parked cars. The snow pushed there by any plow is a dense, packed version of ""normal"" fallen snow. When the temperatures are significantly below freezing this packed snow takes some of the characteristics of solid ice. Its removal is nearly impossible without mechanical means. Recently, windrows created in residential neighborhoods by city operated snow plows have resulted in the snow plow operators being assaulted by angry homeowners. 

The largest roads and highways are the first to be cleared; roads with steep hills or other dangers are also often a priority. Streets used by buses and other mass transit are also often given higher priorities. It often takes many hours, or even days, to cover every street in a city. In some places, a snow emergency will be declared, where automobile owners are instructed to remove their vehicles from the street (or one side of a street). If cars are in the way when the plows come around, they may be hauled away by tow trucks.  Some communities have standing snow emergency rules in winter, in which vehicles may not be parked on streets overnight, whether it snows or not. After smaller snow storms only main roads are cleared while residential ones are left to be melted by passing traffic. Decisions on immediate removal versus ""natural melting"" can be hard to make because the inconvenience to citizens and the economy in general must be weighed against the immediate effect on the snow removal budget at that particular moment in the season.

It is estimated that Canada spends $1 billion annually on snow removal.[16] In large cities with heavy snowfalls like Montreal and Ottawa, the snow clearing expense for each season is an important part of the seasonal public works budget and each snow storm provokes a major logistical operation involving thousands of employees working in shifts 24 hours a day. The effort can vary greatly depending on the amount of snow.  Montreal gets about 225 cm of snow each winter and spends more than $158 million Canadian (2013)[17] each year to remove it.  Toronto, with about 50 per cent more population and 28 per cent more road surface, gets only 125 cm of snow a year and spends about half that.[18] The higher cost in Montreal is due to the need to perform ""snow removal"" (the loading stage where snow is hauled away) consistently, necessitated by both the higher snowfall amounts and fewer melting days there, as opposed to simple ""snow clearing"" that usually suffices in other cities with less snowfall.

In Helsinki, Finland, the amount of snow transported from streets and properties to snow dump sites during the winter of 2009–2010 was 210,000 truckloads, equaling over 3 million cubic meters.[19]

Snow removal impacts the design of city infrastructure.  Where possible, street boulevards are wider to accommodate the windrows and sidewalks are not right next to the street.  Fire hydrants will have tall flags to locate them under the windrows.  Reflective traffic lane markers embedded in the roadbed is not possible (or much harder) due to risk of damage by plows.  Access to snow dumping locations (e.g. ravines) by heavy equipment is also planned.

The employees who take part in snow removal are generally the same workers who do road maintenance work during the summer months, but in some US cities garbage trucks are also equipped with plows and used for snow removal.  Many smaller US communities sign contracts with insurance companies, under which the insurance company assumes the risk of a heavy winter.  The insurance company of course sets the rates such that averaged over time they will make a profit; the town is willing to overpay for snow removal in mild winters in order to avoid the risk of running dramatically over budget in the occasional severe winter.

Large organizations such as universities and airports also often have their own mechanized snow clearing force.  Public transit systems generally clear bus stops while post offices clear around mail boxes. Railroads have their own snow clearing devices such as rotary snowplows.

Airports, with their associated runways, taxiways and ramp areas are an exception to the use of salt, as the metals used in aircraft construction will corrode causing safety issues.

A snow dump site is a location where snow is dumped as a part of the snow removal process. Designated sites are sometimes required to prevent water and ground pollution because the snow collected on roads typically contain a variety of grit, de-icing chemicals, vehicle fluids, engine emissions, and litter. Some pollutants become diluted and wash away with the melt-water and some concentrate at the dump site.[20] Dumping into fresh water is ""...almost universally prohibited due to the serious impact that deicer salts can have on freshwater aquatic life.""[20] In the United States dumping snow into water bodies is not specifically prohibited by the Clean Water Act or the Ocean Dumping Act. The states and local governments determine their own dumping policies.

Snow dump site selection is based on the availability of suitable land and pollution prevention factors that may include distances from lakes and streams, installation of silt fences, soil and aquifer type, and other factors. Some sites may use filters and settling ponds to help prevent the pollution from spreading. Occasionally an excessive amount of snow must be dumped and is sometimes allowed to be legally dumped into water bodies on an ""emergency only"" basis.

The surface is treated primarily by snow removal. Roads are also treated by spreading various materials on the surface. These materials generally fall into two categories: chemical and inert. Chemical (including salt) distribution induces freezing-point depression, causing ice and snow to melt at a lower temperature. Chemical treatment can be applied as a preventive measure and/or after snowfall. Inert materials (i.e. sand, brash, slag) make the surface irregular to improve traction. Both types can be applied together, but the inert materials tend to lower traction once the snow and ice has melted.

Chemical treatment materials include:

In the European Union, 98% of chemical treatment materials used in 2000 were sodium chloride in various forms. It is effective down to −5 °C, at the most −7 °C. For colder temperatures, calcium chloride (CaCl2) is added to NaCl in some countries, but deployment is limited as it costs about 6 times as much as sodium chloride. Other substances were used rarely and experimentally. Alternative substances (urea, alcohols, glycols) are often used at airports.[23]  In recent years, Geomelt, a combination of salt brine and beet juice that is otherwise considered a waste product has been used for pretreatment.[24] In Wisconsin, USA, surplus brine from cheese making has been used for this purpose.[25]

Inert spreadings can be:

The choice of treatment may include consideration of the effect on vegetation, pets and other animals, the local watershed, and effectiveness with regard to speed and temperature.  Some chemicals can degrade concrete, metals, and other materials.  The resulting meltwater and slush can cause frost heaving if it re-freezes, which can also damage pavement. Inert materials can damage vehicles and create dust.

As an example, in the Czech Republic during the winter season of 2000/2001, net material expenditure for road treatment was: 168 000 tonnes of salt (mostly NaCl), 348 000 tonnes of sand and crushed stone and 91 000 tonnes of other materials like slag. In Ireland, the annual expenditure of salt was 30 000 tonnes. Switzerland reports their annual expenditure as 600 grammes of salt to every square metre of roads on average.[23]

De-icing chemicals and inert materials need to be selected and applied with care.

Chemicals may react with infrastructure, the environment, and vehicles. Chlorides corrode steel and aluminum in reinforced concrete, structures and vehicles. Acetates can cause asphalt stripping, weakening the bond between asphalt binder and aggregate. Sand and grit can clog pavement joints and cracks, preventing pavement from expanding in the summer and increasing stress in the pavement.[26]

Salts can be toxic to plants and aquatic life, including the trees lining the side of the roads. Sand can alter aquatic habitats where roads are near  streams and lakes. Acetates can reduce oxygen levels in smaller water bodies, stressing aquatic animal life. Sand can be ground by tires into very fine particulate matter and become airborne, contributing to air pollution.[27][28]


"
General Snow Removal Services,"Snow removal or snow clearing is the job of removing snow after a snowfall to make travel easier and safer.  This is done both by individual households and by governments institutions, and commercial businesses.

De-icing is defined as removal of existing snow, ice or frost from a roadway, airport runway, roof, or other surface. It includes both mechanical means, such as plowing, vacuuming or scraping, and chemical means, such as application of salt or other ice-melting chemicals. Anti-icing is treatment with ice-melting chemicals before or during the onset of a storm in order to prevent or delay the formation and adhesion of ice and snow to the surface. Brine, or wetted salt, is usually applied shortly before the beginning of a snowstorm. When properly performed, anti-icing can significantly reduce the amount of salt required and allow easier removal by mechanical methods, including plowing.[1]

The de-icing of roads has historically been accomplished by snowplows or specially designed dump trucks that spread salt, often mixed with sand and gravel, onto slick roads. Rock salt is normally used because it is inexpensive and readily available in large quantities. However, brine freezes at −18 °C (0 °F), and so it is ineffective at these low temperatures. It also has a strong tendency to cause corrosion, rusting the steel used in most vehicles and the rebar in concrete bridges. More recent snowmelters use other salts, such as calcium chloride and magnesium chloride, which not only decrease the freezing point of water to a much lower temperature[2] but also produce an exothermic reaction, whose dissipated heat further aids in melting. In addition, they are somewhat safer for concrete sidewalks, but excess should still be removed.[disputed – discuss]

Recently, organic compounds have been developed that reduce the environmental impact associated with salts and that have longer residual effects when spread on roadways, usually in conjunction with salt brines or solids. These compounds are generated as byproducts of agricultural operations, such as sugar beet refining or ethanol distillation.[3] A mixture of some selection of these organic compounds with a combination of salts results in a substance that is both more easily spread and more effective at lower temperatures (−34 °C or −30 °F).[4]

Since the 1990s, use of liquid chemical melters has been increasing, sprayed on roads by nozzles instead of a spinning spreader used with salts. Liquid melters are more effective at preventing the ice from bonding to the surface than melting through existing ice.

Several proprietary products incorporate anti-icing chemicals into the pavement. Verglimit incorporates calcium chloride granules into asphalt pavement. The granules are continually exposed by traffic wear, and release calcium chloride onto the surface. This prevents snow and ice from sticking to the pavement [5]
Cargill SafeLane is a proprietary pavement surface treatment that absorbs anti-icing brines, to be released during a storm or other icing event. It also provides a high-friction surface, increasing traction.[6]

In Niigata, Japan, relatively inexpensive hot water bubbles up through holes in the pavement to melt snow, though this solution is only practical within a city or town. Some individual buildings may melt snow and ice with electric heating elements buried in the pavement, or even on a roof to prevent ice dams on the shingles, or to keep massive chunks of snow and dangerous icicles from collapsing on anyone below. Small areas of pavement can be kept ice-free by circulating heated liquids in embedded piping systems.

Most snow removal by individuals is clearance of driveways and walkways. After heavy snowfalls, snow may be removed from roofs to reduce the risk of structural damage due to the weight.

In places with light snow, brooms or other light instruments can be used to brush off snow from walks and other surfaces. In regions with more precipitation, snow is commonly removed with snow shovels, a large lightweight shovel used to push snow and lift it, and snow scoops or sleigh shovels, a large and deep hopper-like implement fitted with a wide handle and designed to scoop up a load of snow and slide it on any slippery surface to another location without lifting. Other tools include snow pushers and shovels with one or more wheels.

Compared to other tasks of equal importance, snow removal can present unusual age dynamics pertaining to overall community safety and efficient cooperation of skills. The relatively simple act of manual snow shoveling is statistically safer for teenagers than for adults, safer for boys than for men, and safer for children than for senior citizens.  Shovelling entails a considerable amount of physical effort and can strain the back and the heart for those who lack good physical health, have heart problems, or who are largely sedentary. Each year many seniors and middle aged persons die from heart attacks while shovelling snow.[7]

Snow blowers are often used by people unwilling or unable to perform this labour, people with large driveways or other substantial surfaces and people who live in areas with long lasting winters with large amounts of snowfall. Others may hire a contractor with a plow bearing truck or a shovel.[8] After a large snowfall, businessmen with plow trucks often drive through cities offering to plow for money.

Removing ice is more difficult. Snow blowers are usually ineffective at clearing ice. Picks are sometimes used, but a solid spade can break through most ice. There is always the risk of damaging the pavement with these instruments. Icy areas can be covered with salt or some other substance, bags of which are widely available.

A recent technological advance is the snowmelt system that heats the pavement from below and melts snow and ice after a period of time. Such systems are expensive to install and operate and they are not cost effective in areas with very low winter temperatures and large snowfalls.

Some governments, Boy Scout troops, and adult volunteer agencies offer free snow clearing for the elderly and others in need. In some cities, snow clearing for elder and handicapped residents counts towards community service hours assigned as a punishment for minor offences.[9]

In many places, laws require homeowners to clear snow from the public sidewalk in front of their house, as well as a pathway on their own property to their mailbox. Traditionally, this creates a unique opportunity for neighborhood youth to make legitimate money. Homeowners who fail to clear sidewalks, depending on the jurisdiction's laws, may experience fines and may be civilly liable for injuries suffered by another on a surface that they were required to clear. In some jurisdictions, such as New York, even private home owners who do clear their pathways can be held civilly liable for others' injuries incurred by falling in areas that have been cleared.[10]

Cleaning off and freeing one's vehicle is another matter. Some people who need their vehicles will only do barely what is necessary in order to drive the vehicle and remove it from its space. Failure to clear all the snow and ice from a vehicle causes hazards by impairing the driver's visibility, and ice from the roofs of driven vehicles can cause crashes.[11] In some jurisdictions, motorists who fail to clear snow from their vehicle entirely may be fined.[12] Others may be more thorough in this process.

In some urban residential areas with curbside parking, residents use objects to mark the spaces they dug out so they can reclaim their space upon their return.

A leaf blower is sometimes used to blow light powder snow from vehicles, driveways, walkways, and decks. This only works well for very light powder snow.

In some countries, keeping sidewalks clear and safe in winter is a duty of the owner of the contiguous land or building. The owner can be an individual inhabitant, in case of a family house, but also the municipality, municipal district or their specific organization or a housing co-operative or some other company (especially if some office or industrial object is concerned). Owners of large buildings or building complexes generally have mechanized snow-removal equipment, but individual house owners mostly clean the sidewalk with hand tools.

One example of the longstanding debate over the obligation of snow removal comes from the Czech Republic. In Prague, evidence of such a duty is documented since 1838.[13] The decree of the government of the Protectorate of Bohemia and Moravia No. 1/1943 Sb. said that sidewalk cleaning in residential areas of municipalities with more than 5000 inhabitants, of district cities and of other specified municipalities is a duty of the owner or user of the contiguous land. The municipality was empowered to undertake this duty at the expense of the contiguous land owner. The Czechoslovak Road Act No. 135/1961 Sb. (§23) adopted such legal regulations for all municipalities, but municipality offices could modify them. The new Road Act of the Czech Republic, No. 13/1997 Sb. (§9 art. 4) left this enactment in place and stated that maintenance of a road, path, track etc. is an obligation of its owner without any exception. Despite this, §27 art. 4 attached to the owner of contiguous land the liability for harm caused by defects of cleaning. In its 2002 and 2003 annual reports, the Czech Public Defender of Rights made a claim for there being a discrepancy between the theoretical and the practical interpretations of the act and recommended that an unequivocal formulation be enacted. This discrepancy was repeatedly handled by the courts, and the Supreme Administrative Court on 27 June 2005 and the Constitutional Court on 3 January 2007 each stated that the cleaning duty results indirectly from the stated liability for harm. Those who impugn the duty argued that it is a residue of feudal corvée or of the totalitarian Nazi and communist regimes, that nowadays, compulsory labour mandated by law is in conflict with the Charter of Fundamental Rights and Basic Freedoms and that systematic municipal cleaning is more effective than cleaning by individuals. On 6 December 2007, the Senate of the Czech Republic proposed at the instance of its Constitutional Committee to remove the controversial article from §27 of the Road Act of 1997. The Czech Government gave support to it by a narrow majority. In a previous vote and after heated debate, the Chamber of Deputies of the Czech Republic had sanctioned this change by a margin of 116 to 31 amongst the 190 members present. Since 16 April 2009, the changes made by Act No. 97/2009 Sb. mandate that sidewalk cleaning is an obligation only of the owner of the walkway or road, i.e. generally the municipality. Despite the abolition of the duty, many people, including its opponents, declared that they will continue the winter cleaning of municipal sidewalks and paths, but instead will do so voluntarily and on their own behalf.

As was mentioned during the discussion in the Czech Parliament in a statement by the Czech Association of Cities and Municipalities, a similar duty belongs to owners of contiguous land exists in many other modern countries, e.g. Austria, France, the United States and some cities in Bavaria.

Hiring a contractor with a winter service vehicle or a shovel.

In many high elevation or heavy snow accumulating areas, companies with snow removal equipment offer to provide services to remove the snow. Contractors may work on a per-time basis, full season contract, or will-call status. Per-time service (or per-push) is usually invoiced monthly and customers will be charged for each time services are provided.  Some companies will charge per-time and per-inch where the depth of the snow is even taken into account.  A full season contract is quoted and paid upfront at the start of the season and services will be provided automatically according to the contracted terms. Terms may sometimes differ between companies. For example, some full season contracts will expire after a certain amount of trips where others are unlimited.  And finally, will-call service is where the client makes contact with the snow removal company to initiate a single clearing.  This is not an automatic service and charges are usually higher for will-call jobs.

Snow removal services may include driveway and parking area snow removal, walkway and deck handwork, and roof clearing. Allowing snow to accumulate - especially on roofs where the weight of the snow may cause the structure to collapse - can be very dangerous.  Contractors use hand shovels, walk behind snowblowers (or snow throwers), truck plows, skid-steers, light-weight tractors, and heavy front-end loaders.  Many times, these machines will require use of tire chains to perform their tasks.  Snow may be pushed by plowing methods or blown to an area of the property by snowblowers.  Contractors may apply sand or salt in some locations to help melt ice accumulations.

Many snow removal contractors will require installation of snow poles or snow staking along the driveway.  This is to keep equipment out of the landscaping and to help identify the perimeter of an area.

Cities clear snow on a much larger scale than individuals.[14] Most cities in areas that get regular snowfall maintain a fleet of snow clearing vehicles. The first to be dispatched are gritters who do some plowing but also salt the road.  The salt, via freezing point depression, helps melt the snow and ice and also gives vehicles more traction. Later, usually when the snow has ceased falling, snow plows, front end loaders with snowplow attachments, and graders cover every street pushing snow to the side of the road. Salt trucks often then return to deal with any remaining ice and snow. The trucks generally travel much faster than the plows, averaging between 30 and 40 kilometers per hour.   Most cities thus have at least twice as many plows as trucks. Smaller narrow body plows, with Caterpillar tracks or huge snow tires salt and clear sidewalks in some cities, but in many others with less snowfall and/or less pedestrian traffic individuals are tasked with clearing the sidewalk in front of their homes. Ecological movements often oppose this use of salt because of the damage it does when it eventually washes off the roads and spreads to the environment in general. Credit for the concept that municipalities should remove snow from public roadways usually goes to Edward N. Hines, a celebrated early 20th century transportation thinker who also was the first to put a painted center line stripe on an automobile-era road.

In cities where snow steadily accumulates over the winter it is also necessary to remove the piles of snow that build up on the side of the roads known as windrows or snowbanks. There are a number of methods of doing this. Pulling snow is done when temperatures rise high enough for traffic to melt snow. The windrows are then broken up and spread over the road. Casting is the moving of snow by means of a shovel or plow to nearby public lands. On boulevards or highways winging back is done, which consists of pushing the snow banks further from the road. The most expensive option, but necessary when there are no nearby places to dump the snow, is to haul it away (known as the loading stage). This is most often done by large self-propelled snowblowers that gather the piles of snow at the side of the road and load it into dump trucks. The snow is then dumped on the outskirts of town, or in a nearby lake, river or harbor. (Some jurisdictions have banned dumping snow into local bodies of water for environmental reasons since the collected snow is contaminated with melting salt, motor oil, and other substances from the roads they were removed from.) Snow melting machines may be cheaper than moving snow, depending on the cost of fuel and the ambient temperature.[15]

The windrows created by the plows in residential areas often block driveways and imprison parked cars. The snow pushed there by any plow is a dense, packed version of ""normal"" fallen snow. When the temperatures are significantly below freezing this packed snow takes some of the characteristics of solid ice. Its removal is nearly impossible without mechanical means. Recently, windrows created in residential neighborhoods by city operated snow plows have resulted in the snow plow operators being assaulted by angry homeowners. 

The largest roads and highways are the first to be cleared; roads with steep hills or other dangers are also often a priority. Streets used by buses and other mass transit are also often given higher priorities. It often takes many hours, or even days, to cover every street in a city. In some places, a snow emergency will be declared, where automobile owners are instructed to remove their vehicles from the street (or one side of a street). If cars are in the way when the plows come around, they may be hauled away by tow trucks.  Some communities have standing snow emergency rules in winter, in which vehicles may not be parked on streets overnight, whether it snows or not. After smaller snow storms only main roads are cleared while residential ones are left to be melted by passing traffic. Decisions on immediate removal versus ""natural melting"" can be hard to make because the inconvenience to citizens and the economy in general must be weighed against the immediate effect on the snow removal budget at that particular moment in the season.

It is estimated that Canada spends $1 billion annually on snow removal.[16] In large cities with heavy snowfalls like Montreal and Ottawa, the snow clearing expense for each season is an important part of the seasonal public works budget and each snow storm provokes a major logistical operation involving thousands of employees working in shifts 24 hours a day. The effort can vary greatly depending on the amount of snow.  Montreal gets about 225 cm of snow each winter and spends more than $158 million Canadian (2013)[17] each year to remove it.  Toronto, with about 50 per cent more population and 28 per cent more road surface, gets only 125 cm of snow a year and spends about half that.[18] The higher cost in Montreal is due to the need to perform ""snow removal"" (the loading stage where snow is hauled away) consistently, necessitated by both the higher snowfall amounts and fewer melting days there, as opposed to simple ""snow clearing"" that usually suffices in other cities with less snowfall.

In Helsinki, Finland, the amount of snow transported from streets and properties to snow dump sites during the winter of 2009–2010 was 210,000 truckloads, equaling over 3 million cubic meters.[19]

Snow removal impacts the design of city infrastructure.  Where possible, street boulevards are wider to accommodate the windrows and sidewalks are not right next to the street.  Fire hydrants will have tall flags to locate them under the windrows.  Reflective traffic lane markers embedded in the roadbed is not possible (or much harder) due to risk of damage by plows.  Access to snow dumping locations (e.g. ravines) by heavy equipment is also planned.

The employees who take part in snow removal are generally the same workers who do road maintenance work during the summer months, but in some US cities garbage trucks are also equipped with plows and used for snow removal.  Many smaller US communities sign contracts with insurance companies, under which the insurance company assumes the risk of a heavy winter.  The insurance company of course sets the rates such that averaged over time they will make a profit; the town is willing to overpay for snow removal in mild winters in order to avoid the risk of running dramatically over budget in the occasional severe winter.

Large organizations such as universities and airports also often have their own mechanized snow clearing force.  Public transit systems generally clear bus stops while post offices clear around mail boxes. Railroads have their own snow clearing devices such as rotary snowplows.

Airports, with their associated runways, taxiways and ramp areas are an exception to the use of salt, as the metals used in aircraft construction will corrode causing safety issues.

A snow dump site is a location where snow is dumped as a part of the snow removal process. Designated sites are sometimes required to prevent water and ground pollution because the snow collected on roads typically contain a variety of grit, de-icing chemicals, vehicle fluids, engine emissions, and litter. Some pollutants become diluted and wash away with the melt-water and some concentrate at the dump site.[20] Dumping into fresh water is ""...almost universally prohibited due to the serious impact that deicer salts can have on freshwater aquatic life.""[20] In the United States dumping snow into water bodies is not specifically prohibited by the Clean Water Act or the Ocean Dumping Act. The states and local governments determine their own dumping policies.

Snow dump site selection is based on the availability of suitable land and pollution prevention factors that may include distances from lakes and streams, installation of silt fences, soil and aquifer type, and other factors. Some sites may use filters and settling ponds to help prevent the pollution from spreading. Occasionally an excessive amount of snow must be dumped and is sometimes allowed to be legally dumped into water bodies on an ""emergency only"" basis.

The surface is treated primarily by snow removal. Roads are also treated by spreading various materials on the surface. These materials generally fall into two categories: chemical and inert. Chemical (including salt) distribution induces freezing-point depression, causing ice and snow to melt at a lower temperature. Chemical treatment can be applied as a preventive measure and/or after snowfall. Inert materials (i.e. sand, brash, slag) make the surface irregular to improve traction. Both types can be applied together, but the inert materials tend to lower traction once the snow and ice has melted.

Chemical treatment materials include:

In the European Union, 98% of chemical treatment materials used in 2000 were sodium chloride in various forms. It is effective down to −5 °C, at the most −7 °C. For colder temperatures, calcium chloride (CaCl2) is added to NaCl in some countries, but deployment is limited as it costs about 6 times as much as sodium chloride. Other substances were used rarely and experimentally. Alternative substances (urea, alcohols, glycols) are often used at airports.[23]  In recent years, Geomelt, a combination of salt brine and beet juice that is otherwise considered a waste product has been used for pretreatment.[24] In Wisconsin, USA, surplus brine from cheese making has been used for this purpose.[25]

Inert spreadings can be:

The choice of treatment may include consideration of the effect on vegetation, pets and other animals, the local watershed, and effectiveness with regard to speed and temperature.  Some chemicals can degrade concrete, metals, and other materials.  The resulting meltwater and slush can cause frost heaving if it re-freezes, which can also damage pavement. Inert materials can damage vehicles and create dust.

As an example, in the Czech Republic during the winter season of 2000/2001, net material expenditure for road treatment was: 168 000 tonnes of salt (mostly NaCl), 348 000 tonnes of sand and crushed stone and 91 000 tonnes of other materials like slag. In Ireland, the annual expenditure of salt was 30 000 tonnes. Switzerland reports their annual expenditure as 600 grammes of salt to every square metre of roads on average.[23]

De-icing chemicals and inert materials need to be selected and applied with care.

Chemicals may react with infrastructure, the environment, and vehicles. Chlorides corrode steel and aluminum in reinforced concrete, structures and vehicles. Acetates can cause asphalt stripping, weakening the bond between asphalt binder and aggregate. Sand and grit can clog pavement joints and cracks, preventing pavement from expanding in the summer and increasing stress in the pavement.[26]

Salts can be toxic to plants and aquatic life, including the trees lining the side of the roads. Sand can alter aquatic habitats where roads are near  streams and lakes. Acetates can reduce oxygen levels in smaller water bodies, stressing aquatic animal life. Sand can be ground by tires into very fine particulate matter and become airborne, contributing to air pollution.[27][28]


"
Land Leveling Services,"Land development is the alteration of landscape in any number of ways such as:

Land development has a history dating to Neolithic times around 8,000 BC. From the dawn of civilization, the process of land development has elaborated the progress of improvements on a piece of land based on codes and regulations, particularly housing complexes.

In an economic context, land development is also sometimes advertised as land improvement or land amelioration. It refers to investment  making land more usable by humans. For accounting purposes it refers to any variety of projects that increase the value of the process . Most are depreciable, but some land improvements are not able to be depreciated because a useful life cannot be determined. Home building and containment[clarification needed] are two of the most common and the oldest types of development.

In an urban context, land development furthermore includes:

A landowner or developer of a project of any size, will often want to maximise profits, minimise risk, and control cash flow. This ""profitable  energy"" means identifying and developing the best scheme for the local marketplace, whilst satisfying the local planning process.

Development analysis puts development prospects and the development process itself under the microscope, identifying where enhancements and improvements can be introduced. These improvements aim to align with best design practice, political sensitivities, and the inevitable social requirements of a project, with the overarching objective of increasing land values and profit margins on behalf of the landowner or developer.[1]

Development analysis can add significantly to the value of land and development, and as such is a crucial tool for landowners and developers. It is an essential step in Kevin A. Lynch's 1960 book The Image of the City, and is considered to be essential to realizing the value potential of land.[2] The landowner can share in additional planning gain (significant value uplift) via an awareness of the land's development potential. This is done via a residual development appraisal or residual valuation. The residual appraisal calculates the sale value of the end product (the gross development value or GDV) and hypothetically deducts costs, including planning and construction costs, finance costs and developer's profit. The ""residue"", or leftover proportion, represents the land value. Therefore, in maximizing the GDV (that which one could build on the land), land value is concurrently enhanced.

Land value is highly sensitive to supply and demand (for the end product), build costs, planning and affordable housing contributions, and so on. Understanding the intricacies of the development system and the effect of ""value drivers"" can result in massive differences in the landowner's sale value.

Land development puts more emphasis on the expected economic development as a result of the process; ""land conversion"" tries to focus on the general physical and biological aspects of the land use change. ""Land improvement"" in the economic sense can often lead to land degradation from the ecological perspective. Land development and the change in land value does not usually take into account changes in the ecology of the developed area. While conversion of (rural) land with a vegetation carpet to building land may result in a rise in economic growth and rising land prices, the irreversibility of lost flora and fauna because of habitat destruction, the loss of ecosystem services and resulting decline in environmental value is only considered a priori in environmental full-cost accounting.

Conversion to building land is as a rule associated with road building, which in itself already brings topsoil abrasion,[3] soil compaction[4] and modification of the soil's chemical composition through soil stabilization, creation of impervious surfaces and, subsequently, (polluted) surface runoff water.

Construction activity often effectively seals off a larger part of the soil from rainfall and the nutrient cycle, so that the soil below buildings and roads is effectively ""consumed"" and made infertile.

With the notable exception of attempts at rooftop gardening and hanging gardens in green buildings (possibly as constituents of green urbanism), vegetative cover of higher plants is lost to concrete and asphalt surfaces, complementary interspersed garden and park areas notwithstanding.[citation needed]

New creation of farmland (or 'agricultural land conversion') will rely on the conversion and development of previous forests, savannas or grassland. Recreation of farmland from wasteland, deserts or previous impervious surfaces is considerably less frequent because of the degraded or missing fertile soil in the latter. Starting from forests, land is made arable by assarting or slash-and-burn. 
Agricultural development furthermore includes: 

Because the newly created farmland is more prone to erosion than soil stabilized by tree roots, such a conversion may mean irreversible crossing of an ecological threshold.

The resulting deforestation is also not easily compensated for by reforestation or afforestation. This is because plantations of other trees as a means for water conservation and protection against wind erosion (shelterbelts), as a rule, lack the biodiversity of the lost forest, especially when realized as monocultures.[5][6][7][8]  These deforestation consequences may have lasting effects on the environment including soil stabilization and erosion control measures that may not be as effective in preserving topsoil as the previous intact vegetation.

Massive land conversion without proper consideration of ecological and geological consequences may lead to disastrous results, such as: 

While deleterious effects can be particularly visible when land is developed for industrial or mining usage, agro-industrial and settlement use can also have a massive and sometimes irreversible impact on the affected ecosystem.[9]

Examples of land restoration/land rehabilitation counted as land development in the strict sense are still rare. However, renaturation, reforestation, stream restoration may all contribute to a healthier environment and quality of life, especially in densely populated regions. The same is true for planned vegetation like parks and gardens, but restoration plays a particular role, because it reverses previous conversions to built and agricultural areas.

The environmental impact of land use and development is a substantial consideration for land development projects. On the local level an environmental impact report (EIR) may be necessary.[definition needed] In the United States, federally funded projects typically require preparation of an environmental impact statement (EIS). The concerns of private citizens or political action committees (PACs)[further explanation needed] can influence the scope, or even cancel, a project based on concerns like the loss of an endangered species’ habitat.[citation needed]

In most cases, the land development project will be allowed to proceed if mitigation requirements are met.[citation needed] Mitigation banking is the most prevalent example, and necessitates that the habitat will have to be replaced at a greater rate than it is removed.  This increase in total area helps to establish the new ecosystem, though it will require time to reach maturity.[citation needed]

The extent, and type of land use directly affects wildlife habitat and thereby impacts local and global biodiversity.[10] Human alteration of landscapes from natural vegetation (e.g. wilderness) to any other use can result in habitat loss, degradation, and fragmentation, all of which can have devastating effects on biodiversity.[11] Land conversion is the single greatest cause of extinction of terrestrial species.[12] An example of land conversion being a chief cause of the critically endangered status of a carnivore is the reduction in habitat for the African wild dog, Lycaon pictus.[13]

Deforestation is also the reason for loss of a natural habitat, with large numbers of trees being cut down for residential and commercial use. Urban growth has become a problem for forests and agriculture, the expansion of structures prevents natural resources from producing in their environment.[14]  To prevent the loss of wildlife the forests must maintain a stable climate and the land must remain unaffected by development. [citation needed]  Furthermore, forests can be sustained by different forest management techniques such as reforestation and preservation. Reforestation is a reactive approach designed to replant previously logged trees within the forest boundary in attempts to re-stabilize this ecosystem. Preservation, on the other hand, is a proactive idea that promotes the concept of leaving the forest without using this area for its ecosystem goods and services.[15] Both of these methods to mitigate deforestation are being used throughout the world.[citation needed]

The U.S. Forest Service predicts that urban and developing terrain in the U.S. will expand by 41 percent in 2060.[16] These conditions cause displacement for the wildlife and limited resources for the environment to maintain a sustainable balance.[17]
"
Residential Drain Cleaning,"A drain cleaner, also known as drain opener, refers to a person, device, or product used to unblock sewer pipes or clear clogged wastewater drains. This term typically applies to chemical, enzymatic, or mechanical tools such as commercial chemical cleaners, plumber’s snakes, drain augers, bio-enzyme solutions, or toilet plungers. In some contexts, it may also refer to a plumber or professional who specializes in drain cleaning and maintenance.  

Chemical drain cleaners, plungers, handheld drain augers, and air burst drain cleaners are typically used to address clogs in single drain, such as sinks, toilets, tubs, or shower drains. These tools are effective at removing soft obstructions like hair and grease that accumulate near the drain inlet. However, excessive use of chemical drain cleaners can lead to pipe damage. In contrast, enzymatic drain cleaners rely on natural enzymes to break down organic matter such as grease, hair, and food particles, offering a more environmentally friendly solution that avoids harsh chemicals.[1]

If more than one plumbing fixture is clogged then electric drain cleaners, battery powered drain cleaners, sewer jetters or such mechanical devices are usually required to clear obstructions along the entire length of the drain piping system, that is, from fixture drain inlets through the main building drains and lateral piping outside the building to the collector sewer mains.

The history of drain cleaners parallels the development of common drain systems themselves.[2] As a result, there is not an extensive history of cleaners in the US, as municipal plumbing systems were not readily available in middle-class American homes until the early 20th century. Prior to this time, Americans often discarded the dirty water collected in basins after use. Limited piping systems gradually developed with lead materials, but after WWI when the poisonous properties of lead became more well-known, piping was reconstructed with galvanized iron.[citation needed]

Galvanized iron is actually steel covered in a protective layer of zinc, but it was soon discovered that this zinc layer naturally corroded due to exposure to the atmosphere and rainwater, as well as cement, runoff, etc. Once corrosion occurred down to the base metal, porous plaques and rust would form, leading to sediment build-up that would gradually clog these drains.

The problems with corroding galvanized iron pipes eventually led to their replacement by copper or plastic (PVC) piping by the 1960s. Natural substances such as hair, grease, or other oils continued to be an issue in drain clogs, encouraging the development of chemical drain cleaners as well as mechanical tools to clear drains.[3]

These commercial products usually contain corrosive acids or alkalis to unclog organic materials, including proteins, lipids and carbohydrates. 

Enzyme drain cleaners are biodegradable cleaning solutions designed to clear clogs and maintain drainage systems by employing natural enzymatic reactions. 

The enzymatic process facilitates the decomposition of these materials into smaller, water-soluble molecules,[4] which are then more easily flushed without harming the plumbing infrastructure or the environment. While enzyme drain cleaners are effective for regular maintenance and preventing minor clogs, they may not be suitable for severe blockages.

Handheld drain augers are typically designed to clean portions of a drain within 25 feet (7.6 m) of the drain opening. The springy, flexible cable of a handheld drain auger is pushed into a drain while the operator rotates a drum that anchors the cable. Similar to handheld augers, drain rods can be used for clearing blockages in long, straight runs of pipe.

Many handheld augers have cables which are thin enough to pass through common sink traps, but manufacturers do not recommend using handheld drain augers in toilets because of their potential to scratch ceramic surfaces. Instead, a special closet auger (from ""water closet"") should be used.

Advantages of handheld drain augers include low relative cost and ready availability of these tools in hardware stores. However, drawbacks include a reach that is normally limited to 25 feet (7.6 m), and the potential for the twisting cable to scratch the ceramic surfaces of plumbing fixtures. They are also only effective on small-diameter pipes (40–50 millimetres (1.6–2.0 in)) rather than main sewer pipes (110 millimetres (4.3 in)).

Safety considerations include protective gloves and eye protection, and practicing good hygiene after coming into contact with drain fluids.

Air burst drain cleaners use accelerated carbon dioxide, air or other gas to rupture the clog membrane. Accelerated gas creates a force on standing water that can dislodge clogs that accumulate close to drain openings.

Advantages of air burst drain cleaners include the potential to immediately clear clogs and slow-running drains, in contrast to chemical cleaners that can take more time to work. Air burst cleaners can dislodge obstructions that are further away from drain openings than can a plunger, and in contrast to drain augers do not risk scratching the ceramic surfaces of sinks, bathtubs and toilets.

Disadvantages of air burst drain cleaners include a limited cleaning range in pipes that do not contain standing water and, in general, ineffectiveness for unclogging blocked main sewer drains.

Safety considerations for air burst drain cleaners include a requirement to wear eye protection and, when using an air burst cleaner that uses compressed gas cartridges, careful handling of unused cartridges.

Hydro-mechanical drain cleans use high-pressure water to break up obstructions and flush these smaller particles down the drain.

Most municipal building codes mandate that drain plumbing increase in diameter as it moves closer to the municipal sewer system. i.e., most kitchen sinks evacuate water with a 1+1⁄2-inch drain pipe, which feeds into a larger 4-inch drain pipe on the main plumbing stack before heading to a septic tank or to the city sewage system. This means that, barring intrusion by tree roots or other debris into buried piping, the vast majority of household drain clogs occur in the smallest-diameter piping, usually in the pop-up or drain trap, where they can be reached easily by a hydro-mechanical device's water hose.

Advantages of hydro-mechanical drain cleaners are their eco-friendliness (most use only tap water), their ability to dislodge and remove clogs like sand or cat litter that 'back-fill when using a conventional snake, and their friendliness to plumbing joints. Unlike air-burst cleaners, hydro-mechanical drain cleaners do not pressurize plumbing joints. On some models of hydro-mechanical drain cleaner both hot and cold water can be used, providing added cleaning power for fat, protein, or other easily melting drain clogs.

Disadvantages of hydro-mechanical drain cleaners included limited reach into drain plumbing, and the necessity of a water source to act as the motive agent.

Safety considerations for hydro-mechanical drain cleaners include the risk of injury from high-pressure water coming into contact with skin or delicate areas of the body (i.e., eyes, and face).

Electric drain cleaners, also called plumber's snakes, use the mechanical force of an electric motor to twist a flexible cable or spring in a clockwise direction and drive it into a pipe. Electric drain cleaners are commonly available with cable lengths of up to 40 metres and can go as far as 80 metres.

Advantages of electric drain cleaners include the ability to clean long sections of sewer drain, the ability to remove solid objects such as tree roots and jewelry, and ready availability through hardware stores and tool rental counters. Machines using springs can easily negotiate multiple 90-degree bends while maintaining their effectiveness and without damaging the pipe.

Disadvantages of electric drain cleaners include high relative cost and weight, and the considerable physical effort that may be required to control the cable.

Safety considerations for electric drain cleaners include the requirement to wear work gloves and eye protection, to carefully control the cable during operation to avoid overstressing it, to use appropriate caution when working around rotating machinery, and to use properly grounded electrical outlets.[5]

Sewer jetting is the process of shooting high powered streams of water through the drain, down into the sewer in order to blast away any debris blocking the passage of water. This is more effective than using a snake, blades, or even drain rods because, first the water is shot at such a high intensity that the force isn't even comparable to manual labour, secondly the water is much more capable of bending around curved or angular pipes to reach all the tight spots.[citation needed]

A sewer jetter is composed of a controlled high-pressure water source such as a pressure washer or reciprocating displacement pump, a flexible high-pressure line (called a jetter hose, which connects the high-pressure engine to the mini-reel) of up to hundreds of metres (several hundred feet) in length, the Mini-Reel (a hose reel which can be taken a distance from the engine) and a nozzle that uses hydraulic force to pull the line into sewer drains, clean the sides of pipes, and flush out residue. High-pressure sewer jetters can be mounted on trolleys, inside vans or on trailers. The power of a sewer jetter ranges from 1,000 psi (68 atm) to 5,000 psi (340 atm). 

Sewer jetter nozzles come in different sizes and applications; a bullet-type nozzle with a streamlined profile can clear a hole for the larger root cutting nozzle. Root-cutter nozzles are designed to cut roots with a spinning nozzle that shoots a jet stream horizontally inside the pipe. High pressure sewer jetters with root-cutting nozzles can clear a hole through the center of a root-infested sewer line and with its rear-facing jet streams cut the roots and clean the pipe walls, flushing the root debris through the sewer line. The sewer jetter has been labeled as a technological advancement of the plumber's snake (also known as an electric eel) drain clearing method.[citation needed]

Portable sewer jetters and pressure washer sewer jetter attachments are primarily used by service personnel and homeowners to remove soft obstructions throughout the length of a building's sewer drain and to prevent the recurrence of clogs by cleaning the sides of drain pipes and flushing out residue. Pressure washer sewer jetter attachments are generally lower in cost and weight than electric drain cleaners with an equivalent reach, and can present a lower risk of scratching plumbing fixtures.[6]

Truck and trailer-mounted sewer jetters used by municipalities and larger service companies benefit from the high hydraulic horsepower delivered by powerful displacement pumps and so can remove tree roots and other solid obstructions.[citation needed]

Advantages of sewer jetters include the relative ease of penetrating long sewer lines and the ability to remove residue that accumulates along the sides of sewer pipes, thereby reducing the need for subsequent drain cleaning.

Disadvantages of pressure washer sewer jetter attachments and many portable jetters include an inability to extract tree roots and other hard obstructions. Disadvantages of truck- and trailer-mounted sewer jetters include high relative cost and weight, and the requirement for extensive training to comply with manufacturers' safety guidelines.

Safety considerations for sewer jetters include a requirement to wear protective gloves and eye protection, to avoid contact with sewer drain fluids, and to ensure that the jetter nozzle operates only inside the sewer pipe.[7] Furthermore, larger truck- and trailer-mounted units that operate with sufficient power to cut tree roots require extensive training and strict adherence to manufacturers' safety guidelines to avoid serious injury.[8]
"
Commercial Drain Cleaning,"A drain cleaner, also known as drain opener, refers to a person, device, or product used to unblock sewer pipes or clear clogged wastewater drains. This term typically applies to chemical, enzymatic, or mechanical tools such as commercial chemical cleaners, plumber’s snakes, drain augers, bio-enzyme solutions, or toilet plungers. In some contexts, it may also refer to a plumber or professional who specializes in drain cleaning and maintenance.  

Chemical drain cleaners, plungers, handheld drain augers, and air burst drain cleaners are typically used to address clogs in single drain, such as sinks, toilets, tubs, or shower drains. These tools are effective at removing soft obstructions like hair and grease that accumulate near the drain inlet. However, excessive use of chemical drain cleaners can lead to pipe damage. In contrast, enzymatic drain cleaners rely on natural enzymes to break down organic matter such as grease, hair, and food particles, offering a more environmentally friendly solution that avoids harsh chemicals.[1]

If more than one plumbing fixture is clogged then electric drain cleaners, battery powered drain cleaners, sewer jetters or such mechanical devices are usually required to clear obstructions along the entire length of the drain piping system, that is, from fixture drain inlets through the main building drains and lateral piping outside the building to the collector sewer mains.

The history of drain cleaners parallels the development of common drain systems themselves.[2] As a result, there is not an extensive history of cleaners in the US, as municipal plumbing systems were not readily available in middle-class American homes until the early 20th century. Prior to this time, Americans often discarded the dirty water collected in basins after use. Limited piping systems gradually developed with lead materials, but after WWI when the poisonous properties of lead became more well-known, piping was reconstructed with galvanized iron.[citation needed]

Galvanized iron is actually steel covered in a protective layer of zinc, but it was soon discovered that this zinc layer naturally corroded due to exposure to the atmosphere and rainwater, as well as cement, runoff, etc. Once corrosion occurred down to the base metal, porous plaques and rust would form, leading to sediment build-up that would gradually clog these drains.

The problems with corroding galvanized iron pipes eventually led to their replacement by copper or plastic (PVC) piping by the 1960s. Natural substances such as hair, grease, or other oils continued to be an issue in drain clogs, encouraging the development of chemical drain cleaners as well as mechanical tools to clear drains.[3]

These commercial products usually contain corrosive acids or alkalis to unclog organic materials, including proteins, lipids and carbohydrates. 

Enzyme drain cleaners are biodegradable cleaning solutions designed to clear clogs and maintain drainage systems by employing natural enzymatic reactions. 

The enzymatic process facilitates the decomposition of these materials into smaller, water-soluble molecules,[4] which are then more easily flushed without harming the plumbing infrastructure or the environment. While enzyme drain cleaners are effective for regular maintenance and preventing minor clogs, they may not be suitable for severe blockages.

Handheld drain augers are typically designed to clean portions of a drain within 25 feet (7.6 m) of the drain opening. The springy, flexible cable of a handheld drain auger is pushed into a drain while the operator rotates a drum that anchors the cable. Similar to handheld augers, drain rods can be used for clearing blockages in long, straight runs of pipe.

Many handheld augers have cables which are thin enough to pass through common sink traps, but manufacturers do not recommend using handheld drain augers in toilets because of their potential to scratch ceramic surfaces. Instead, a special closet auger (from ""water closet"") should be used.

Advantages of handheld drain augers include low relative cost and ready availability of these tools in hardware stores. However, drawbacks include a reach that is normally limited to 25 feet (7.6 m), and the potential for the twisting cable to scratch the ceramic surfaces of plumbing fixtures. They are also only effective on small-diameter pipes (40–50 millimetres (1.6–2.0 in)) rather than main sewer pipes (110 millimetres (4.3 in)).

Safety considerations include protective gloves and eye protection, and practicing good hygiene after coming into contact with drain fluids.

Air burst drain cleaners use accelerated carbon dioxide, air or other gas to rupture the clog membrane. Accelerated gas creates a force on standing water that can dislodge clogs that accumulate close to drain openings.

Advantages of air burst drain cleaners include the potential to immediately clear clogs and slow-running drains, in contrast to chemical cleaners that can take more time to work. Air burst cleaners can dislodge obstructions that are further away from drain openings than can a plunger, and in contrast to drain augers do not risk scratching the ceramic surfaces of sinks, bathtubs and toilets.

Disadvantages of air burst drain cleaners include a limited cleaning range in pipes that do not contain standing water and, in general, ineffectiveness for unclogging blocked main sewer drains.

Safety considerations for air burst drain cleaners include a requirement to wear eye protection and, when using an air burst cleaner that uses compressed gas cartridges, careful handling of unused cartridges.

Hydro-mechanical drain cleans use high-pressure water to break up obstructions and flush these smaller particles down the drain.

Most municipal building codes mandate that drain plumbing increase in diameter as it moves closer to the municipal sewer system. i.e., most kitchen sinks evacuate water with a 1+1⁄2-inch drain pipe, which feeds into a larger 4-inch drain pipe on the main plumbing stack before heading to a septic tank or to the city sewage system. This means that, barring intrusion by tree roots or other debris into buried piping, the vast majority of household drain clogs occur in the smallest-diameter piping, usually in the pop-up or drain trap, where they can be reached easily by a hydro-mechanical device's water hose.

Advantages of hydro-mechanical drain cleaners are their eco-friendliness (most use only tap water), their ability to dislodge and remove clogs like sand or cat litter that 'back-fill when using a conventional snake, and their friendliness to plumbing joints. Unlike air-burst cleaners, hydro-mechanical drain cleaners do not pressurize plumbing joints. On some models of hydro-mechanical drain cleaner both hot and cold water can be used, providing added cleaning power for fat, protein, or other easily melting drain clogs.

Disadvantages of hydro-mechanical drain cleaners included limited reach into drain plumbing, and the necessity of a water source to act as the motive agent.

Safety considerations for hydro-mechanical drain cleaners include the risk of injury from high-pressure water coming into contact with skin or delicate areas of the body (i.e., eyes, and face).

Electric drain cleaners, also called plumber's snakes, use the mechanical force of an electric motor to twist a flexible cable or spring in a clockwise direction and drive it into a pipe. Electric drain cleaners are commonly available with cable lengths of up to 40 metres and can go as far as 80 metres.

Advantages of electric drain cleaners include the ability to clean long sections of sewer drain, the ability to remove solid objects such as tree roots and jewelry, and ready availability through hardware stores and tool rental counters. Machines using springs can easily negotiate multiple 90-degree bends while maintaining their effectiveness and without damaging the pipe.

Disadvantages of electric drain cleaners include high relative cost and weight, and the considerable physical effort that may be required to control the cable.

Safety considerations for electric drain cleaners include the requirement to wear work gloves and eye protection, to carefully control the cable during operation to avoid overstressing it, to use appropriate caution when working around rotating machinery, and to use properly grounded electrical outlets.[5]

Sewer jetting is the process of shooting high powered streams of water through the drain, down into the sewer in order to blast away any debris blocking the passage of water. This is more effective than using a snake, blades, or even drain rods because, first the water is shot at such a high intensity that the force isn't even comparable to manual labour, secondly the water is much more capable of bending around curved or angular pipes to reach all the tight spots.[citation needed]

A sewer jetter is composed of a controlled high-pressure water source such as a pressure washer or reciprocating displacement pump, a flexible high-pressure line (called a jetter hose, which connects the high-pressure engine to the mini-reel) of up to hundreds of metres (several hundred feet) in length, the Mini-Reel (a hose reel which can be taken a distance from the engine) and a nozzle that uses hydraulic force to pull the line into sewer drains, clean the sides of pipes, and flush out residue. High-pressure sewer jetters can be mounted on trolleys, inside vans or on trailers. The power of a sewer jetter ranges from 1,000 psi (68 atm) to 5,000 psi (340 atm). 

Sewer jetter nozzles come in different sizes and applications; a bullet-type nozzle with a streamlined profile can clear a hole for the larger root cutting nozzle. Root-cutter nozzles are designed to cut roots with a spinning nozzle that shoots a jet stream horizontally inside the pipe. High pressure sewer jetters with root-cutting nozzles can clear a hole through the center of a root-infested sewer line and with its rear-facing jet streams cut the roots and clean the pipe walls, flushing the root debris through the sewer line. The sewer jetter has been labeled as a technological advancement of the plumber's snake (also known as an electric eel) drain clearing method.[citation needed]

Portable sewer jetters and pressure washer sewer jetter attachments are primarily used by service personnel and homeowners to remove soft obstructions throughout the length of a building's sewer drain and to prevent the recurrence of clogs by cleaning the sides of drain pipes and flushing out residue. Pressure washer sewer jetter attachments are generally lower in cost and weight than electric drain cleaners with an equivalent reach, and can present a lower risk of scratching plumbing fixtures.[6]

Truck and trailer-mounted sewer jetters used by municipalities and larger service companies benefit from the high hydraulic horsepower delivered by powerful displacement pumps and so can remove tree roots and other solid obstructions.[citation needed]

Advantages of sewer jetters include the relative ease of penetrating long sewer lines and the ability to remove residue that accumulates along the sides of sewer pipes, thereby reducing the need for subsequent drain cleaning.

Disadvantages of pressure washer sewer jetter attachments and many portable jetters include an inability to extract tree roots and other hard obstructions. Disadvantages of truck- and trailer-mounted sewer jetters include high relative cost and weight, and the requirement for extensive training to comply with manufacturers' safety guidelines.

Safety considerations for sewer jetters include a requirement to wear protective gloves and eye protection, to avoid contact with sewer drain fluids, and to ensure that the jetter nozzle operates only inside the sewer pipe.[7] Furthermore, larger truck- and trailer-mounted units that operate with sufficient power to cut tree roots require extensive training and strict adherence to manufacturers' safety guidelines to avoid serious injury.[8]
"
Street Cleaning Operations,"A street sweeper or street cleaner is a person or machine that cleans streets.

People have worked in cities as ""sanitation workers"" since sanitation and waste removal became a priority. A street-sweeping person would use a broom and shovel to clean off litter, animal waste and filth that accumulated on streets. Later, water hoses were used to wash the streets.

Street sweepers as machines were created in the 19th century to do the job easier. Today, modern street sweepers are mounted on truck bodies and can vacuum debris that accumulates in streets.

The need for rubbish to be removed from roads in built-up areas has existed for centuries.

Sometimes a local law in a town or city ordered the owner or occupier of each address to clean the length of that road that passed his address.

Sometimes when much traffic was horse-drawn vehicles or ridden horses, there were street cleaners who selectively removed horse droppings because of their value as fertilizer on nearby rural areas.

By the 1840s, Manchester, England, had become known as the first industrial city. Manchester had one of the largest textile industries of that time. As a result, the robust metropolis was said to be England's unhealthiest place to live.[1] In response to this unsanitary environment, Joseph Whitworth invented the mechanical street sweeper. The street sweeper was designed with the primary objective to remove rubbish from streets in order to maintain aesthetic goals and safety.[2][3]

The very first street sweeping machine was patented in 1849 by its inventor, C.S. Bishop. For a long time, street sweepers were just rotating disks covered with wire bristles. These rotating disks served as mechanical brooms that swept the dirt on the streets.[4]

The first self-propelled sweeper vehicle patented in the US, driven by a steam engine and intended for cleaning railroad tracks, was patented in 1868, patent No. 79606. Eureka C. Bowne was the first known woman to get a patent for a street sweeper, in 1879, patent No. 222447. ""Her success was great"", wrote Matilda Joslyn Gage in The North American Review, volume 136, issue 318, May 1883.[5]

In 1896, African-American inventor Charles Brooks improved on then-conventional street sweeping inventions by making the front brushes of different lengths, and by including a mechanism for collection and disposal of debris. The revolving front brushes could also be replaced with a scraper to remove snow or ice. Brooks was granted a U.S. patent for the invention in 1896.[6] Most of the more than 300 street sweeper patents issued in the United States before 1900, including the one in Brooks' patent, had no engine on board. The wheels on the cart turned gears or chains which drove the brush and belt.

John M. Murphy called at the offices of American Tower and Tank Company in Elgin, Illinois, in the fall of 1911. He had a plan of a motor-driven pickup street sweeper. The American Tower and Tank Company had been formed in 1903 by Charles A. Whiting and James Todd. They called in a recently acquired silent partner, Daniel M. Todd, and it was decided to hire  Murphy and begin the development of his idea. That started what has become the Elgin Sweeper Company.[7]

After two years of trial, development, experimentation, and research, a sweeper was achieved which Murphy was satisfied performed all of the sweeping functions in the manner he had envisioned – one which partners James and Daniel M. Todd and Charles A. Whiting were willing to risk a reputation gained from 30 years' manufacturing experience.[7]

In the fall of 1913, the city of Boise, Idaho, purchased the first Elgin Sweeper, following a demonstration. Boise Street Commissioner, Thomas Finegan, made a comparison showing a savings of $2,716.77 from the Elgin motorized sweeper when used rather than a horse-drawn sweeper.[7]

Following its introduction and initial sales,  Murphy continued improving his sweeper. In 1917, US patents were filed and issues for J. M. Murphy, Street Sweeping machine No. 1,239,293.[7]

The goal of simple debris removal did not change until the 1970s, when policymakers began to reflect concern for water quality. In the United States, the lag time in which street sweepers responded can be pinpointed to the Runoff Report of 1998.[8] As older street sweepers were only effective in removing large particles of road debris, small particles of debris remained behind in large quantities.[9] The remaining debris was not seen as an aesthetic issue because rain would wash them away. Today, small particles are known to carry a substantial portion of the stormwater pollutant load.

Street sweeping can be an effective measure in reducing pollutants in stormwater runoff.[10] The Environmental Protection Agency considers street sweeping a best practice in protecting water quality.

Street sweepers are capable of collecting small particles of debris.[2] Many street sweepers produced today are PM10 and PM2.5 certified,[3] meaning that they are capable of collecting and holding particulate matter sized less than 10μm and even down to 2.5μm.[11]

Despite advancements in street sweeping technology, the mechanical broom type street sweeper accounts for approximately 90 percent of all street sweepers used in the United States today.[12] In 2018, Boschung, a Swiss street sweeper manufacturer, launched the Urban-Sweeper S2.0, the first fully electric street sweeper releasing zero emissions.
"
Conveyor System Installation,"A conveyor system is a common piece of mechanical handling equipment that moves materials from one location to another. Conveyors are especially useful in applications involving the transport of heavy or bulky materials. Conveyor systems allow quick and efficient transport for a wide variety of materials, which make them very popular in the material handling and packaging industries. They also have popular consumer applications, as they are often found in supermarkets and airports, constituting the final leg of item/ bag delivery to customers. Many kinds of conveying systems are available and are used according to the various needs of different industries. There are chain conveyors (floor and overhead) as well. Chain conveyors consist of enclosed tracks, I-Beam, towline, power  & free, and hand pushed trolleys.

Conveyor systems are used widespread across a range of industries due to the numerous benefits they provide.

Conveyor systems are commonly used in many industries, including the Mining, automotive, agricultural, computer, electronic, food processing, aerospace, pharmaceutical, chemical, bottling and canning, print finishing and packaging. Although a wide variety of materials can be conveyed, some of the most common include food items such as beans and nuts, bottles and cans, automotive components, scrap metal, pills and powders, wood and furniture and grain and animal feed. Many factors are important in the accurate selection of a conveyor system. It is important to know how the conveyor system will be used beforehand. Some individual areas that are helpful to consider are the required conveyor operations, such as transport, accumulation and sorting, the material sizes, weights and shapes and where the loading and pickup points need to be.

A conveyor system is often the lifeline to a company's ability to effectively move its product in a timely fashion. The steps that a company can take to ensure that it performs at peak capacity, include regular inspections and system audits, close monitoring of motors and reducers, keeping key parts in stock, and proper training of personnel.

Increasing the service life of a conveyor system involves: choosing the right conveyor type, the right system design and paying attention to regular maintenance practices.

A conveyor system that is designed properly will last a long time with proper maintenance. Overhead conveyor systems have been used in numerous applications from shop displays, assembly lines to paint finishing plants and more.

Conveyor systems require materials suited to the displacement of heavy loads and the wear-resistance to hold-up over time without seizing due to deformation. Where static control is a factor, special materials designed to either dissipate or conduct electrical charges are used. Examples of conveyor handling materials include UHMW, nylon, Nylatron NSM, HDPE, Tivar, Tivar ESd, and polyurethane.

As far as growth is concerned the material handling and conveyor system makers are getting utmost exposure in the industries like automotive, pharmaceutical, packaging and different production plants. The portable conveyors are likewise growing fast in the construction sector and by the year 2014 the purchase rate for conveyor systems in North America, Europe and Asia is likely to grow even further. The most commonly purchased types of conveyors are line-shaft roller conveyors, chain conveyors and conveyor belts at packaging factories and industrial plants where usually product finishing and monitoring are carried. Commercial and civil sectors are increasingly implementing conveyors at airports, shopping malls, etc.

Every pneumatic system uses pipes or ducts called transport lines that carry a mixture of materials and a stream of air. These materials are free flowing powdery materials like cement and fly ash. Products are moved through tubes by air pressure. Pneumatic conveyors are either carrier systems or dilute-phase systems; carrier systems simply push items from one entry point to one exit point, such as the money-exchanging pneumatic tubes used at a bank drive-through window. Dilute-phase systems use push-pull pressure to guide materials through various entry and exit points. Air compressors or blowers can be used to generate the air flow.
Three systems used to generate high-velocity air stream:

A vibrating conveyor is a machine with a solid conveying surface which is turned up on the side to form a trough. They are used extensively in food-grade applications to convey dry bulk solids[1] where sanitation, washdown, and low maintenance are essential. Vibrating conveyors are also suitable for harsh, very hot, dirty, or corrosive environments. They can be used to convey newly-cast metal parts which may reach upwards of 1,500 °F (820 °C). Due to the fixed nature of the conveying pans vibrating conveyors can also perform tasks such as sorting, screening, classifying and orienting parts. Vibrating conveyors have been built to convey material at angles exceeding 45° from horizontal using special pan shapes. Flat pans will convey most materials at a 5° incline from horizontal line.

The flexible conveyor is based on a conveyor beam in aluminum or stainless steel, with low-friction slide rails guiding a plastic multi-flexing chain. Products to be conveyed travel directly on the conveyor, or on pallets/carriers. These conveyors can be worked around obstacles and keep production lines flowing. They are made at varying levels and can work in multiple environments. They are used in food packaging, case packing, and pharmaceutical industries and also in large retail stores such as Wal-Mart and Kmart.

Like vertical conveyors, spiral conveyors raise and lower materials to different levels of a facility. In contrast, spiral conveyors are able to transport material loads in a continuous flow. A helical spiral or screw rotates within a sealed tube and the speed makes the product in the conveyor rotate with the screw. The tumbling effect provides a homogeneous mix of particles in the conveyor, which is essential when feeding pre-mixed ingredients and maintaining mixing integrity. Industries that require a higher output of materials - food and beverage, retail case packaging, pharmaceuticals - typically incorporate these conveyors into their systems over standard vertical conveyors due to their ability to facilitate high throughput. Most spiral conveyors also have a lower angle of incline or decline (11 degrees or less) to prevent sliding and tumbling during operation.

Vertical conveyors, also commonly referred to as freight lifts and material lifts, are conveyor systems used to raise or lower materials to different levels of a facility during the handling process. Examples of these conveyors applied in the industrial assembly process include transporting materials to different floors. While similar in look to freight elevators, vertical conveyors are not equipped to transport people, only materials.

Vertical lift conveyors contain two adjacent, parallel conveyors for simultaneous upward movement of adjacent surfaces of the parallel conveyors. One of the conveyors normally has spaced apart flights (pans) for transporting bulk food items. The dual conveyors rotate in opposite directions, but are operated from one gear box to ensure equal belt speed. One of the conveyors is pivotally hinged to the other conveyor for swinging the attached conveyor away from the remaining conveyor for access to the facing surfaces of the parallel conveyors.[2] Vertical lift conveyors can be manually or automatically loaded and controlled.[3] Almost all vertical conveyors can be systematically integrated with horizontal conveyors, since both of these conveyor systems work in tandem to create a cohesive material handling assembly line.

Like spiral conveyors, vertical conveyors that use forks can transport material loads in a continuous flow. With these forks the load can be taken from one horizontal conveyor and put down on another horizontal conveyor on a different level. By adding more forks, more products can be lifted at the same time. Conventional vertical conveyors must have input and output of material loads moving in the same direction. By using forks many combinations of different input- and output- levels in different directions are possible. A vertical conveyor with forks can even be used as a vertical sorter. Compared to a spiral conveyor a vertical conveyor - with or without forks - takes up less space.

Vertical reciprocating conveyors (or VRC) are another type of unit handling system. Typical applications include moving unit loads between floor levels, working with multiple accumulation conveyors, and interfacing overhead conveyors line. Common material to be conveyed includes pallets, sacks, custom fixtures or product racks and more.[4]

Motorized Drive Roller (MDR) conveyor utilize drive rollers that have a Brushless DC (BLDC) motor embedded within a conveyor roller tube. A single motorized roller tube is then mechanically linked to a small number of non-powered rollers to create a controllable zone of powered conveyor. A linear collection of these individually powered zones are arranged end to end to form a line of contiguous conveyor.  The mechanical performance (torque, speed, efficiency, etc.) of drive rollers equipped with BLDC motors is right in the range of that needed for roller conveyor zones when they need to convey general use carton boxes of the size and weight seen in typical modern warehouse and distribution applications. A typical motorized roller conveyor zone can handle carton items weighing up to approximately 35 kg (75 lbs.).  

Heavy-duty roller conveyors are used for moving items that weigh at least 500 pounds (230 kg). This type of conveyor makes the handling of such heavy equipment/products easier and more time effective. Many of the heavy duty roller conveyors can move as fast as 75 feet per minute (23 m/min).

Other types of heavy-duty roller conveyors are gravity roller conveyors, chain-driven live roller conveyors, pallet accumulation conveyors, multi-strand chain conveyors, and chain and roller transfers.

Gravity roller conveyors are easy to use and are used in many different types of industries such as automotive and retail.

Chain-driven live roller conveyors are used for single or bi-directional material handling. Large, heavy loads are moved by chain driven live roller conveyors.

Pallet accumulation conveyors are powered through a mechanical clutch. This is used instead of individually powered and controlled sections of conveyors.

Multi-strand chain conveyors are used for double-pitch roller chains. Products that cannot be moved on traditional roller conveyors can be moved by a multi-strand chain conveyor.

Chain and roller conveyors are short runs of two or more strands of double-pitch chain conveyors built into a chain-driven line roller conveyor. These pop up under the load and move the load off of the conveyor.

It usually consists of two fluid power cylinders [5] or also can use a motor driven cam.[6] For the cylinder driven fluid power type, one axis is for vertical motion and the other for horizontal. Both cam and fluid power types require nests at each station to retain the part that is being moved. The beam is raised, raising the part from its station nest and holding the part in a nest on the walking beam, then moved horizontally, transporting the part to the next nest, then lowered vertically, placing the part in the next station's nest. The beam is then returned to its home position while it is in the lowered position out of the way of the parts. This type of conveying system is useful for parts that need to be accurately physically located or relatively heavy parts. All stations are equidistance and require a nest to retain the part.

M.Marcu-Pipeline Conveyors (theory,photos,state of the art 1990-Pneumatic Pipeline conveyors with wheeled containers) at page 45 in: ""Material handling in pyrometallurgy: proceedings of the International Symposium on Materials Handling in Pyrometallurgy, Hamilton, Ontario, August 26–30, 1990-Pergamon press""
"
Low-Rise Signage Installation,"Traffic signs or road signs are signs erected at the side of or above roads to give instructions or provide information to road users. The earliest signs were simple wooden or stone milestones. Later, signs with directional arms were introduced, for example the fingerposts in the United Kingdom and their wooden counterparts in Saxony.

With traffic volumes increasing since the 1930s, many countries have adopted pictorial signs or otherwise simplified and standardized their signs to overcome language barriers, and enhance traffic safety. Such pictorial signs use symbols (often silhouettes) in place of words and are usually based on international protocols. Such signs were first developed in Europe, and have been adopted by most countries to varying degrees.

International conventions such as Vienna Convention on Road Signs and Signals and Geneva Convention on Road Traffic have helped to achieve a degree of uniformity in traffic signing in various countries.[1] Countries have also unilaterally (to some extent) followed other countries in order to avoid confusion.

Traffic signs can be grouped into several types. For example, Annexe 1 of the Vienna Convention on Road Signs and Signals (1968), which on 30 June 2004 had 52 signatory countries, defines eight categories of signs:

In the United States, Canada, Australia, and New Zealand signs are categorized as follows:

In the United States, the categories, placement, and graphic standards for traffic signs and pavement markings are legally defined in the Federal Highway Administration's Manual on Uniform Traffic Control Devices as the standard.

A rather informal distinction among the directional signs is the one between advance directional signs, interchange directional signs, and reassurance signs. Advance directional signs appear at a certain distance from the interchange, giving information for each direction. A number of countries do not give information for the road ahead (so-called ""pull-through"" signs), and only for the directions left and right. Advance directional signs enable drivers to take precautions for the exit (e.g., switch lanes, double check whether this is the correct exit, slow down).
They often do not appear on lesser roads, but are normally posted on expressways and motorways, as drivers would be missing exits without them. While each nation has its own system, the first approach sign for a motorway exit is mostly placed at least 1,000 metres (3,300 ft) from the actual interchange. After that sign, one or two additional advance directional signs typically follow before the actual interchange itself.

The earliest road signs were milestones, giving distance or direction; for example, the Romans erected stone columns throughout their empire giving the distance to Rome. According to Strabo, Mauryas erected signboards at distance of 10 stades to mark their roads.[2] In the Middle Ages, multidirectional signs at intersections became common, giving directions to cities and towns.

In 1686, the first known Traffic Regulation Act in Europe was established by King Peter II of Portugal. This act foresaw the placement of priority signs in the narrowest streets of Lisbon, stating which traffic should back up to give way. One of these signs still exists at Salvador street, in the neighborhood of Alfama.[3]

The first modern road signs erected on a wide scale were designed for riders of high or ""ordinary"" bicycles in the late 1870s and early 1880s. These machines were fast, silent and their nature made them difficult to control, moreover their riders travelled considerable distances and often preferred to tour on unfamiliar roads. For such riders, cycling organizations began to erect signs that warned of potential hazards ahead (particularly steep hills), rather than merely giving distance or directions to places, thereby contributing the sign type that defines ""modern"" traffic signs.

The development of automobiles encouraged more complex signage systems using more than just text-based notices. One of the first modern-day road sign systems was devised by the Italian Touring Club in 1895. By 1900, a Congress of the International League of Touring Organizations in Paris was considering proposals for standardization of road signage. In 1903 the British government introduced four ""national"" signs based on shape, but the basic patterns of most traffic signs were set at the 1908 World Road Congress in Paris.[citation needed] In 1909, nine European governments agreed on the use of four pictorial symbols, indicating ""bump"", ""curve"", ""intersection"", and ""grade-level railroad crossing"". The intensive work on international road signs that took place between 1926 and 1949 eventually led to the development of the European road sign system. Both Britain and the United States developed their own road signage systems, both of which were adopted or modified by many other nations in their respective spheres of influence. The UK adopted a version of the European road signs in 1964 and, over past decades, North American signage began using some symbols and graphics mixed in with English.

In the U.S., the first road signs were erected by the American Automobile Association (AAA). Starting in 1906, regional AAA clubs began paying for and installing wooden signs to help motorists find their way. In 1914, AAA started a cohesive transcontinental signage project, installing more than 4,000 signs in one stretch between Los Angeles and Kansas City alone.[4]

Over the years, change was gradual. Pre-industrial signs were stone or wood, but with the development of Darby's method of smelting iron using coke-painted cast iron became favoured in the late 18th and 19th centuries. Cast iron continued to be used until the mid-20th century, but it was gradually displaced by aluminium or other materials and processes, such as vitreous enamelled and/or pressed malleable iron, or (later) steel. Since 1945 most signs have been made from sheet aluminium with adhesive plastic coatings; these are normally retroreflective for nighttime and low-light visibility. Before the development of reflective plastics, reflectivity was provided by glass reflectors set into the lettering and symbols.

New generations of traffic signs based on electronic displays can also change their text (or, in some countries, symbols) to provide for ""intelligent control"" linked to automated traffic sensors or remote manual input. In over 20 countries, real-time Traffic Message Channel incident warnings are conveyed directly to vehicle navigation systems using inaudible signals carried via FM radio, 3G cellular data and satellite broadcasts. Finally, cars can pay tolls and trucks pass safety screening checks using video numberplate scanning, or RFID transponders in windshields linked to antennae over the road, in support of on-board signalling, toll collection, and travel time monitoring.

Yet another ""medium"" for transferring information ordinarily associated with visible signs is RIAS (Remote Infrared Audible Signage), e.g., ""talking signs"" for print-handicapped (including blind/low-vision/illiterate) people. These are infra-red transmitters serving the same purpose as the usual graphic signs when received by an appropriate device such as a hand-held receiver or one built into a cell phone.

Then, finally, in 1914, the world's first electric traffic signal is put into place on the corner of Euclid Avenue and East 105th Street in Cleveland, Ohio, on August 5.[citation needed]

Typefaces used on traffic signs vary by location, with some typefaces being designed specifically for the purpose of being used on traffic signs and based on attributes that aid viewing from a distance. A typeface chosen for a traffic sign is selected based on its readability, which is essential for conveying information to drivers quickly and accurately at high speeds and long distances.

Factors such as clear letterforms, lines of copy, appropriate spacing, and simplicity contribute to readability. Increased X-height and counters specifically help with letter distinction and reduced halation, which especially affects aging drivers. In cases of halation, certain letters can blur and look like others, such as a lowercase ""e"" appearing as an ""a"", ""c"", or ""o"".[5][6]

In 1997, a design team at T.D. Larson Transportation Institute began testing Clearview, a typeface designed to improve readability and halation issues with the FHWA Standard Alphabet, also known as Highway Gothic, which is the standard typeface for highway signs in the U.S.[7][8]

The adoption of Clearview for traffic signs over Highway Gothic has been slow since its initial proposal. Country-wide adoption faced resistance from both local governments and the Federal Highway Administration (FHWA), citing concerns about consistency and cost, along with doubts of the studies done on Clearview’s improved readability. As stated by the FHWA, ""This process (of designing Clearview) did not result in a necessarily better set of letter styles for highway signing, but rather a different set of letter styles with increased letter height and different letter spacing that was not comparable to the Standard Alphabets.""[9]

The FHWA allowed use of Clearview to be approved on an interim basis as opposed to national change, where local governments could decide to submit a request to the FHWA for approval to update their signs with Clearview, but in 2016 they rescinded this approval, wanting to limit confusion and inconsistency that could come from a mix of two typefaces being used. In 2018, they again allowed interim approval of Clearview, with Highway Gothic remaining the standard.[9][10]

Cars are beginning to feature cameras with automatic traffic sign recognition, beginning 2008 with the Opel Insignia. It mainly recognizes speed limits and no-overtaking areas.[11] It also uses GPS and a database over speed limits, which is useful in the many countries which signpost city speed limits with a city name sign, not a speed limit sign.

Rail traffic has often a lot of differences between countries and often not much similarity with road signs. Rail traffic has professional drivers who have much longer education than what's normal for road driving licenses. Differences between neighboring countries cause problems for cross border traffic and causes need for additional education for drivers.
"
High-Rise Signage Installation,"Digital signage is a segment of electronic signage. Digital displays use technologies such as LCD, LED, OLED, projection and e-paper to display digital images, video, web pages, weather data, restaurant menus, or text. It is used in various settings to enhance user experience, share information, support community interaction, provide announcements and display advertisements.[1] They can be found in public spaces, transportation systems, museums, stadiums, retail stores, hotels, restaurants and corporate buildings etc., to provide wayfinding, exhibitions, marketing and outdoor advertising. They are used as a network of electronic displays that are centrally managed and individually addressable for the display of text, animated or video messages for advertising, information, entertainment and merchandising to targeted audiences.[2]

The many different uses of digital signage allow a business to accomplish a variety of goals. Some of the most common applications include:

The global digital signage market was valued at approximately USD 28.5 billion in 2024. It is projected to grow at a CAGR of 5.56% from 2025 to 2033, reaching USD 48.95 billion by 2033. This growth is driven by the increasing demand for visually engaging and interactive content, technological updates, and the rise of smart cities.[6] The expansion and improvement of urban transport systems, public infrastructure, and commercial buildings are also contributing to the growth of the digital signage market.[7] North America currently dominates the global digital signage market with 37.2% of the market share in 2024. It is expected to maintain its lead in the coming years.[8] The European and Asia-Pacific markets are also experiencing steady growth, supported by various regional factors that are creating new opportunities in the global digital signage market.[9]

The predominant market users of digital signage are restaurants,[10] retailers,[11] office buildings,[12] regional state and local planning authorities, public transport services,[13] educational institutions, and various different industries.

Digital signage continues to grow in the United States, supported by key sectors such as retail, healthcare, and education. 21% increase in the tourism sector by 2022 has led to the widespread adoption of interactive advertising and kiosks in hotels and tourist destinations. It is also becoming more accessible to small and medium-sized enterprises (SMEs), thanks to advancements in OLED, 4K displays, and cloud-based systems, with AI and IoT further enhancing user interaction.

In Europe, digital signage is widely used in retail, government, and tourism sectors. The United Kingdom received 3 million foreign visitors in June 2022, further driving demand for tourist information systems and smart city displays. With the integration of artificial intelligence and energy-efficient technologies, digital signage plays an increasingly diverse role in areas such as traffic management, healthcare, and education.

Digital signage is expanding rapidly across the Asia-Pacific region, with widespread applications in retail, hospitality, and transportation hubs. India’s Smart City programme is over 90% complete, promoting the digital transformation of public information services. As internet availability improves and device costs decline, more small and medium-sized enterprises are adopting digital signage solutions.

In Latin America, digital signage is becoming more widely adopted in the retail and hospitality industries. Retail sales in Mexico reached USD 78.4 billion in 2023, increasing the demand for personalised and dynamic content. With the adoption of cloud-based systems and video displays, digital signage is becoming a practical tool for schools, governments, and businesses to support environmental communication initiatives.

Digital signage is experiencing strong growth in the Middle East and Africa, particularly in the transport, retail, and fast food sectors. 25% increase in food e-commerce in 2021 has contributed to the rise of interactive advertising and digital menu boards. As smart city development progresses, digital signage plays an increasingly important role in public messaging and brand engagement.

Digital signage is used in restaurants through an interactive menu screen that rotates promotional offers. Restaurants use digital signage both indoors and outdoors, with the latter needing a form[14] of weather protection depending on the components of the hardware. Outdoor usage of digital signage is most prevalent in drive-through that allows the customer to browse through the entire menu at a glance while also placing their order with an interactive touchscreen.[15] Indoor digital signage is used for the display of menus. Prior to the integration of digital signage, restaurants manually updated the cafeteria menu, which is in itself a full-time job, especially if the menu is updated daily. With digital signage, restaurants do not have to manually update the menu feed, with live menu feed from digital signage solutions.[buzzword][16] According to a survey conducted by quick-service restaurants and casual restaurant operators, over 20% of restaurant operators experience a 5% sales lift after incorporating digital signage in their service sectors.[17]

Digital signage is widely used in shopping malls as a form of directory or map display. Uses of digital signage include a wayfinding kiosk, enabling the customer to find their path through an interactive touchscreen. Recent digital signage have begun combining interactive advertisement with wayfinding application. This will offer shoppers who interact with the advertisement of the tenant in the shopping mall to the store. Another usage is disseminating relevant information such as the schedule of an event or campaign.[18]

The hospitality industry uses digital signage to display crucial information at a convenient and visible location for all its patron. A digital signage is capable of functioning as a virtual concierge in hotels and as entertainment for conferences during waiting room. Digital signage is also used in hotels as a form of wayfinding, to guide a large group of people for a conference to the correct room. Digital signage is used to provide a simple method to update information that is continuously changing such as expo information.

Digital signage is placed in the lobby, concession stands and displays advertisement before the movie begins. This informs customers about other theater offerings and scheduling, increasing concession sales and gaining other sources of revenue.[citation needed]

Transport is a growing sector for digital signage with practical solutions, such as wayfinding, as well as out-of-home advertising.

One specific use of digital signage is for out-of-home advertising in which video content, advertisements, and/or messages are displayed on digital signs with the goal of delivering targeted messages, to specific locations and/or consumers, at specific times. This is often called ""digital out of home"" (DOOH).[19][20]

Digital signage can be used for school cafeteria menu boards, digitized noticeboards, conference centres[21] and as an element of smart campuses. In April 2022, Visix announced that Wichita State University had adopted the AxisTV Signage Suite digital signage system for its Metroplex Conference Centre. The installation includes a cloud-based content management platform, digital signage players, and e-paper room signs to dynamically display information about campus events.[22]

Interactive digital signage allows end users to interact with digital content via touchscreens, body sensors or QR codes via smartphones.[23]

Digital signs can interact with mobile phones using SMS messaging and Bluetooth. SMS can be used to post messages on the displays, while Bluetooth allows users to interact directly with what they see on screen. In addition to mobile interactivity, networks are also using technology that integrates social and location-based media interactivity. This technology enables end users to upload photos and messages to social networks as well as text messages.

The widespread use of smartphones led to the development of screen–smart device interaction technologies. These allow smartphone users to interact directly with the digital signage screen, for example, participate in a poll, play a game, or share social network content. JPEG images and MPEG4 videos remain the dominant digital content formats for the digital signage industry. For interactive content, HTML5 and Unity3D are widely used due to their popularity among web developers and multimedia designers.

Context-aware digital signage leverages technologies such as sensors, cameras, beacons, RFID technologies, software programs and network connectivity including the Internet of Things (IoT) to monitor the ambient environment, process information and deliver promotional messages based on environmental cues.[24] Many digital signage products include cameras and gather shopper demographic data by estimating the age, gender and economic status of passers-by and use this information to update signage as well as to provide back-end analytics and shopper profiles.

Digital signs rely on a variety of hardware to deliver the content. The components of a typical digital sign installation include one or more display screens, one or more media players, and a content management server. Sometimes two or more of these components are present in a single device but typically there is a display screen, a media player, and a content management server that is connected to the media player over a network. One content management server may support multiple media players and one media player may support multiple screens. Stand-alone digital sign devices combine all three functions in one device and no network connection is needed. Digital signage media players run on a variety of operating systems including Windows, Linux, Android and iOS. 

Rapidly dropping prices for large plasma and LCD screens have led to a growing increase in the number of digital sign installations.[25] An array of these displays is known as a video wall. With the release of the HDMI 2.1 standard in 2017, wall resolution can reach 4K at higher refresh rates, 8K displays, up to 10K in some applications.[26]

Digital signage displays use content management systems and digital media distribution systems which can either be run from personal computers and servers or regional/national media hosting providers. In many digital sign applications, content must be regularly updated to ensure that the correct messages are being displayed. This can either be done manually as and when needed, through a scheduling system, using a data feed from a content provider (e.g. Canadian Press, Data Call Technologies, Bloomberg LP, Thomson Reuters, AHN), or an in-house data source.

Whenever the display, media player and content server are located apart there is a need for audio-video wiring between the display and the media player and between the media player and the content server. The connection from media player to display is normally a VGA, DVI, HDMI or Component video connection. Sometimes this signal is distributed over Cat 5 cables using transmitter and receiver baluns allowing for greater distances between display and player and simplified wiring. The connection from media player to the content server is usually a wired Ethernet connection although some installations use wireless Wi-Fi networking.

Some digital signage with the ability to offer interactive media content will come with a usage reporting function. Each interaction users have made with the digital signage such as the photos that were taken, the number of games that were played will be recorded in the digital signage and produced in the form of a usage report. From the report, owners of the digital signage will be able to gauge the effectiveness of the particular advertisement or media content that was in play at the specific hour based on the number of times interaction has been made. Furthermore, if the digital signage is integrated with kinect, the signage will be able to determine the proximity of the consumer to the display and their demographic details such as age, gender for further analytic and consumer behavior study.

Digital content is managed via display control software. This control software can be a stand-alone dedicated program or integrated with hardware. New messages can be created from an inventory of audio, video, image, graphics, words and phrases which are assembled in different combinations and permutations to yield new messages in real-time.

Digital content displayed on the signage is presented in one of the following formats:

Prior to the advent of digital signage throughout the industries, electronic paper were used as display devices. Electronic paper were used to hold static texts and images indefinitely without electricity. The disadvantage to electronic paper as a form of digital signage is the limited reach of information transmission. Users that need to update the information will need to be in the same retail store, or be within the proximity for shopping malls. This required manual work from store staffs and shopping mall staff in maintaining the device with the latest information.

First generation of digital signage display utilize LED board, projection screens or other emerging display types like interactive surfaces or organic LED screens (OLED). A dot matrix display digital signage will relay the information within a database. All the information must be inputted manually by a person before the message display is updated. This form of digital signage is most commonly used in both train stations, airports, and other areas where information must be conveyed to the mass public. The downside of having dot matrix digital signage is the lack of media player. This digital signage will not be able to play multimedia content.

The second generation of digital signage is able to play multimedia content and is controlled by a centralized management system. Digital audiovisual (av) content is reproduced on TVs and monitor displays of a digital sign network from at least one media player (usually a small computer unit, but DVD players and other types of media sources may also be used). Various hardware and software options exist. These range from portable media players that can output JPG slide shows or loops of MPEG-2 video to networks consisting of multiple players and servers that offer control over enterprise-wide or campus-wide displays at many venues from a single location. The former are ideal for small groups of displays that can be updated via USB flash drive, SD card or CD-ROM. Another option is the use of D.A.N. (Digital Advertising Network) players that connect directly to the monitor and to the internet, to a WAN (Wide Area Network), or to a LAN (Local Area Network). This allows the end user the ability to manage multiple D.A.N. players from any location. The end user can create new advertising or edit existing advertisements and then upload changes to the D.A.N. via the internet or other networking options.

APIs for some digital sign software allow customized content management interfaces through which end-users can manage their content from one location.

More advanced digital sign software allows content to be automatically created by the media players (computers) and servers on a minute-by-minute basis, combining real-time data, from news to weather, prices, transport schedules, etc., with av content to produce the most up-to-date content.[28]

The current generation of digital signage builds onto the previous generation with the added function to interact with the system. Users will be able to interact with the advertisement, scroll through the product menu, or share their information online via the new generation of digital signage. The interactive digital signage opens up interaction and ability to collect more personalize information. Some common uses of interactive digital signage are for users to take a picture and then connect to their Facebook, Twitter, Instagram, or other Social media platform to share the photo they have taken. Products such as people counting or facial recognition are leading to developments where adverts on digital signage boards can change to display gender specific adverts or screens can direct shoppers to specific locations to enable queue management.


"
Tank Installation Services,"The tank services industry exists to assist companies in maintaining their tanks.

Regular maintenance, as well as other services are required for many types of above ground storage tank systems used in the energy and petro-chemical industry.

Some of the areas that tank service companies provide assistance with are:

Companies that specialise in Tank Services, oil tank removals, replacement and installations in the UK follow detailed rules & regulations. OFTEC governs a comprehensive set of guidelines and regulations on fuel tank installations. Tank Servicing companies follow a combination of both Building Regulations and OFTEC Guidelines to provide a fully compliant safe installation.

At the start of any project, an API certified above ground storage tank inspector should be on hand to provide inspection and consultation services, and to ensure compliance with applicable codes and standards in the region. Each tank upgrading project has specific requirements and presents unique challenges. Proper procedure will ensure project safety, tank integrity, and the best utilization of resources and materials. A complete engineering analysis should be performed to ensure that tank components will not be overstressed during lifting and remedial operations. A professional engineer will develop project-specific drawings and procedures, and will supervise each stage of the work.

An analysis of the situation will result in recommendations for the appropriate remedial action, and what repair work should be performed, possibly including but not limited to:

Storage tanks should be updated to meet the highest local environmental standards, including the installation of release prevention barriers and leak detection.

Several different technologies exist for lifting tanks. The conventional methods involve hydraulic jacking equipment, while another method utilizes an airbag lifting system to elevate the tank from its base to allow for remedial action.

Some companies choose to relocate existing tanks, rather than embark on new tank construction. Tank relocation services can be utilized on land or via navigable waterways. There can be cost benefits to relocating an existing tank, depending on variables such as distance and the condition of the tank.

This engineering-related article is a stub. You can help Wikipedia by expanding it."
Residential Communication Equipment Installation,"A residential gateway is a small consumer-grade gateway which bridges network access between connected local area network (LAN) hosts to a wide area network (WAN) (such as the Internet) via a modem, or directly connects to a WAN (as in EttH), while routing. The WAN is a larger computer network, generally operated by an Internet service provider.

The term residential gateway was popularized by Clifford Holliday in 1997 through his paper entitled ""The residential gateway"".[1]

Multiple devices have been described as residential gateways:

A modem (e.g. DSL modem, cable modem) by itself provides none of the functions of a router.[3] It merely allows ATM or PPP or PPPoE traffic to be transmitted across telephone lines, cable wires, optical fibers, wireless radio frequencies, or other physical layers.[4] On the receiving end is another modem that re-converts the transmission format back into digital data packets.[5]
This allows network bridging using telephone, cable, optical, and radio connection methods. The modem also provides handshake protocols, so that the devices on each end of the connection are able to recognize each other.[6] However, a modem generally provides few other network functions.

A cellular wireless access point can function in a similar fashion to a modem. It can allow a direct connection from a home LAN to a WWAN, if a wireless router or access point is present on the WAN as well and tethering is allowed.

Many modems now incorporate the features mentioned below and thus are appropriately described as residential gateways, such as some Internet providers which offer a cable modem router combo.[8]

A residential gateway usually provides

It may also provide other functions such as Dynamic DNS,[13] and converged triple play services such as TV and telephony.

Most gateways are self-contained components, using internally stored firmware. They are generally platform-independent, i.e., they can serve any operating system.

Wireless routers perform the same functions as a wired router and base station, but allow connectivity for wireless devices with the LAN, or as a bridge between the wireless router and another wireless router for a meshnet (the wireless router-wireless router connection can be within the LAN or can be between the LAN and WWAN).[14]

Low-cost production and requirement for user friendliness make gateways vulnerable to network attacks, which resulted in large clusters of such devices being taken over and used to launch DDoS attacks.[15] A majority of the vulnerabilities were present in the web administration frontends of the routers, allowing unauthorized control either via default passwords, vendor backdoors, or web vulnerabilities.[16]
"
Commercial Communication Equipment Installation,"
In telecommunications, a customer-premises equipment or customer-provided equipment (CPE) is any terminal and associated equipment located at a subscriber's premises and connected with a carrier's telecommunication circuit at the demarcation point (""demarc""). The demarc is a point established in a building or complex to separate customer equipment from the equipment located in either the distribution infrastructure or central office of the communications service provider.

CPE generally refers to devices such as telephones, routers, network switches, residential gateways (RG), set-top boxes, fixed mobile convergence products, home networking adapters and Internet access gateways that enable consumers to access providers' communication services and distribute them in a residence or enterprise with a local area network (LAN).

A CPE can be an active equipment, as the ones mentioned above, or passive equipment such as analog telephone adapters (ATA) or xDSL-splitters. This includes key telephone systems and most private branch exchanges. Excluded from the CPE category are overvoltage protection equipment and pay telephones. Other types of materials that are necessary for the delivery of the telecommunication service, but are not defined as equipment, such as manuals and cable packages, and cable adapters are instead referred to as CPE-peripherals.

CPE can refer to devices purchased by the subscriber, or to those provided by the operator or service provider.

The two phrases, ""customer-premises equipment"" and ""customer-provided equipment"", reflect the history of this equipment.

Under the Bell System monopoly in the United States (post Communications Act of 1934), the Bell System owned the telephones, and one could not attach privately owned or supplied devices to the network, or to the station apparatus. Telephones were located on customers' premises, hence, customer-premises equipment.  In the U.S. Federal Communications Commission (FCC) proceeding the Second Computer Inquiry, the FCC ruled that telecommunications carriers could no longer bundle CPE with telecommunications service, uncoupling the market power of the telecommunications service monopoly from the CPE market, and creating a competitive CPE market.[1]

With the gradual breakup of the Bell monopoly, starting with Hush-A-Phone v. United States [1956], which allowed some non-Bell owned equipment to be connected to the network (a process called interconnection), equipment on customers' premises became increasingly owned by customers. Indeed, subscribers were eventually permitted to purchase telephones – hence, customer-provided equipment.

In the pay-TV industry many operators and service providers offer subscribers a set-top box with which to receive video services, in return for a monthly fee.  As offerings have evolved to include multiple services [voice and data] operators have increasingly given consumers the opportunity to rent or buy additional devices like access modems, internet gateways and video extenders that enable them to access multiple services, and distribute them to a range of consumer electronics devices in the home.

The growth of multiple system operators, offering triple or quad-play services, required the development of hybrid CPE to make it easy for subscribers to access voice, video and data services. The development of this technology was led by Pay TV operators looking for a way to deliver video services via both traditional broadcast and broadband IP networks.  Spain's Telefonica was the first operator to launch a hybrid broadcast and broadband TV service in 2003 with its Movistar TV DTT/IPTV offering,[2] while Polish satellite operator 'n' was the first to offer its subscribers a Three-way hybrid (or Tri-brid) broadcast and broadband TV service,[3] which launched in 2009

The term set-back box is used in the digital TV industry to describe a piece of consumer hardware that enables them to access both linear broadcast and internet-based video content, plus a range of interactive services like Electronic Programme Guides (EPG), Pay Per View (PPV) and video on demand (VOD) as well as internet browsing, and view them on a large screen television set. Unlike standard set-top boxes, which sit on top of or below the TV, a set-back box has a smaller form factor to enable it to be mounted to the rear of the display panel flat panel TV, hiding it from view.

A residential gateway is a networking device used to connect devices in the home to the Internet or other wide area network (WAN).
It is an umbrella term, used to cover multi-function networking appliances used in homes, which may combine a DSL modem or cable modem, a network switch, a consumer-grade router, and a wireless access point. In the past, such functions were provided by separate devices, but in recent years technological convergence has enabled multiple functions to be merged into a single device.

One of the first home gateway devices to be launched was selected by Telecom Italia to enable the operator to offer triple play services in 2002: Along with a SIP VoIP handset for making voice calls, it enabled subscribers to access voice, video and data services over a 10MB symmetrical ADSL fiber connection.

The virtual gateway concept enables consumers to access video and data services and distribute them around their homes using software rather than hardware.  The first virtual gateway was introduced in 2010 by Advanced Digital Broadcast at the IBC exhibition in Amsterdam.[4][5][6] The ADB Virtual Gateway uses software that resides within the middleware and is based on open standards, including DLNA home networking and the DTCP-IP standard, to ensure that all content, including paid-for encrypted content like Pay TV services, can only be accessed by secure CE devices.[7]

A subscriber unit, or SU is a broadband radio that is installed at a business or residential location to connect to an access point to send/receive high speed data wired or wirelessly.  Devices commonly referred to as a subscriber unit include cable modems, access gateways, home networking adapters and mobile phones. Example brands and vendors include SpeedTouch, DrayTek, Ubee Interactive, 2Wire and Efficient Networks.

CPE may also refer to any devices that terminate a WAN circuit, such as an ISDN, E-carrier/T-carrier, DSL, or metro Ethernet. This includes any customer-owned hardware at the customer's site: routers, firewalls, network switches, PBXs, VoIP gateways, sometimes CSU/DSU and modems.

Application areas
"
Low-Rise Glass Installation,"Architectural glass is glass that is used as a building material. It is most typically used as transparent glazing material in the building envelope, including windows in the external walls. Glass is also used for internal partitions and as an architectural feature. When used in buildings, glass is often of a safety type, which include reinforced, toughened and laminated glasses.

Glass casting is the process in which glass objects are cast by directing molten glass into a mould where it solidifies. The technique has been used since the Egyptian period. Modern cast glass is formed by a variety of processes such as kiln casting, or casting into sand, graphite or metal moulds. Cast glass windows, albeit with poor optical qualities, began to appear in the most important buildings in Rome and the most luxurious villas of Herculaneum and Pompeii.[10]

One of the earliest methods of glass window manufacture was the crown glass method. Hot blown glass was cut open opposite the pipe, then rapidly spun on a table before it could cool. Centrifugal force shaped the hot globe of glass into a round, flat sheet. The sheet would then be broken off the pipe and trimmed to form a rectangular window to fit into a frame.

At the center of a piece of crown glass, a thick remnant of the original blown bottle neck would remain, hence the name ""bullseye"". Optical distortions produced by the bullseye could be reduced by grinding the glass. The development of diaper latticed windows was in part because three regular diamond-shaped panes could be conveniently cut from a piece of Crown glass, with minimum waste and with minimum distortion.

This method for manufacturing flat glass panels was very expensive and could not be used to make large panes. It was replaced in the 19th century by the cylinder, sheet, and rolled plate processes, but it is still used in traditional construction and restoration.

In this manufacturing process, glass is blown into a cylindrical iron mould. The ends are cut off and a cut is made down the side of the cylinder. The cut cylinder is then placed in an oven where the cylinder unrolls into a flat glass sheets.

Drawn Sheet glass was made by dipping a leader into a vat of molten glass then pulling that leader straight up while a film of glass hardened just out of the vat – this is known as the Fourcault process.  This film or ribbon was pulled up continuously held by tractors on both edges while it cooled.  After 12 metres or so it was cut off the vertical ribbon and tipped down to be further cut.  This glass is clear but has thickness variations due to small temperature changes just out of the vat as it was hardening. These variations cause lines of slight distortions. This glass may still be seen in older houses.  Float glass replaced this process.

Irving Wightman Colburn developed a similar method independently. He began experimenting with the method in 1899, and started production in 1906. He went bankrupt, but was bought by Michael Joseph Owens. Because the method was imperfect, they kept refining it till 1916 when they felt it was perfect, and opened a glass factory based on the technology the year after.[11]

In 1838, James Hartley was granted a patent for Hartley's Patent Rolled Plate, manufactured by a new cast glass process. The glass is taken from the furnace in large iron ladles, which are carried upon slings running on overhead rails; from the ladle the glass is thrown upon the cast-iron bed of a rolling-table; and is rolled into sheet by an iron roller, the process being similar to that employed in making plate-glass, but on a smaller scale. The sheet thus rolled is roughly trimmed while hot and soft, so as to remove those portions of glass which have been spoiled by immediate contact with the ladle, and the sheet, still soft, is pushed into the open mouth of an annealing tunnel or temperature-controlled oven called a lehr, down which it is carried by a system of rollers.

The polished plate glass process starts with sheet or rolled plate glass.  This glass is dimensionally inaccurate and often created visual distortions.  These rough panes were ground flat and then polished clear.  This was a fairly expensive process.

Before the float process, mirrors were plate glass as sheet glass had visual distortions that were akin to those seen in amusement park or funfair mirrors.

In 1918 the Belgian engineer Emil Bicheroux improved the plate glass manufacturing by pouring molten glass between two rollers, which resulted in more even thickness and fewer undulations, and reduced the need for grinding and polishing. This process was further improved in the US.[12]

The elaborate patterns found on figured (or 'Cathedral') rolled-plate glass are produced in a similar fashion to the rolled plate glass process except that the plate is cast between two rollers, one of which carries a pattern. On occasion, both rollers can carry a pattern. The pattern is impressed upon the sheet by a printing roller which is brought down upon the glass as it leaves the main rolls while still soft. This glass shows a pattern in high relief. The glass is then annealed in a lehr.

The glass used for this purpose is typically whiter in colour than the clear glasses used for other applications.

Only some of the figured glasses may be toughened, dependent on the depth of the embossed pattern.  Single rolled figured glass, where the pattern is only imprinted into one surface, may be laminated to produce a safety glass.  The much less common 'double rolled figured glass', where the pattern is embossed into both surfaces, can not be made into a safety glass but will already be thicker than average figured plate to accommodate both patterned faces.  The finished thickness being dependent on the imprinted design.

Ninety percent of the world's flat glass is produced by the float glass process[citation needed] invented in the 1950s by Sir Alastair Pilkington of Pilkington Glass, in which molten glass is poured onto one end of a molten tin bath. The glass floats on the tin, and levels out as it spreads along the bath, giving a smooth face to both sides. The glass cools and slowly solidifies as it travels over the molten tin and leaves the tin bath in a continuous ribbon. The glass is then annealed by cooling in an oven called a lehr. The finished product has near-perfect parallel surfaces.

The side of the glass that has been in contact with the tin has a very small amount of the tin embedded in its surface. This quality makes that side of the glass easier to be coated in order to turn it into a mirror, however that side is also softer and easier to scratch.

Glass is produced in standard metric thicknesses of 2, 3, 4, 5, 6, 8, 10, 12, 15, 19 and 25 mm, with 10mm being the most popular sizing in the architectural industry. Molten glass floating on tin in a nitrogen/hydrogen atmosphere will spread out to a thickness of about 6 mm and stop due to surface tension. Thinner glass is made by stretching the glass while it floats on the tin and cools. Similarly, thicker glass is pushed back and not permitted to expand as it cools on the tin.

Toughened (or tempered) glass is made from standard Float Glass to create an impact resistant, safety glass. Broken float glass yields sharp, hazardous shards. The toughening process introduces tensions between internal and external surfaces to increase its strength and ensure in the case of breakages the glass shatters into small, harmless pieces of glass. The cut glass panels are put into a toughening furnace. Here the glass panels are heated to upward of 600 degrees C and then the surfaces are cooled rapidly with cold air. This produces tensile stresses on the surface of the glass with the warmer internal glass particles. As the top thickness of the glass cools it contracts and forces the corresponding glass elements to contract to introduce stresses into the glass panel and increasing strength.[13]

Prism glass is architectural glass which bends light. It was frequently used around the turn of the 20th century to provide natural light to underground spaces and areas far from windows.[14] Prism glass can be found on sidewalks, where it is known as vault lighting,[15] in windows, partitions, and canopies, where it is known as prism tiles, and as deck prisms, which were used to light spaces below deck on sailing ships. It could be highly ornamented; Frank Lloyd Wright created over forty different designs for prism tiles.[16] Modern architectural prism lighting is generally done with a plastic film applied to ordinary window glass.
[17]

Glass block, also known as glass brick, is an architectural element made from glass used in areas where privacy or visual obscuration is desired while admitting light, such as underground parking garages, washrooms, and municipal swimming baths. Glass block was originally developed in the early 1900s to provide natural light in industrial factories.

Annealed glass is glass without internal stresses caused by heat treatment, i.e., rapid cooling, or by toughening or heat strengthening. Glass becomes annealed if it is heated above a transition point then allowed to cool slowly, without being quenched. Float glass is annealed during the process of manufacture. However, most toughened glass is made from float glass that has been specially heat-treated.

Annealed glass breaks into large, jagged shards that can cause serious injury and is considered a hazard in architectural applications. Building codes in many parts of the world restrict the use of annealed glass in areas where there is a high risk of breakage and injury, for example in bathrooms, door panels, fire exits and at low heights in schools or domestic houses. Safety glass, such as laminated or tempered must be used in these settings to reduce risk of injury.

Laminated glass is manufactured by bonding two or more layers of glass together with an interlayer, such as PVB, under heat and pressure, to create a single sheet of glass. When broken, the interlayer keeps the layers of glass bonded and prevents it from breaking apart. The interlayer can also give the glass a higher sound insulation rating.

There are several types of laminated glasses manufactured using different types of glass and interlayers which produce different results when broken.

Laminated glass that is made up of annealed glass is normally used when safety is a concern, but tempering is not an option. Windshields are typically laminated glasses. When broken, the PVB layer prevents the glass from breaking apart, creating a ""spider web"" cracking pattern.

Tempered laminated glass is designed to shatter into small pieces, preventing possible injury. When both pieces of glass are broken it produces a ""wet blanket"" effect and it will fall out of its opening.

Heat strengthened laminated glass is stronger than annealed, but not as strong as tempered. It is often used where security is a concern. It has a larger break pattern than tempered, but because it holds its shape (unlike the ""wet blanket"" effect of tempered laminated glass) it remains in the opening and can withstand more force for a longer period of time, making it much more difficult to get through.

Laminated glass has similar properties to ballistic glass, but the two should not be confused. Both are made using a PVB interlayer, but they have drastically different tensile strength. Ballistic glass and laminated glass are both rated to different standards and have a different shatter pattern.[18]

Heat-strengthened glass, or tempered glass, is glass that has been heat treated to induce surface compression, but not to the extent of causing it to ""dice"" on breaking in the manner of tempered glass. On breaking, heat-strengthened glass breaks into sharp pieces that are typically somewhat smaller than those found on breaking annealed glass, and is intermediate in strength between annealed and toughened glasses.

Heat-strengthened glass can take a strong direct hit without shattering, but has a weak edge. By simply tapping the edge of heat-strengthened glass with a solid object, it is possible to shatter the entire sheet.

Chemically strengthened glass is a type of glass that has increased strength.  When broken it still shatters in long pointed splinters similar to float (annealed) glass.  For this reason, it is not considered a safety glass and must be laminated if a safety glass is required. Chemically strengthened glass is typically six to eight times the strength of annealed glass.

The glass is chemically strengthened by submerging the glass in a bath containing a potassium salt (typically potassium nitrate) at 450 °C (842 °F). This causes sodium ions in the glass surface to be replaced by potassium ions from the bath solution.

Unlike toughened glass, chemically strengthened glass may be cut after strengthening, but loses its added strength within the region of approximately 20 mm of the cut.  Similarly, when the surface of chemically strengthened glass is deeply scratched, this area loses its additional strength.

Chemically strengthened glass was used on some fighter aircraft canopies.

Glass coated with a low-emissivity substance can reflect radiant infrared energy, encouraging radiant heat to remain on the same side of the glass from which it originated, while letting visible light pass. This often results in more efficient windows because radiant heat originating from indoors in winter is reflected back inside, while infrared heat radiation from the sun during summer is reflected away, keeping it cooler inside.

Electrically heatable glass is a relatively new product, which helps to find solutions while designing buildings and vehicles.
The idea of heating glass is based on usage of energy-efficient low-emissive glass that is generally simple silicate glass with special metallic oxides coating. Heatable glass can be used in all kinds of standard glazing systems, made of wood, plastic, aluminum or steel.

A recent (2001 Pilkington Glass) innovation is so-called self-cleaning glass, aimed at building, automotive and other technical applications. A nanometre-scale coating of titanium dioxide on the outer surface of glass introduces two mechanisms which lead to the self-cleaning property. The first is a photo-catalytic effect, in which ultra-violet rays catalyse the breakdown of organic compounds on the window surface; the second is a hydrophilic effect in which water is attracted to the surface of the glass, forming a thin sheet which washes away the broken-down organic compounds.

Insulating glass, or double glazing, consists of a window or glazing element of two or more layers of glazing separated by a spacer along the edge and sealed to create a dead air space between the layers.  This type of glazing has functions of thermal insulation and noise reduction. When the space is filled with an inert gas it is part of energy conservation sustainable architecture design for low energy buildings.

A 1994 innovation for insulated glazing is evacuated glass, which as yet is produced commercially only in Japan and China.[19] The extreme thinness of evacuated glazing offers many new architectural possibilities, particularly in building conservation and historicist architecture, where evacuated glazing can replace traditional single glazing, which is much less energy-efficient.

An evacuated glazing unit is made by sealing the edges of two glass sheets, typically by using a solder glass, and evacuating the space inside with a vacuum pump. The evacuated space between the two sheets can be very shallow and yet be a good insulator, yielding insulative window glass with nominal thicknesses as low as 6 mm overall. The reasons for this low thickness are deceptively complex, but the potential insulation is good essentially because there can be no convection or gaseous conduction in a vacuum.

Unfortunately, evacuated glazing does have some disadvantages; its manufacture is complicated and difficult. For example, a necessary stage in the manufacture of evacuated glazing is outgassing; that is, heating it to liberate any gases adsorbed on the inner surfaces, which could otherwise later escape and destroy the vacuum. This heating process currently means that evacuated glazing cannot be toughened or heat-strengthened. If an evacuated safety glass is required, the glass must be laminated. The high temperatures necessary for outgassing also tend to destroy the highly effective ""soft"" low-emissivity coatings that are often applied to one or both of the internal surfaces (i.e. the ones facing the air gap) of other forms of modern insulative glazing, in order to prevent loss of heat through infrared radiation. Slightly less effective ""hard"" coatings are still suitable for evacuated glazing, however.

Furthermore, because of the atmospheric pressure present on the outside of an evacuated glazing unit, its two glass sheets must somehow be held apart in order to prevent them flexing together and touching each other, which would defeat the object of evacuating the unit. The task of holding the panes apart is performed by a grid of spacers, which typically consist of small stainless steel discs that are placed around 20 mm apart. The spacers are small enough that they are visible only at very close distances, typically up to 1 m. However, the fact that the spacers will conduct some heat often leads in cold weather to the formation of temporary, grid-shaped patterns on the surface of an evacuated window, consisting either of small circles of interior condensation centred around the spacers, where the glass is slightly colder than average, or, when there is dew outside, small circles on the exterior face of the glass, in which the dew is absent because the spacers make the glass near them slightly warmer.

The conduction of heat between the panes, caused by the spacers, tends to limit evacuated glazing's overall insulative effectiveness. Nevertheless, evacuated glazing is still as insulative as much thicker conventional double glazing and tends to be stronger, since the two constituent glass sheets are pressed together by the atmosphere, and hence react practically as one thick sheet to bending forces. Evacuated glazing also offers very good sound insulation in comparison with other popular types of window glazing.

One type of heat reduction glass uses radiative cooling. This glass includes a 1.2 micron-thick transparent radiative cooler (TRC) layer of silica, alumina, and titanium oxide upon glass coated with contact lens polymer. The layer permits only visible light to cross, cutting buildings’ cooling costs by as much as one-third. The developers used machine learning and quantum computing to rapidly test models and identify the best alternative.[20]

The most current building code enforced in most jurisdictions in the United States is the 2006 International Building Code (IBC, 2006). The 2006 IBC references for the 2005 edition of the standard Minimum Design Loads for buildings and other Structures prepared by the American Society of Civil Engineers (ASCE, 2005) for its seismic provisions. ASCE 7-05 contains specific requirements for nonstructural components including requirements for architectural glass.[21]

If incorrectly designed, concave surfaces with extensive amounts of glass can act as solar concentrators depending on the angle of the sun, potentially injuring people and damaging property.[22]
"
High-Rise Glass Installation,"Architectural glass is glass that is used as a building material. It is most typically used as transparent glazing material in the building envelope, including windows in the external walls. Glass is also used for internal partitions and as an architectural feature. When used in buildings, glass is often of a safety type, which include reinforced, toughened and laminated glasses.

Glass casting is the process in which glass objects are cast by directing molten glass into a mould where it solidifies. The technique has been used since the Egyptian period. Modern cast glass is formed by a variety of processes such as kiln casting, or casting into sand, graphite or metal moulds. Cast glass windows, albeit with poor optical qualities, began to appear in the most important buildings in Rome and the most luxurious villas of Herculaneum and Pompeii.[10]

One of the earliest methods of glass window manufacture was the crown glass method. Hot blown glass was cut open opposite the pipe, then rapidly spun on a table before it could cool. Centrifugal force shaped the hot globe of glass into a round, flat sheet. The sheet would then be broken off the pipe and trimmed to form a rectangular window to fit into a frame.

At the center of a piece of crown glass, a thick remnant of the original blown bottle neck would remain, hence the name ""bullseye"". Optical distortions produced by the bullseye could be reduced by grinding the glass. The development of diaper latticed windows was in part because three regular diamond-shaped panes could be conveniently cut from a piece of Crown glass, with minimum waste and with minimum distortion.

This method for manufacturing flat glass panels was very expensive and could not be used to make large panes. It was replaced in the 19th century by the cylinder, sheet, and rolled plate processes, but it is still used in traditional construction and restoration.

In this manufacturing process, glass is blown into a cylindrical iron mould. The ends are cut off and a cut is made down the side of the cylinder. The cut cylinder is then placed in an oven where the cylinder unrolls into a flat glass sheets.

Drawn Sheet glass was made by dipping a leader into a vat of molten glass then pulling that leader straight up while a film of glass hardened just out of the vat – this is known as the Fourcault process.  This film or ribbon was pulled up continuously held by tractors on both edges while it cooled.  After 12 metres or so it was cut off the vertical ribbon and tipped down to be further cut.  This glass is clear but has thickness variations due to small temperature changes just out of the vat as it was hardening. These variations cause lines of slight distortions. This glass may still be seen in older houses.  Float glass replaced this process.

Irving Wightman Colburn developed a similar method independently. He began experimenting with the method in 1899, and started production in 1906. He went bankrupt, but was bought by Michael Joseph Owens. Because the method was imperfect, they kept refining it till 1916 when they felt it was perfect, and opened a glass factory based on the technology the year after.[11]

In 1838, James Hartley was granted a patent for Hartley's Patent Rolled Plate, manufactured by a new cast glass process. The glass is taken from the furnace in large iron ladles, which are carried upon slings running on overhead rails; from the ladle the glass is thrown upon the cast-iron bed of a rolling-table; and is rolled into sheet by an iron roller, the process being similar to that employed in making plate-glass, but on a smaller scale. The sheet thus rolled is roughly trimmed while hot and soft, so as to remove those portions of glass which have been spoiled by immediate contact with the ladle, and the sheet, still soft, is pushed into the open mouth of an annealing tunnel or temperature-controlled oven called a lehr, down which it is carried by a system of rollers.

The polished plate glass process starts with sheet or rolled plate glass.  This glass is dimensionally inaccurate and often created visual distortions.  These rough panes were ground flat and then polished clear.  This was a fairly expensive process.

Before the float process, mirrors were plate glass as sheet glass had visual distortions that were akin to those seen in amusement park or funfair mirrors.

In 1918 the Belgian engineer Emil Bicheroux improved the plate glass manufacturing by pouring molten glass between two rollers, which resulted in more even thickness and fewer undulations, and reduced the need for grinding and polishing. This process was further improved in the US.[12]

The elaborate patterns found on figured (or 'Cathedral') rolled-plate glass are produced in a similar fashion to the rolled plate glass process except that the plate is cast between two rollers, one of which carries a pattern. On occasion, both rollers can carry a pattern. The pattern is impressed upon the sheet by a printing roller which is brought down upon the glass as it leaves the main rolls while still soft. This glass shows a pattern in high relief. The glass is then annealed in a lehr.

The glass used for this purpose is typically whiter in colour than the clear glasses used for other applications.

Only some of the figured glasses may be toughened, dependent on the depth of the embossed pattern.  Single rolled figured glass, where the pattern is only imprinted into one surface, may be laminated to produce a safety glass.  The much less common 'double rolled figured glass', where the pattern is embossed into both surfaces, can not be made into a safety glass but will already be thicker than average figured plate to accommodate both patterned faces.  The finished thickness being dependent on the imprinted design.

Ninety percent of the world's flat glass is produced by the float glass process[citation needed] invented in the 1950s by Sir Alastair Pilkington of Pilkington Glass, in which molten glass is poured onto one end of a molten tin bath. The glass floats on the tin, and levels out as it spreads along the bath, giving a smooth face to both sides. The glass cools and slowly solidifies as it travels over the molten tin and leaves the tin bath in a continuous ribbon. The glass is then annealed by cooling in an oven called a lehr. The finished product has near-perfect parallel surfaces.

The side of the glass that has been in contact with the tin has a very small amount of the tin embedded in its surface. This quality makes that side of the glass easier to be coated in order to turn it into a mirror, however that side is also softer and easier to scratch.

Glass is produced in standard metric thicknesses of 2, 3, 4, 5, 6, 8, 10, 12, 15, 19 and 25 mm, with 10mm being the most popular sizing in the architectural industry. Molten glass floating on tin in a nitrogen/hydrogen atmosphere will spread out to a thickness of about 6 mm and stop due to surface tension. Thinner glass is made by stretching the glass while it floats on the tin and cools. Similarly, thicker glass is pushed back and not permitted to expand as it cools on the tin.

Toughened (or tempered) glass is made from standard Float Glass to create an impact resistant, safety glass. Broken float glass yields sharp, hazardous shards. The toughening process introduces tensions between internal and external surfaces to increase its strength and ensure in the case of breakages the glass shatters into small, harmless pieces of glass. The cut glass panels are put into a toughening furnace. Here the glass panels are heated to upward of 600 degrees C and then the surfaces are cooled rapidly with cold air. This produces tensile stresses on the surface of the glass with the warmer internal glass particles. As the top thickness of the glass cools it contracts and forces the corresponding glass elements to contract to introduce stresses into the glass panel and increasing strength.[13]

Prism glass is architectural glass which bends light. It was frequently used around the turn of the 20th century to provide natural light to underground spaces and areas far from windows.[14] Prism glass can be found on sidewalks, where it is known as vault lighting,[15] in windows, partitions, and canopies, where it is known as prism tiles, and as deck prisms, which were used to light spaces below deck on sailing ships. It could be highly ornamented; Frank Lloyd Wright created over forty different designs for prism tiles.[16] Modern architectural prism lighting is generally done with a plastic film applied to ordinary window glass.
[17]

Glass block, also known as glass brick, is an architectural element made from glass used in areas where privacy or visual obscuration is desired while admitting light, such as underground parking garages, washrooms, and municipal swimming baths. Glass block was originally developed in the early 1900s to provide natural light in industrial factories.

Annealed glass is glass without internal stresses caused by heat treatment, i.e., rapid cooling, or by toughening or heat strengthening. Glass becomes annealed if it is heated above a transition point then allowed to cool slowly, without being quenched. Float glass is annealed during the process of manufacture. However, most toughened glass is made from float glass that has been specially heat-treated.

Annealed glass breaks into large, jagged shards that can cause serious injury and is considered a hazard in architectural applications. Building codes in many parts of the world restrict the use of annealed glass in areas where there is a high risk of breakage and injury, for example in bathrooms, door panels, fire exits and at low heights in schools or domestic houses. Safety glass, such as laminated or tempered must be used in these settings to reduce risk of injury.

Laminated glass is manufactured by bonding two or more layers of glass together with an interlayer, such as PVB, under heat and pressure, to create a single sheet of glass. When broken, the interlayer keeps the layers of glass bonded and prevents it from breaking apart. The interlayer can also give the glass a higher sound insulation rating.

There are several types of laminated glasses manufactured using different types of glass and interlayers which produce different results when broken.

Laminated glass that is made up of annealed glass is normally used when safety is a concern, but tempering is not an option. Windshields are typically laminated glasses. When broken, the PVB layer prevents the glass from breaking apart, creating a ""spider web"" cracking pattern.

Tempered laminated glass is designed to shatter into small pieces, preventing possible injury. When both pieces of glass are broken it produces a ""wet blanket"" effect and it will fall out of its opening.

Heat strengthened laminated glass is stronger than annealed, but not as strong as tempered. It is often used where security is a concern. It has a larger break pattern than tempered, but because it holds its shape (unlike the ""wet blanket"" effect of tempered laminated glass) it remains in the opening and can withstand more force for a longer period of time, making it much more difficult to get through.

Laminated glass has similar properties to ballistic glass, but the two should not be confused. Both are made using a PVB interlayer, but they have drastically different tensile strength. Ballistic glass and laminated glass are both rated to different standards and have a different shatter pattern.[18]

Heat-strengthened glass, or tempered glass, is glass that has been heat treated to induce surface compression, but not to the extent of causing it to ""dice"" on breaking in the manner of tempered glass. On breaking, heat-strengthened glass breaks into sharp pieces that are typically somewhat smaller than those found on breaking annealed glass, and is intermediate in strength between annealed and toughened glasses.

Heat-strengthened glass can take a strong direct hit without shattering, but has a weak edge. By simply tapping the edge of heat-strengthened glass with a solid object, it is possible to shatter the entire sheet.

Chemically strengthened glass is a type of glass that has increased strength.  When broken it still shatters in long pointed splinters similar to float (annealed) glass.  For this reason, it is not considered a safety glass and must be laminated if a safety glass is required. Chemically strengthened glass is typically six to eight times the strength of annealed glass.

The glass is chemically strengthened by submerging the glass in a bath containing a potassium salt (typically potassium nitrate) at 450 °C (842 °F). This causes sodium ions in the glass surface to be replaced by potassium ions from the bath solution.

Unlike toughened glass, chemically strengthened glass may be cut after strengthening, but loses its added strength within the region of approximately 20 mm of the cut.  Similarly, when the surface of chemically strengthened glass is deeply scratched, this area loses its additional strength.

Chemically strengthened glass was used on some fighter aircraft canopies.

Glass coated with a low-emissivity substance can reflect radiant infrared energy, encouraging radiant heat to remain on the same side of the glass from which it originated, while letting visible light pass. This often results in more efficient windows because radiant heat originating from indoors in winter is reflected back inside, while infrared heat radiation from the sun during summer is reflected away, keeping it cooler inside.

Electrically heatable glass is a relatively new product, which helps to find solutions while designing buildings and vehicles.
The idea of heating glass is based on usage of energy-efficient low-emissive glass that is generally simple silicate glass with special metallic oxides coating. Heatable glass can be used in all kinds of standard glazing systems, made of wood, plastic, aluminum or steel.

A recent (2001 Pilkington Glass) innovation is so-called self-cleaning glass, aimed at building, automotive and other technical applications. A nanometre-scale coating of titanium dioxide on the outer surface of glass introduces two mechanisms which lead to the self-cleaning property. The first is a photo-catalytic effect, in which ultra-violet rays catalyse the breakdown of organic compounds on the window surface; the second is a hydrophilic effect in which water is attracted to the surface of the glass, forming a thin sheet which washes away the broken-down organic compounds.

Insulating glass, or double glazing, consists of a window or glazing element of two or more layers of glazing separated by a spacer along the edge and sealed to create a dead air space between the layers.  This type of glazing has functions of thermal insulation and noise reduction. When the space is filled with an inert gas it is part of energy conservation sustainable architecture design for low energy buildings.

A 1994 innovation for insulated glazing is evacuated glass, which as yet is produced commercially only in Japan and China.[19] The extreme thinness of evacuated glazing offers many new architectural possibilities, particularly in building conservation and historicist architecture, where evacuated glazing can replace traditional single glazing, which is much less energy-efficient.

An evacuated glazing unit is made by sealing the edges of two glass sheets, typically by using a solder glass, and evacuating the space inside with a vacuum pump. The evacuated space between the two sheets can be very shallow and yet be a good insulator, yielding insulative window glass with nominal thicknesses as low as 6 mm overall. The reasons for this low thickness are deceptively complex, but the potential insulation is good essentially because there can be no convection or gaseous conduction in a vacuum.

Unfortunately, evacuated glazing does have some disadvantages; its manufacture is complicated and difficult. For example, a necessary stage in the manufacture of evacuated glazing is outgassing; that is, heating it to liberate any gases adsorbed on the inner surfaces, which could otherwise later escape and destroy the vacuum. This heating process currently means that evacuated glazing cannot be toughened or heat-strengthened. If an evacuated safety glass is required, the glass must be laminated. The high temperatures necessary for outgassing also tend to destroy the highly effective ""soft"" low-emissivity coatings that are often applied to one or both of the internal surfaces (i.e. the ones facing the air gap) of other forms of modern insulative glazing, in order to prevent loss of heat through infrared radiation. Slightly less effective ""hard"" coatings are still suitable for evacuated glazing, however.

Furthermore, because of the atmospheric pressure present on the outside of an evacuated glazing unit, its two glass sheets must somehow be held apart in order to prevent them flexing together and touching each other, which would defeat the object of evacuating the unit. The task of holding the panes apart is performed by a grid of spacers, which typically consist of small stainless steel discs that are placed around 20 mm apart. The spacers are small enough that they are visible only at very close distances, typically up to 1 m. However, the fact that the spacers will conduct some heat often leads in cold weather to the formation of temporary, grid-shaped patterns on the surface of an evacuated window, consisting either of small circles of interior condensation centred around the spacers, where the glass is slightly colder than average, or, when there is dew outside, small circles on the exterior face of the glass, in which the dew is absent because the spacers make the glass near them slightly warmer.

The conduction of heat between the panes, caused by the spacers, tends to limit evacuated glazing's overall insulative effectiveness. Nevertheless, evacuated glazing is still as insulative as much thicker conventional double glazing and tends to be stronger, since the two constituent glass sheets are pressed together by the atmosphere, and hence react practically as one thick sheet to bending forces. Evacuated glazing also offers very good sound insulation in comparison with other popular types of window glazing.

One type of heat reduction glass uses radiative cooling. This glass includes a 1.2 micron-thick transparent radiative cooler (TRC) layer of silica, alumina, and titanium oxide upon glass coated with contact lens polymer. The layer permits only visible light to cross, cutting buildings’ cooling costs by as much as one-third. The developers used machine learning and quantum computing to rapidly test models and identify the best alternative.[20]

The most current building code enforced in most jurisdictions in the United States is the 2006 International Building Code (IBC, 2006). The 2006 IBC references for the 2005 edition of the standard Minimum Design Loads for buildings and other Structures prepared by the American Society of Civil Engineers (ASCE, 2005) for its seismic provisions. ASCE 7-05 contains specific requirements for nonstructural components including requirements for architectural glass.[21]

If incorrectly designed, concave surfaces with extensive amounts of glass can act as solar concentrators depending on the angle of the sun, potentially injuring people and damaging property.[22]
"
Industrial Machinery Installation,"The machine industry or machinery industry is a subsector of the industry, that produces and maintains machines for consumers, the industry, and most other companies in the economy.

This machine industry traditionally belongs to the heavy industry. Nowadays, many smaller industrial manufacturing companies in this branch are considered part of the light industry. Most machine tool manufacturers in the machinery industry are called machine factories.

The machine industry is a subsector of the industry that produces a range of products from power tools, different types of machines, and domestic technology to factory equipment etc. On the one hand the machine industry provides:

These means of production are called capital goods, because a certain amount of capital is invested. Much of those production machines require regular maintenance, which becomes supplied specialized companies in the machine industry.

On the other end the machinery industry supplies consumer goods, including kitchen appliances, refrigerators, washers, dryers and a like. Production of radio and television, however, is generally considered belonging to the electrical equipment industry. The machinery industry itself is a major customer of the steel industry.

The production of the machinery industry varies widely from single-unit production and series production to mass production.[1] Single-unit production is about constructing unique products, which are specified in specific customer requirements. Due to modular design such devices and machines can often be manufactured in small series, which significantly reduces the costs. From a certain stage in the production, the specific customer requirements are built-in, and the unique product is created.[1]

The machinery industry came into existence during the Industrial Revolution. Companies in this emerging field grew out of iron foundries, shipyards, forges and repair shops.[2] Often companies were a combination of machine factory and shipyard. Early in the 20th century several motorcycle and automobile manufacturers began their own machine factories.

Prior to the industrial revolution a variety of machines existed such as clocks, weapons and running gear for mills (watermill, windmill, horse mill etc.) Production of these machines were on much smaller scale in artisan workshops mostly for the local or regional market. With the advent of the industrial revolution manufacturing began of composite tools with more complex construction, such as steam engines and steam generators for the evolving industry and transport.[2] In addition, the emerging machine factories started making machines for production machines as textile machinery, compressors, agricultural machinery, and engines for ships.

During the first decades of the industrial revolution in England, from 1750, there was a concentration of labor usually in not yet mechanized factories. Many new machines were invented, which were initially made by the inventors themselves. Early in the 18th century, the first steam engines, the Newcomen engine, came into use throughout Britain and Europe, principally to pump water out of mines.

In the 1770s James Watt significantly improved this design. He introduced a steam engine easy employable to supply a large amounts of energy, which set the mechanization of factories underway. In England certain cities concentrated on making specific products, such as specific types of textiles or pottery. Around these cities specialized machinery industry arose in order to enable the mechanization of the plants. Hereby late in the 18th century arose the first machinery industry in the UK and also in Germany and Belgium.

The Industrial Revolution received a further boost with the upcoming railways. These arose at the beginning of the 19th century in England as innovation in the mining industry. The work in coal mines was hard and dangerous, and so there was a great need for tools to ease this work. In 1804, Richard Trevithick placed the first steam engine on rails, and was in 1825 the Stockton and Darlington Railway was opened, intended to transport coals from the mine to the port. In 1835 the first train drove in continental Europe between Mechelen and Brussels, and in the Netherlands in 1839 the first train drove between Amsterdam and Haarlem. For the machinery industry this brought all sorts of new work with new machinery for metallurgy, machine tool for metalworking, production of steam engines for trains with all its necessities etc.

In time the market for the machine industry became wider, specialized products were manufactured for a greater national and often international market. For example, it was not uncommon in the second half of the 19th century that American steelmakers ordered their production in England, where new steelmaking techniques were more advanced. In the far east Japan would import these product until the early 1930s, the creation of an own machinery industry got underway.

The term ""machinery industry"" came into existence later in the 19th century. One of the first times this branch of industry was recognized as such, and was investigated, was in a production statistics of 1907 created by the British Ministry of Trade and Industry. In this statistic the output of the engineering industry, was divided into forty different categories, including for example, agricultural machinery, machinery for the textile industry and equipment, and parts for train and tram.[3]

The inventions of new propulsion techniques based on electric motors, internal combustion engines and gas turbines brought a new generation of machines in the 20th century from cars to household appliances. Not only the product range of the machinery industry increased considerably, but especially smaller machines could also deliver products in much greater numbers fabricated in mass production.  With the rise of mass production in other parts of the industry, there was also a high demand for manufacturing and production systems, to increase the entire production.

Shortage of labor in agriculture and industry at the beginning of the second half of the 20th century, raised the need for further mechanization of production, which required for more specific machines. The rise of the computer made further automation of production possible, which in turn set new demands on the machinery industry.

The machinery industry produces different kind of products, for example, engines, pumps, logistics equipment; for different kind of markets from the agriculture industry, food & beverage industry, manufacturing industry, health industry, and amusement industry till different branches of the consumer market. As such companies in the machine industry can be classified by product of market.
[4]

In the world of today, all kinds of Industry classifications exists. Some classifications recognize the machine industry as a specific class, and offer a subdivision for this field. For example, the Dutch Standard Industrial Classification of 1993, developed by the  Statistics Netherlands, give the following breakdown of the machinery industry:

This composition of the machinery industry has been significantly altered with the latest revision of the Dutch Standard Industrial Classification of 1993. The Standard Industrial Classification of 1974 broke down the machinery industry into nine sectors:

It may be clear that classification is by markets, and the more recent classification is by product.

The machine industry makes a very diverse range of products. A selection:

In ASEAN, the machine industry is a vital part of the region's economic structure. As of 2023, the industry employed approximately 2.5 million people across member countries. The sector generated a combined turnover of around $120 billion, with about 65% of this revenue coming from exports.

The ASEAN region is home to roughly 12,000 active companies in the machine industry, with the majority being small and medium-sized enterprises. Around 90% of these companies employ fewer than 500 people. On average, each employee in this sector generates approximately $48,000 in revenue annually.

Prominent companies in the ASEAN machine industry include ST Engineering (Singapore), Aikawa Iron Works (Thailand), and San Miguel Corporation (Philippines). The industry's significant growth can be attributed to favorable macroeconomic policies, open trade regimes, and increasing demand for manufactured goods both within the region and globally.

ASEAN's machine industry has been significantly influenced by advancements in automation and digital infrastructure, contributing to increased productivity and competitiveness on a global scale [5]

In Germany, in 2011 about 900,000 people were employed[6]  in the machine industry and an estimated of 300,000 abroad. The combined turnover of the sector was €238 billion, of which 60% came from export. There were about 6,600 active companies, and 95% of those companies employed less than 500 people. Each employee generated an average of 148,000 Euro. Some of the largest companies in Germany are DMG Mori Seiki AG, GEA Group, Siemens AG, and ThyssenKrupp.

In the French machinery industry in 2009 about 650,000 people were employed, and the sector generated a turnover of 44 billion euros. Because of the crisis, the turnover of the sector had fallen by 15 percent. Due to stronger consumer spending and continuing demand from the energy sector and transport sector, the damage of the crisis was still limited.[7]
Alternatively, some companies decided to focus their request on used industrial equipment. This guarantee attractive prices and better time delivery.[8][9]

In Japan, the machine industry plays a significant role in the economy, employing a considerable number of people and generating substantial revenue. As of 2023, approximately 1.5 million people were employed in the machine tool industry. The combined turnover of the sector was estimated to be around $60 billion, with about 70% of the revenue coming from exports.[10][11]

Japan is home to about 3,000 active companies in the machine industry, with 85% of these companies employing fewer than 500 people. The average revenue generated per employee in the industry is approximately $40,000. Some of the largest and most influential companies in Japan's machine industry include Fanuc Corporation, Mitsubishi Electric Corporation, and DMG Mori Seiki Co., Ltd.

In the Netherlands in 1996, a total of some 93,000 workers were employed in the machinery industry, with approximately 2,500 companies present. In 1000 of these companies there were working 20 or more employees.[1] In the Netherlands, according to the Chamber of Commerce in this subsector of the industry in 2011 some 15,000 companies were active.[12] Some of the largest companies in the Netherlands are Lely, Philips and Stork B.V.

U.S. machinery industries had total domestic and foreign sales of $413.7 billion in 2011.  The United States is the world’s largest market for machinery, as well as the third-largest supplier. American manufacturers held a 58.5 percent share of the U.S. domestic market.[13]

 Media related to Machinery industry at Wikimedia Commons
"
Agricultural Machinery Installation,"Agricultural machinery relates to the mechanical structures and devices used in farming or other agriculture. There are many types of such equipment, from hand tools and power tools to tractors and the farm implements that they tow or operate. Machinery is used in both organic and nonorganic farming. Especially since the advent of mechanised agriculture, agricultural machinery is an indispensable part of how the world is fed.

Agricultural machinery can be regarded as part of wider agricultural automation technologies, which includes the more advanced digital equipment and agricultural robotics.[1] While robots have the potential to automate the three key steps involved in any agricultural operation (diagnosis, decision-making and performing), conventional motorized machinery is used principally to automate only the performing step where diagnosis and decision-making are conducted by humans based on observations and experience.[1]

With the coming of the Industrial Revolution and the development of more complicated machines, farming methods took a great leap forward.[2] Instead of harvesting grain by hand with a sharp blade, wheeled machines cut a continuous swath. Instead of threshing the grain by beating it with sticks, threshing machines separated the seeds from the heads and stalks. The first tractors appeared in the late 19th century.[3]

Power for agricultural machinery was originally supplied by ox or other  domesticated animals.  With the invention of steam power came the portable engine, and later the traction engine, a multipurpose, mobile energy source that was the ground-crawling cousin to the steam locomotive.  Agricultural steam engines took over the heavy pulling work of oxen, and were also equipped with a pulley that could power stationary machines via the use of a long belt. The steam-powered machines were low-powered by today's standards but because of their size and their low gear ratios, they could provide a large drawbar pull. The slow speed of steam-powered machines led farmers to comment that tractors had two speeds: ""slow, and damn slow"".

The internal combustion engine; first the petrol engine, and later diesel engines; became the main source of power for the next generation of tractors. These engines also contributed to the development of the self-propelled combine harvester and thresher, or the combine harvester (also shortened to 'combine'). Instead of cutting the grain stalks and transporting them to a stationary threshing machine, these combines cut, threshed, and separated the grain while moving continuously throughout the field.

Tractors do the majority of work on a modern farm.  They are used to push/pull implements—machines that till the ground, plant seeds, and perform other tasks. Tillage implements prepare the soil for planting by loosening the soil and killing weeds or competing plants.  The best-known is the plow, the ancient implement that was upgraded in 1838 by John Deere.  Plows are now used less frequently in the U.S. than formerly, with offset disks used instead to turn over the soil, and chisels used to gain the depth needed to retain moisture.

Combine is a machine designed to efficiently harvest a variety of grain crops. The name derives from its combining four separate harvesting operations—reaping, threshing, gathering, and winnowing—into a single process. Among the crops harvested with a combine are wheat, rice, oats, rye, barley, corn (maize), sorghum, soybeans, flax (linseed), sunflowers and rapeseed.[4]

The most common type of seeder is called a planter, and spaces seeds out equally in long rows, which are usually two to three feet apart.  Some crops are planted by drills, which put out much more seed in rows less than a foot apart, blanketing the field with crops.  Transplanters automate the task of transplanting seedlings to the field.  With the widespread use of plastic mulch, plastic mulch layers, transplanters, and seeders lay down long rows of plastic, and plant through them automatically.

After planting, other agricultural machinery such as self-propelled sprayers can be used to apply fertilizer and pesticides. Agriculture sprayer application is a method to protect crops from weeds by using herbicides, fungicides, and insecticides. Spraying or planting a cover crop are ways to mix weed growth.[5]

Planting crop hay balers can be used to tightly package grass or alfalfa into a storable form for the winter months. Modern irrigation relies on machinery. Engines, pumps and other specialized gear provide water quickly and in high volumes to large areas of land. Similar types of equipment such as agriculture sprayers can be used to deliver fertilizers and pesticides.

Besides the tractor, other vehicles have been adapted for use in farming, including trucks, airplanes, and helicopters, such as for transporting crops and making equipment mobile, to aerial spraying and livestock herd management.

The basic technology of agricultural machines has changed little in the last century. Though modern harvesters and planters may do a better job or be slightly tweaked from their predecessors, the combine of today still cuts, threshes, and separates grain in the same way it has always been done. However, technology is changing the way that humans operate the machines, as computer monitoring systems, GPS locators and self-steer programs allow the most advanced tractors and implements to be more precise and less wasteful in the use of fuel, seed, or fertilizer. In the foreseeable future, there may be mass production of driverless tractors, which use GPS maps and electronic sensors.

The Food and Agriculture Organization of the United Nations (FAO) defines agricultural automation as the use of machinery and equipment in agricultural operations to improve their diagnosis, decision-making, or performance, reducing the drudgery of agricultural work and improving the timeliness, and potentially the precision, of agricultural operations.[1][6]

The technological evolution in agriculture has been a journey from manual tools to animal traction, then to motorized mechanization, and further to digital equipment. This progression has culminated in the use of robotics with artificial intelligence (AI). Motorized mechanization, for instance, automates operations like ploughing, seeding, fertilizing, milking, feeding, and irrigating, thereby significantly reducing manual labor.[7] With the advent of digital automation technologies, it has become possible to automate diagnosis and decision-making. For instance, autonomous crop robots can harvest and seed crops, and drones can collect information to help automate input applications.[1][6] Tractors, on the other hand, can be transformed into automated vehicles that can sow fields independently. < ref name= "":1""/>

A 2023 report by the United States Department of Agriculture (USDA) revealed that over 50% of corn, cotton, rice, sorghum, soybeans, and winter wheat in the United States is planted using automated guidance systems. These systems, which utilize technology to autonomously steer farm equipment, only require supervision from a farmer. This is a clear example of how agricultural automation is being implemented in real-world farming scenarios.[8]

Many farmers are upset by their inability to fix the new types of high-tech farm equipment.[9] This is due mostly to companies using intellectual property law to prevent farmers from having the legal right to fix their equipment (or gain access to the information to allow them to do it).[10] In October 2015 an exemption was added to the DMCA to allow inspection and modification of the software in cars and other vehicles including agricultural machinery.[11]

The Open Source Agriculture movement counts different initiatives and organizations such as Farm Labs which is a network in Europe,[12] l'Atelier Paysan which is a cooperative to teach farmers in France how to build and repair their tools,[13][14] and Ekylibre which is an open-source company to provide farmers in France with open source software (SaaS) to manage farming operations.[14][15] In the United States, the MIT Media Lab's Open Agriculture Initiative seeks to foster ""the creation of an open-source ecosystem of technologies that enable and promote transparency, networked experimentation, education, and hyper-local production"".[16] It develops the Personal Food Computer, an educational project to create a ""controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber"". It includes the development of Open Phenom,[17] an open source library with open data sets for climate recipes which link the phenotype response of plants (taste, nutrition) to environmental variables, biological, genetic and resource-related necessary for cultivation (input).[18] Plants with the same genetics can naturally vary in color, size, texture, growth rate, yield, flavor, and nutrient density according to the environmental conditions in which they are produced.

 This article incorporates text from a free content work. Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from In Brief to The State of Food and Agriculture 2022 – Leveraging automation in agriculture for transforming agrifood systems​, FAO, FAO.  
"
Grain Handling Machinery Installation,"Agricultural machinery relates to the mechanical structures and devices used in farming or other agriculture. There are many types of such equipment, from hand tools and power tools to tractors and the farm implements that they tow or operate. Machinery is used in both organic and nonorganic farming. Especially since the advent of mechanised agriculture, agricultural machinery is an indispensable part of how the world is fed.

Agricultural machinery can be regarded as part of wider agricultural automation technologies, which includes the more advanced digital equipment and agricultural robotics.[1] While robots have the potential to automate the three key steps involved in any agricultural operation (diagnosis, decision-making and performing), conventional motorized machinery is used principally to automate only the performing step where diagnosis and decision-making are conducted by humans based on observations and experience.[1]

With the coming of the Industrial Revolution and the development of more complicated machines, farming methods took a great leap forward.[2] Instead of harvesting grain by hand with a sharp blade, wheeled machines cut a continuous swath. Instead of threshing the grain by beating it with sticks, threshing machines separated the seeds from the heads and stalks. The first tractors appeared in the late 19th century.[3]

Power for agricultural machinery was originally supplied by ox or other  domesticated animals.  With the invention of steam power came the portable engine, and later the traction engine, a multipurpose, mobile energy source that was the ground-crawling cousin to the steam locomotive.  Agricultural steam engines took over the heavy pulling work of oxen, and were also equipped with a pulley that could power stationary machines via the use of a long belt. The steam-powered machines were low-powered by today's standards but because of their size and their low gear ratios, they could provide a large drawbar pull. The slow speed of steam-powered machines led farmers to comment that tractors had two speeds: ""slow, and damn slow"".

The internal combustion engine; first the petrol engine, and later diesel engines; became the main source of power for the next generation of tractors. These engines also contributed to the development of the self-propelled combine harvester and thresher, or the combine harvester (also shortened to 'combine'). Instead of cutting the grain stalks and transporting them to a stationary threshing machine, these combines cut, threshed, and separated the grain while moving continuously throughout the field.

Tractors do the majority of work on a modern farm.  They are used to push/pull implements—machines that till the ground, plant seeds, and perform other tasks. Tillage implements prepare the soil for planting by loosening the soil and killing weeds or competing plants.  The best-known is the plow, the ancient implement that was upgraded in 1838 by John Deere.  Plows are now used less frequently in the U.S. than formerly, with offset disks used instead to turn over the soil, and chisels used to gain the depth needed to retain moisture.

Combine is a machine designed to efficiently harvest a variety of grain crops. The name derives from its combining four separate harvesting operations—reaping, threshing, gathering, and winnowing—into a single process. Among the crops harvested with a combine are wheat, rice, oats, rye, barley, corn (maize), sorghum, soybeans, flax (linseed), sunflowers and rapeseed.[4]

The most common type of seeder is called a planter, and spaces seeds out equally in long rows, which are usually two to three feet apart.  Some crops are planted by drills, which put out much more seed in rows less than a foot apart, blanketing the field with crops.  Transplanters automate the task of transplanting seedlings to the field.  With the widespread use of plastic mulch, plastic mulch layers, transplanters, and seeders lay down long rows of plastic, and plant through them automatically.

After planting, other agricultural machinery such as self-propelled sprayers can be used to apply fertilizer and pesticides. Agriculture sprayer application is a method to protect crops from weeds by using herbicides, fungicides, and insecticides. Spraying or planting a cover crop are ways to mix weed growth.[5]

Planting crop hay balers can be used to tightly package grass or alfalfa into a storable form for the winter months. Modern irrigation relies on machinery. Engines, pumps and other specialized gear provide water quickly and in high volumes to large areas of land. Similar types of equipment such as agriculture sprayers can be used to deliver fertilizers and pesticides.

Besides the tractor, other vehicles have been adapted for use in farming, including trucks, airplanes, and helicopters, such as for transporting crops and making equipment mobile, to aerial spraying and livestock herd management.

The basic technology of agricultural machines has changed little in the last century. Though modern harvesters and planters may do a better job or be slightly tweaked from their predecessors, the combine of today still cuts, threshes, and separates grain in the same way it has always been done. However, technology is changing the way that humans operate the machines, as computer monitoring systems, GPS locators and self-steer programs allow the most advanced tractors and implements to be more precise and less wasteful in the use of fuel, seed, or fertilizer. In the foreseeable future, there may be mass production of driverless tractors, which use GPS maps and electronic sensors.

The Food and Agriculture Organization of the United Nations (FAO) defines agricultural automation as the use of machinery and equipment in agricultural operations to improve their diagnosis, decision-making, or performance, reducing the drudgery of agricultural work and improving the timeliness, and potentially the precision, of agricultural operations.[1][6]

The technological evolution in agriculture has been a journey from manual tools to animal traction, then to motorized mechanization, and further to digital equipment. This progression has culminated in the use of robotics with artificial intelligence (AI). Motorized mechanization, for instance, automates operations like ploughing, seeding, fertilizing, milking, feeding, and irrigating, thereby significantly reducing manual labor.[7] With the advent of digital automation technologies, it has become possible to automate diagnosis and decision-making. For instance, autonomous crop robots can harvest and seed crops, and drones can collect information to help automate input applications.[1][6] Tractors, on the other hand, can be transformed into automated vehicles that can sow fields independently. < ref name= "":1""/>

A 2023 report by the United States Department of Agriculture (USDA) revealed that over 50% of corn, cotton, rice, sorghum, soybeans, and winter wheat in the United States is planted using automated guidance systems. These systems, which utilize technology to autonomously steer farm equipment, only require supervision from a farmer. This is a clear example of how agricultural automation is being implemented in real-world farming scenarios.[8]

Many farmers are upset by their inability to fix the new types of high-tech farm equipment.[9] This is due mostly to companies using intellectual property law to prevent farmers from having the legal right to fix their equipment (or gain access to the information to allow them to do it).[10] In October 2015 an exemption was added to the DMCA to allow inspection and modification of the software in cars and other vehicles including agricultural machinery.[11]

The Open Source Agriculture movement counts different initiatives and organizations such as Farm Labs which is a network in Europe,[12] l'Atelier Paysan which is a cooperative to teach farmers in France how to build and repair their tools,[13][14] and Ekylibre which is an open-source company to provide farmers in France with open source software (SaaS) to manage farming operations.[14][15] In the United States, the MIT Media Lab's Open Agriculture Initiative seeks to foster ""the creation of an open-source ecosystem of technologies that enable and promote transparency, networked experimentation, education, and hyper-local production"".[16] It develops the Personal Food Computer, an educational project to create a ""controlled environment agriculture technology platform that uses robotic systems to control and monitor climate, energy, and plant growth inside of a specialized growing chamber"". It includes the development of Open Phenom,[17] an open source library with open data sets for climate recipes which link the phenotype response of plants (taste, nutrition) to environmental variables, biological, genetic and resource-related necessary for cultivation (input).[18] Plants with the same genetics can naturally vary in color, size, texture, growth rate, yield, flavor, and nutrient density according to the environmental conditions in which they are produced.

 This article incorporates text from a free content work. Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from In Brief to The State of Food and Agriculture 2022 – Leveraging automation in agriculture for transforming agrifood systems​, FAO, FAO.  
"
Dock and Pier Construction,"The word dock (from Dutch  dok) in American English refers to one or a group of human-made structures that are involved in the handling of boats or ships (usually on or near a shore). In British English, the term is not used the same way as in American English; it is used to mean the area of water that is next to or around a wharf or quay. The exact meaning varies among different variants of the English language.

""Dock"" may also refer to a dockyard (also known as a shipyard) where the loading, unloading, building, or repairing of ships occurs.

The earliest known docks were those discovered in Wadi al-Jarf, an ancient Egyptian harbor, of  Pharaoh Khufu, dating from c.2500 BC located on the Red Sea coast.[1][2] Archaeologists also discovered anchors and storage jars near the site.[3]

A dock from Lothal in India dates from 2400 BC[4][5] and was located away from the main current to avoid deposition of silt.[6] Modern oceanographers have observed that the ancient Harappans must have possessed great knowledge relating to tides in order to build such a dock on the ever-shifting course of the Sabarmati, as well as exemplary hydrography and maritime engineering.[6] This is the earliest known dock found in the world equipped to berth and service ships.[6]

It is speculated that Lothal engineers studied tidal movements and their effects on brick-built structures, since the walls are of kiln-burnt bricks.[7] This knowledge also enabled them to select Lothal's location in the first place, as the Gulf of Khambhat has the highest tidal amplitude and ships can be sluiced through flow tides in the river estuary.[7] The engineers built a trapezoidal structure, with north–south arms of average 21.8 metres (71.5 ft), and east–west arms of 37 metres (121 ft).[7]

In British English, a dock is an enclosed area of water used for loading, unloading, building or repairing ships. Such a dock may be created by building enclosing harbour walls into an existing natural water space, or by excavation within what would otherwise be dry land.

There are specific types of dock structures where the water level is controlled:

Where the water level is not controlled berths may be:

A dockyard (or shipyard) consists of one or more docks, usually with other structures.

In American English, dock is technically synonymous with pier or wharf—any human-made structure in the water intended for people to be on. However, in modern use, pier is generally used to refer to structures originally intended for industrial use, such as seafood processing or shipping, and more recently for cruise ships, and dock is used for almost everything else, often with a qualifier, such as ferry dock, swimming dock, ore dock and others. However, pier is also commonly used to refer to wooden or metal structures that extend into the ocean from beaches and are used, for the most part, to accommodate fishing in the ocean without using a boat.

In American English, the term for the water area between piers is slip.

In the cottage country of Canada and the United States, a dock is a wooden platform built over water, with one end secured to the shore.  The platform is used for the boarding and offloading of small boats.


"
Road and Highway Construction,"A road is a thoroughfare used primarily for movement of traffic. Roads differ from streets, whose primary use is local access. They also differ from stroads, which combine the features of streets and roads. Most modern roads are paved.

The words ""road"" and ""street"" are commonly considered to be interchangeable, but the distinction is important in urban design.

There are many types of roads, including parkways, avenues, controlled-access highways (freeways, motorways, and expressways), tollways, interstates, highways, and local roads.

The primary features of roads include lanes, sidewalks (pavement), roadways (carriageways), medians, shoulders, verges, bike paths (cycle paths), and shared-use paths.

Historically many roads were simply recognizable routes without any formal construction or some maintenance.[1]

The Organization for Economic Co-operation and Development (OECD) defines a road as ""a line of communication (travelled way) using a stabilized base other than rails or air strips open to public traffic, primarily for the use of road motor vehicles running on their own wheels"", which includes ""bridges, tunnels, supporting structures, junctions, crossings, interchanges, and toll roads, but not cycle paths"".[2]

The Eurostat, ITF and UNECE Glossary for Transport Statistics Illustrated defines a road as a ""Line of communication (traveled way) open to public traffic, primarily for the use of road motor vehicles, using a stabilized base other than rails or air strips. [...] Included are paved roads and other roads with a stabilized base, e.g. gravel roads. Roads also cover streets, bridges, tunnels, supporting structures, junctions, crossings and interchanges. Toll roads are also included. Excluded are dedicated cycle lanes.""[3]

The 1968 Vienna Convention on Road Traffic defines a road as the entire surface of any way or street open to public traffic.[4]

In urban areas roads may diverge through a city or village and be named as streets, serving a dual function as urban space easement and route.[5] Modern roads are normally smoothed, paved, or otherwise prepared to allow easy travel.[6]

Part 2, Division 1, clauses 11–13 of the National Transport Commission Regulations 2006 defines a road in Australia as 'an area that is open to or used by the public and is developed for, or has as one of its main uses, the driving or riding of motor vehicles.'[7]

Further, it defines a shoulder (typical an area of the road outside the edge line, or the curb) and a road-related area which includes green areas separating roads, areas designated for cyclists and areas generally accessible to the public for driving, riding or parking vehicles.

In New Zealand, the definition of a road is broad in common law[8] where the statutory definition includes areas the public has access to, by right or not.[9] Beaches, publicly accessible car parks and yards (even if privately owned), river beds, road shoulders (verges), wharves and bridges are included.[10] However, the definition of a road for insurance purposes may be restricted to reduce risk.

In the United Kingdom The Highway Code details rules for ""road users"", but there is some ambiguity between the terms highway and road.[11] For the purposes of the English law, Highways Act 1980, which covers England and Wales but not Scotland or Northern Ireland, road is ""any length of highway or of any other road to which the public has access, and includes bridges over which a road passes"".[12] This includes footpaths, bridleways and cycle tracks, and also road and driveways on private land and many car parks.[13] Vehicle Excise Duty, a road use tax, is payable on some vehicles used on the public road.[13]

The definition of a road depends on the definition of a highway; there is no formal definition for a highway in the relevant Act. A 1984 ruling said ""the land over which a public right of way exists is known as a highway; and although most highways have been made up into roads, and most easements of way exist over footpaths, the presence or absence of a made road has nothing to do with the distinction.[14][15] Another legal view is that while a highway historically included footpaths, bridleways, driftways, etc., it can now be used to mean those ways that allow the movement of motor vehicles, and the term rights of way can be used to cover the wider usage.[16]

In the United States, laws distinguish between public roads, which are open to public use,[17] and private roads, which are privately controlled.[18]

The assertion that the first pathways were the trails made by animals has not been universally accepted; in many cases animals do not follow constant paths.[1] Some believe that some roads originated from following animal trails.[25][26] The Icknield Way may exemplify this type of road origination, where human and animal both selected the same natural line.[27] By about 10,000 BC human travelers used rough roads/pathways.[1]


In transport engineering, subgrade is the native material underneath a constructed road.
Road construction requires the creation of an engineered continuous right-of-way or roadbed, overcoming geographic obstacles and having grades low enough to permit vehicle or foot travel,[42]: 15  and may be required to meet standards set by law[43] or official guidelines.[44] The process is often begun with the removal of earth and rock by digging or blasting, construction of embankments, bridges and tunnels, and removal of vegetation (this may involve deforestation) and followed by the laying of pavement material. A variety of road building equipment is employed in road building.[45][46]

After design, approval, planning, legal, and environmental considerations have been addressed alignment of the road is set out by a surveyor.[37] The radii and gradient are designed and staked out to best suit the natural ground levels and minimize the amount of cut and fill.[44]: 34  Great care is taken to preserve reference benchmarks.[44]: 59 

Roads are designed and built for primary use by vehicular and pedestrian traffic. Storm drainage and environmental considerations are a major concern. Erosion and sediment controls are constructed to prevent detrimental effects. Drainage lines are laid with sealed joints in the road easement with runoff coefficients and characteristics adequate for the land zoning and storm water system. Drainage systems must be capable of carrying the ultimate design flow from the upstream catchment with approval for the outfall from the appropriate authority to a watercourse, creek, river or the sea for drainage discharge.[44]: 38–40 

A borrow pit (source for obtaining fill, gravel, and rock) and a water source should be located near or in reasonable distance to the road construction site. Approval from local authorities may be required to draw water or for working (crushing and screening) of materials for construction needs. The topsoil and vegetation is removed from the borrow pit and stockpiled for subsequent rehabilitation of the extraction area. Side slopes in the excavation area not steeper than one vertical to two horizontal for safety reasons.[44]: 53–56 

Old road surfaces, fences, and buildings may need to be removed before construction can begin. Trees in the road construction area may be marked for retention. These protected trees should not have the topsoil within the area of the tree's drip line removed and the area should be kept clear of construction material and equipment. Compensation or replacement may be required if a protected tree is damaged. Much of the vegetation may be mulched and put aside for use during reinstatement. The topsoil is usually stripped and stockpiled nearby for rehabilitation of newly constructed embankments along the road. Stumps and roots are removed and holes filled as required before the earthwork begins. Final rehabilitation after road construction is completed will include seeding, planting, watering and other activities to reinstate the area to be consistent with the untouched surrounding areas.[44]: 66–67 

Processes during earthwork include excavation, removal of material to spoil, filling, compacting, construction and trimming. If rock or other unsuitable material is discovered it is removed, moisture content is managed and replaced with standard fill compacted to meet the design requirements (generally 90–95% relative compaction). Blasting is not frequently used to excavate the roadbed as the intact rock structure forms an ideal road base. When a depression must be filled to come up to the road grade the native bed is compacted after the topsoil has been removed. The fill is made by the ""compacted layer method"" where a layer of fill is spread then compacted to specifications, under saturated conditions. The process is repeated until the desired grade is reached.[44]: 68–69 

General fill material should be free of organics, meet minimum California bearing ratio (CBR) results and have a low plasticity index. The lower fill generally comprises sand or a sand-rich mixture with fine gravel, which acts as an inhibitor to the growth of plants or other vegetable matter. The compacted fill also serves as lower-stratum drainage. Select second fill (sieved) should be composed of gravel, decomposed rock or broken rock below a specified particle size and be free of large lumps of clay. Sand clay fill may also be used. The roadbed must be ""proof rolled"" after each layer of fill is compacted. If a roller passes over an area without creating visible deformation or spring the section is deemed to comply.[44]: 70–72 

Geosynthetics such as geotextiles, geogrids, and geocells are frequently used in the various pavement layers to improve road quality. These materials and methods are used in low-traffic private roadways as well as public roads and highways.[47] Geosynthetics perform four main functions in roads: separation, reinforcement, filtration, and drainage; which increase the pavement performance, reduce construction costs and decrease maintenance.[48][self-published source]

The completed roadway is finished by paving or left with a gravel or other natural surface. The type of road surface is dependent on economic factors and expected usage. Safety improvements such as traffic signs, crash barriers, raised pavement markers and other forms of road surface marking are installed.

According to a May 2009 report by the American Association of State Highway and Transportation Officials (AASHTO) and TRIP – a national transportation research organization – driving on rough roads costs the average American motorist approximately $400 a year in extra vehicle operating costs. Drivers living in urban areas with populations more than 250,000 are paying upwards of $750 more annually because of accelerated vehicle deterioration, increased maintenance, additional fuel consumption, and tire wear caused by poor road conditions.

When a single carriageway road is converted into dual carriageway by building a second separate carriageway alongside the first, it is usually referred to as duplication,[49] twinning or doubling. The original carriageway is changed from two-way to become one-way, while the new carriageway is one-way in the opposite direction. In the same way as converting railway lines from single track to double track, the new carriageway is not always constructed directly alongside the existing carriageway.

Roads that are intended for use by a particular mode of transport can be reallocated for another mode of transport,[50] i.e. by using traffic signs. For instance, in the ongoing road space reallocation effort, some roads (particularly in city centers) which are intended for use by cars are increasingly being repurposed for cycling and/or walking.[51][52][53]

Like all structures, roads deteriorate over time. Deterioration is primarily due to environmental effects such as frost heaves, thermal cracking and oxidation often contribute, however accumulated damage from vehicles also contributes.[54] According to a series of experiments carried out in the late 1950s, called the AASHO Road Test, it was empirically determined that the effective damage done to the road is roughly proportional to the fourth power of axle weight.[55] A typical tractor-trailer weighing 80,000 pounds (36.287 t) with 8,000 pounds (3.629 t) on the steer axle and 36,000 pounds (16.329 t) on both of the tandem axle groups is expected to do 7,800 times more damage than a passenger vehicle with 2,000 pounds (0.907 t) on each axle. Potholes on roads are caused by rain damage and vehicle braking or related construction work.

Pavements are designed for an expected service life or design life. In some parts of the United Kingdom the standard design life is 40 years for new bitumen and concrete pavement. Maintenance is considered in the whole life cost of the road with service at 10, 20 and 30-year milestones.[56] Roads can be and are designed for a variety of lives (8-, 15-, 30-, and 60-year designs). When pavement lasts longer than its intended life, it may have been overbuilt, and the original costs may have been too high. When a pavement fails before its intended design life, the owner may have excessive repair and rehabilitation costs. Some asphalt pavements are designed as perpetual pavements with an expected structural life in excess of 50 years.[57]

Many asphalt pavements built over 35 years ago, despite not being specifically designed as a perpetual pavement, have remained in good condition long past their design life.[58] Many concrete pavements built since the 1950s have significantly outlived their intended design lives.[59] Some roads like Chicago's Wacker Drive, a major two-level (and at one point, three-level) roadway in the downtown area, are being rebuilt with a designed service life of 100 years.[60]

Virtually all roads require some form of maintenance before they come to the end of their service life. Pro-active agencies use pavement management techniques to continually monitor road conditions and schedule preventive maintenance treatments as needed to prolong the lifespan of their roads. Technically advanced agencies monitor the road network surface condition with sophisticated equipment such as laser/inertial profilometers. These measurements include road curvature, cross slope, asperity, roughness, rutting and texture. Software algorithms use this data to recommend maintenance or new construction.

Maintenance treatments for asphalt concrete generally include thin asphalt overlays, crack sealing, surface rejuvenating, fog sealing, micro milling or diamond grinding and surface treatments. Thin surfacing preserves, protects and improves the functional condition of the road while reducing the need for routing maintenance, leading to extended service life without increasing structural capacity.[61]

Older concrete pavements that develop faults can be repaired with a dowel bar retrofit, in which slots are cut in the pavement at each joint, and dowel bars are placed in the slots, which are then filled with concrete patching material. This can extend the life of the concrete pavement for 15 years.[62]

Failure to maintain roads properly can create significant costs to society. A 2009 report released by the American Association of State Highway and Transportation Officials estimated that about 50% of the roads in the US are in bad condition, with urban areas worse. The report estimates that urban drivers pay an average of $746/year on vehicle repairs while the average US motorist pays about $335/year. In contrast, the average motorist pays about $171/year in road maintenance taxes (based on 600 gallons/year and $0.285/gallon tax).

Distress and serviceability loss on concrete roads can be caused by loss of support due to voids beneath the concrete pavement slabs. The voids usually occur near cracks or joints due to surface water infiltration. The most common causes of voids are pumping, consolidation, subgrade failure and bridge approach failure. Slab stabilization is a non-destructive method of solving this problem and is usually employed with other concrete pavement restoration methods including patching and diamond grinding. The technique restores support to concrete slabs by filing small voids that develop underneath the concrete slab at joints, cracks or the pavement edge.

The process consists of pumping a cementitious grout or polyurethane mixture through holes drilled through the slab. The grout can fill small voids beneath the slab and/or sub-base. The grout also displaces free water and helps keep water from saturating and weakening support under the joints and slab edge after stabilization is complete. The three steps for this method after finding the voids are locating and drilling holes, grout injection and post-testing the stabilized slabs.

Slab stabilization does not correct depressions, increase the design structural capacity, stop erosion or eliminate faulting. It does, however, restore the slab support, therefore, decreasing deflections under the load. Stabilization should only be performed at joints and cracks where the loss of support exists. Visual inspection is the simplest manner to find voids. Signs that repair is needed are transverse joint faulting, corner breaks and shoulder drop off and lines at or near joints and cracks. Deflection testing is another common procedure used to locate voids. It is recommended to do this testing at night as during cooler temperatures, joints open, aggregate interlock diminishes and load deflections are at their highest.

Ground penetrating radar pulses electromagnetic waves into the pavement and measures and graphically displays the reflected signal. This can reveal voids and other defects.

The epoxy/core test, detects voids by visual and mechanical methods. It consists of drilling a 25 to 50 millimeter hole through the pavement into the sub-base with a dry-bit roto-hammer. Next, a two-part epoxy is poured into the hole – dyed for visual clarity. Once the epoxy hardens, technicians drill through the hole. If a void is present, the epoxy will stick to the core and provide physical evidence.

Common stabilization materials include pozzolan-cement grout and polyurethane. The requirements for slab stabilization are strength and the ability to flow into or expand to fill small voids. Colloidal mixing equipment is necessary to use the pozzolan-cement grouts. The contractor must place the grout using a positive-displacement injection pump or a non-pulsing progressive cavity pump. A drill is also necessary but it must produce a clean hole with no surface spalling or breakouts. The injection devices must include a grout packer capable of sealing the hole. The injection device must also have a return hose or a fast-control reverse switch, in case workers detect slab movement on the uplift gauge. The uplift beam helps to monitor the slab deflection and has to have sensitive dial gauges.[63][64]

Also called joint and crack repair, this method's purpose is to minimize infiltration of surface water and incompressible material into the joint system. Joint sealants are also used to reduce dowel bar corrosion in concrete pavement restoration techniques. Successful resealing consists of old sealant removal, shaping and cleaning the reservoir, installing the backer rod and installing the sealant. Sawing, manual removal, plowing and cutting are methods used to remove the old sealant. Saws are used to shape the reservoir. When cleaning the reservoir, no dust, dirt or traces of old sealant should remain. Thus, it is recommended to water wash, sand-blast and then air blow to remove any sand, dirt or dust. The backer rod installation requires a double-wheeled, steel roller to insert the rod to the desired depth. After inserting the backer rod, the sealant is placed into the joint. There are various materials to choose for this method including hot pour bituminous liquid, silicone and preformed compression seals.[63][65][66][67]

Careful design and construction of roads can increase road traffic safety and reduce the harm (deaths, injuries, and property damage) on the highway system from traffic collisions.

On neighborhood roads traffic calming, safety barriers, pedestrian crossings and cycle lanes can help protect pedestrians, cyclists, and drivers.

Lane markers in some countries and states are marked with Cat's eyes or Botts dots. Botts dots are not used where it is icy in the winter, because frost and snowplows can break the glue that holds them to the road, although they can be embedded in short, shallow trenches carved in the roadway, as is done in the mountainous regions of California.

For major roads risk can be reduced by providing limited access from properties and local roads, grade separated junctions and median dividers between opposite-direction traffic to reduce the likelihood of head-on collisions.

The placement of energy attenuation devices (e.g. guardrails, wide grassy areas, sand barrels) is also common. Some road fixtures such as road signs and fire hydrants are designed to collapse on impact. Light poles are designed to break at the base rather than violently stop a car that hits them. Highway authorities may also remove larger trees from the immediate vicinity of the road. During heavy rains, if the elevation of the road surface is not higher than the surrounding landscape, it may result in flooding.[68]

Speed limits can improve road traffic safety and reduce the number of road traffic casualties from traffic collisions. In their World report on road traffic injury prevention report, the World Health Organization (WHO) identify speed control as one of various interventions likely to contribute to a reduction in road casualties.

Road conditions are the collection of factors describing the ease of driving on a particular stretch of road, or on the roads of a particular locality, including the quality of the pavement surface, potholes, road markings, and weather. It has been reported that ""[p]roblems of transportation participants and road conditions are the main factors that lead to road traffic accidents"".[69] It has further been specifically noted that ""weather conditions and road conditions are interlinked as weather conditions affect the road conditions"".[70] Specific aspects of road conditions can be of particular importance for particular purposes. For example, for autonomous vehicles such as self-driving cars, significant road conditions can include ""shadowing and lighting changes, road surface texture changes, and road markings consisting of circular reflectors, dashed lines, and solid lines"".[71]

Various government agencies and private entities, including local news services, track and report on road conditions to the public so that drivers going through a particular area can be aware of hazards that may exist in that area. News agencies, in turn, rely on tips from area residents with respect to certain aspects of road conditions in their coverage area.[72]

Careful design and construction of a road can reduce any negative environmental impacts.
Water management systems can be used to reduce the effect of pollutants from roads.[73][74] Rainwater and snowmelt running off of roads tends to pick up gasoline, motor oil, heavy metals, trash and other pollutants and result in water pollution. Road runoff is a major source of nickel, copper, zinc, cadmium, lead and polycyclic aromatic hydrocarbons (PAHs), which are created as combustion byproducts of gasoline and other fossil fuels.[75]

De-icing chemicals and sand can run off into roadsides, contaminate groundwater and pollute surface waters;[76] and road salts can be toxic to sensitive plants and animals.[77] Sand applied to icy roads can be ground up by traffic into fine particulates and contribute to air pollution.

Roads are a chief source of noise pollution. In the early 1970s, it was recognized that design of roads can be conducted to influence and minimize noise generation.[78] Noise barriers can reduce noise pollution near built-up areas. Regulations can restrict the use of engine braking.

Motor vehicle emissions contribute air pollution. Concentrations of air pollutants and adverse respiratory health effects are greater near the road than at some distance away from the road.[79] Road dust kicked up by vehicles may trigger allergic reactions.[80] In addition, on-road transportation greenhouse gas emissions are the largest single cause of climate change, scientists say.[81]

Traffic flows on the right or on the left side of the road depending on the country.[82] In countries where traffic flows on the right, traffic signs are mostly on the right side of the road, roundabouts and traffic circles go counter-clockwise/anti-clockwise, and pedestrians crossing a two-way road should watch out for traffic from the left first.[83] In countries where traffic flows on the left, the reverse is true.

About 33% of the world by population drive on the left, and 67% keep right. By road distances, about 28% drive on the left, and 72% on the right,[84] even though originally most traffic drove on the left worldwide.[85]

Transport economics is used to understand both the relationship between the transport system and the wider economy and the complex effects of the road network structure when there are multiple paths and competing modes for both personal and freight (road/rail/air/ferry) and where induced demand can result in increased on decreased transport levels when road provision is increased by building new roads or decreased (for example California State Route 480). Roads are generally built and maintained by the public sector using taxation although implementation may be through private contractors).[86][87] or occasionally using road tolls.[88]

Public-private partnerships are a way for communities to address the rising cost by injecting private funds into the infrastructure. There are four main ones:[89]

Society depends heavily on efficient roads. In the European Union (EU) 44% of all goods are moved by trucks over roads and 85% of all people are transported by cars, buses or coaches on roads.[90] The term was also commonly used to refer to roadsteads, waterways that lent themselves to use by shipping.

According to the New York State Thruway Authority,[91] some sample per-mile costs to construct multi-lane roads in several US northeastern states were:

The United States has the largest network of roads of any country with 4,050,717 miles (6,518,997 km) as of 2009.[92] The Republic of India has the second-largest road system globally with 4,689,842 kilometres (2,914,133 miles) of road (2013).[93] The People's Republic of China is third with 3,583,715 kilometres (2,226,817 mi) of road (2007). The Federative Republic of Brazil has the fourth-largest road system in the world with 1,751,868 kilometres (1,088,560 mi) (2002). See List of countries by road network size. When looking only at expressways, the National Trunk Highway System (NTHS) in China has a total length of 45,000 kilometres (28,000 mi) at the end of 2006, and 60,300 km at the end of 2008, second only to the United States with 90,000 kilometres (56,000 mi) in 2005. However, as of 2017, China has 130,000 km of Expressways.[94][95]

Eurasia, Africa, North America, South America, and Australia each have an extensive road network that connects most cities.
The North and South American road networks are separated by the Darién Gap, the only interruption in the Pan-American Highway. Eurasia and Africa are connected by roads on the Sinai Peninsula. The European Peninsula is connected to the Scandinavian Peninsula by the Øresund Bridge, and both have many connections to the mainland of Eurasia, including the bridges over the Bosphorus. Antarctica has very few roads and no continent-bridging network, though there are a few ice roads between bases, such as the South Pole Traverse. Bahrain is the only island country to be connected to a continental network by road (the King Fahd Causeway to Saudi Arabia).

Even well-connected road networks are controlled by many different legal jurisdictions, and laws such as which side of the road to drive on vary accordingly.

Many populated domestic islands are connected to the mainland by bridges. A very long example is the 113 mi (182 km) Overseas Highway connecting many of the Florida Keys with the continental United States.

Even on mainlands, some settlements have no roads connecting with the primary continental network, due to natural obstacles like mountains or wetlands, or high cost compared to the population served. Unpaved roads or lack of roads are more common in developing countries, and these can become impassible in wet conditions. As of 2014, only 43% of rural Africans have access to an all-season road.[96] Due to steepness, mud, snow, or fords, roads can sometimes be passable only to four-wheel drive vehicles, those with snow chains or snow tires, or those capable of deep wading or amphibious operation.

Most disconnected settlements have local road networks connecting ports, buildings, and other points of interest.

Where demand for travel by road vehicle to a disconnected island or mainland settlement is high, roll-on/roll-off ferries are commonly available if the journey is relatively short. For long-distance trips, passengers usually travel by air and rent a car upon arrival. If facilities are available, vehicles and cargo can also be shipped to many disconnected settlements by boat, or air transport at much greater expense. The island of Great Britain is connected to the European road network by Eurotunnel Shuttle – an example of a car shuttle train which is a service used in other parts of Europe to travel under mountains and over wetlands.

In polar areas, disconnected settlements are often more easily reached by snowmobile or dogsled in cold weather, which can produce sea ice that blocks ports, and bad weather that prevents flying. For example, resupply aircraft are only flown to Amundsen–Scott South Pole Station October to February, and many residents of coastal Alaska have bulk cargo shipped in only during the warmer months. Permanent darkness during the winter can also make long-distance travel more dangerous in polar areas. Continental road networks do reach into these areas, such as the Dalton Highway to the North Slope of Alaska, the R21 highway to Murmansk in Russia, and many roads in Scandinavia (though due to fjords water transport is sometimes faster). Large areas of Alaska, Canada, Greenland, and Siberia are sparsely connected. For example, all 25 communities of Nunavut are disconnected from each other and the main North American road network.[97]

Road transport of people and cargo by may also be obstructed by border controls and travel restrictions. For example, travel from other parts of Asia to South Korea would require passage through the hostile country of North Korea. Moving between most countries in Africa and Eurasia would require passing through Egypt and Israel, which is a politically sensitive area.

Some places are intentionally car-free, and roads (if present) might be used by bicycles or pedestrians.

Roads are under construction to many remote places, such as the villages of the Annapurna Circuit, and a road was completed in 2013 to Mêdog County. Additional intercontinental and transoceanic fixed links have been proposed, including a Bering Strait crossing that would connect Eurasia-Africa and North America, a Malacca Strait Bridge to the largest island of Indonesia from Asia, and a Strait of Gibraltar crossing to connect Europe and Africa directly.
"
Road Maintenance Services,"A road maintenance depot is a depot used by road maintenance agencies for storing works equipment and organising maintenance operations. Road maintenance depots can range in size from small sheds storing just a few pieces of equipment, to vast buildings housing computer and closed-circuit television systems, allowing operators to monitor conditions across the road network.

Road maintenance depots carry gear for a number of tasks, including road works, snow removal, planting of road verge and central reservations and storm drain maintenance. Most depots will have limited accommodation facilities for staff who are on-call, particularly during heavy winter storms, when travel between the worker's home and the depot may be restricted. Road maintenance depots also include garages and repair shops for the fleets of vehicles stored within, and large depots keep supplies of fuel and road salt for drivers.[1]

Depots carry a wide range of vehicles to cover most eventualities, depending on the location of the depot; small urban depots carry street sweeper vehicles and small gully emptiers, while larger rural and motorway-based depots hold fleets of winter service vehicles and engineering vehicles, and often tow trucks and breakdown vehicles for rescuing broken down or stranded equipment.[1] Other vehicles commonly kept at depots include lawnmowers, sprayers and road markers.

Along with garages, most depots also have either salt barns or brine tanks, to store de-icing agents for use in winter months, and filling stations to refuel vehicles, especially those that use red diesel, which is not available at public filling stations. Larger depots have vehicle washes and repair shops to maintain the fleet, and a cafeteria and on-call room for workers.

Road maintenance depots in Germany are known as Straßenmeisterei or Autobahnmeisterei. Responsibility for operating the depot depends on the type of road covered by the catchment area of the depot; those that cover the autobahn network and major road are owned by the Federal Government, while those that cover more urban roads are operated by the local city administration.[2]

All public road maintenance depots in the United Kingdom are owned by the Highways Agency or its contractors, although the depot on the privately owned M6 Toll is run by the operators of the road.[3] Most are located along the edge of motorways, and are signposted ""Works Exit"" or ""Works Access Only"". These signs are also used to disguise sliproads leading to sensitive military institutions such as RAF Welford and to dissuade members of the public from using emergency evacuation routes and short cuts designed for emergency vehicles.[4]
"
Pipeline Construction Services,"A pipeline is a system of pipes for long-distance transportation of a liquid or gas, typically to a market area for consumption. The latest data from 2014 gives a total of slightly less than 2,175,000 miles (3,500,000 km) of pipeline in 120 countries around the world.[1] The United States had 65%, Russia had 8%, and Canada had 3%, thus 76% of all pipeline were in these three countries.[1] The main attribute to pollution from pipelines is caused by corrosion and leakage.[2]

Pipeline and Gas Journal's worldwide survey figures indicate that 118,623 miles (190,905 km) of pipelines are planned and under construction. Of these, 88,976 miles (143,193 km) represent projects in the planning and design phase; 29,647 miles (47,712 km) reflect pipelines in various stages of construction. Liquids and gases are transported in pipelines, and any chemically stable substance can be sent through a pipeline.[3]


Pipelines exist for the transport of crude and refined petroleum, fuels – such as oil, natural gas and biofuels – and other fluids including sewage, slurry, water, beer, hot water or steam for shorter distances and even pneumatic systems which allow for the generation of suction pressure for useful work and in transporting solid objects.[4] Pipelines are useful for transporting water for drinking or irrigation over long distances when it needs to move over hills, or where canals or channels are poor choices due to considerations of evaporation, pollution, or environmental impact. 
Oil pipelines are made from steel or plastic tubes which are usually buried. The oil is moved through the pipelines by pump stations along the pipeline. Natural gas (and similar gaseous fuels) are pressurized into liquids known as natural gas liquids (NGLs).[5] Natural gas pipelines are constructed of carbon steel. Hydrogen pipeline transport is the transportation of hydrogen through a pipe. Pipelines are one of the safest ways of transporting materials as compared to road or rail, and hence in war, pipelines are often the target of military attacks.[6]

It is well documented when the first crude oil pipeline was built. Credit for the development of pipeline transport belongs indisputably to the Oil Transport Association, which first constructed a 2-inch (51 mm) wrought iron pipeline over a 6-mile (9.7 km) track from an oil field in Pennsylvania to a railroad station in Oil Creek, in the 1860s. Some of the first major submarine pipelines were constructed across the English Channel in 1944 during Operation Pluto. These provided an estimated 8 per cent of all petroleum products used by the Allies on the Western Front between the Normandy landings and the end of Second World War.

Pipelines are generally the most economical way to transport large quantities of oil, refined oil products or natural gas over land. For example, in 2014, pipeline transport of crude oil cost about $5 per barrel, while rail transport cost about $10 to $15 per barrel.[7] Trucking has even higher costs due to the additional labor required; employment on completed pipelines represents only ""1% of that of the trucking industry."".[8]

In the United States, 70% of crude oil and petroleum products are shipped by pipeline. (23% are by ship, 4% by truck, and 3% by rail) In Canada for natural gas and petroleum products, 97% are shipped by pipeline.[7]

Natural gas (and similar gaseous fuels) are lightly pressurized into liquids known as Natural Gas Liquids (NGLs). Small NGL processing facilities can be located in oil fields so the butane and propane liquid under light pressure of 125 pounds per square inch (860 kPa), can be shipped by rail, truck or pipeline. Propane can be used as a fuel in oil fields to heat various facilities used by the oil drillers or equipment and trucks used in the oil patch. EG: Propane will convert from a gas to a liquid under light pressure, 100 psi, give or take depending on temperature, and is pumped into cars and trucks at less than 125 psi (860 kPa) at retail stations. Pipelines and rail cars use about double that pressure to pump at 250 psi (1,700 kPa).

The distance to ship propane to markets is much shorter, as thousands of natural-gas processing plants are located in or near oil fields. Many Bakken Basin oil companies in North Dakota, Montana, Manitoba and Saskatchewan gas fields separate the NGLs in the field, allowing the drillers to sell propane directly to small wholesalers, eliminating the large refinery control of product and prices for propane or butane.

The most recent major pipeline to start operating in North America is a TransCanada natural gas line going north across the Niagara region bridges. This gas line carries Marcellus shale gas from Pennsylvania and other tied in methane or natural gas sources into the Canadian province of Ontario. It began operations in the fall of 2012, supplying 16 percent of all the natural gas used in Ontario.[citation needed]

This new US-supplied natural gas displaces the natural gas formerly shipped to Ontario from western Canada in Alberta and Manitoba, thus dropping the government regulated pipeline shipping charges because of the significantly shorter distance from gas source to consumer. To avoid delays and US government regulation, many small, medium and large oil producers in North Dakota have decided to run an oil pipeline north to Canada to meet up with a Canadian oil pipeline shipping oil from west to east. This allows the Bakken Basin and Three Forks oil producers to get higher negotiated prices for their oil because they will not be restricted to just one wholesale market in the US. The distance from the biggest oil patch in North Dakota, in Williston, North Dakota, is only about 85 miles or 137 kilometers to the Canada–US border and Manitoba. Mutual funds and joint ventures are the largest investors in new oil and gas pipelines. In the fall of 2012, the US began exporting propane to Europe, known as LPG, as wholesale prices there are much higher than in North America. Additionally, a pipeline is currently being constructed from North Dakota to Illinois, commonly known as the Dakota Access Pipeline.[9]

As more North American pipelines are built, even more exports of LNG, propane, butane, and other natural gas products occur on all three US coasts. To give insight, North Dakota Bakken region's oil production has grown by 600% from 2007 to 2015.[10] North Dakota oil companies are shipping huge amounts of oil by tanker rail car as they can direct the oil to the market that gives the best price, and rail cars can be used to avoid a congested oil pipeline to get the oil to a different pipeline in order to get the oil to market faster or to a different less busy oil refinery. However, pipelines provide a cheaper means to transport by volume.

Enbridge in Canada is applying to reverse an oil pipeline going from east-to-west (Line 9) and expanding it and using it to ship western Canadian bitumen oil eastward.[11] From a presently rated 250,000 barrels equivalent per day pipeline, it will be expanded to between 1.0 and 1.3 million barrels per day. It will bring western oil to refineries in Ontario, Michigan, Ohio, Pennsylvania, Quebec and New York by early 2014. New Brunswick will also refine some of this western Canadian crude and export some crude and refined oil to Europe from its deep water oil ULCC loading port.

Although pipelines can be built under the sea, that process is economically and technically demanding, so the majority of oil at sea is transported by tanker ships. Similarly, it is often more economically feasible to transport natural gas in the form of LNG, however the break-even point between LNG and pipelines would depend on the volume of natural gas and the distance it travels.[12]

The market size for oil and gas pipeline construction experienced tremendous growth prior to the economic downturn in 2008. After faltering in 2009, demand for pipeline expansion and updating increased the following year as energy production grew.[13] By 2012, almost 32,000 miles (51500 km) of North American pipeline were being planned or under construction.[14] When pipelines are constrained, additional pipeline product transportation options may include the use of drag reducing agents, or by transporting product via truck or rail.

Oil pipelines are made from steel or plastic tubes with inner diameter typically from 4 to 48 inches (100 to 1,220 mm). Most pipelines are typically buried at a depth of about 3 to 6 feet (0.91 to 1.83 m). To protect pipes from impact, abrasion, and corrosion, a variety of methods are used. These can include wood lagging (wood slats), concrete coating, rockshield, high-density polyethylene, imported sand padding, sacrificial cathodes and padding machines.[15]

Crude oil contains varying amounts of paraffin wax and in colder climates wax buildup may occur within a pipeline. Often these pipelines are inspected and cleaned using pigging, the practice of using devices known as ""pigs"" to perform various maintenance operations on a pipeline. The devices are also known as ""scrapers"" or ""Go-devils"". ""Smart pigs"" (also known as ""intelligent"" or ""intelligence"" pigs) are used to detect anomalies in the pipe such as dents, metal loss caused by corrosion, cracking or other mechanical damage.[16] These devices are launched from pig-launcher stations and travel through the pipeline to be received at any other station down-stream, either cleaning wax deposits and material that may have accumulated inside the line or inspecting and recording the condition of the line.

For natural gas, pipelines are constructed of carbon steel and vary in size from 2 to 60 inches (51 to 1,524 mm) in diameter, depending on the type of pipeline. The gas is pressurized by compressor stations and is odorless unless mixed with a mercaptan odorant where required by a regulating authority.

Until damaged during the Russian invasion of Ukraine,[17] the Russian–Ukrainian Transammiak line was the longest ammonia pipeline in the world, at 2,500 km.[18] It connected the TogliattiAzot facility in Russia to the exporting Black Sea-port of Odesa in Ukraine.

Pipelines have been used for transportation of ethanol in Brazil, and there are several ethanol pipeline projects in Brazil and the United States.[19] The main problems related to the transport of ethanol by pipeline are its corrosive nature and tendency to absorb water and impurities in pipelines, which are not problems with oil and natural gas.[19][20] Insufficient volumes and cost-effectiveness are other considerations limiting construction of ethanol pipelines.[20][21]

In the US minimal amounts of ethanol are transported by pipeline. Most ethanol is shipped by rail, the main alternatives being truck and barge. Delivering ethanol by pipeline is the most desirable option, but ethanol's affinity for water and solvent properties require the use of a dedicated pipeline, or significant cleanup of existing pipelines.

Slurry pipelines are sometimes used to transport coal or ore from mines. The material to be transported is closely mixed with water before being introduced to the pipeline; at the far end, the material must be dried. One example is a 525-kilometre (326 mi) slurry pipeline which is planned to transport iron ore from the Minas-Rio mine (producing 26.5 million tonnes per year) to the Port of Açu in Brazil.[22] An existing example is the 85-kilometre (53 mi) Savage River Slurry pipeline in Tasmania, Australia, possibly the world's first when it was built in 1967. It includes a 366-metre (1,201 ft) bridge span at 167 metres (548 ft) above the Savage River.[23][24]

Hydrogen pipeline transport is a transportation of hydrogen through a pipe as part of the hydrogen infrastructure. Hydrogen pipeline transport is used to connect the point of hydrogen production or delivery of hydrogen with the point of demand, with transport costs similar to CNG,[25] the technology is proven.[26] Most hydrogen is produced at the place of demand with every 50 to 100 miles (160 km) an industrial production facility.[27] The 1938 Rhine-Ruhr 240-kilometre (150 mi) hydrogen pipeline is still in operation.[28] As of 2004[update], there are 900 miles (1,400 km) of low pressure hydrogen pipelines in the US and 930 miles (1,500 km) in Europe.

Two millennia ago, the ancient Romans made use of large aqueducts to transport water from higher elevations by building the aqueducts in graduated segments that allowed gravity to push the water along until it reached its destination. Hundreds of these were built throughout Europe and elsewhere, and along with flour mills were considered the lifeline of the Roman Empire. The ancient Chinese also made use of channels and pipe systems for public works. The famous Han dynasty court eunuch Zhang Rang (d. 189 AD) once ordered the engineer Bi Lan to construct a series of square-pallet chain pumps outside the capital city of Luoyang.[29] These chain pumps serviced the imperial palaces and living quarters of the capital city as the water lifted by the chain pumps was brought in by a stoneware pipe system.[29][30]

Pipelines are useful for transporting water for drinking or irrigation over long distances when it needs to move over hills, or where canals or channels are poor choices due to considerations of evaporation, pollution, or environmental impact.

The 530 km (330 miles) Goldfields Water Supply Scheme in Western Australia using 750 mm (30 inch) pipe and completed in 1903 was the largest water supply scheme of its time.[31][32]

Examples of significant water pipelines in South Australia are the Morgan-Whyalla pipeline (completed 1944) and Mannum-Adelaide pipeline (completed 1955) pipelines, both part of the larger Snowy Mountains scheme.[33]

Two Los Angeles, California aqueducts, the Owens Valley aqueduct (completed 1913) and the Second Los Angeles Aqueduct (completed 1970), include extensive use of pipelines.

The Great Manmade River of Libya supplies 3,680,000 cubic metres (4,810,000 cu yd) of water each day to Tripoli, Benghazi, Sirte, and several other cities in Libya. The pipeline is over 2,800 kilometres (1,700 mi) long, and is connected to wells tapping an aquifer over 500 metres (1,600 ft) underground.[34]

District heating or teleheating systems consist of a network of insulated feed and return pipes which transport heated water, pressurized hot water, or sometimes steam to the customer. While steam is hottest and may be used in industrial processes due to its higher temperature, it is less efficient to produce and transport due to greater heat losses. Heat transfer oils are generally not used for economic and ecological reasons. The typical annual loss of thermal energy through distribution is around 10%, as seen in Norway's district heating network.[36]

District heating pipelines are normally installed underground, with some exceptions. Within the system, heat storage may be installed to even out peak load demands. Heat is transferred into the central heating of the dwellings through heat exchangers at heat substations, without mixing of the fluids in either system.

Bars in the Veltins-Arena, a major football ground in Gelsenkirchen, Germany, are interconnected by a 5-kilometre (3.1 mi) long beer pipeline. In Randers city in Denmark, the so-called Thor Beer pipeline was operated. Originally, copper pipes ran directly from the brewery, but when the brewery moved out of the city in the 1990s, Thor Beer replaced it with a giant tank.

A three-kilometer beer pipeline was completed in Bruges, Belgium in September 2016 to reduce truck traffic on the city streets.[37]

The village of Hallstatt in Austria, which is known for its long history of salt mining, claims to contain ""the oldest industrial pipeline in the world"", dating back to 1595.[38] It was constructed from 13,000 hollowed-out tree trunks to transport brine 40 kilometres (25 mi) from Hallstatt to Ebensee.[39]

Between 1978 and 1994, a 15 km milk pipeline ran between the Dutch island of Ameland and Holwerd on the mainland, of which 8 km was beneath the Wadden Sea. Every day, 30,000 litres of milk produced on the island were transported to be processed on the mainland. In 1994, the pipeline was abandoned.[40]

Rather than transporting fluids, pneumatic tubes are usually used to transport solids in a cylindrical container by compressed air or by partial vacuum. They were most popular in the late 19th and early 20th centuries, and were used to transport small solid objects within a building, e.g. documents in an office or money in a bank. By the 21st century, pneumatic tube transport had been mostly superseded by digital solutions for transporting information, but is still used in cases where convenience and speed in a local environment are important. Hospitals, for example, use them to deliver drugs and specimens.[41]

In places, a pipeline may have to cross water expanses, such as small seas, straits and rivers.[42] In many instances, they lie entirely on the seabed. These pipelines are referred to as ""marine"" pipelines (also, ""submarine"" or ""offshore"" pipelines). They are used primarily to carry oil or gas, but transportation of water is also important.[42] In offshore projects, a distinction is made between a ""flowline"" and a pipeline.[42][43][44] The former is an intrafield pipeline, in the sense that it is used to connect subsea wellheads, manifolds and the platform within a particular development field. The latter, sometimes referred to as an ""export pipeline"", is used to bring the resource to shore.[43] The construction and maintenance of marine pipelines imply logistical challenges that are different from those onland, mainly because of wave and current dynamics, along with other geohazards.

In general, pipelines can be classified in three categories depending on purpose:

When a pipeline is built, the construction project not only covers the civil engineering work to lay the pipeline and build the pump/compressor stations, it also has to cover all the work related to the installation of the field devices that will support remote operation.

The pipeline is routed along what is known as a ""right of way"". Pipelines are generally developed and built using the following stages:

Russia has ""Pipeline Troops"" as part of the Rear Services, who are trained to build and repair pipelines. Russia is the only country to have Pipeline Troops.[46]

The U.S. government, mainly through the EPA, the FERC and others, reviews proposed pipeline projects in order to comply with the Clean Water Act, the National Environmental Policy Act, other laws and, in some cases, municipal laws.[47][48] The Biden administration has sought to permit the respective states and tribal groups to appraise and potentially block the proposed projects.[49]

Field devices are instrumentation, data gathering units and communication systems. The field instrumentation includes flow, pressure, and temperature gauges/transmitters, and other devices to measure the relevant data required. These instruments are installed along the pipeline on some specific locations, such as injection or delivery stations, pump stations (liquid pipelines) or compressor stations (gas pipelines), and block valve stations.

The information measured by these field instruments is then gathered in local remote terminal units (RTU) that transfer the field data to a central location in real time using communication systems, such as satellite channels, microwave links, or cellular phone connections.

Pipelines are controlled and operated remotely, from what is usually known as the ""Main Control Room"". In this center, all the data related to field measurement is consolidated in one central database. The data is received from multiple RTUs along the pipeline. It is common to find RTUs installed at every station along the pipeline.

The SCADA system at the Main Control Room receives all the field data and presents it to the pipeline operator through a set of screens or Human Machine Interface, showing the operational conditions of the pipeline. The operator can monitor the hydraulic conditions of the line, as well as send operational commands (open/close valves, turn on/off compressors or pumps, change setpoints, etc.) through the SCADA system to the field.

To optimize and secure the operation of these assets, some pipeline companies are using what is called ""Advanced Pipeline Applications"", which are software tools installed on top of the SCADA system, that provide extended functionality to perform leak detection, leak location, batch tracking (liquid lines), pig tracking, composition tracking, predictive modeling, look ahead modeling, and operator training.

Pipeline networks are composed of several pieces of equipment that operate together to move products from location to location. The main elements of a pipeline system are:

Since oil and gas pipelines are an important asset of the economic development of almost any country, it has been required either by government regulations or internal policies to ensure the safety of the assets, and the population and environment where these pipelines run.

Pipeline companies face government regulation, environmental constraints and social situations. Government regulations may define minimum staff to run the operation, operator training requirements, pipeline facilities, technology and applications required to ensure operational safety. For example, in the State of Washington it is mandatory for pipeline operators to be able to detect and locate leaks of 8 percent of maximum flow within fifteen minutes or less. Social factors also affect the operation of pipelines. Product theft is sometimes also a problem for pipeline companies. In this case, the detection levels should be under two percent of maximum flow, with a high expectation for location accuracy.

Various technologies and strategies have been implemented for monitoring pipelines, from physically walking the lines to satellite surveillance. The most common technology to protect pipelines from occasional leaks is Computational Pipeline Monitoring or CPM. CPM takes information from the field related to pressures, flows, and temperatures to estimate the hydraulic behavior of the product being transported. Once the estimation is completed, the results are compared to other field references to detect the presence of an anomaly or unexpected situation, which may be related to a leak.

The American Petroleum Institute has published several articles related to the performance of CPM in liquids pipelines. The API Publications are:

Where a pipeline containing passes under a road or railway, it is usually enclosed in a protective casing. This casing is vented to the atmosphere to prevent the build-up of flammable gases or corrosive substances, and to allow the air inside the casing to be sampled to detect leaks. The casing vent, a pipe protruding from the ground, often doubles as a warning marker called a casing vent marker.[50]

Pipelines are generally laid underground because temperature is less variable. Because pipelines are usually metal, this helps to reduce the expansion and shrinkage that can occur with weather changes.[51] However, in some cases it is necessary to cross a valley or a river on a pipeline bridge. Pipelines for centralized heating systems are often laid on the ground or overhead. Pipelines for petroleum running through permafrost areas as Trans-Alaska-Pipeline are often run overhead in order to avoid melting the frozen ground by hot petroleum which would result in sinking the pipeline in the ground.

Maintenance of pipelines includes checking cathodic protection levels for the proper range, surveillance for construction, erosion, or leaks by foot, land vehicle, boat, or air, and running cleaning pigs when there is anything carried in the pipeline that is corrosive.

US pipeline maintenance rules are covered in Code of Federal Regulations(CFR) sections, 49 CFR 192 for natural gas pipelines, and 49 CFR 195 for petroleum liquid pipelines.

In the US, onshore and offshore pipelines used to transport oil and gas are regulated by the Pipeline and Hazardous Materials Safety Administration (PHMSA). Certain offshore pipelines used to produce oil and gas are regulated by the Minerals Management Service (MMS). In Canada, pipelines are regulated by either the provincial regulators or, if they cross provincial boundaries or the Canada–US border, by the National Energy Board (NEB). Government regulations in Canada and the United States require that buried fuel pipelines must be protected from corrosion. Often, the most economical method of corrosion control is by use of pipeline coating in conjunction with cathodic protection and technology to monitor the pipeline. Above ground, cathodic protection is not an option. The coating is the only external protection.

Pipelines for major energy resources (petroleum and natural gas) are not merely an element of trade. They connect to issues of geopolitics and international security as well, and the construction, placement, and control of oil and gas pipelines often figure prominently in state interests and actions. A notable example of pipeline politics occurred at the beginning of the year 2009, wherein a dispute between Russia and Ukraine ostensibly over pricing led to a major political crisis. Russian state-owned gas company Gazprom cut off natural gas supplies to Ukraine after talks between it and the Ukrainian government fell through. In addition to cutting off supplies to Ukraine, Russian gas flowing through Ukraine—which included nearly all supplies to Southeastern Europe and some supplies to Central and Western Europe—was cut off, creating a major crisis in several countries heavily dependent on Russian gas as fuel. Russia was accused of using the dispute as leverage in its attempt to keep other powers, and particularly the European Union, from interfering in its ""near abroad"".

Because the solvent fraction of dilbit typically comprises volatile aromatics such as naptha and benzene, reasonably rapid carrier vaporization can be expected to follow an above-ground spill—ostensibly enabling timely intervention by leaving only a viscous residue that is slow to migrate. Effective protocols to minimize exposure to petrochemical vapours are well-established, and oil spilled from the pipeline would be unlikely to reach the aquifer unless incomplete remediation were followed by the introduction of another carrier (e.g. a series of torrential downpours).

The introduction of benzene and other volatile organic compounds (collectively BTEX) to the subterranean environment compounds the threat posed by a pipeline leak. Particularly if followed by rain, a pipeline breach would result in BTEX dissolution and equilibration of benzene in water, followed by percolation of the admixture into the aquifer. Benzene can cause many health problems and is carcinogenic with EPA Maximum Contaminant Level (MCL) set at 5 μg/L for potable water.[52] Although it is not well studied, single benzene exposure events have been linked to acute carcinogenesis.[53] Additionally, the exposure of livestock, mainly cattle, to benzene has been shown to cause many health issues, such as neurotoxicity, fetal damage and fatal poisoning.[54]

The entire surface of an above-ground pipeline can be directly examined for material breach. Pooled petroleum is unambiguous, readily spotted, and indicates the location of required repairs. Because the effectiveness of remote inspection is limited by the cost of monitoring equipment, gaps between sensors, and data that requires interpretation, small leaks in buried pipe can sometimes go undetected.

Pipeline developers do not always prioritize effective surveillance against leaks. Buried pipes draw fewer complaints. They are insulated from extremes in ambient temperature, they are shielded from ultraviolet rays, and they are less exposed to photodegradation. Buried pipes are isolated from airborne debris, electrical storms, tornadoes, hurricanes, hail, and acid rain. They are protected from nesting birds, rutting mammals, and stray buckshot. Buried pipe is less vulnerable to accident damage (e.g. automobile collisions) and less accessible to vandals, saboteurs, and terrorists.

Previous work[55] has shown that a 'worst-case exposure scenario' can be limited to a specific set of conditions. Based on the advanced detection methods and pipeline shut-off SOP developed by TransCanada, the risk of a substantive or large release over a short period of time contaminating groundwater with benzene is unlikely.[56] Detection, shutoff, and remediation procedures would limit the dissolution and transport of benzene. Therefore, the exposure of benzene would be limited to leaks that are below the limit of detection and go unnoticed for extended periods of time.[55] Leak detection is monitored through a SCADA system that assesses pressure and volume flow every 5 seconds. A pinhole leak that releases small quantities that cannot be detected by the SCADA system (<1.5% flow) could accumulate into a substantive spill.[56] Detection of pinhole leaks would come from a visual or olfactory inspection, aerial surveying, or mass-balance inconsistencies.[56] It is assumed that pinhole leaks are discovered within the 14-day inspection interval, however snow cover and location (e.g. remote, deep) could delay detection. Benzene typically makes up 0.1 – 1.0% of oil and will have varying degrees of volatility and dissolution based on environmental factors.

Even with pipeline leak volumes within SCADA detection limits, sometimes pipeline leaks are misinterpreted by pipeline operators to be pump malfunctions, or other problems. The Enbridge Line 6B crude oil pipeline failure in Marshall, Michigan, on July 25, 2010, was thought by operators in Edmonton to be from column separation of the dilbit in that pipeline. The leak in wetlands along the Kalamazoo River was only confirmed 17 hours after it happened by a local gas company employee.

Although the Pipeline and Hazardous Materials Safety Administration (PHMSA) has standard baseline incident frequencies to estimate the number of spills, TransCanada altered these assumptions based on improved pipeline design, operation, and safety.[56] Whether these adjustments are justified is debatable as these assumptions resulted in a nearly 10-fold decrease in spill estimates.[55] Given that the pipeline crosses 247 miles of the Ogallala Aquifer,[57] or 14.5% of the entire pipeline length, and the 50-year life of the entire pipeline is expected to have between 11 – 91 spills,[55] approximately 1.6 – 13.2 spills can be expected to occur over the aquifer. An estimate of 13.2 spills over the aquifer, each lasting 14 days, results in 184 days of potential exposure over the 50 year lifetime of the pipeline.
In the reduced-scope worst-case exposure scenario, the volume of a pinhole leak at 1.5% of max flow-rate for 14 days has been estimated at 189,000 barrels or 7.9 million gallons of oil.[55] According to PHMSA's incident database,[58] only 0.5% of all spills in the last 10 years were >10,000 barrels.

Benzene is considered a light aromatic hydrocarbon with high solubility and high volatility.[clarification needed] It is unclear how temperature and depth would impact the volatility of benzene, so assumptions have been made that benzene in oil (1% weight by volume) would not volatilize before equilibrating with water.[55]

Using the octanol-water partition coefficient and a 100-year precipitation event for the area, a worst-case estimate of 75 mg/L of benzene is anticipated to flow toward the aquifer.[55] The actual movement of the plume through groundwater systems is not well described, although one estimate is that up to 4.9 billion gallons of water in the Ogallala Aquifer could become contaminated with benzene at concentrations above the MCL.[55] The Final Environmental Impact Statement from the State Department does not include a quantitative analysis because it assumed that most benzene will volatilize.[56]

One of the major concerns over dilbit is the difficulty in cleaning it up.[59] When the aforementioned Enbridge Line 6B crude oil pipeline ruptured in Marshall, Michigan in 2010, at least 843,000 gallons of dilbit were spilled.[60] After detection of the leak, booms and vacuum trucks were deployed. Heavy rains caused the river to overtop existing dams, and carried dilbit 30 miles downstream before the spill was contained. Remediation work collected over 1.1 million gallons of oil and almost 200,000 cubic yards of oil-contaminated sediment and debris from the Kalamazoo River system. However, oil was still being found in affected waters in October 2012.[61]

Pipelines can help ensure a country's economic well-being and as such present a likely target of terrorists or wartime adversaries.
Fossil fuels can be transported by pipeline, rail, truck or ship, though natural gas requires compression or liquefaction to make vehicle transport economical. For transport of crude oil via these four modes, various reports rank pipelines as proportionately causing less human death and property damage than rail and truck and spilling less oil than truck.[7]

Pipelines conveying flammable or explosive material, such as natural gas or oil, pose special safety concerns. While corrosion, pressure, and equipment failure are common causes, excavation damage is also a leading accident type that can be avoided by calling 811 before digging near pipelines.[62]

Pipelines can be the target of vandalism, sabotage, or even terrorist attacks. For example, between early 2011 and July 2012, a natural gas pipeline connecting Egypt to Israel and Jordan was attacked 15 times.[74] In 2019, a fuel pipeline north of Mexico City exploded after fuel thieves tapped into the line. At least sixty-six people were reported to have been killed.[75] In war, pipelines are often the target of military attacks, as destruction of pipelines can seriously disrupt enemy logistics. On 26 September 2022, a series of explosions and subsequent major gas leaks occurred on the Nord Stream 1 and Nord Stream 2 pipelines that run to Europe from Russia under the Baltic Sea. The leaks are believed to have been caused by an act of sabotage.[76][77][78]
"
New Ground Pipeline Installation,"A pipeline is a system of pipes for long-distance transportation of a liquid or gas, typically to a market area for consumption. The latest data from 2014 gives a total of slightly less than 2,175,000 miles (3,500,000 km) of pipeline in 120 countries around the world.[1] The United States had 65%, Russia had 8%, and Canada had 3%, thus 76% of all pipeline were in these three countries.[1] The main attribute to pollution from pipelines is caused by corrosion and leakage.[2]

Pipeline and Gas Journal's worldwide survey figures indicate that 118,623 miles (190,905 km) of pipelines are planned and under construction. Of these, 88,976 miles (143,193 km) represent projects in the planning and design phase; 29,647 miles (47,712 km) reflect pipelines in various stages of construction. Liquids and gases are transported in pipelines, and any chemically stable substance can be sent through a pipeline.[3]


Pipelines exist for the transport of crude and refined petroleum, fuels – such as oil, natural gas and biofuels – and other fluids including sewage, slurry, water, beer, hot water or steam for shorter distances and even pneumatic systems which allow for the generation of suction pressure for useful work and in transporting solid objects.[4] Pipelines are useful for transporting water for drinking or irrigation over long distances when it needs to move over hills, or where canals or channels are poor choices due to considerations of evaporation, pollution, or environmental impact. 
Oil pipelines are made from steel or plastic tubes which are usually buried. The oil is moved through the pipelines by pump stations along the pipeline. Natural gas (and similar gaseous fuels) are pressurized into liquids known as natural gas liquids (NGLs).[5] Natural gas pipelines are constructed of carbon steel. Hydrogen pipeline transport is the transportation of hydrogen through a pipe. Pipelines are one of the safest ways of transporting materials as compared to road or rail, and hence in war, pipelines are often the target of military attacks.[6]

It is well documented when the first crude oil pipeline was built. Credit for the development of pipeline transport belongs indisputably to the Oil Transport Association, which first constructed a 2-inch (51 mm) wrought iron pipeline over a 6-mile (9.7 km) track from an oil field in Pennsylvania to a railroad station in Oil Creek, in the 1860s. Some of the first major submarine pipelines were constructed across the English Channel in 1944 during Operation Pluto. These provided an estimated 8 per cent of all petroleum products used by the Allies on the Western Front between the Normandy landings and the end of Second World War.

Pipelines are generally the most economical way to transport large quantities of oil, refined oil products or natural gas over land. For example, in 2014, pipeline transport of crude oil cost about $5 per barrel, while rail transport cost about $10 to $15 per barrel.[7] Trucking has even higher costs due to the additional labor required; employment on completed pipelines represents only ""1% of that of the trucking industry."".[8]

In the United States, 70% of crude oil and petroleum products are shipped by pipeline. (23% are by ship, 4% by truck, and 3% by rail) In Canada for natural gas and petroleum products, 97% are shipped by pipeline.[7]

Natural gas (and similar gaseous fuels) are lightly pressurized into liquids known as Natural Gas Liquids (NGLs). Small NGL processing facilities can be located in oil fields so the butane and propane liquid under light pressure of 125 pounds per square inch (860 kPa), can be shipped by rail, truck or pipeline. Propane can be used as a fuel in oil fields to heat various facilities used by the oil drillers or equipment and trucks used in the oil patch. EG: Propane will convert from a gas to a liquid under light pressure, 100 psi, give or take depending on temperature, and is pumped into cars and trucks at less than 125 psi (860 kPa) at retail stations. Pipelines and rail cars use about double that pressure to pump at 250 psi (1,700 kPa).

The distance to ship propane to markets is much shorter, as thousands of natural-gas processing plants are located in or near oil fields. Many Bakken Basin oil companies in North Dakota, Montana, Manitoba and Saskatchewan gas fields separate the NGLs in the field, allowing the drillers to sell propane directly to small wholesalers, eliminating the large refinery control of product and prices for propane or butane.

The most recent major pipeline to start operating in North America is a TransCanada natural gas line going north across the Niagara region bridges. This gas line carries Marcellus shale gas from Pennsylvania and other tied in methane or natural gas sources into the Canadian province of Ontario. It began operations in the fall of 2012, supplying 16 percent of all the natural gas used in Ontario.[citation needed]

This new US-supplied natural gas displaces the natural gas formerly shipped to Ontario from western Canada in Alberta and Manitoba, thus dropping the government regulated pipeline shipping charges because of the significantly shorter distance from gas source to consumer. To avoid delays and US government regulation, many small, medium and large oil producers in North Dakota have decided to run an oil pipeline north to Canada to meet up with a Canadian oil pipeline shipping oil from west to east. This allows the Bakken Basin and Three Forks oil producers to get higher negotiated prices for their oil because they will not be restricted to just one wholesale market in the US. The distance from the biggest oil patch in North Dakota, in Williston, North Dakota, is only about 85 miles or 137 kilometers to the Canada–US border and Manitoba. Mutual funds and joint ventures are the largest investors in new oil and gas pipelines. In the fall of 2012, the US began exporting propane to Europe, known as LPG, as wholesale prices there are much higher than in North America. Additionally, a pipeline is currently being constructed from North Dakota to Illinois, commonly known as the Dakota Access Pipeline.[9]

As more North American pipelines are built, even more exports of LNG, propane, butane, and other natural gas products occur on all three US coasts. To give insight, North Dakota Bakken region's oil production has grown by 600% from 2007 to 2015.[10] North Dakota oil companies are shipping huge amounts of oil by tanker rail car as they can direct the oil to the market that gives the best price, and rail cars can be used to avoid a congested oil pipeline to get the oil to a different pipeline in order to get the oil to market faster or to a different less busy oil refinery. However, pipelines provide a cheaper means to transport by volume.

Enbridge in Canada is applying to reverse an oil pipeline going from east-to-west (Line 9) and expanding it and using it to ship western Canadian bitumen oil eastward.[11] From a presently rated 250,000 barrels equivalent per day pipeline, it will be expanded to between 1.0 and 1.3 million barrels per day. It will bring western oil to refineries in Ontario, Michigan, Ohio, Pennsylvania, Quebec and New York by early 2014. New Brunswick will also refine some of this western Canadian crude and export some crude and refined oil to Europe from its deep water oil ULCC loading port.

Although pipelines can be built under the sea, that process is economically and technically demanding, so the majority of oil at sea is transported by tanker ships. Similarly, it is often more economically feasible to transport natural gas in the form of LNG, however the break-even point between LNG and pipelines would depend on the volume of natural gas and the distance it travels.[12]

The market size for oil and gas pipeline construction experienced tremendous growth prior to the economic downturn in 2008. After faltering in 2009, demand for pipeline expansion and updating increased the following year as energy production grew.[13] By 2012, almost 32,000 miles (51500 km) of North American pipeline were being planned or under construction.[14] When pipelines are constrained, additional pipeline product transportation options may include the use of drag reducing agents, or by transporting product via truck or rail.

Oil pipelines are made from steel or plastic tubes with inner diameter typically from 4 to 48 inches (100 to 1,220 mm). Most pipelines are typically buried at a depth of about 3 to 6 feet (0.91 to 1.83 m). To protect pipes from impact, abrasion, and corrosion, a variety of methods are used. These can include wood lagging (wood slats), concrete coating, rockshield, high-density polyethylene, imported sand padding, sacrificial cathodes and padding machines.[15]

Crude oil contains varying amounts of paraffin wax and in colder climates wax buildup may occur within a pipeline. Often these pipelines are inspected and cleaned using pigging, the practice of using devices known as ""pigs"" to perform various maintenance operations on a pipeline. The devices are also known as ""scrapers"" or ""Go-devils"". ""Smart pigs"" (also known as ""intelligent"" or ""intelligence"" pigs) are used to detect anomalies in the pipe such as dents, metal loss caused by corrosion, cracking or other mechanical damage.[16] These devices are launched from pig-launcher stations and travel through the pipeline to be received at any other station down-stream, either cleaning wax deposits and material that may have accumulated inside the line or inspecting and recording the condition of the line.

For natural gas, pipelines are constructed of carbon steel and vary in size from 2 to 60 inches (51 to 1,524 mm) in diameter, depending on the type of pipeline. The gas is pressurized by compressor stations and is odorless unless mixed with a mercaptan odorant where required by a regulating authority.

Until damaged during the Russian invasion of Ukraine,[17] the Russian–Ukrainian Transammiak line was the longest ammonia pipeline in the world, at 2,500 km.[18] It connected the TogliattiAzot facility in Russia to the exporting Black Sea-port of Odesa in Ukraine.

Pipelines have been used for transportation of ethanol in Brazil, and there are several ethanol pipeline projects in Brazil and the United States.[19] The main problems related to the transport of ethanol by pipeline are its corrosive nature and tendency to absorb water and impurities in pipelines, which are not problems with oil and natural gas.[19][20] Insufficient volumes and cost-effectiveness are other considerations limiting construction of ethanol pipelines.[20][21]

In the US minimal amounts of ethanol are transported by pipeline. Most ethanol is shipped by rail, the main alternatives being truck and barge. Delivering ethanol by pipeline is the most desirable option, but ethanol's affinity for water and solvent properties require the use of a dedicated pipeline, or significant cleanup of existing pipelines.

Slurry pipelines are sometimes used to transport coal or ore from mines. The material to be transported is closely mixed with water before being introduced to the pipeline; at the far end, the material must be dried. One example is a 525-kilometre (326 mi) slurry pipeline which is planned to transport iron ore from the Minas-Rio mine (producing 26.5 million tonnes per year) to the Port of Açu in Brazil.[22] An existing example is the 85-kilometre (53 mi) Savage River Slurry pipeline in Tasmania, Australia, possibly the world's first when it was built in 1967. It includes a 366-metre (1,201 ft) bridge span at 167 metres (548 ft) above the Savage River.[23][24]

Hydrogen pipeline transport is a transportation of hydrogen through a pipe as part of the hydrogen infrastructure. Hydrogen pipeline transport is used to connect the point of hydrogen production or delivery of hydrogen with the point of demand, with transport costs similar to CNG,[25] the technology is proven.[26] Most hydrogen is produced at the place of demand with every 50 to 100 miles (160 km) an industrial production facility.[27] The 1938 Rhine-Ruhr 240-kilometre (150 mi) hydrogen pipeline is still in operation.[28] As of 2004[update], there are 900 miles (1,400 km) of low pressure hydrogen pipelines in the US and 930 miles (1,500 km) in Europe.

Two millennia ago, the ancient Romans made use of large aqueducts to transport water from higher elevations by building the aqueducts in graduated segments that allowed gravity to push the water along until it reached its destination. Hundreds of these were built throughout Europe and elsewhere, and along with flour mills were considered the lifeline of the Roman Empire. The ancient Chinese also made use of channels and pipe systems for public works. The famous Han dynasty court eunuch Zhang Rang (d. 189 AD) once ordered the engineer Bi Lan to construct a series of square-pallet chain pumps outside the capital city of Luoyang.[29] These chain pumps serviced the imperial palaces and living quarters of the capital city as the water lifted by the chain pumps was brought in by a stoneware pipe system.[29][30]

Pipelines are useful for transporting water for drinking or irrigation over long distances when it needs to move over hills, or where canals or channels are poor choices due to considerations of evaporation, pollution, or environmental impact.

The 530 km (330 miles) Goldfields Water Supply Scheme in Western Australia using 750 mm (30 inch) pipe and completed in 1903 was the largest water supply scheme of its time.[31][32]

Examples of significant water pipelines in South Australia are the Morgan-Whyalla pipeline (completed 1944) and Mannum-Adelaide pipeline (completed 1955) pipelines, both part of the larger Snowy Mountains scheme.[33]

Two Los Angeles, California aqueducts, the Owens Valley aqueduct (completed 1913) and the Second Los Angeles Aqueduct (completed 1970), include extensive use of pipelines.

The Great Manmade River of Libya supplies 3,680,000 cubic metres (4,810,000 cu yd) of water each day to Tripoli, Benghazi, Sirte, and several other cities in Libya. The pipeline is over 2,800 kilometres (1,700 mi) long, and is connected to wells tapping an aquifer over 500 metres (1,600 ft) underground.[34]

District heating or teleheating systems consist of a network of insulated feed and return pipes which transport heated water, pressurized hot water, or sometimes steam to the customer. While steam is hottest and may be used in industrial processes due to its higher temperature, it is less efficient to produce and transport due to greater heat losses. Heat transfer oils are generally not used for economic and ecological reasons. The typical annual loss of thermal energy through distribution is around 10%, as seen in Norway's district heating network.[36]

District heating pipelines are normally installed underground, with some exceptions. Within the system, heat storage may be installed to even out peak load demands. Heat is transferred into the central heating of the dwellings through heat exchangers at heat substations, without mixing of the fluids in either system.

Bars in the Veltins-Arena, a major football ground in Gelsenkirchen, Germany, are interconnected by a 5-kilometre (3.1 mi) long beer pipeline. In Randers city in Denmark, the so-called Thor Beer pipeline was operated. Originally, copper pipes ran directly from the brewery, but when the brewery moved out of the city in the 1990s, Thor Beer replaced it with a giant tank.

A three-kilometer beer pipeline was completed in Bruges, Belgium in September 2016 to reduce truck traffic on the city streets.[37]

The village of Hallstatt in Austria, which is known for its long history of salt mining, claims to contain ""the oldest industrial pipeline in the world"", dating back to 1595.[38] It was constructed from 13,000 hollowed-out tree trunks to transport brine 40 kilometres (25 mi) from Hallstatt to Ebensee.[39]

Between 1978 and 1994, a 15 km milk pipeline ran between the Dutch island of Ameland and Holwerd on the mainland, of which 8 km was beneath the Wadden Sea. Every day, 30,000 litres of milk produced on the island were transported to be processed on the mainland. In 1994, the pipeline was abandoned.[40]

Rather than transporting fluids, pneumatic tubes are usually used to transport solids in a cylindrical container by compressed air or by partial vacuum. They were most popular in the late 19th and early 20th centuries, and were used to transport small solid objects within a building, e.g. documents in an office or money in a bank. By the 21st century, pneumatic tube transport had been mostly superseded by digital solutions for transporting information, but is still used in cases where convenience and speed in a local environment are important. Hospitals, for example, use them to deliver drugs and specimens.[41]

In places, a pipeline may have to cross water expanses, such as small seas, straits and rivers.[42] In many instances, they lie entirely on the seabed. These pipelines are referred to as ""marine"" pipelines (also, ""submarine"" or ""offshore"" pipelines). They are used primarily to carry oil or gas, but transportation of water is also important.[42] In offshore projects, a distinction is made between a ""flowline"" and a pipeline.[42][43][44] The former is an intrafield pipeline, in the sense that it is used to connect subsea wellheads, manifolds and the platform within a particular development field. The latter, sometimes referred to as an ""export pipeline"", is used to bring the resource to shore.[43] The construction and maintenance of marine pipelines imply logistical challenges that are different from those onland, mainly because of wave and current dynamics, along with other geohazards.

In general, pipelines can be classified in three categories depending on purpose:

When a pipeline is built, the construction project not only covers the civil engineering work to lay the pipeline and build the pump/compressor stations, it also has to cover all the work related to the installation of the field devices that will support remote operation.

The pipeline is routed along what is known as a ""right of way"". Pipelines are generally developed and built using the following stages:

Russia has ""Pipeline Troops"" as part of the Rear Services, who are trained to build and repair pipelines. Russia is the only country to have Pipeline Troops.[46]

The U.S. government, mainly through the EPA, the FERC and others, reviews proposed pipeline projects in order to comply with the Clean Water Act, the National Environmental Policy Act, other laws and, in some cases, municipal laws.[47][48] The Biden administration has sought to permit the respective states and tribal groups to appraise and potentially block the proposed projects.[49]

Field devices are instrumentation, data gathering units and communication systems. The field instrumentation includes flow, pressure, and temperature gauges/transmitters, and other devices to measure the relevant data required. These instruments are installed along the pipeline on some specific locations, such as injection or delivery stations, pump stations (liquid pipelines) or compressor stations (gas pipelines), and block valve stations.

The information measured by these field instruments is then gathered in local remote terminal units (RTU) that transfer the field data to a central location in real time using communication systems, such as satellite channels, microwave links, or cellular phone connections.

Pipelines are controlled and operated remotely, from what is usually known as the ""Main Control Room"". In this center, all the data related to field measurement is consolidated in one central database. The data is received from multiple RTUs along the pipeline. It is common to find RTUs installed at every station along the pipeline.

The SCADA system at the Main Control Room receives all the field data and presents it to the pipeline operator through a set of screens or Human Machine Interface, showing the operational conditions of the pipeline. The operator can monitor the hydraulic conditions of the line, as well as send operational commands (open/close valves, turn on/off compressors or pumps, change setpoints, etc.) through the SCADA system to the field.

To optimize and secure the operation of these assets, some pipeline companies are using what is called ""Advanced Pipeline Applications"", which are software tools installed on top of the SCADA system, that provide extended functionality to perform leak detection, leak location, batch tracking (liquid lines), pig tracking, composition tracking, predictive modeling, look ahead modeling, and operator training.

Pipeline networks are composed of several pieces of equipment that operate together to move products from location to location. The main elements of a pipeline system are:

Since oil and gas pipelines are an important asset of the economic development of almost any country, it has been required either by government regulations or internal policies to ensure the safety of the assets, and the population and environment where these pipelines run.

Pipeline companies face government regulation, environmental constraints and social situations. Government regulations may define minimum staff to run the operation, operator training requirements, pipeline facilities, technology and applications required to ensure operational safety. For example, in the State of Washington it is mandatory for pipeline operators to be able to detect and locate leaks of 8 percent of maximum flow within fifteen minutes or less. Social factors also affect the operation of pipelines. Product theft is sometimes also a problem for pipeline companies. In this case, the detection levels should be under two percent of maximum flow, with a high expectation for location accuracy.

Various technologies and strategies have been implemented for monitoring pipelines, from physically walking the lines to satellite surveillance. The most common technology to protect pipelines from occasional leaks is Computational Pipeline Monitoring or CPM. CPM takes information from the field related to pressures, flows, and temperatures to estimate the hydraulic behavior of the product being transported. Once the estimation is completed, the results are compared to other field references to detect the presence of an anomaly or unexpected situation, which may be related to a leak.

The American Petroleum Institute has published several articles related to the performance of CPM in liquids pipelines. The API Publications are:

Where a pipeline containing passes under a road or railway, it is usually enclosed in a protective casing. This casing is vented to the atmosphere to prevent the build-up of flammable gases or corrosive substances, and to allow the air inside the casing to be sampled to detect leaks. The casing vent, a pipe protruding from the ground, often doubles as a warning marker called a casing vent marker.[50]

Pipelines are generally laid underground because temperature is less variable. Because pipelines are usually metal, this helps to reduce the expansion and shrinkage that can occur with weather changes.[51] However, in some cases it is necessary to cross a valley or a river on a pipeline bridge. Pipelines for centralized heating systems are often laid on the ground or overhead. Pipelines for petroleum running through permafrost areas as Trans-Alaska-Pipeline are often run overhead in order to avoid melting the frozen ground by hot petroleum which would result in sinking the pipeline in the ground.

Maintenance of pipelines includes checking cathodic protection levels for the proper range, surveillance for construction, erosion, or leaks by foot, land vehicle, boat, or air, and running cleaning pigs when there is anything carried in the pipeline that is corrosive.

US pipeline maintenance rules are covered in Code of Federal Regulations(CFR) sections, 49 CFR 192 for natural gas pipelines, and 49 CFR 195 for petroleum liquid pipelines.

In the US, onshore and offshore pipelines used to transport oil and gas are regulated by the Pipeline and Hazardous Materials Safety Administration (PHMSA). Certain offshore pipelines used to produce oil and gas are regulated by the Minerals Management Service (MMS). In Canada, pipelines are regulated by either the provincial regulators or, if they cross provincial boundaries or the Canada–US border, by the National Energy Board (NEB). Government regulations in Canada and the United States require that buried fuel pipelines must be protected from corrosion. Often, the most economical method of corrosion control is by use of pipeline coating in conjunction with cathodic protection and technology to monitor the pipeline. Above ground, cathodic protection is not an option. The coating is the only external protection.

Pipelines for major energy resources (petroleum and natural gas) are not merely an element of trade. They connect to issues of geopolitics and international security as well, and the construction, placement, and control of oil and gas pipelines often figure prominently in state interests and actions. A notable example of pipeline politics occurred at the beginning of the year 2009, wherein a dispute between Russia and Ukraine ostensibly over pricing led to a major political crisis. Russian state-owned gas company Gazprom cut off natural gas supplies to Ukraine after talks between it and the Ukrainian government fell through. In addition to cutting off supplies to Ukraine, Russian gas flowing through Ukraine—which included nearly all supplies to Southeastern Europe and some supplies to Central and Western Europe—was cut off, creating a major crisis in several countries heavily dependent on Russian gas as fuel. Russia was accused of using the dispute as leverage in its attempt to keep other powers, and particularly the European Union, from interfering in its ""near abroad"".

Because the solvent fraction of dilbit typically comprises volatile aromatics such as naptha and benzene, reasonably rapid carrier vaporization can be expected to follow an above-ground spill—ostensibly enabling timely intervention by leaving only a viscous residue that is slow to migrate. Effective protocols to minimize exposure to petrochemical vapours are well-established, and oil spilled from the pipeline would be unlikely to reach the aquifer unless incomplete remediation were followed by the introduction of another carrier (e.g. a series of torrential downpours).

The introduction of benzene and other volatile organic compounds (collectively BTEX) to the subterranean environment compounds the threat posed by a pipeline leak. Particularly if followed by rain, a pipeline breach would result in BTEX dissolution and equilibration of benzene in water, followed by percolation of the admixture into the aquifer. Benzene can cause many health problems and is carcinogenic with EPA Maximum Contaminant Level (MCL) set at 5 μg/L for potable water.[52] Although it is not well studied, single benzene exposure events have been linked to acute carcinogenesis.[53] Additionally, the exposure of livestock, mainly cattle, to benzene has been shown to cause many health issues, such as neurotoxicity, fetal damage and fatal poisoning.[54]

The entire surface of an above-ground pipeline can be directly examined for material breach. Pooled petroleum is unambiguous, readily spotted, and indicates the location of required repairs. Because the effectiveness of remote inspection is limited by the cost of monitoring equipment, gaps between sensors, and data that requires interpretation, small leaks in buried pipe can sometimes go undetected.

Pipeline developers do not always prioritize effective surveillance against leaks. Buried pipes draw fewer complaints. They are insulated from extremes in ambient temperature, they are shielded from ultraviolet rays, and they are less exposed to photodegradation. Buried pipes are isolated from airborne debris, electrical storms, tornadoes, hurricanes, hail, and acid rain. They are protected from nesting birds, rutting mammals, and stray buckshot. Buried pipe is less vulnerable to accident damage (e.g. automobile collisions) and less accessible to vandals, saboteurs, and terrorists.

Previous work[55] has shown that a 'worst-case exposure scenario' can be limited to a specific set of conditions. Based on the advanced detection methods and pipeline shut-off SOP developed by TransCanada, the risk of a substantive or large release over a short period of time contaminating groundwater with benzene is unlikely.[56] Detection, shutoff, and remediation procedures would limit the dissolution and transport of benzene. Therefore, the exposure of benzene would be limited to leaks that are below the limit of detection and go unnoticed for extended periods of time.[55] Leak detection is monitored through a SCADA system that assesses pressure and volume flow every 5 seconds. A pinhole leak that releases small quantities that cannot be detected by the SCADA system (<1.5% flow) could accumulate into a substantive spill.[56] Detection of pinhole leaks would come from a visual or olfactory inspection, aerial surveying, or mass-balance inconsistencies.[56] It is assumed that pinhole leaks are discovered within the 14-day inspection interval, however snow cover and location (e.g. remote, deep) could delay detection. Benzene typically makes up 0.1 – 1.0% of oil and will have varying degrees of volatility and dissolution based on environmental factors.

Even with pipeline leak volumes within SCADA detection limits, sometimes pipeline leaks are misinterpreted by pipeline operators to be pump malfunctions, or other problems. The Enbridge Line 6B crude oil pipeline failure in Marshall, Michigan, on July 25, 2010, was thought by operators in Edmonton to be from column separation of the dilbit in that pipeline. The leak in wetlands along the Kalamazoo River was only confirmed 17 hours after it happened by a local gas company employee.

Although the Pipeline and Hazardous Materials Safety Administration (PHMSA) has standard baseline incident frequencies to estimate the number of spills, TransCanada altered these assumptions based on improved pipeline design, operation, and safety.[56] Whether these adjustments are justified is debatable as these assumptions resulted in a nearly 10-fold decrease in spill estimates.[55] Given that the pipeline crosses 247 miles of the Ogallala Aquifer,[57] or 14.5% of the entire pipeline length, and the 50-year life of the entire pipeline is expected to have between 11 – 91 spills,[55] approximately 1.6 – 13.2 spills can be expected to occur over the aquifer. An estimate of 13.2 spills over the aquifer, each lasting 14 days, results in 184 days of potential exposure over the 50 year lifetime of the pipeline.
In the reduced-scope worst-case exposure scenario, the volume of a pinhole leak at 1.5% of max flow-rate for 14 days has been estimated at 189,000 barrels or 7.9 million gallons of oil.[55] According to PHMSA's incident database,[58] only 0.5% of all spills in the last 10 years were >10,000 barrels.

Benzene is considered a light aromatic hydrocarbon with high solubility and high volatility.[clarification needed] It is unclear how temperature and depth would impact the volatility of benzene, so assumptions have been made that benzene in oil (1% weight by volume) would not volatilize before equilibrating with water.[55]

Using the octanol-water partition coefficient and a 100-year precipitation event for the area, a worst-case estimate of 75 mg/L of benzene is anticipated to flow toward the aquifer.[55] The actual movement of the plume through groundwater systems is not well described, although one estimate is that up to 4.9 billion gallons of water in the Ogallala Aquifer could become contaminated with benzene at concentrations above the MCL.[55] The Final Environmental Impact Statement from the State Department does not include a quantitative analysis because it assumed that most benzene will volatilize.[56]

One of the major concerns over dilbit is the difficulty in cleaning it up.[59] When the aforementioned Enbridge Line 6B crude oil pipeline ruptured in Marshall, Michigan in 2010, at least 843,000 gallons of dilbit were spilled.[60] After detection of the leak, booms and vacuum trucks were deployed. Heavy rains caused the river to overtop existing dams, and carried dilbit 30 miles downstream before the spill was contained. Remediation work collected over 1.1 million gallons of oil and almost 200,000 cubic yards of oil-contaminated sediment and debris from the Kalamazoo River system. However, oil was still being found in affected waters in October 2012.[61]

Pipelines can help ensure a country's economic well-being and as such present a likely target of terrorists or wartime adversaries.
Fossil fuels can be transported by pipeline, rail, truck or ship, though natural gas requires compression or liquefaction to make vehicle transport economical. For transport of crude oil via these four modes, various reports rank pipelines as proportionately causing less human death and property damage than rail and truck and spilling less oil than truck.[7]

Pipelines conveying flammable or explosive material, such as natural gas or oil, pose special safety concerns. While corrosion, pressure, and equipment failure are common causes, excavation damage is also a leading accident type that can be avoided by calling 811 before digging near pipelines.[62]

Pipelines can be the target of vandalism, sabotage, or even terrorist attacks. For example, between early 2011 and July 2012, a natural gas pipeline connecting Egypt to Israel and Jordan was attacked 15 times.[74] In 2019, a fuel pipeline north of Mexico City exploded after fuel thieves tapped into the line. At least sixty-six people were reported to have been killed.[75] In war, pipelines are often the target of military attacks, as destruction of pipelines can seriously disrupt enemy logistics. On 26 September 2022, a series of explosions and subsequent major gas leaks occurred on the Nord Stream 1 and Nord Stream 2 pipelines that run to Europe from Russia under the Baltic Sea. The leaks are believed to have been caused by an act of sabotage.[76][77][78]
"
Excavation Services,"Excavators are heavy construction equipment primarily consisting of a boom, dipper (or stick), bucket, and cab on a rotating platform known as the ""house"".[1]

The modern excavator's house sits atop an undercarriage with tracks or wheels, being an evolution of the steam shovel (which itself evolved into the power shovel when steam was replaced by diesel and electric power). All excavation-related movement and functions of a hydraulic excavator are accomplished through the use of hydraulic fluid, with hydraulic cylinders and hydraulic motors,[2] which replaced winches, chains, and steel ropes.[3]  Another principle change was the direction of the digging action, with modern excavators pulling their buckets toward them like a dragline rather than pushing them away to fill them the way the first powered shovels did.

Excavators are also called diggers, scoopers, mechanical shovels, or 360-degree excavators (sometimes abbreviated simply to ""360""). Tracked excavators are sometimes called ""trackhoes"" by analogy to the backhoe.[4] In the UK, wheeled excavators are sometimes known as ""rubber ducks"".[5]

Excavators are used in many ways:

Modern hydraulic excavators come in a wide variety of sizes. The smaller ones are called mini or compact excavators.[7] For example, Caterpillar's smallest mini-excavator weighs 2,060 pounds (930 kg) and has 13 hp;[8] their largest model is the largest excavator available (developed and produced by the Orenstein & Koppel, Germany, until the takeover 2011 by Caterpillar, named »RH400«), the CAT 6090, which weighs in excess of 2,160,510 pounds (979,990 kg), has 4500 hp, and a bucket as large as 52.0 m3.

Hydraulic excavators usually couple engine power to (commonly) three hydraulic pumps rather than to mechanical drivetrains. The two main pumps supply oil at high pressure (up to 5000 psi, 345 bar) for the arms, swing motor, track motors and accessories while the third is a lower pressure (≈700 psi, 48 bar) pump for pilot control of the spool valves; this third circuit allows for reduced physical effort when operating the controls. Generally, the 3 pumps used in excavators consist of 2 variable displacement piston pumps and a gear pump. The arrangement of the pumps in the excavator unit changes with different manufacturers using different formats.

The three main sections of an excavator are the undercarriage, the house and the arm. The boom, the front part that is attached to the cab itself and holds the arm, is also used. The undercarriage includes tracks, track frame, and final drives, which have a hydraulic motor and gearing providing the drive to the individual tracks. The undercarriage, especially frequently for a mini-excavator, can also have blade similar to that of a bulldozer. The house includes the operator cab, counterweight, engine, fuel and hydraulic oil tanks. The house attaches to the undercarriage by way of a center pin. High-pressure oil is supplied to the tracks' hydraulic motors through a hydraulic swivel at the axis of the pin, allowing the machine to slew 360° unhindered and thus provides the left-and-right movement.[9] The arm provides the up-and-down and closer-and-further (or digging movement) movements. Arms typically consist of a boom, stick and bucket with three joints between them and the house.

The boom attaches to the house and provides the up-and-down movement. It can be one of several different configurations:

Attached to the end of the boom is the stick (or dipper arm). The stick provides the digging movement needed to pull the bucket through the ground. The stick length is optional depending whether reach (longer stick) or break-out power (shorter stick) is required. Most common is mono stick but there are also, for example, telescopic sticks. The largest form ever of an excavator, the dragline excavator, eliminated the dipper in favor of a line and winch.

On the end of the stick is usually a bucket.  A wide, large capacity (mud) bucket with a straight cutting edge is used for cleanup and levelling or where the material to be dug is soft, and teeth are not required.  A general purpose (GP) bucket is generally smaller, stronger, and has hardened side cutters and teeth used to break through hard ground and rocks. Buckets have numerous shapes and sizes for various applications. There are also many other attachments that are available to be attached to the excavator for boring, ripping, crushing, cutting, lifting, etc. Attachments can be attached with pins similar to other parts of the arm or with some variety of quick coupler. Excavators in Scandinavia often feature a tiltrotator which allows attachments rotate 360 degrees and tilt +/- 45 degrees, in order to increase the flexibility and precision of the excavator.

Before the 1990s, all excavators had a long or conventional counterweight that hung off the rear of the machine to provide more digging force and lifting capacity. This became a nuisance when working in confined areas.  In 1993 Yanmar launched the world's first Zero Tail Swing excavator,[10] which allows the counterweight to stay inside the width of the tracks as it slews, thus being safer and more user friendly when used in a confined space. This type of machine is now widely used throughout the world.

There are two main types of control configuration used in excavators to control the boom and bucket, each distributing the four primary digging functions across two x-y joysticks. This allows a skilled operator to control all four functions simultaneously.  The most popular configuration in the US is the SAE controls configuration while in other parts of the world, the ISO control configuration is more common.  Some manufacturers such as Takeuchi have switches that allow the operator to select which control configuration to use.

Hydraulic excavators now perform tasks well beyond bucket excavation. With the advent of hydraulic-powered attachments such as a breaker, a cutter, a grapple or an auger,a crusher and screening buckets[11] the excavator is frequently used in many applications other than excavation. Many excavators feature a quick coupler for simplified attachment mounting, increasing the machine's utilization on the jobsite. Excavators are usually employed together with loaders and bulldozers. Most wheeled, compact and some medium-sized (11 to 18-tonne) excavators have a backfill (or dozer) blade. This is a horizontal bulldozer-like blade attached to the undercarriage and is used for leveling and pushing removed material back into a hole.

As of July 2021, current excavator manufacturers include:

 Korea

 China

 France

 United States

 Denmark

 Sweden

 United Kingdom

 India

 Germany

 Russia

  Switzerland-Germany

 Iran

 Algeria

   Italy-USA-Netherlands

 Indonesia

 Turkey
"
Residential Plumbing Services,"

Plumbing is any system that conveys fluids for a wide range of applications. Plumbing uses pipes, valves, plumbing fixtures, tanks, and other apparatuses to convey fluids.[1] Heating and cooling (HVAC), waste removal, and potable water delivery are among the most common uses for plumbing, but it is not limited to these applications.[2]  The word derives from the Latin for lead, plumbum, as the first effective pipes used in the Roman era were lead pipes.[3]

In the developed world, plumbing infrastructure is critical to public health and sanitation.[4][5]

Boilermakers and pipefitters are not plumbers although they work with piping as part of their trade and their work can include some plumbing.

Plumbing originated during ancient civilizations, as they developed public baths and needed to provide potable water and wastewater removal for larger numbers of people.[6]

The Mesopotamians introduced the world to clay sewer pipes around 4000 BCE, with the earliest examples found in the Temple of Bel at Nippur and at Eshnunna,[7] used to remove wastewater from sites, and capture rainwater, in wells. The city of Uruk contains the oldest known examples of brick constructed Latrines, constructed atop interconnecting fired clay sewer pipes, c. 3200 BCE.[8][9] Clay pipes were later used in the Hittite city of Hattusa.[10] They had easily detachable and replaceable segments, and allowed for cleaning.

Standardized earthen plumbing pipes with broad flanges making use of asphalt for preventing leakages appeared in the urban settlements of the Indus Valley civilization by 2700 BC.[11]

Copper piping appeared in Egypt by 2400 BCE, with the Pyramid of Sahure and adjoining temple complex at Abusir, found to be connected by a copper waste pipe.[12]

The word  ""plumber"" dates from the Roman Empire.[13] The Latin for lead is plumbum.  Roman roofs used lead in conduits and drain pipes[14] and some were also covered with lead. Lead was also used for piping and for making baths.[15]

Plumbing reached its early apex in ancient Rome, which saw the introduction of expansive systems of aqueducts, tile wastewater removal, and widespread use of lead pipes. The Romans used lead pipe inscriptions to prevent water theft. With the Fall of Rome both water supply and sanitation stagnated—or regressed—for well over 1,000 years.  Improvement was very slow, with little effective progress made until the growth of modern densely populated cities in the 1800s. During this period, public health authorities began pressing for better waste disposal systems to be installed, to prevent or control epidemics of disease. Earlier, the waste disposal system had consisted of collecting waste and dumping it on the ground or into a river. Eventually the development of separate, underground water and sewage systems eliminated open sewage ditches and cesspools.

In post-classical Kilwa the wealthy enjoyed indoor plumbing in their stone homes.[16][17]

Most large cities today pipe solid wastes to sewage treatment plants in order to separate and partially purify the water, before emptying into streams or other bodies of water. For potable water use, galvanized iron piping was commonplace in the United States from the late 1800s until around 1960. After that period, copper piping took over, first soft copper with flared fittings, then with rigid copper tubing using soldered fittings.

The use of lead for potable water declined sharply after World War II because of increased awareness of the dangers of lead poisoning. At this time, copper piping was introduced as a better and safer alternative to lead pipes.[18]

The major categories of plumbing systems or subsystems are:[19]

A water pipe is a pipe or tube, frequently made of plastic or metal,[a] that carries pressurized and treated fresh water to a building (as part of a municipal water system), as well as inside the building.

Lead was the favoured material for water pipes for many centuries because its malleability made it practical to work into the desired shape. Such use was so common that the word ""plumbing"" derives from plumbum, the Latin word for lead. This was a source of lead-related health problems in the years before the health hazards of ingesting lead were fully understood; among these were stillbirths and high rates of infant mortality. Lead water pipes were still widely used in the early 20th century and remain in many households. Lead-tin alloy solder was commonly used to join copper pipes, but modern practice uses tin-antimony alloy solder instead in order to eliminate lead hazards.[20]

Despite the Romans' common use of lead pipes, their aqueducts rarely poisoned people. Unlike other parts of the world where lead pipes cause poisoning, the Roman water had so much calcium in it that a layer of plaque prevented the water contacting the lead itself. What often causes confusion is the large amount of evidence of widespread lead poisoning, particularly amongst those who would have had easy access to piped water,[21] an unfortunate result of lead being used in cookware and as an additive to processed food and drink (for example as a preservative in wine).[22] Roman lead pipe inscriptions provided information on the owner to prevent water theft.

Wooden pipes were used in London and elsewhere during the 16th and 17th centuries. The pipes were hollowed-out logs which were tapered at the end with a small hole in which the water would pass through.[23] The multiple pipes were then sealed together with hot animal fat. Wooden pipes were  used in Philadelphia,[24] Boston, and Montreal in the 1800s. Built-up wooden tubes were widely used in the US during the 20th century. These pipes (used in place of corrugated iron or reinforced concrete pipes) were made of sections cut from short lengths of wood. Locking of adjacent rings with hardwood dowel pins produced a flexible structure. About 100,000 feet of these wooden pipes were installed during WW2 in drainage culverts, storm sewers and conduits, under highways and at army camps, naval stations, airfields and ordnance plants.

Cast iron and ductile iron pipe was long a lower-cost alternative to copper before the advent of durable plastic materials but special non-conductive fittings must be used where transitions are to be made to other metallic pipes (except for terminal fittings) in order to avoid corrosion owing to electrochemical reactions between dissimilar metals (see galvanic cell).[25]

Bronze fittings and short pipe segments are commonly used in combination with various materials.[26]

The difference between pipes and tubes is a matter of sizing. For instance, PVC pipe for plumbing applications and galvanized steel pipe are measured in iron pipe size (IPS). Copper tube, CPVC, PeX and other tubing is measured nominally, basically an average diameter. These sizing schemes allow for universal adaptation of transitional fittings. For instance, 1/2"" PeX tubing is the same size as 1/2"" copper tubing. 1/2"" PVC on the other hand is not the same size as 1/2"" tubing, and therefore requires either a threaded male or female adapter to connect them.  When used in agricultural irrigation, the singular form ""pipe"" is often used as a plural.[27]

Pipe is available in rigid joints, which come in various lengths depending on the material. Tubing, in particular copper, comes in rigid hard tempered joints or soft tempered (annealed) rolls. PeX and CPVC tubing also comes in rigid joints or flexible rolls. The temper of the copper, whether it is a rigid joint or flexible roll, does not affect the sizing.[27]

The thicknesses of the water pipe and tube walls can vary. Because piping and tubing are commodities, having a greater wall thickness implies higher initial cost. Thicker walled pipe generally implies greater durability and higher pressure tolerances. Pipe wall thickness is denoted by various schedules or for large bore polyethylene pipe in the UK by the Standard Dimension Ratio (SDR), defined as the ratio of the pipe diameter to its wall thickness. Pipe wall thickness increases with schedule, and is available in schedules 20, 40, 80, and higher in special cases. The schedule is largely determined by the operating pressure of the system, with higher pressures commanding greater thickness. Copper tubing is available in four wall thicknesses: type DWV (thinnest wall; only allowed as drain pipe per UPC), type 'M' (thin; typically only allowed as drain pipe by IPC code), type 'L' (thicker, standard duty for water lines and water service), and type 'K' (thickest, typically used underground between the main and the meter).

Wall thickness does not affect pipe or tubing size.[28] 1/2"" L copper has the same outer diameter as 1/2"" K or M copper. The same applies to pipe schedules. As a result, a slight increase in pressure losses is realized due to a decrease in flowpath as wall thickness is increased. In other words, 1 foot of 1/2"" L copper has slightly less volume than 1 foot of 1/2 M copper.[29]

Water systems of ancient times relied on gravity for the supply of water, using pipes or channels usually made of clay, lead, bamboo, wood, or stone. Hollowed wooden logs wrapped in steel banding were used for plumbing pipes, particularly water mains. Logs were used for water distribution in England close to 500 years ago. US cities began using hollowed logs in the late 1700s through the 1800s.  Today, most plumbing supply pipe is made out of steel, copper, and plastic; most waste (also known as ""soil"")[30] out of steel, copper, plastic, and cast iron.[30]

The straight sections of plumbing systems are called ""pipes"" or ""tubes"". A pipe is typically formed via casting or welding, whereas a tube is made through extrusion. Pipe normally has thicker walls and may be threaded or welded, while tubing is thinner-walled and requires special joining techniques such as brazing, compression fitting, crimping, or for plastics, solvent welding.  These joining techniques are discussed in more detail in the piping and plumbing fittings article.

Galvanized steel potable water supply and distribution pipes are commonly found with nominal pipe sizes from 3⁄8 inch (9.5 mm) to 2 inches (51 mm). It is rarely used today for new construction residential plumbing. Steel pipe has National Pipe Thread (NPT) standard tapered male threads, which connect with female tapered threads on elbows, tees, couplers, valves, and other fittings. Galvanized steel (often known simply as ""galv"" or ""iron"" in the plumbing trade) is relatively expensive, and difficult to work with due to weight and requirement of a pipe threader. It remains in common use for repair of existing ""galv"" systems and to satisfy building code non-combustibility requirements typically found in hotels, apartment buildings and other commercial applications. It is also extremely durable and resistant to mechanical abuse.  Black lacquered steel pipe is the most widely used pipe material for fire sprinklers and natural gas.

Most typical single family home systems will not require supply piping larger than 3⁄4 inch (19 mm) due to expense as well as steel piping's tendency to become obstructed from internal rusting and mineral deposits forming on the inside of the pipe over time once the internal galvanizing zinc coating has degraded. In potable water distribution service, galvanized steel pipe has a service life of about 30 to 50 years, although it is not uncommon for it to be less in geographic areas with corrosive water contaminants.

Copper pipe and tubing was widely used for domestic water systems in the latter half of the twentieth century. Demand for copper products has fallen due to the dramatic increase in the price of copper, resulting in increased demand for alternative products including PEX and stainless steel.

Plastic pipe is in wide use for domestic water supply and drain-waste-vent (DWV) pipe. Principal types include:
Polyvinyl chloride (PVC) was produced experimentally in the 19th century but did not become practical to manufacture until 1926, when Waldo Semon of BF Goodrich Co. developed a method to plasticize PVC, making it easier to process. PVC pipe began to be manufactured in the 1940s and was in wide use for Drain-Waste-Vent piping during the reconstruction of Germany and Japan following WWII. In the 1950s, plastics manufacturers in Western Europe and Japan began producing acrylonitrile butadiene styrene (ABS) pipe. The method for producing cross-linked polyethylene (PEX) was also developed in the 1950s. Plastic supply pipes have become increasingly common, with a variety of materials and fittings employed.

Present-day water-supply systems use a network of high-pressure pumps, and pipes in buildings are now made of copper,[34] brass, plastic (particularly cross-linked polyethylene called PEX, which is estimated to be used in 60% of single-family homes[35]), or other nontoxic material. Due to its toxicity, most cities moved away from lead water-supply piping by the 1920s in the United States,[36] although lead pipes were approved by national plumbing codes into the 1980s,[37] and lead was used in plumbing solder for drinking water until it was banned in 1986.[36] Drain and vent lines are made of plastic, steel, cast iron, or lead.[38][39]

In addition to lengths of pipe or tubing, pipe fittings such as valves, elbows, tees, and unions. are used in plumbing systems.[40] Pipe and fittings are held in place with pipe hangers and strapping.

Plumbing fixtures are exchangeable devices that use water and can be connected to a building's plumbing system. They are considered to be ""fixtures"", in that they are semi-permanent parts of buildings, not usually owned or maintained separately.  Plumbing fixtures are seen by and designed for the end-users. Some examples of fixtures include water closets[41] (also known as toilets), urinals, bidets, showers, bathtubs, utility and kitchen sinks, drinking fountains, ice makers, humidifiers, air washers, fountains, and eye wash stations.

Threaded pipe joints are sealed with thread seal tape or pipe dope. Many plumbing fixtures are sealed to their mounting surfaces with plumber's putty.[42]

Plumbing equipment includes devices often  behind walls or in utility spaces which are not seen by the general public. It includes water meters, pumps, expansion tanks, back flow preventers, water filters, UV sterilization lights, water softeners, water heaters, heat exchangers, gauges, and control systems.

There are many tools a plumber needs to do a good plumbing job. While many simple plumbing tasks can be completed with a few common hand held tools, other more complex jobs require specialised tools, designed specifically to make the job easier.

Specialized plumbing tools include pipe wrenches, flaring pliers, pipe vise, pipe bending machine, pipe cutter, dies, and joining tools such as soldering torches and crimp tools. New tools have been developed to help plumbers fix problems more efficiently. For example, plumbers use video cameras for inspections of hidden leaks or other problems; they also use hydro jets, and high pressure hydraulic pumps connected to steel cables for trench-less sewer line replacement.

Flooding from excessive rain or clogged sewers may require specialized equipment, such as a heavy duty pumper truck designed to vacuum raw sewage.[citation needed]

Bacteria have been shown to live in ""premises plumbing systems"". The latter refers to the ""pipes and fixtures within a building that transport water to taps after it is delivered by the utility"".[43] Community water systems have been known for centuries to spread waterborne diseases like typhoid and cholera.  However, ""opportunistic premises plumbing pathogens"" have been recognized only more recently: Legionella pneumophila, discovered in 1976, Mycobacterium avium, and Pseudomonas aeruginosa are the most commonly tracked bacteria, which people with depressed immunity can inhale or ingest and may become infected with.[44]
Some of the locations where these opportunistic pathogens can grow include faucets, shower heads, water heaters and along pipe walls. Reasons that favor their growth are ""high surface-to-volume ratio, intermittent stagnation, low disinfectant residual, and warming cycles"". A high surface-to-volume ratio, i.e. a relatively large surface area allows the bacteria to form a biofilm, which protects them from disinfection.[44]

Much of the plumbing work in populated areas is regulated by government or quasi-government agencies due to the direct impact on the public's health, safety, and welfare. Plumbing installation and repair work on residences and other buildings generally must be done according to plumbing and building codes to protect the inhabitants of the buildings and to ensure safe, quality construction to future buyers. If permits are required for work, plumbing contractors typically secure them from the authorities on behalf of home or building owners.[citation needed]

In Australia, the national governing body for plumbing regulation is the Australian Building Codes Board. They are responsible for the creation of the National Construction Code (NCC), Volume 3 of which, the Plumbing Regulations 2008[45] and the Plumbing Code of Australia,[46] pertains to plumbing.

Each Government at the state level has their own Authority and regulations in place for licensing plumbers. They are also responsible for the interpretation, administration and enforcement of the regulations outlined in the NCC.[47] These Authorities are usually established for the sole purpose of regulating plumbing activities in their respective states/territories. However, several state level regulation acts are quite outdated, with some still operating on local policies introduced more than a decade ago. This has led to an increase in plumbing regulatory issues not covered under current policy, and as such, many policies are currently being updated to cover these more modern issues. The updates include changed to the minimum experience and training requirements for licensing, additional work standards for new and more specific kinds of plumbing, as well as adopting the Plumbing Code of Australia into state regulations in an effort to standardise plumbing regulations across the country.

In Norway, new domestic plumbing installed since 1997 has had to satisfy the requirement that it should be easily accessible for replacement after installation.[48] This has led to the development of the pipe-in-pipe system as a de facto requirement for domestic plumbing.

In the United Kingdom the professional body is the Chartered Institute of Plumbing and Heating Engineering (educational charity status) and it is true that the trade still remains virtually ungoverned;[49] there are no systems in place to monitor or control the activities of unqualified plumbers or those home owners who choose to undertake installation and maintenance works themselves, despite the health and safety issues which arise from such works when they are undertaken incorrectly; see Health Aspects of Plumbing (HAP) published jointly by the World Health Organization (WHO) and the World Plumbing Council (WPC).[50][51] WPC has subsequently appointed a representative to the World Health Organization to take forward various projects related to Health Aspects of Plumbing.[52]

In the United States, plumbing codes and licensing are generally controlled by state and local governments. At the national level, the Environmental Protection Agency has set guidelines about what constitutes lead-free plumbing fittings and pipes, in order to comply with the Safe Drinking Water Act.[53]

Some widely used Standards in the United States are:[citation needed]

In Canada, plumbing is a regulated trade requiring specific technical training and certification. Standards and regulations for plumbing are overseen at the provincial and territorial level, each having its distinct governing body:

[[Category:Bathrooms]
"
Commercial Plumbing Services,"

Plumbing is any system that conveys fluids for a wide range of applications. Plumbing uses pipes, valves, plumbing fixtures, tanks, and other apparatuses to convey fluids.[1] Heating and cooling (HVAC), waste removal, and potable water delivery are among the most common uses for plumbing, but it is not limited to these applications.[2]  The word derives from the Latin for lead, plumbum, as the first effective pipes used in the Roman era were lead pipes.[3]

In the developed world, plumbing infrastructure is critical to public health and sanitation.[4][5]

Boilermakers and pipefitters are not plumbers although they work with piping as part of their trade and their work can include some plumbing.

Plumbing originated during ancient civilizations, as they developed public baths and needed to provide potable water and wastewater removal for larger numbers of people.[6]

The Mesopotamians introduced the world to clay sewer pipes around 4000 BCE, with the earliest examples found in the Temple of Bel at Nippur and at Eshnunna,[7] used to remove wastewater from sites, and capture rainwater, in wells. The city of Uruk contains the oldest known examples of brick constructed Latrines, constructed atop interconnecting fired clay sewer pipes, c. 3200 BCE.[8][9] Clay pipes were later used in the Hittite city of Hattusa.[10] They had easily detachable and replaceable segments, and allowed for cleaning.

Standardized earthen plumbing pipes with broad flanges making use of asphalt for preventing leakages appeared in the urban settlements of the Indus Valley civilization by 2700 BC.[11]

Copper piping appeared in Egypt by 2400 BCE, with the Pyramid of Sahure and adjoining temple complex at Abusir, found to be connected by a copper waste pipe.[12]

The word  ""plumber"" dates from the Roman Empire.[13] The Latin for lead is plumbum.  Roman roofs used lead in conduits and drain pipes[14] and some were also covered with lead. Lead was also used for piping and for making baths.[15]

Plumbing reached its early apex in ancient Rome, which saw the introduction of expansive systems of aqueducts, tile wastewater removal, and widespread use of lead pipes. The Romans used lead pipe inscriptions to prevent water theft. With the Fall of Rome both water supply and sanitation stagnated—or regressed—for well over 1,000 years.  Improvement was very slow, with little effective progress made until the growth of modern densely populated cities in the 1800s. During this period, public health authorities began pressing for better waste disposal systems to be installed, to prevent or control epidemics of disease. Earlier, the waste disposal system had consisted of collecting waste and dumping it on the ground or into a river. Eventually the development of separate, underground water and sewage systems eliminated open sewage ditches and cesspools.

In post-classical Kilwa the wealthy enjoyed indoor plumbing in their stone homes.[16][17]

Most large cities today pipe solid wastes to sewage treatment plants in order to separate and partially purify the water, before emptying into streams or other bodies of water. For potable water use, galvanized iron piping was commonplace in the United States from the late 1800s until around 1960. After that period, copper piping took over, first soft copper with flared fittings, then with rigid copper tubing using soldered fittings.

The use of lead for potable water declined sharply after World War II because of increased awareness of the dangers of lead poisoning. At this time, copper piping was introduced as a better and safer alternative to lead pipes.[18]

The major categories of plumbing systems or subsystems are:[19]

A water pipe is a pipe or tube, frequently made of plastic or metal,[a] that carries pressurized and treated fresh water to a building (as part of a municipal water system), as well as inside the building.

Lead was the favoured material for water pipes for many centuries because its malleability made it practical to work into the desired shape. Such use was so common that the word ""plumbing"" derives from plumbum, the Latin word for lead. This was a source of lead-related health problems in the years before the health hazards of ingesting lead were fully understood; among these were stillbirths and high rates of infant mortality. Lead water pipes were still widely used in the early 20th century and remain in many households. Lead-tin alloy solder was commonly used to join copper pipes, but modern practice uses tin-antimony alloy solder instead in order to eliminate lead hazards.[20]

Despite the Romans' common use of lead pipes, their aqueducts rarely poisoned people. Unlike other parts of the world where lead pipes cause poisoning, the Roman water had so much calcium in it that a layer of plaque prevented the water contacting the lead itself. What often causes confusion is the large amount of evidence of widespread lead poisoning, particularly amongst those who would have had easy access to piped water,[21] an unfortunate result of lead being used in cookware and as an additive to processed food and drink (for example as a preservative in wine).[22] Roman lead pipe inscriptions provided information on the owner to prevent water theft.

Wooden pipes were used in London and elsewhere during the 16th and 17th centuries. The pipes were hollowed-out logs which were tapered at the end with a small hole in which the water would pass through.[23] The multiple pipes were then sealed together with hot animal fat. Wooden pipes were  used in Philadelphia,[24] Boston, and Montreal in the 1800s. Built-up wooden tubes were widely used in the US during the 20th century. These pipes (used in place of corrugated iron or reinforced concrete pipes) were made of sections cut from short lengths of wood. Locking of adjacent rings with hardwood dowel pins produced a flexible structure. About 100,000 feet of these wooden pipes were installed during WW2 in drainage culverts, storm sewers and conduits, under highways and at army camps, naval stations, airfields and ordnance plants.

Cast iron and ductile iron pipe was long a lower-cost alternative to copper before the advent of durable plastic materials but special non-conductive fittings must be used where transitions are to be made to other metallic pipes (except for terminal fittings) in order to avoid corrosion owing to electrochemical reactions between dissimilar metals (see galvanic cell).[25]

Bronze fittings and short pipe segments are commonly used in combination with various materials.[26]

The difference between pipes and tubes is a matter of sizing. For instance, PVC pipe for plumbing applications and galvanized steel pipe are measured in iron pipe size (IPS). Copper tube, CPVC, PeX and other tubing is measured nominally, basically an average diameter. These sizing schemes allow for universal adaptation of transitional fittings. For instance, 1/2"" PeX tubing is the same size as 1/2"" copper tubing. 1/2"" PVC on the other hand is not the same size as 1/2"" tubing, and therefore requires either a threaded male or female adapter to connect them.  When used in agricultural irrigation, the singular form ""pipe"" is often used as a plural.[27]

Pipe is available in rigid joints, which come in various lengths depending on the material. Tubing, in particular copper, comes in rigid hard tempered joints or soft tempered (annealed) rolls. PeX and CPVC tubing also comes in rigid joints or flexible rolls. The temper of the copper, whether it is a rigid joint or flexible roll, does not affect the sizing.[27]

The thicknesses of the water pipe and tube walls can vary. Because piping and tubing are commodities, having a greater wall thickness implies higher initial cost. Thicker walled pipe generally implies greater durability and higher pressure tolerances. Pipe wall thickness is denoted by various schedules or for large bore polyethylene pipe in the UK by the Standard Dimension Ratio (SDR), defined as the ratio of the pipe diameter to its wall thickness. Pipe wall thickness increases with schedule, and is available in schedules 20, 40, 80, and higher in special cases. The schedule is largely determined by the operating pressure of the system, with higher pressures commanding greater thickness. Copper tubing is available in four wall thicknesses: type DWV (thinnest wall; only allowed as drain pipe per UPC), type 'M' (thin; typically only allowed as drain pipe by IPC code), type 'L' (thicker, standard duty for water lines and water service), and type 'K' (thickest, typically used underground between the main and the meter).

Wall thickness does not affect pipe or tubing size.[28] 1/2"" L copper has the same outer diameter as 1/2"" K or M copper. The same applies to pipe schedules. As a result, a slight increase in pressure losses is realized due to a decrease in flowpath as wall thickness is increased. In other words, 1 foot of 1/2"" L copper has slightly less volume than 1 foot of 1/2 M copper.[29]

Water systems of ancient times relied on gravity for the supply of water, using pipes or channels usually made of clay, lead, bamboo, wood, or stone. Hollowed wooden logs wrapped in steel banding were used for plumbing pipes, particularly water mains. Logs were used for water distribution in England close to 500 years ago. US cities began using hollowed logs in the late 1700s through the 1800s.  Today, most plumbing supply pipe is made out of steel, copper, and plastic; most waste (also known as ""soil"")[30] out of steel, copper, plastic, and cast iron.[30]

The straight sections of plumbing systems are called ""pipes"" or ""tubes"". A pipe is typically formed via casting or welding, whereas a tube is made through extrusion. Pipe normally has thicker walls and may be threaded or welded, while tubing is thinner-walled and requires special joining techniques such as brazing, compression fitting, crimping, or for plastics, solvent welding.  These joining techniques are discussed in more detail in the piping and plumbing fittings article.

Galvanized steel potable water supply and distribution pipes are commonly found with nominal pipe sizes from 3⁄8 inch (9.5 mm) to 2 inches (51 mm). It is rarely used today for new construction residential plumbing. Steel pipe has National Pipe Thread (NPT) standard tapered male threads, which connect with female tapered threads on elbows, tees, couplers, valves, and other fittings. Galvanized steel (often known simply as ""galv"" or ""iron"" in the plumbing trade) is relatively expensive, and difficult to work with due to weight and requirement of a pipe threader. It remains in common use for repair of existing ""galv"" systems and to satisfy building code non-combustibility requirements typically found in hotels, apartment buildings and other commercial applications. It is also extremely durable and resistant to mechanical abuse.  Black lacquered steel pipe is the most widely used pipe material for fire sprinklers and natural gas.

Most typical single family home systems will not require supply piping larger than 3⁄4 inch (19 mm) due to expense as well as steel piping's tendency to become obstructed from internal rusting and mineral deposits forming on the inside of the pipe over time once the internal galvanizing zinc coating has degraded. In potable water distribution service, galvanized steel pipe has a service life of about 30 to 50 years, although it is not uncommon for it to be less in geographic areas with corrosive water contaminants.

Copper pipe and tubing was widely used for domestic water systems in the latter half of the twentieth century. Demand for copper products has fallen due to the dramatic increase in the price of copper, resulting in increased demand for alternative products including PEX and stainless steel.

Plastic pipe is in wide use for domestic water supply and drain-waste-vent (DWV) pipe. Principal types include:
Polyvinyl chloride (PVC) was produced experimentally in the 19th century but did not become practical to manufacture until 1926, when Waldo Semon of BF Goodrich Co. developed a method to plasticize PVC, making it easier to process. PVC pipe began to be manufactured in the 1940s and was in wide use for Drain-Waste-Vent piping during the reconstruction of Germany and Japan following WWII. In the 1950s, plastics manufacturers in Western Europe and Japan began producing acrylonitrile butadiene styrene (ABS) pipe. The method for producing cross-linked polyethylene (PEX) was also developed in the 1950s. Plastic supply pipes have become increasingly common, with a variety of materials and fittings employed.

Present-day water-supply systems use a network of high-pressure pumps, and pipes in buildings are now made of copper,[34] brass, plastic (particularly cross-linked polyethylene called PEX, which is estimated to be used in 60% of single-family homes[35]), or other nontoxic material. Due to its toxicity, most cities moved away from lead water-supply piping by the 1920s in the United States,[36] although lead pipes were approved by national plumbing codes into the 1980s,[37] and lead was used in plumbing solder for drinking water until it was banned in 1986.[36] Drain and vent lines are made of plastic, steel, cast iron, or lead.[38][39]

In addition to lengths of pipe or tubing, pipe fittings such as valves, elbows, tees, and unions. are used in plumbing systems.[40] Pipe and fittings are held in place with pipe hangers and strapping.

Plumbing fixtures are exchangeable devices that use water and can be connected to a building's plumbing system. They are considered to be ""fixtures"", in that they are semi-permanent parts of buildings, not usually owned or maintained separately.  Plumbing fixtures are seen by and designed for the end-users. Some examples of fixtures include water closets[41] (also known as toilets), urinals, bidets, showers, bathtubs, utility and kitchen sinks, drinking fountains, ice makers, humidifiers, air washers, fountains, and eye wash stations.

Threaded pipe joints are sealed with thread seal tape or pipe dope. Many plumbing fixtures are sealed to their mounting surfaces with plumber's putty.[42]

Plumbing equipment includes devices often  behind walls or in utility spaces which are not seen by the general public. It includes water meters, pumps, expansion tanks, back flow preventers, water filters, UV sterilization lights, water softeners, water heaters, heat exchangers, gauges, and control systems.

There are many tools a plumber needs to do a good plumbing job. While many simple plumbing tasks can be completed with a few common hand held tools, other more complex jobs require specialised tools, designed specifically to make the job easier.

Specialized plumbing tools include pipe wrenches, flaring pliers, pipe vise, pipe bending machine, pipe cutter, dies, and joining tools such as soldering torches and crimp tools. New tools have been developed to help plumbers fix problems more efficiently. For example, plumbers use video cameras for inspections of hidden leaks or other problems; they also use hydro jets, and high pressure hydraulic pumps connected to steel cables for trench-less sewer line replacement.

Flooding from excessive rain or clogged sewers may require specialized equipment, such as a heavy duty pumper truck designed to vacuum raw sewage.[citation needed]

Bacteria have been shown to live in ""premises plumbing systems"". The latter refers to the ""pipes and fixtures within a building that transport water to taps after it is delivered by the utility"".[43] Community water systems have been known for centuries to spread waterborne diseases like typhoid and cholera.  However, ""opportunistic premises plumbing pathogens"" have been recognized only more recently: Legionella pneumophila, discovered in 1976, Mycobacterium avium, and Pseudomonas aeruginosa are the most commonly tracked bacteria, which people with depressed immunity can inhale or ingest and may become infected with.[44]
Some of the locations where these opportunistic pathogens can grow include faucets, shower heads, water heaters and along pipe walls. Reasons that favor their growth are ""high surface-to-volume ratio, intermittent stagnation, low disinfectant residual, and warming cycles"". A high surface-to-volume ratio, i.e. a relatively large surface area allows the bacteria to form a biofilm, which protects them from disinfection.[44]

Much of the plumbing work in populated areas is regulated by government or quasi-government agencies due to the direct impact on the public's health, safety, and welfare. Plumbing installation and repair work on residences and other buildings generally must be done according to plumbing and building codes to protect the inhabitants of the buildings and to ensure safe, quality construction to future buyers. If permits are required for work, plumbing contractors typically secure them from the authorities on behalf of home or building owners.[citation needed]

In Australia, the national governing body for plumbing regulation is the Australian Building Codes Board. They are responsible for the creation of the National Construction Code (NCC), Volume 3 of which, the Plumbing Regulations 2008[45] and the Plumbing Code of Australia,[46] pertains to plumbing.

Each Government at the state level has their own Authority and regulations in place for licensing plumbers. They are also responsible for the interpretation, administration and enforcement of the regulations outlined in the NCC.[47] These Authorities are usually established for the sole purpose of regulating plumbing activities in their respective states/territories. However, several state level regulation acts are quite outdated, with some still operating on local policies introduced more than a decade ago. This has led to an increase in plumbing regulatory issues not covered under current policy, and as such, many policies are currently being updated to cover these more modern issues. The updates include changed to the minimum experience and training requirements for licensing, additional work standards for new and more specific kinds of plumbing, as well as adopting the Plumbing Code of Australia into state regulations in an effort to standardise plumbing regulations across the country.

In Norway, new domestic plumbing installed since 1997 has had to satisfy the requirement that it should be easily accessible for replacement after installation.[48] This has led to the development of the pipe-in-pipe system as a de facto requirement for domestic plumbing.

In the United Kingdom the professional body is the Chartered Institute of Plumbing and Heating Engineering (educational charity status) and it is true that the trade still remains virtually ungoverned;[49] there are no systems in place to monitor or control the activities of unqualified plumbers or those home owners who choose to undertake installation and maintenance works themselves, despite the health and safety issues which arise from such works when they are undertaken incorrectly; see Health Aspects of Plumbing (HAP) published jointly by the World Health Organization (WHO) and the World Plumbing Council (WPC).[50][51] WPC has subsequently appointed a representative to the World Health Organization to take forward various projects related to Health Aspects of Plumbing.[52]

In the United States, plumbing codes and licensing are generally controlled by state and local governments. At the national level, the Environmental Protection Agency has set guidelines about what constitutes lead-free plumbing fittings and pipes, in order to comply with the Safe Drinking Water Act.[53]

Some widely used Standards in the United States are:[citation needed]

In Canada, plumbing is a regulated trade requiring specific technical training and certification. Standards and regulations for plumbing are overseen at the provincial and territorial level, each having its distinct governing body:

[[Category:Bathrooms]
"
Industrial Plumbing Services,"

Plumbing is any system that conveys fluids for a wide range of applications. Plumbing uses pipes, valves, plumbing fixtures, tanks, and other apparatuses to convey fluids.[1] Heating and cooling (HVAC), waste removal, and potable water delivery are among the most common uses for plumbing, but it is not limited to these applications.[2]  The word derives from the Latin for lead, plumbum, as the first effective pipes used in the Roman era were lead pipes.[3]

In the developed world, plumbing infrastructure is critical to public health and sanitation.[4][5]

Boilermakers and pipefitters are not plumbers although they work with piping as part of their trade and their work can include some plumbing.

Plumbing originated during ancient civilizations, as they developed public baths and needed to provide potable water and wastewater removal for larger numbers of people.[6]

The Mesopotamians introduced the world to clay sewer pipes around 4000 BCE, with the earliest examples found in the Temple of Bel at Nippur and at Eshnunna,[7] used to remove wastewater from sites, and capture rainwater, in wells. The city of Uruk contains the oldest known examples of brick constructed Latrines, constructed atop interconnecting fired clay sewer pipes, c. 3200 BCE.[8][9] Clay pipes were later used in the Hittite city of Hattusa.[10] They had easily detachable and replaceable segments, and allowed for cleaning.

Standardized earthen plumbing pipes with broad flanges making use of asphalt for preventing leakages appeared in the urban settlements of the Indus Valley civilization by 2700 BC.[11]

Copper piping appeared in Egypt by 2400 BCE, with the Pyramid of Sahure and adjoining temple complex at Abusir, found to be connected by a copper waste pipe.[12]

The word  ""plumber"" dates from the Roman Empire.[13] The Latin for lead is plumbum.  Roman roofs used lead in conduits and drain pipes[14] and some were also covered with lead. Lead was also used for piping and for making baths.[15]

Plumbing reached its early apex in ancient Rome, which saw the introduction of expansive systems of aqueducts, tile wastewater removal, and widespread use of lead pipes. The Romans used lead pipe inscriptions to prevent water theft. With the Fall of Rome both water supply and sanitation stagnated—or regressed—for well over 1,000 years.  Improvement was very slow, with little effective progress made until the growth of modern densely populated cities in the 1800s. During this period, public health authorities began pressing for better waste disposal systems to be installed, to prevent or control epidemics of disease. Earlier, the waste disposal system had consisted of collecting waste and dumping it on the ground or into a river. Eventually the development of separate, underground water and sewage systems eliminated open sewage ditches and cesspools.

In post-classical Kilwa the wealthy enjoyed indoor plumbing in their stone homes.[16][17]

Most large cities today pipe solid wastes to sewage treatment plants in order to separate and partially purify the water, before emptying into streams or other bodies of water. For potable water use, galvanized iron piping was commonplace in the United States from the late 1800s until around 1960. After that period, copper piping took over, first soft copper with flared fittings, then with rigid copper tubing using soldered fittings.

The use of lead for potable water declined sharply after World War II because of increased awareness of the dangers of lead poisoning. At this time, copper piping was introduced as a better and safer alternative to lead pipes.[18]

The major categories of plumbing systems or subsystems are:[19]

A water pipe is a pipe or tube, frequently made of plastic or metal,[a] that carries pressurized and treated fresh water to a building (as part of a municipal water system), as well as inside the building.

Lead was the favoured material for water pipes for many centuries because its malleability made it practical to work into the desired shape. Such use was so common that the word ""plumbing"" derives from plumbum, the Latin word for lead. This was a source of lead-related health problems in the years before the health hazards of ingesting lead were fully understood; among these were stillbirths and high rates of infant mortality. Lead water pipes were still widely used in the early 20th century and remain in many households. Lead-tin alloy solder was commonly used to join copper pipes, but modern practice uses tin-antimony alloy solder instead in order to eliminate lead hazards.[20]

Despite the Romans' common use of lead pipes, their aqueducts rarely poisoned people. Unlike other parts of the world where lead pipes cause poisoning, the Roman water had so much calcium in it that a layer of plaque prevented the water contacting the lead itself. What often causes confusion is the large amount of evidence of widespread lead poisoning, particularly amongst those who would have had easy access to piped water,[21] an unfortunate result of lead being used in cookware and as an additive to processed food and drink (for example as a preservative in wine).[22] Roman lead pipe inscriptions provided information on the owner to prevent water theft.

Wooden pipes were used in London and elsewhere during the 16th and 17th centuries. The pipes were hollowed-out logs which were tapered at the end with a small hole in which the water would pass through.[23] The multiple pipes were then sealed together with hot animal fat. Wooden pipes were  used in Philadelphia,[24] Boston, and Montreal in the 1800s. Built-up wooden tubes were widely used in the US during the 20th century. These pipes (used in place of corrugated iron or reinforced concrete pipes) were made of sections cut from short lengths of wood. Locking of adjacent rings with hardwood dowel pins produced a flexible structure. About 100,000 feet of these wooden pipes were installed during WW2 in drainage culverts, storm sewers and conduits, under highways and at army camps, naval stations, airfields and ordnance plants.

Cast iron and ductile iron pipe was long a lower-cost alternative to copper before the advent of durable plastic materials but special non-conductive fittings must be used where transitions are to be made to other metallic pipes (except for terminal fittings) in order to avoid corrosion owing to electrochemical reactions between dissimilar metals (see galvanic cell).[25]

Bronze fittings and short pipe segments are commonly used in combination with various materials.[26]

The difference between pipes and tubes is a matter of sizing. For instance, PVC pipe for plumbing applications and galvanized steel pipe are measured in iron pipe size (IPS). Copper tube, CPVC, PeX and other tubing is measured nominally, basically an average diameter. These sizing schemes allow for universal adaptation of transitional fittings. For instance, 1/2"" PeX tubing is the same size as 1/2"" copper tubing. 1/2"" PVC on the other hand is not the same size as 1/2"" tubing, and therefore requires either a threaded male or female adapter to connect them.  When used in agricultural irrigation, the singular form ""pipe"" is often used as a plural.[27]

Pipe is available in rigid joints, which come in various lengths depending on the material. Tubing, in particular copper, comes in rigid hard tempered joints or soft tempered (annealed) rolls. PeX and CPVC tubing also comes in rigid joints or flexible rolls. The temper of the copper, whether it is a rigid joint or flexible roll, does not affect the sizing.[27]

The thicknesses of the water pipe and tube walls can vary. Because piping and tubing are commodities, having a greater wall thickness implies higher initial cost. Thicker walled pipe generally implies greater durability and higher pressure tolerances. Pipe wall thickness is denoted by various schedules or for large bore polyethylene pipe in the UK by the Standard Dimension Ratio (SDR), defined as the ratio of the pipe diameter to its wall thickness. Pipe wall thickness increases with schedule, and is available in schedules 20, 40, 80, and higher in special cases. The schedule is largely determined by the operating pressure of the system, with higher pressures commanding greater thickness. Copper tubing is available in four wall thicknesses: type DWV (thinnest wall; only allowed as drain pipe per UPC), type 'M' (thin; typically only allowed as drain pipe by IPC code), type 'L' (thicker, standard duty for water lines and water service), and type 'K' (thickest, typically used underground between the main and the meter).

Wall thickness does not affect pipe or tubing size.[28] 1/2"" L copper has the same outer diameter as 1/2"" K or M copper. The same applies to pipe schedules. As a result, a slight increase in pressure losses is realized due to a decrease in flowpath as wall thickness is increased. In other words, 1 foot of 1/2"" L copper has slightly less volume than 1 foot of 1/2 M copper.[29]

Water systems of ancient times relied on gravity for the supply of water, using pipes or channels usually made of clay, lead, bamboo, wood, or stone. Hollowed wooden logs wrapped in steel banding were used for plumbing pipes, particularly water mains. Logs were used for water distribution in England close to 500 years ago. US cities began using hollowed logs in the late 1700s through the 1800s.  Today, most plumbing supply pipe is made out of steel, copper, and plastic; most waste (also known as ""soil"")[30] out of steel, copper, plastic, and cast iron.[30]

The straight sections of plumbing systems are called ""pipes"" or ""tubes"". A pipe is typically formed via casting or welding, whereas a tube is made through extrusion. Pipe normally has thicker walls and may be threaded or welded, while tubing is thinner-walled and requires special joining techniques such as brazing, compression fitting, crimping, or for plastics, solvent welding.  These joining techniques are discussed in more detail in the piping and plumbing fittings article.

Galvanized steel potable water supply and distribution pipes are commonly found with nominal pipe sizes from 3⁄8 inch (9.5 mm) to 2 inches (51 mm). It is rarely used today for new construction residential plumbing. Steel pipe has National Pipe Thread (NPT) standard tapered male threads, which connect with female tapered threads on elbows, tees, couplers, valves, and other fittings. Galvanized steel (often known simply as ""galv"" or ""iron"" in the plumbing trade) is relatively expensive, and difficult to work with due to weight and requirement of a pipe threader. It remains in common use for repair of existing ""galv"" systems and to satisfy building code non-combustibility requirements typically found in hotels, apartment buildings and other commercial applications. It is also extremely durable and resistant to mechanical abuse.  Black lacquered steel pipe is the most widely used pipe material for fire sprinklers and natural gas.

Most typical single family home systems will not require supply piping larger than 3⁄4 inch (19 mm) due to expense as well as steel piping's tendency to become obstructed from internal rusting and mineral deposits forming on the inside of the pipe over time once the internal galvanizing zinc coating has degraded. In potable water distribution service, galvanized steel pipe has a service life of about 30 to 50 years, although it is not uncommon for it to be less in geographic areas with corrosive water contaminants.

Copper pipe and tubing was widely used for domestic water systems in the latter half of the twentieth century. Demand for copper products has fallen due to the dramatic increase in the price of copper, resulting in increased demand for alternative products including PEX and stainless steel.

Plastic pipe is in wide use for domestic water supply and drain-waste-vent (DWV) pipe. Principal types include:
Polyvinyl chloride (PVC) was produced experimentally in the 19th century but did not become practical to manufacture until 1926, when Waldo Semon of BF Goodrich Co. developed a method to plasticize PVC, making it easier to process. PVC pipe began to be manufactured in the 1940s and was in wide use for Drain-Waste-Vent piping during the reconstruction of Germany and Japan following WWII. In the 1950s, plastics manufacturers in Western Europe and Japan began producing acrylonitrile butadiene styrene (ABS) pipe. The method for producing cross-linked polyethylene (PEX) was also developed in the 1950s. Plastic supply pipes have become increasingly common, with a variety of materials and fittings employed.

Present-day water-supply systems use a network of high-pressure pumps, and pipes in buildings are now made of copper,[34] brass, plastic (particularly cross-linked polyethylene called PEX, which is estimated to be used in 60% of single-family homes[35]), or other nontoxic material. Due to its toxicity, most cities moved away from lead water-supply piping by the 1920s in the United States,[36] although lead pipes were approved by national plumbing codes into the 1980s,[37] and lead was used in plumbing solder for drinking water until it was banned in 1986.[36] Drain and vent lines are made of plastic, steel, cast iron, or lead.[38][39]

In addition to lengths of pipe or tubing, pipe fittings such as valves, elbows, tees, and unions. are used in plumbing systems.[40] Pipe and fittings are held in place with pipe hangers and strapping.

Plumbing fixtures are exchangeable devices that use water and can be connected to a building's plumbing system. They are considered to be ""fixtures"", in that they are semi-permanent parts of buildings, not usually owned or maintained separately.  Plumbing fixtures are seen by and designed for the end-users. Some examples of fixtures include water closets[41] (also known as toilets), urinals, bidets, showers, bathtubs, utility and kitchen sinks, drinking fountains, ice makers, humidifiers, air washers, fountains, and eye wash stations.

Threaded pipe joints are sealed with thread seal tape or pipe dope. Many plumbing fixtures are sealed to their mounting surfaces with plumber's putty.[42]

Plumbing equipment includes devices often  behind walls or in utility spaces which are not seen by the general public. It includes water meters, pumps, expansion tanks, back flow preventers, water filters, UV sterilization lights, water softeners, water heaters, heat exchangers, gauges, and control systems.

There are many tools a plumber needs to do a good plumbing job. While many simple plumbing tasks can be completed with a few common hand held tools, other more complex jobs require specialised tools, designed specifically to make the job easier.

Specialized plumbing tools include pipe wrenches, flaring pliers, pipe vise, pipe bending machine, pipe cutter, dies, and joining tools such as soldering torches and crimp tools. New tools have been developed to help plumbers fix problems more efficiently. For example, plumbers use video cameras for inspections of hidden leaks or other problems; they also use hydro jets, and high pressure hydraulic pumps connected to steel cables for trench-less sewer line replacement.

Flooding from excessive rain or clogged sewers may require specialized equipment, such as a heavy duty pumper truck designed to vacuum raw sewage.[citation needed]

Bacteria have been shown to live in ""premises plumbing systems"". The latter refers to the ""pipes and fixtures within a building that transport water to taps after it is delivered by the utility"".[43] Community water systems have been known for centuries to spread waterborne diseases like typhoid and cholera.  However, ""opportunistic premises plumbing pathogens"" have been recognized only more recently: Legionella pneumophila, discovered in 1976, Mycobacterium avium, and Pseudomonas aeruginosa are the most commonly tracked bacteria, which people with depressed immunity can inhale or ingest and may become infected with.[44]
Some of the locations where these opportunistic pathogens can grow include faucets, shower heads, water heaters and along pipe walls. Reasons that favor their growth are ""high surface-to-volume ratio, intermittent stagnation, low disinfectant residual, and warming cycles"". A high surface-to-volume ratio, i.e. a relatively large surface area allows the bacteria to form a biofilm, which protects them from disinfection.[44]

Much of the plumbing work in populated areas is regulated by government or quasi-government agencies due to the direct impact on the public's health, safety, and welfare. Plumbing installation and repair work on residences and other buildings generally must be done according to plumbing and building codes to protect the inhabitants of the buildings and to ensure safe, quality construction to future buyers. If permits are required for work, plumbing contractors typically secure them from the authorities on behalf of home or building owners.[citation needed]

In Australia, the national governing body for plumbing regulation is the Australian Building Codes Board. They are responsible for the creation of the National Construction Code (NCC), Volume 3 of which, the Plumbing Regulations 2008[45] and the Plumbing Code of Australia,[46] pertains to plumbing.

Each Government at the state level has their own Authority and regulations in place for licensing plumbers. They are also responsible for the interpretation, administration and enforcement of the regulations outlined in the NCC.[47] These Authorities are usually established for the sole purpose of regulating plumbing activities in their respective states/territories. However, several state level regulation acts are quite outdated, with some still operating on local policies introduced more than a decade ago. This has led to an increase in plumbing regulatory issues not covered under current policy, and as such, many policies are currently being updated to cover these more modern issues. The updates include changed to the minimum experience and training requirements for licensing, additional work standards for new and more specific kinds of plumbing, as well as adopting the Plumbing Code of Australia into state regulations in an effort to standardise plumbing regulations across the country.

In Norway, new domestic plumbing installed since 1997 has had to satisfy the requirement that it should be easily accessible for replacement after installation.[48] This has led to the development of the pipe-in-pipe system as a de facto requirement for domestic plumbing.

In the United Kingdom the professional body is the Chartered Institute of Plumbing and Heating Engineering (educational charity status) and it is true that the trade still remains virtually ungoverned;[49] there are no systems in place to monitor or control the activities of unqualified plumbers or those home owners who choose to undertake installation and maintenance works themselves, despite the health and safety issues which arise from such works when they are undertaken incorrectly; see Health Aspects of Plumbing (HAP) published jointly by the World Health Organization (WHO) and the World Plumbing Council (WPC).[50][51] WPC has subsequently appointed a representative to the World Health Organization to take forward various projects related to Health Aspects of Plumbing.[52]

In the United States, plumbing codes and licensing are generally controlled by state and local governments. At the national level, the Environmental Protection Agency has set guidelines about what constitutes lead-free plumbing fittings and pipes, in order to comply with the Safe Drinking Water Act.[53]

Some widely used Standards in the United States are:[citation needed]

In Canada, plumbing is a regulated trade requiring specific technical training and certification. Standards and regulations for plumbing are overseen at the provincial and territorial level, each having its distinct governing body:

[[Category:Bathrooms]
"
Boiler Installation Services,"A boiler is a closed vessel in which fluid (generally water) is heated. The fluid does not necessarily boil. The heated or vaporized fluid exits the boiler for use in various processes or heating applications,[1][page needed][2][page needed] including water heating, central heating, boiler-based power generation, cooking, and sanitation.

In a fossil fuel power plant using a steam cycle for power generation, the primary heat source will be combustion of coal, oil, or natural gas. In some cases byproduct fuel such as the carbon monoxide rich offgasses of a coke battery can be burned to heat a boiler; biofuels such as bagasse, where economically available, can also be used. In a nuclear power plant, boilers called steam generators are heated by the heat produced by nuclear fission. Where a large volume of hot gas is available from some process, a heat recovery steam generator  or recovery boiler can use the heat to produce steam, with little or no extra fuel consumed; such a configuration is common in a combined cycle power plant where a gas turbine and a steam boiler are used. In all cases the combustion product waste gases are separate from the working fluid of the steam cycle, making these systems examples of external combustion engines.

The pressure vessel of a boiler is usually made of steel (or alloy steel), or historically of wrought iron. Stainless steel, especially of the austenitic types, is not used in wetted parts of boilers due to corrosion and stress corrosion cracking.[3][page needed] However, ferritic stainless steel is often used in superheater sections that will not be exposed to boiling water, and electrically-heated stainless steel shell boilers are allowed under the European ""Pressure Equipment Directive"" for production of steam for sterilizers and disinfectors.[4]

In live steam models, copper or brass is often used because it is more easily fabricated in smaller size boilers. Historically, copper was often used for fireboxes (particularly for steam locomotives), because of its better formability and higher thermal conductivity; however, in more recent times, the high price of copper often makes this an uneconomic choice and cheaper substitutes (such as steel) are used instead.[citation needed]

For much of the Victorian ""age of steam"", the only material used for boilermaking was the highest grade of wrought iron, with assembly by riveting. This iron was often obtained from specialist ironworks, such as those in the Cleator Moor (UK) area, noted for the high quality of their rolled plate, which was especially suitable for use in critical applications such as high-pressure boilers. In the 20th century, design practice moved towards the use of steel, with welded construction, which is stronger and cheaper, and can be fabricated more quickly and with less labour. Wrought iron boilers corrode far more slowly than their modern-day steel counterparts, and are less susceptible to localized pitting and stress-corrosion. That makes the longevity of older wrought-iron boilers far superior to that of welded steel boilers.[citation needed]

Cast iron may be used for the heating vessel of domestic water heaters. Although such heaters are usually termed ""boilers"" in some countries, their purpose is usually to produce hot water, not steam, and so they run at low pressure and try to avoid boiling. The brittleness of cast iron makes it impractical for high-pressure steam boilers.

The source of heat for a boiler is combustion of any of several fuels, such as wood, coal, oil, or natural gas. Electric steam boilers use resistance- or immersion-type heating elements. Nuclear fission is also used as a heat source for generating steam, either directly (BWR) or, in most cases, in specialised heat exchangers called ""steam generators"" (PWR). Heat recovery steam generators (HRSGs) use the heat rejected from other processes such as gas turbine.[citation needed]

There are two methods to measure the boiler efficiency in the ASME performance test code (PTC) for boilers ASME PTC 4[5] and for HRSG ASME PTC 4.4 and EN 12952-15[6] for water tube boilers:

Direct method of boiler efficiency test is more usable or more common.

where

To measure the boiler efficiency in indirect method, parameter like these are needed:

Boilers can be classified into the following configurations:

To define and secure boilers safely, some professional specialized organizations such as the American Society of Mechanical Engineers (ASME) develop standards and regulation codes. For instance, the ASME Boiler and Pressure Vessel Code is a standard providing a wide range of rules and directives to ensure compliance of the boilers and other pressure vessels with safety, security and design standards.[8]

Historically, boilers were a source of many serious injuries and property destruction due to poorly understood engineering principles. Thin and brittle metal shells can rupture, while poorly welded or riveted seams could open up, leading to a violent eruption of the pressurized steam.  When water is converted to steam it expands to over 1,000 times its original volume and travels down steam pipes at over 100 kilometres per hour (62 mph). Because of this, steam is an efficient method of moving energy and heat around a site from a central boiler house to where it is needed, but without the right boiler feedwater treatment, a steam-raising plant will suffer from scale formation and corrosion. At best, this increases energy costs and can lead to poor quality steam, reduced efficiency, shorter plant life and unreliable operation. At worst, it can lead to catastrophic failure and loss of life. Collapsed or dislodged boiler tubes can also spray scalding-hot steam and smoke out of the air intake and firing chute, injuring the firemen who load the coal into the fire chamber. Extremely large boilers providing hundreds of horsepower to operate factories can potentially demolish entire buildings.[9]

A boiler that has a loss of feed water and is permitted to boil dry can be extremely dangerous. If feed water is then sent into the empty boiler, the small cascade of incoming water instantly boils on contact with the superheated metal shell and leads to a violent explosion that cannot be controlled even by safety steam valves. Draining of the boiler can also happen if a leak occurs in the steam supply lines that is larger than the make-up water supply could replace. The Hartford Loop was invented in 1919 by the Hartford Steam Boiler Inspection and Insurance Company as a method to help prevent this condition from occurring, and thereby reduce their insurance claims.[10][11]

When water is boiled the result is saturated steam, also referred to as ""wet steam.""  Saturated steam, while mostly consisting of water vapor, carries some unevaporated water in the form of droplets.  Saturated steam is useful for many purposes, such as cooking, heating and sanitation, but is not desirable when steam is expected to convey energy to machinery, such as a ship's propulsion system or the ""motion"" of a steam locomotive.  This is because unavoidable temperature and/or pressure loss that occurs as steam travels from the boiler to the machinery will cause some condensation, resulting in liquid water being carried into the machinery.  The water entrained in the steam may damage turbine blades or in the case of a reciprocating steam engine, may cause serious mechanical damage due to hydrostatic lock.

Superheated steam boilers evaporate the water and then further heat the steam in a superheater, causing the discharged steam temperature to be substantially above the boiling temperature at the boiler's operating pressure. As the resulting ""dry steam"" is much hotter than needed to stay in the vaporous state it will not contain any significant unevaporated water.  Also, higher steam pressure will be possible than with saturated steam, enabling the steam to carry more energy.  Although superheating adds more energy to the steam in the form of heat there is no effect on pressure, which is determined by the rate at which steam is drawn from the boiler and the pressure settings of the safety valves.[12]  The fuel consumption required to generate superheated steam is greater than that required to generate an equivalent volume of saturated steam.  However, the overall energy efficiency of the steam plant (the combination of boiler, superheater, piping and machinery) generally will be improved enough to more than offset the increased fuel consumption.

Superheater operation is similar to that of the coils on an air conditioning unit, although for a different purpose. The steam piping is directed through the flue gas path in the boiler furnace, an area in which the temperature is typically between 1,300 and 1,600 degrees Celsius (2,372 and 2,912 degrees Fahrenheit). Some superheaters are radiant type, which as the name suggests, they absorb heat by radiation.  Others are convection type, absorbing heat from a fluid. Some are a combination of the two types. Through either method, the extreme heat in the flue gas path will also heat the superheater steam piping and the steam within.

The design of any superheated steam plant presents several engineering challenges due to the high working temperatures and pressures.  One consideration is the introduction of feedwater to the boiler.  The pump used to charge the boiler must be able to overcome the boiler's operating pressure, else water will not flow.  As a superheated boiler is usually operated at high pressure, the corresponding feedwater pressure must be even higher, demanding a more robust pump design.

Another consideration is safety.  High pressure, superheated steam can be extremely dangerous if it unintentionally escapes.  To give the reader some perspective, the steam plants used in many U.S. Navy destroyers built during World War II operated at 600 psi (4,100 kPa; 41 bar) pressure and 850 degrees Fahrenheit (454 degrees Celsius) superheat.  In the event of a major rupture of the system, an ever-present hazard in a warship during combat, the enormous energy release of escaping superheated steam, expanding to more than 1600 times its confined volume, would be equivalent to a cataclysmic explosion, whose effects would be exacerbated by the steam release occurring in a confined space, such as a ship's engine room.  Also, small leaks that are not visible at the point of leakage could be lethal if an individual were to step into the escaping steam's path.  Hence designers endeavor to give the steam-handling components of the system as much strength as possible to maintain integrity.  Special methods of coupling steam pipes together are used to prevent leaks, with very high pressure systems employing welded joints to avoided leakage problems with threaded or gasketed connections.

Supercritical steam generators are frequently used for the production of electric power. They operate at supercritical pressure. In contrast to a ""subcritical boiler"", a supercritical steam generator operates at such a high pressure (over 3,200 psi or 22 MPa) that the physical turbulence that characterizes boiling ceases to occur; the fluid is neither liquid nor gas but a super-critical fluid. There is no generation of steam bubbles within the water, because the pressure is above the critical pressure point at which steam bubbles can form. As the fluid expands through the turbine stages, its thermodynamic state drops below the critical point as it does work turning the turbine which turns the electrical generator from which power is ultimately extracted. The fluid at that point may be a mix of steam and liquid droplets as it passes into the condenser. This results in slightly less fuel use and therefore less greenhouse gas production. The term ""boiler"" should not be used for a supercritical pressure steam generator, as no ""boiling"" occurs in this device.

A fuel-heated boiler must provide air to oxidize its fuel. Early boilers provided this stream of air, or draught, through the natural action of convection in a chimney connected to the exhaust of the combustion chamber. Since the heated flue gas is less dense than the ambient air surrounding the boiler, the flue gas rises in the chimney, pulling denser, fresh air into the combustion chamber.[citation needed]

Most modern boilers depend on mechanical draught rather than natural draught. This is because natural draught is subject to outside air conditions and temperature of flue gases leaving the furnace, as well as the chimney height. All these factors make proper draught hard to attain and therefore make mechanical draught equipment much more reliable and economical.[citation needed]

Types of draught can also be divided into induced draught, where exhaust gases are pulled out of the boiler; forced draught, where fresh air is pushed into the boiler; and balanced draught, where both effects are employed. Natural draught through the use of a chimney is a type of induced draught; mechanical draught can be induced, forced or balanced.

There are two types of mechanical induced draught. The first is through use of a steam jet. The steam jet oriented in the direction of flue gas flow induces flue gases into the stack and allows for a greater flue gas velocity increasing the overall draught in the furnace. This method was common on steam driven locomotives which could not have tall chimneys. The second method is by simply using an induced draught fan (ID fan) which removes flue gases from the furnace and forces the exhaust gas up the stack. Almost all induced draught furnaces operate with a slightly negative pressure.

Mechanical forced draught is provided by means of a fan forcing air into the combustion chamber. Air is often passed through an air heater; which, as the name suggests, heats the air going into the furnace to increase the overall efficiency of the boiler. Dampers are used to control the quantity of air admitted to the furnace. Forced draught furnaces usually have a positive pressure.

Balanced draught is obtained through use of both induced and forced draught. This is more common with larger boilers where the flue gases have to travel a long distance through many boiler passes. The induced draught fan works in conjunction with the forced draught fan allowing the furnace pressure to be maintained slightly below atmospheric.
"
Boiler Repair Services,"A boiler is a closed vessel in which fluid (generally water) is heated. The fluid does not necessarily boil. The heated or vaporized fluid exits the boiler for use in various processes or heating applications,[1][page needed][2][page needed] including water heating, central heating, boiler-based power generation, cooking, and sanitation.

In a fossil fuel power plant using a steam cycle for power generation, the primary heat source will be combustion of coal, oil, or natural gas. In some cases byproduct fuel such as the carbon monoxide rich offgasses of a coke battery can be burned to heat a boiler; biofuels such as bagasse, where economically available, can also be used. In a nuclear power plant, boilers called steam generators are heated by the heat produced by nuclear fission. Where a large volume of hot gas is available from some process, a heat recovery steam generator  or recovery boiler can use the heat to produce steam, with little or no extra fuel consumed; such a configuration is common in a combined cycle power plant where a gas turbine and a steam boiler are used. In all cases the combustion product waste gases are separate from the working fluid of the steam cycle, making these systems examples of external combustion engines.

The pressure vessel of a boiler is usually made of steel (or alloy steel), or historically of wrought iron. Stainless steel, especially of the austenitic types, is not used in wetted parts of boilers due to corrosion and stress corrosion cracking.[3][page needed] However, ferritic stainless steel is often used in superheater sections that will not be exposed to boiling water, and electrically-heated stainless steel shell boilers are allowed under the European ""Pressure Equipment Directive"" for production of steam for sterilizers and disinfectors.[4]

In live steam models, copper or brass is often used because it is more easily fabricated in smaller size boilers. Historically, copper was often used for fireboxes (particularly for steam locomotives), because of its better formability and higher thermal conductivity; however, in more recent times, the high price of copper often makes this an uneconomic choice and cheaper substitutes (such as steel) are used instead.[citation needed]

For much of the Victorian ""age of steam"", the only material used for boilermaking was the highest grade of wrought iron, with assembly by riveting. This iron was often obtained from specialist ironworks, such as those in the Cleator Moor (UK) area, noted for the high quality of their rolled plate, which was especially suitable for use in critical applications such as high-pressure boilers. In the 20th century, design practice moved towards the use of steel, with welded construction, which is stronger and cheaper, and can be fabricated more quickly and with less labour. Wrought iron boilers corrode far more slowly than their modern-day steel counterparts, and are less susceptible to localized pitting and stress-corrosion. That makes the longevity of older wrought-iron boilers far superior to that of welded steel boilers.[citation needed]

Cast iron may be used for the heating vessel of domestic water heaters. Although such heaters are usually termed ""boilers"" in some countries, their purpose is usually to produce hot water, not steam, and so they run at low pressure and try to avoid boiling. The brittleness of cast iron makes it impractical for high-pressure steam boilers.

The source of heat for a boiler is combustion of any of several fuels, such as wood, coal, oil, or natural gas. Electric steam boilers use resistance- or immersion-type heating elements. Nuclear fission is also used as a heat source for generating steam, either directly (BWR) or, in most cases, in specialised heat exchangers called ""steam generators"" (PWR). Heat recovery steam generators (HRSGs) use the heat rejected from other processes such as gas turbine.[citation needed]

There are two methods to measure the boiler efficiency in the ASME performance test code (PTC) for boilers ASME PTC 4[5] and for HRSG ASME PTC 4.4 and EN 12952-15[6] for water tube boilers:

Direct method of boiler efficiency test is more usable or more common.

where

To measure the boiler efficiency in indirect method, parameter like these are needed:

Boilers can be classified into the following configurations:

To define and secure boilers safely, some professional specialized organizations such as the American Society of Mechanical Engineers (ASME) develop standards and regulation codes. For instance, the ASME Boiler and Pressure Vessel Code is a standard providing a wide range of rules and directives to ensure compliance of the boilers and other pressure vessels with safety, security and design standards.[8]

Historically, boilers were a source of many serious injuries and property destruction due to poorly understood engineering principles. Thin and brittle metal shells can rupture, while poorly welded or riveted seams could open up, leading to a violent eruption of the pressurized steam.  When water is converted to steam it expands to over 1,000 times its original volume and travels down steam pipes at over 100 kilometres per hour (62 mph). Because of this, steam is an efficient method of moving energy and heat around a site from a central boiler house to where it is needed, but without the right boiler feedwater treatment, a steam-raising plant will suffer from scale formation and corrosion. At best, this increases energy costs and can lead to poor quality steam, reduced efficiency, shorter plant life and unreliable operation. At worst, it can lead to catastrophic failure and loss of life. Collapsed or dislodged boiler tubes can also spray scalding-hot steam and smoke out of the air intake and firing chute, injuring the firemen who load the coal into the fire chamber. Extremely large boilers providing hundreds of horsepower to operate factories can potentially demolish entire buildings.[9]

A boiler that has a loss of feed water and is permitted to boil dry can be extremely dangerous. If feed water is then sent into the empty boiler, the small cascade of incoming water instantly boils on contact with the superheated metal shell and leads to a violent explosion that cannot be controlled even by safety steam valves. Draining of the boiler can also happen if a leak occurs in the steam supply lines that is larger than the make-up water supply could replace. The Hartford Loop was invented in 1919 by the Hartford Steam Boiler Inspection and Insurance Company as a method to help prevent this condition from occurring, and thereby reduce their insurance claims.[10][11]

When water is boiled the result is saturated steam, also referred to as ""wet steam.""  Saturated steam, while mostly consisting of water vapor, carries some unevaporated water in the form of droplets.  Saturated steam is useful for many purposes, such as cooking, heating and sanitation, but is not desirable when steam is expected to convey energy to machinery, such as a ship's propulsion system or the ""motion"" of a steam locomotive.  This is because unavoidable temperature and/or pressure loss that occurs as steam travels from the boiler to the machinery will cause some condensation, resulting in liquid water being carried into the machinery.  The water entrained in the steam may damage turbine blades or in the case of a reciprocating steam engine, may cause serious mechanical damage due to hydrostatic lock.

Superheated steam boilers evaporate the water and then further heat the steam in a superheater, causing the discharged steam temperature to be substantially above the boiling temperature at the boiler's operating pressure. As the resulting ""dry steam"" is much hotter than needed to stay in the vaporous state it will not contain any significant unevaporated water.  Also, higher steam pressure will be possible than with saturated steam, enabling the steam to carry more energy.  Although superheating adds more energy to the steam in the form of heat there is no effect on pressure, which is determined by the rate at which steam is drawn from the boiler and the pressure settings of the safety valves.[12]  The fuel consumption required to generate superheated steam is greater than that required to generate an equivalent volume of saturated steam.  However, the overall energy efficiency of the steam plant (the combination of boiler, superheater, piping and machinery) generally will be improved enough to more than offset the increased fuel consumption.

Superheater operation is similar to that of the coils on an air conditioning unit, although for a different purpose. The steam piping is directed through the flue gas path in the boiler furnace, an area in which the temperature is typically between 1,300 and 1,600 degrees Celsius (2,372 and 2,912 degrees Fahrenheit). Some superheaters are radiant type, which as the name suggests, they absorb heat by radiation.  Others are convection type, absorbing heat from a fluid. Some are a combination of the two types. Through either method, the extreme heat in the flue gas path will also heat the superheater steam piping and the steam within.

The design of any superheated steam plant presents several engineering challenges due to the high working temperatures and pressures.  One consideration is the introduction of feedwater to the boiler.  The pump used to charge the boiler must be able to overcome the boiler's operating pressure, else water will not flow.  As a superheated boiler is usually operated at high pressure, the corresponding feedwater pressure must be even higher, demanding a more robust pump design.

Another consideration is safety.  High pressure, superheated steam can be extremely dangerous if it unintentionally escapes.  To give the reader some perspective, the steam plants used in many U.S. Navy destroyers built during World War II operated at 600 psi (4,100 kPa; 41 bar) pressure and 850 degrees Fahrenheit (454 degrees Celsius) superheat.  In the event of a major rupture of the system, an ever-present hazard in a warship during combat, the enormous energy release of escaping superheated steam, expanding to more than 1600 times its confined volume, would be equivalent to a cataclysmic explosion, whose effects would be exacerbated by the steam release occurring in a confined space, such as a ship's engine room.  Also, small leaks that are not visible at the point of leakage could be lethal if an individual were to step into the escaping steam's path.  Hence designers endeavor to give the steam-handling components of the system as much strength as possible to maintain integrity.  Special methods of coupling steam pipes together are used to prevent leaks, with very high pressure systems employing welded joints to avoided leakage problems with threaded or gasketed connections.

Supercritical steam generators are frequently used for the production of electric power. They operate at supercritical pressure. In contrast to a ""subcritical boiler"", a supercritical steam generator operates at such a high pressure (over 3,200 psi or 22 MPa) that the physical turbulence that characterizes boiling ceases to occur; the fluid is neither liquid nor gas but a super-critical fluid. There is no generation of steam bubbles within the water, because the pressure is above the critical pressure point at which steam bubbles can form. As the fluid expands through the turbine stages, its thermodynamic state drops below the critical point as it does work turning the turbine which turns the electrical generator from which power is ultimately extracted. The fluid at that point may be a mix of steam and liquid droplets as it passes into the condenser. This results in slightly less fuel use and therefore less greenhouse gas production. The term ""boiler"" should not be used for a supercritical pressure steam generator, as no ""boiling"" occurs in this device.

A fuel-heated boiler must provide air to oxidize its fuel. Early boilers provided this stream of air, or draught, through the natural action of convection in a chimney connected to the exhaust of the combustion chamber. Since the heated flue gas is less dense than the ambient air surrounding the boiler, the flue gas rises in the chimney, pulling denser, fresh air into the combustion chamber.[citation needed]

Most modern boilers depend on mechanical draught rather than natural draught. This is because natural draught is subject to outside air conditions and temperature of flue gases leaving the furnace, as well as the chimney height. All these factors make proper draught hard to attain and therefore make mechanical draught equipment much more reliable and economical.[citation needed]

Types of draught can also be divided into induced draught, where exhaust gases are pulled out of the boiler; forced draught, where fresh air is pushed into the boiler; and balanced draught, where both effects are employed. Natural draught through the use of a chimney is a type of induced draught; mechanical draught can be induced, forced or balanced.

There are two types of mechanical induced draught. The first is through use of a steam jet. The steam jet oriented in the direction of flue gas flow induces flue gases into the stack and allows for a greater flue gas velocity increasing the overall draught in the furnace. This method was common on steam driven locomotives which could not have tall chimneys. The second method is by simply using an induced draught fan (ID fan) which removes flue gases from the furnace and forces the exhaust gas up the stack. Almost all induced draught furnaces operate with a slightly negative pressure.

Mechanical forced draught is provided by means of a fan forcing air into the combustion chamber. Air is often passed through an air heater; which, as the name suggests, heats the air going into the furnace to increase the overall efficiency of the boiler. Dampers are used to control the quantity of air admitted to the furnace. Forced draught furnaces usually have a positive pressure.

Balanced draught is obtained through use of both induced and forced draught. This is more common with larger boilers where the flue gases have to travel a long distance through many boiler passes. The induced draught fan works in conjunction with the forced draught fan allowing the furnace pressure to be maintained slightly below atmospheric.
"
Steam Services,"

Steam is a digital distribution service and storefront developed by Valve. It was launched as a software client in September 2003 to provide game updates automatically for Valve's games and expanded to distributing third-party titles in late 2005. Steam offers various features, such as game server matchmaking with Valve Anti-Cheat (VAC) measures, social networking, and game streaming services. The Steam client functions include update maintenance, cloud storage, and community features such as direct messaging, an in-game overlay, discussion forums, and a virtual collectable marketplace. The storefront also offers productivity software, game soundtracks, videos, and sells hardware made by Valve, such as the Valve Index and the Steam Deck.

Steamworks, an application programming interface (API) released in 2008, is used by developers to integrate Steam's functions, including digital rights management (DRM), into their products. Several game publishers began distributing their products on Steam that year. Initially developed for Windows, Steam was ported to macOS and Linux in 2010 and 2013 respectively, while a mobile version of Steam for interacting with the service's online features was released on iOS and Android in 2012.

The service is the largest digital distribution platform for PC games, with an estimated 75% of the market share in 2013 according to IHS Screen Digest.[2] By 2017, game purchases through Steam totaled about US$4.3 billion, or at least 18% of global PC game sales according to Steam Spy.[3] By 2021, the service had over 34,000 games with over 132 million monthly active users.[4] Steam's success has led to the development of the Steam Machine gaming PCs in 2015, including the SteamOS Linux distribution and Steam Controller; Steam Link devices for local game streaming; and in 2022, the handheld Steam Deck tailored for running Steam games.

In the early 2000s, Valve was looking for a better way to update its published games,[5] as providing downloadable patches for multiplayer games resulted in most users disconnecting for several days until they had installed the patch. They decided to create a platform that would update games automatically, and implement stronger anti-piracy and anti-cheat measures. They approached several companies, including Microsoft, Yahoo!, and RealNetworks, to build a client with these features, but were rejected.[6]

Valve began its own platform development in 2002, using the working names ""Grid"" and ""Gazelle"".[7][8] The Steam platform was announced at the Game Developers Conference event on March 22, 2002, and released for beta testing that day.[9][10] Prior to the implementation of Steam, Valve had a publishing contract with Sierra Studios; the 2001 version of the contract gave Valve rights to digital distribution of its games.[11] Valve took Sierra and its owners, Vivendi Games, to court in 2002 over a claimed breach of contract. Sierra counter-sued, asserting that Valve had undermined the contract by offering a digital storefront for their games, to compete directly with Sierra.[11]

Steam was released out of beta on September 12, 2003.[12] In November 2004, Half-Life 2 was the first high-profile game to be offered digitally on Steam, requiring installation of the Steam client for retail copies. During this time users faced problems attempting to play; part of legal issues that Valve had with Vivendi, who claimed that physical copies they published could not be activated as to them the game had not been released.[13][7][14][15] The Steam requirement was met with concerns about software ownership and requirements, as well as problems with overloaded servers - demonstrated previously by the Counter-Strike rollout.[16]

In 2005, third-party developers were contracted to release games on Steam, such as Rag Doll Kung Fu and Darwinia.[17][18] In May 2007, ATI included Steam in the ATI Catalyst GPU driver as well as offering a free Steam copy of Half-Life 2: Lost Coast and Half-Life 2: Deathmatch to ATI Radeon owners.[19]

In January 2008, Nvidia promoted Steam in the GeForce GPU driver, as well as offering a free Steam copy of Portal: The First Slice to Nvidia hardware owners.[20] In 2011, some Electronic Arts games, such as Crysis 2, Dragon Age II, and Alice: Madness Returns, were removed from sale because of terms of service that prevented them having their own in-game storefront for downloadable content. These were later launched on the Origin service.[21][22][23][24]

In 2019, Ubisoft announced that it would stop selling future games on Steam, starting with Tom Clancy's The Division 2 because Valve would not modify its revenue sharing model.[25] In May 2019, Microsoft distributed its games on Steam in addition to the Microsoft Store.[26]

In 2020, Electronic Arts started to publish selected games on Steam, and offered its rebranded subscription service EA Play on the platform.[27][28] In 2022, Ubisoft announced that it would return to selling its recent games on Steam, starting with Assassin's Creed Valhalla, stating that it was ""constantly evaluating how to bring our games to different audiences wherever they are"".[29] By 2014, total annual game sales on Steam were estimated at $1.5 billion.[30] By 2018, the service had over 90 million monthly active users.[31] In 2018, its network delivered 15 billion gigabytes of data, compared to less than 4 billion in 2014.[32]

Steam's primary service is to allow its users to purchase games and other software, adding them to a virtual library from which they may be downloaded and installed an unlimited number of times. Initially, Valve was required to be the publisher for these games since they had sole access to Steam's database and engine, but with the introduction of the Steamworks software development kit (SDK) in May 2008, anyone could integrate Steam into their game without Valve's direct involvement.[33]

Valve intended to ""make DRM obsolete"" as games released on Steam had traditional anti-piracy measures, including the assignment and distribution of product keys and support for digital rights management software tools such as SecuROM or non-malicious rootkits. With an update to the Steamworks SDK in March 2009, Valve added ""Custom Executable Generation"" (CEG), which creates a unique, encrypted copy of the game's executable files for the given user, which allows them to install it multiple times and on multiple devices, and make backup copies of their software.[34] Once the software is downloaded and installed, the user must then authenticate through Steam to de-encrypt the executable files to play the game. Normally this is done while connected to the Internet following the user's credential validation, but once they have logged into Steam once, a user can instruct Steam to launch in a special offline mode to be able to play their games without a network connection.[35][36] Developers are not limited to Steam's CEG and may include other forms of DRM (or none at all) and other authentication services than Steam; for example, some games from publisher Ubisoft require the use of their Uplay gaming service.[37]

In September 2008, Valve added support for Steam Cloud, a service that can automatically store saved game and related custom files on Valve's servers; users can access this data from any machine running the Steam client.[38] Users can disable this feature on a per-game and per-account basis.[39] Cloud saving was expanded in January 2022 for Dynamic Cloud Sync, allowing games developed with this feature to store saved states to Steam Cloud while a game is running rather than waiting until the user quit; this was added ahead of the portable Steam Deck unit so that users can save from the Deck and then put the unit into a suspended state.[40] In May 2012, the service added the ability for users to manage their game libraries from remote clients, including computers and mobile devices.[41] Product keys sold through third-party retailers can also be redeemed on Steam.[42] For games that incorporate Steamworks, users can buy redemption codes from other vendors and redeem these in the Steam client to add the title to their libraries. Steam also offers a framework for selling and distributing downloadable content (DLC) for games.[43][44]

In September 2013, Steam introduced the ability to share most games with family members and close friends by authorizing machines to access one's library. Authorized players can install the game locally and play it separately from the owning account. Users can access their saved games and achievements provided the main owner is not playing. When the main player initiates a game while a shared account is using it, the shared account user is allowed a few minutes to either save their progress and close the game or purchase the game for their own account.[45] Within Family View, introduced in January 2014, parents can adjust settings for their children's tied accounts, limiting the functionality and accessibility to the Steam client and purchased games.[46] A more robust implementation of Family Sharing, titled ""Steam Families"", was released in September 2024, allowing up to five members of a household to share games from a single account, including the ability to play different games on those accounts along with different game saves and profiles, and enhanced parental control tools for those accounts.[47][48]

By its acceptable use policy, Valve retains the right to block customers' access to their games and Steam services when Valve's Anti-Cheat (VAC) software determines that the user is cheating in multiplayer games, selling accounts to others, or trading games to exploit regional price differences.[49] Blocking such users initially removed access to their other games, leading to some users with high-value accounts losing access because of minor infractions.[50] Valve later changed its policy to be similar to that of Electronic Arts' Origin platform, in which blocked users can still access their games but are heavily restricted, limited to playing in offline mode and unable to participate in Steam Community features.[51] Customers also lose access to their games and Steam account if they refuse to accept changes to Steam's end user license agreements; this last occurred in August 2012.[52] In April 2015, Valve began allowing developers to set bans on players for their games, but enacted and enforced at the Steam level, which allowed them to police their own gaming communities in a customizable manner.[53]

The Steam client includes a digital storefront called the Steam Store through which users can purchase games. Once the game is bought, a software license is permanently attached to the user's Steam account, allowing them to download the software on any compatible device. Game licenses can be given to other accounts under certain conditions. Content is delivered from an international network of servers using a proprietary file transfer protocol.[54] Products sold on Steam are available for sale in different currencies, which changes depending on the user's location.[55] In December 2010, the storefront began supporting WebMoney for payments,[56] and from April 2016 until December 2017 supported Bitcoin payments before dropping support due to high value fluctuations and costly service fees.[57][58] The Steam storefront validates the user's region; the purchase of games may be restricted to specific regions because of release dates, game classification, or agreements with publishers. Since 2010, the Steam Translation Server project allows Steam users to assist with the translation of the Steam client, storefront, and a selected library of Steam games for twenty-eight languages.[59] In October 2018, official support for Vietnamese and Latin American Spanish was added, in addition to Steam's then 26 languages.[60] Steam also allows users to purchase downloadable content for games, and for some specific games such as Team Fortress 2, the ability to purchase in-game inventory items. In February 2015, Steam began to open similar options for in-game item purchases for third-party games.[61] In November 2007, achievements were added, similar to Xbox 360 Achievements.[62]

In conjunction with developers and publishers, Valve frequently provides discounted sales on games on a daily and weekly basis, sometimes oriented around a publisher, genre, or holiday theme, and sometimes allows games to be tried for free during the days of these sales. The site normally offers a large selection of games at a discount during its annual Summer and Holiday sales, including gamification of these sales.[63]

Users of Steam's storefront can also purchase games and other software as gifts for another Steam user. Before May 2017, users could purchase these gifts to be held in their profile's inventory until they opted to gift them. However, this feature enabled a gray market around some games, where a user in a country where the price of a game was substantially lower than elsewhere could stockpile giftable copies to sell to others in regions with much higher prices.[64] In August 2016, Valve changed its gifting policy to require that games with VAC and Game Ban-enabled games be gifted immediately to another Steam user, which also served to combat players that worked around VAC and Game Bans;[65] in May 2017, Valve expanded this policy to all games.[66] The changes also placed limitations on gifts between users of different countries if there is a large difference in pricing.[67] Due to runaway inflation in Argentina and Turkey, Valve eliminated the use of local currency pricing for users in those storefronts in November 2023, instead moving them to a special regional pricing model based on U.S. dollars as a means to provide fair payments to publisher and developers, though these local users saw effective price hikes as high as 2900%.[68]

The Steam store also enables users to redeem store product keys to add software from their library. The keys are sold by third-party providers such as Humble Bundle, distributed as part of a physical release, or given to a user as part of promotions, often used to deliver Kickstarter and other crowdfunding rewards. A grey market exists around Steam keys, where less reputable buyers purchase a large number of Steam keys for a game when it is offered for a low cost, and then resell these keys to users or other third-party sites at a higher price.[69][70] This caused some of these third-party sites, such as G2A, to be embroiled in this grey market.[71] It is possible for publishers to have Valve track down where specific keys have been used and cancel them, removing the product from the user's libraries.[72] Other legitimate storefronts, like Humble Bundle, have set a minimum price that must be spent to obtain Steam keys as to discourage mass purchases.[73] In June 2021, Valve began limiting how frequently Steam users could change their default region to prevent them from purchasing games from outside their home region for cheaper.[74]

In 2013, Steam began to accept player reviews of games. Other users can subsequently rate these reviews as helpful, humorous, or otherwise unhelpful, which are then used to highlight the most useful reviews on the game's Steam store page. Steam also aggregates these reviews and enables users to sort products based on this feedback while browsing the store.[75] In May 2016, Steam further broke out these aggregations between all reviews overall and those made more recently in the last 30 days, a change Valve acknowledges to how game updates, particularly those in Early Access, can alter the impression of a game to users.[76] To prevent observed abuse of the review system by developers or other third-party agents, Valve modified the review system in September 2016 to discount review scores for a game from users that activated the product through a product key rather than directly purchased by the Steam Store, though their reviews remain visible.[77] Alongside this, Valve announced that it would end business relations with any developer or publisher that they found to be abusing the review system.[78] Separately, Valve has taken actions to minimize the effects of review bombs on Steam. In particular, Valve announced in March 2019 that they mark reviews they believe are ""off-topic"" as a result of a review bomb, and eliminate their contribution to summary review scores; the first such games they took action on with this were the Borderlands games after it was announced Borderlands 3 would be a timed-exclusive to the Epic Games Store.[79][80]

Valve added support for free-to-play games on Steam as well as support for in-game microtransactions through the use of Steamworks in June 2011,[81] while support was added in September 2011 for trading in-game items and ""unopened"" gifts between users.[82] Steam Coupons, which was introduced in December 2011, provides single-use coupons that provide a discount to the cost of items. Steam Coupons can be provided to users by developers and publishers; users can trade these coupons between friends in a similar fashion to gifts and in-game items.[83] In May 2015, GameStop began selling Steam Wallet cards.[84] Steam Market, a feature introduced in beta in December 2012 that would allow users to sell virtual items to others via Steam Wallet funds, further extended the idea. Valve levies a transaction fee of 15% on such sales and game publishers that use Steam Market pay a transaction fee. For example, Team Fortress 2—the first game supported at the beta phase—incurred both fees. Full support for other games was expected to be available in early 2013.[85] In April 2013, Valve added subscription-based game support to Steam; the first game to use this service was Darkfall Unholy Wars.[86]

In October 2012, Steam introduced non-gaming applications, which are sold through the service in the same manner as games.[87] Creativity and productivity applications can access the core functions of the Steamworks API, allowing them to use Steam's simplified installation and updating process, and incorporate features including cloud saving and Steam Workshop.[88] Steam also allows game soundtracks to be purchased to be played via Steam Music or integrated with the user's other media players.[89] Valve adjusted its approach to soundtracks in 2020, no longer requiring them to be offered as DLC, meaning that users can buy soundtracks to games they do not own, and publishers can offer soundtracks to games not on Steam.[90]

Valve has also added the ability for publishers to rent and sell digital movies via the service, with initially most being video game documentaries.[91] Following Warner Bros. Entertainment offering the Mad Max films alongside the September 2015 release of the game based on the series,[92] Lionsgate entered into agreement with Valve to rent over one hundred feature films from its catalog through Steam starting in April 2016, with more films following later.[93] In March 2017, Crunchyroll started offering various anime for purchase or rent through Steam.[94] However, by February 2019, Valve shuttered video from its storefront save for videos directly related to gaming content.[95] While available, users could also purchase Steam Machine related hardware.[96]

Valve took a flat 30% share of all revenue generated from direct Steam sales and microtransactions[a] until October 2018 when they changed their policy to reduce the cut to 25% once revenue for a game surpasses US$10 million, and further to 20% at US$50 million.[98] The policy change was seen by journalists as trying to entice larger developers to stay with Steam,[99] while the decision was also met with backlash from indie and other small game developers, as their revenue split remained unchanged.[100][101][102]

While Steam allows developers to offer demo versions of their games at any time, Valve worked with Geoff Keighley in 2019 in conjunction with The Game Awards to hold a week-long Steam Game Festival to feature a large selection of game demos of current and upcoming games, alongside sales for games already released.[103] This event has since been repeated two or three times a year, typically in conjunction with game expositions or award events, and since has been renamed as the Steam Next Fest.[104] Valve expanded support for demo versions of games in July 2024, allowing demos to have their own store page with user reviews and made it easier for user to manage demos within their game library.[105]

A Steam Points system and storefront was added in June 2020, which mirrored similar temporary points systems that had been used in prior sales on the storefront. Users earn points through purchases on Steam or by receiving community recognition for helpful reviews or discussion comments. These points can be redeemed in the separate storefront for cosmetics that apply to the user's profile and chat interface.[106][107]

The popularity of Steam has led to the services being attacked by hackers. An attempt occurred in November 2011, when Valve temporarily closed the community forums, citing potential hacking threats to the service. Days later, Valve reported that the hack had compromised one of its customer databases, potentially allowing the perpetrators to access customer information, including encrypted passwords and credit card details. At that time, Valve was not aware whether the intruders actually accessed this information or discovered the encryption method, but nevertheless warned users to be alert for fraudulent activity.[108][109]

Valve launched Steam Guard in March 2011 with the goal of protecting Steam users against account hijacking via phishing schemes, one of the largest security problems Valve had at the time.[110] Steam Guard was advertised to take advantage of the identity protection provided by Intel's second-generation Core processors and compatible motherboard hardware, which allows users to lock their account to a specific computer. Once locked, activity by that account on other computers must first be approved by the user on the locked computer. Support APIs for Steam Guard are available to third-party developers through Steamworks.[111] Steam Guard also offers two-factor, risk-based authentication that uses a one-time verification code sent to a verified email address associated with the Steam account; this was later expanded to include two-factor authentication through the Steam mobile application, known as Steam Guard Mobile Authenticator.[112]

In 2015, Valve stated that the potential monetary value of virtual goods attached to user accounts had drawn hackers to try to access accounts for financial benefit.[113] Valve reported that in December 2015, around 77,000 accounts per month were hijacked, enabling the hijackers to empty the user's inventory of items through the trading features. To improve security, the company announced that new restrictions would be added in March 2016, under which 15-day holds are placed on traded items unless they activate, and authenticate with Steam Guard Mobile Authenticator.[113][114] After a Counter-Strike: Global Offensive gambling controversy, Valve stated it was cracking down on third-party websites using Steam inventory trading for skin gambling in July 2016.[115]

ReVuln, a commercial vulnerability research firm, published a paper in October 2012 that said the Steam browser protocol was posing a security risk by enabling malicious exploits through a simple user click on a maliciously crafted steam:// URL in a browser.[116][117][118] This was the second serious vulnerability of gaming-related software following a problem with Ubisoft's Uplay.[119] German IT platform Heise online recommended strict separation of gaming and sensitive data, for example using a PC dedicated to gaming, gaming from a second Windows installation, or using a computer account with limited rights dedicated to gaming.[118]

In July 2015, a bug in the software allowed anyone to reset the password to any account by using the ""forgot password"" function of the client. High-profile professional gamers and streamers lost access to their accounts.[120][121] In December 2015, Steam's content delivery network was misconfigured in response to a DDoS attack, causing cached store pages containing personal information to be temporarily exposed for 34,000 users.[122][123]

Valve added new privacy settings to Steam in April 2018, allowing users to hide their activity status, game lists, inventory, and other profile elements. While these changes brought Steam's privacy settings in line with services such as PlayStation Network and the Xbox network, third-party services such as Steam Spy were impacted, due to their reliance on public data to estimate Steam product sales.[124][125]

Valve established a HackerOne bug bounty program in May 2018, a crowdsourced method to test and improve the security features of the Steam client.[126] In August 2019, a security researcher exposed a zero-day vulnerability in the Windows client of Steam, which allowed for any user to run arbitrary code with LocalSystem privileges using just a few simple commands. The vulnerability was then reported to Valve via the program, but it was initially rejected for being ""out-of-scope"". Following a second vulnerability found by the same user, Valve apologized and patched them both, and expanded the program's rules to accept any other similar problems.[127][128]

The Anti-Defamation League published a report that stated the Steam Community platform harbors hateful content in April 2020.[129] In January 2021, a trading card glitch let players generate Steam Wallet funds from free Steam trading cards with bots using Capcom Arcade Stadium and other games, resulting in the game becoming one of the statistically most played titles.[130]

Since November 2013, Steam has allowed for users to review their purchased games and organize them into categories set by the user and add to favorite lists for quick access.[131]
Players can add non-Steam games to their libraries, allowing the game to be easily accessed from the Steam client and providing support where possible for Steam Overlay features. The Steam interface allows for user-defined shortcuts to be added. In this way, third-party modifications and games not purchased through the Steam Store can use Steam features. Valve sponsors and distributes some modifications free of charge;[132] and modifications that use Steamworks can also use any Steam features supported by their parent game. For most games launched from Steam, the client provides an in-game overlay from which the user can access Steam Community lists and participate in chat, manage selected Steam settings, and access a built-in web browser without having to exit the game.[133] Since the beginning of February 2011 as a beta version, the overlay also allows players to take screenshots of the games in process.[134] As a full version on February 24, 2011, this feature was reimplemented so that users could share screenshots on websites of Facebook, Twitter, and Reddit directly from a user's screenshot manager.[135] Store game pages display a score from Metacritic since 2007.[136]

Steam's ""Big Picture"" mode was announced in 2011;[137] public betas started in September 2012 and were integrated into the software in December 2012.[138] Big Picture mode is a 10-foot user interface, which optimizes the Steam display to work on high-definition televisions, allowing the user to control Steam with a gamepad or with a keyboard and mouse. Gabe Newell stated that Big Picture mode was a step towards a dedicated Steam entertainment hardware unit.[139] With the introduction of the Steam Deck, Valve began pushing the new Big Picture mode based on the Steam Deck UI in beta testing in October 2022, and full release in February 2023.[140][141][142] The new UI was also adopted by SteamVR in October 2023.[143]

In 2012, Valve announced Steam for Schools, a free function-limited version of the Steam client for schools.[144] It was part of Valve's initiative to support gamification of learning. It was released alongside free versions of Portal 2 and a standalone program called ""Puzzle Maker"" that allowed teachers and students to create and manipulate levels. It featured additional authentication security that allowed teachers to share and distribute content via a Steam Workshop-type interface but blocks access from students.[145][146]

In-Home Streaming was introduced in May 2014; it allows users to stream games installed on one computer to another on the same home network with low latency.[147] By June 2019, Valve renamed this feature to Remote Play, allowing users to stream games across devices that may be outside of their home network.[148] Steam's ""Remote Play Together"", added in November 2019 after a month of beta testing, gives the ability for local multiplayer games to be played by people in disparate locations, though will not necessary resolve latency problems typical of these types of games.[149][150][151] Remote Play Together was expanded in February 2021 to give the ability to invite non-Steam players to play through a Steam Link app approach.[152]


The Steam client, as part of a social network service, allows users to identify friends and join groups using the Steam Community feature.[153] Through the Steam Chat feature, users can use text chat and peer-to-peer VoIP with other users, identify which games their friends and other group members are playing, and join and invite friends to Steamworks-based multiplayer games that support this feature. Users can participate in forums hosted by Valve to discuss Steam games. Each user has a unique page that shows his or her groups and friends, game library including earned achievements, game wishlists, and other social features; users can choose to keep this information private.[154] In January 2010, Valve reported that 10 million of the 25 million active Steam accounts had signed up to Steam Community.[155] In conjunction with the 2012 Steam Summer Sale, user profiles were updated with Badges reflecting the user's participation in the Steam community and past events.[156] Steam Trading Cards, a system where players earn virtual trading cards based on games they own, were introduced in May 2013. Using them, players can trade with other Steam users on the Steam Community Marketplace and use them to craft ""Badges"", which grant rewards such as discount coupons, and user profile page customization options.[157][158] In 2010, the Steam client became an OpenID provider, allowing third-party websites to use a Steam user's identity without requiring the user to expose his or her Steam credentials.[159][160] In order to prevent abuse, access to most community features is restricted until a one-time payment of at least US$5 is made to Valve. This requirement can be fulfilled by making any purchase of five dollars or more on Steam, or by adding at the same amount to their wallet.[161]

Through Steamworks, Steam provides a means of server browsing for multiplayer games that use the Steam Community features, allowing users to create lobbies with friends or members of common groups. Steamworks also provides Valve Anti-Cheat (VAC), Valve's anti-cheat system; game servers automatically detect and report users who are using cheats in online, multiplayer games.[162] In August 2012, Valve added new features—including dedicated hub pages for games that highlight the best user-created content, top forum posts, and screenshots—to the Community area.[163] In December 2012, a feature where users can upload walkthroughs and guides detailing game strategy was added.[164] Starting in January 2015, the Steam client allowed players to livestream to Steam friends or the public while playing games on the platform.[165][166] For the main event of The International 2018 Dota 2 tournament, Valve launched Steam.tv as a major update to Steam Broadcasting, adding Steam Chat and Steamworks integration for spectating matches played at the event.[167][168] It has also been used for other events, such as a pre-release tournament for the digital card game Artifact and for The Game Awards 2018 and Steam Awards award shows.[169][170][171] Game Recording was added in beta in June 2024 and released in full by November 2024, allowing for recording of gameplay sessions both on demand or as a background recording. Users can then edit and clip footage to share via Steam with other users.[172][173]

In September 2014, Steam Music was added to the Steam client, allowing users to play through music stored on their computer or to stream from a locally networked computer directly in Steam.[174][175] An update to the friends and chat system was released in July 2018, allowing for non-peer-to-peer chats integrated with voice chat and other features that were compared to Discord.[176][177] A standalone mobile app based on this for Android and iOS was released in May 2019.[178]

A major visual overhaul of the Library was released in October 2019,[179] with the goal of aiding users in organizing their games, help showcase what shared games a user's friends are playing, games that are being live-streamed, and new content that may be available, along with more customization options for sorting games. Along with the redesign, Valve launched Steam Events, allowing game developers to communicate when new in-game events are approaching, which appear to players in the Library and game listings.[180][181]

In June 2023, a visual and architectural overhaul was released, unifying the backend functions of the Steam and Steam Deck clients and redesigning the desktop client. As part of this, the in-game overlay received a new customizable design where users can pin windows such as chat or game guides on top of the current game window. It also received several new features, including the ability to create pinnable personal notes stored in the cloud.[182]

Valve provides developers the ability to create storefront pages to help generate interest in their game ahead of release.[183] This is also necessary to fix a release date that functions into Valve's ""build review"", a free service performed by Valve about a week before this release date to make sure the game's launch is trouble-free.[184] Updates in 2020 to Discovery queues have given developers more options for customizing their storefront page and how these pages integrate with users' experiences with the Steam client.[184]

Valve offers Steamworks, an application programming interface (API) that provides development and publishing tools free of charge to game and software developers.[185] Steamworks provides networking and player authentication tools for both server and peer-to-peer multiplayer games, matchmaking services, support for Steam community friends and groups, Steam statistics and achievements, integrated voice communications, and Steam Cloud support, allowing games to integrate with the Steam client. The API also provides anti-cheating devices and digital copy management.[186] In 2016, after introducing the Steam Controller and improvements to the Steam interface to support numerous customization options, the Steamworks API was also updated to provide a generic controller library for developers and these customization features for other third-party controllers, starting with the DualShock 4.[187] Steam's Input API has since been updated to include official support for other console controllers such as the Nintendo Switch Pro Controller in 2018,[188] the Xbox Wireless Controller for the Xbox Series X and Series S consoles, and the PlayStation 5's DualSense, as well as compatible controllers from third-party manufacturers in 2020.[189][190] In November 2020, Valve said the controller usage had more than doubled over the past 2 years.[191] In March 2019, Steam's game server network was opened to third-party developers.[192]

Developers of software available on Steam can track sales of their games through the Steam store. In February 2014, Valve announced that it would begin to allow developers to set up their own sales for their games independent of any sales that Valve may set.[193] Valve may also work with developers to suggest their participation in sales on themed days.[184]

Steam has conducted and partially published a monthly opt-in hardware and software survey between 2007 and 2010.[194][195]

Valve added the ability for developers to sell games under an early access model with a special section of the Steam store, starting in March 2013. This program allows developers to release functional, but not finished, products such as beta versions to the service to allow users to buy the games and help provide testing and feedback towards the final production. Early access also helps to provide funding to the developers to help complete their games.[196] The early access approach allowed more developers to publish games onto the Steam service without the need for Valve's direct curation of games, significantly increasing the number of available games on the service.[197] Valve added Steam Playtest for developers in 2020, allowing them to run closed beta testing for their games prior to a public release.[198]

Developers can request Steam keys of their products to use as they see fit, such as to give away in promotions, to provide to selected users for review, or to give to key resellers for different prioritization. Valve generally honors all such requests, but clarified that they would evaluate some requests to avoid giving keys to games or other offerings that are designed to manipulate the Steam storefront and other features.[199]

Valve enabled the ability for multiple developers to create bundles of games from their offerings in June 2021.[200]

The Steam Workshop is a service that allows users to share user-made content and modifications for video games available on Steam. New levels, art assets, gameplay modifications, or other content may be published to or installed from the Workshop depending on the title. The Workshop was originally used for distribution of new in-game items for Team Fortress 2;[201] it was redesigned to extend support for any game in early 2012, including modifications for The Elder Scrolls V: Skyrim.[202] A May 2012 patch for Portal 2, enabled by a new map-making tool through the Workshop, introduced the ability to share user-created levels.[203] Independently developed games, including Dungeons of Dredmor, are able to provide Workshop support for user-made content.[204] Dota 2 became Valve's third published title available for the Workshop in June 2012; its features include customizable accessories, character skins, and announcer packs.[205] Workshop content may be monetized; Newell said that the Workshop was inspired by gold farming from World of Warcraft to find a way to incentive both players and content creators in video games, and which had informed them of their approach to Team Fortress 2 and their later multiplayer games.[206]

By January 2015, Valve themselves had provided some user-developed Workshop content as paid-for features in Valve-developed games, including Team Fortress 2 and Dota 2; with over $57 million being paid to content creators using the Workshop.[207][208] Valve began allowing developers to use these advanced features in January 2015; both the developer and content generator share the profits of the sale of these items; the feature went live in April 2015, starting with various mods for Skyrim.[207][209][210] This feature was pulled a few days afterward following negative user feedback and reports of pricing and copyright misuse.[211][212][213] Six months later, Valve stated they were still interested in offering this type of functionality in the future.[214] In November 2015, the Steam client was updated with the ability for game developers to offer in-game items for direct sale via the store interface, with Rust being the first game to use the feature.[215][216][217]

SteamVR is a virtual reality hardware and software platform developed by Valve, with a focus on allowing ""room-scale"" experiences using positional tracking base stations, as opposed to those requiring the player to stay in a singular location.[218] SteamVR was first introduced for the Oculus Rift headset in 2014,[219] and later expanded to support other virtual reality headsets.[220][221][218][222] Initially released for support on Windows, macOS, and Linux, Valve dropped macOS support for SteamVR in May 2020.[223] SteamVR 2.0 was released in October 2023, introducing a new overlay interface that is unified with the updated SteamOS and Big Picture mode interfaces.[143]

Until 2012, Valve handpicked games to be included onto the Steam service, limiting these to games that either had a major developer supporting them, or smaller studios with proven track records. Since then, Valve have sought ways to enable more games to be offered through Steam, while pulling away from manually approving games, short of validating that a game runs on the platforms the publisher had indicated.[224] In 2017, Steam development team member Alden Kroll said that Valve knows Steam is in a near-monopoly for game sales on personal computers, and the company does not want to be in a position to determine what gets sold, and thus had tried to find ways to make the process of adding games to Steam outside of their control.[224] At the same time, Valve recognized that unfettered control of games in the service can lead to discovery problems as well as low-quality games.[224]

Valve announced Steam Greenlight to streamline game addition to the service in July 2012 and released the following month.[225] Through Greenlight, Steam users would choose which games were added to the service. Developers were able to submit information about their games, as well as early builds or beta versions, for consideration by users. Users would pledge support for these games, and Valve would make top-pledged games available on Steam.[226] In response to complaints during its first week that finding games to support was made difficult by a flood of inappropriate or false submissions,[227] Valve required developers to pay US$100 to list a game on the service. Those fees were donated to the charity Child's Play.[228] This fee was met with some concern from smaller developers, who often are already working in a deficit and may not have the money to cover such fees.[229] A later modification allowed developers to put conceptual ideas on the Greenlight service to garner interest in potential projects free-of-charge; votes from such projects are visible only to the developer.[230] Valve also allowed non-gaming software to be voted onto the service through Greenlight.[231]

The initial process offered by Greenlight was panned by developers because while they favored the concept, the rate of games that were eventually approved were small.[232] In January 2013, Newell stated that Valve recognized that its role in Greenlight was perceived as a bottleneck, something the company was planning to eliminate in the future through an open marketplace infrastructure.[233][234] On the eve of Greenlight's first anniversary, Valve simultaneously approved 100 games to demonstrate this change of direction.[235]

Valve launched Steam Direct on June 13, 2017, following Greenlight's shutdown the week before.[236] With Steam Direct, a developer or publisher wishing to distribute their game on Steam needs only to complete appropriate identification and tax forms for Valve and then pay a recoupable application fee for each game they intend to publish. Once they apply, a developer must wait thirty days before publishing the game to allow Valve to review the game to ensure it is ""configured correctly, matches the description provided on the store page, and doesn't contain malicious content"".[236]

On announcing its plans for Steam Direct, Valve suggested the fee would be in the range of $100–5,000, meant to encourage earnest software submissions to the service and weed out poor quality games that are treated as shovelware, improving the discovery pipeline to Steam's customers.[237] Smaller developers raised concerns about the Direct fee harming them, and excluding potentially good indie games from reaching the Steam store.[229] Valve opted to set the Direct fee at $100 after reviewing concerns from the community and outlined plans to improve their discovery algorithms and inject more human involvement to help these.[238] Valve refunds the fee should the game exceed $1,000 in sales.[239] In the process of transitioning from Greenlight to Direct, Valve mass-approved most of the 3,400 remaining games that were still in Greenlight, though the company noted that not all of these were at a state to be published. Valve anticipated that the volume of new games added to the service would further increase with Direct in place.[240] Some groups, such as publisher Raw Fury and crowdfunding/investment site Fig, have offered to pay the Direct fee for indie developers who cannot afford it.[241][242] VentureBeat compared the system to the Google Play Store.[243]

Without more direct interaction in the curation process, Valve had looked to find methods to allow players to find games they would be more likely to buy based on previous purchase patterns.[224] Valve has rejected the use of paid advertising or placement on the storefront, which would have created a ""pay to win"" scenario. Instead, the company had relied on algorithms and other automatic features for game discovery, which has allowed for unexpected hits to gain more visibility.[244]

The September 2014 ""Discovery Update"" added tools that would allow existing Steam users to be curators for game recommendations, and sorting functions that presented more popular games and recommended games specific to the user.[245] This Discovery update was considered successful by Valve, as they reported in March 2015 in seeing increased use of the Steam Storefront and an increase in 18% of sales by revenue from just prior to the update.[246] A second Discovery update was released November 2016, giving users more control over what games they want to see or ignore within the Steam Store, alongside tools for developers and publishers to better customize and present their game.[247][248]

By February 2017, Valve reported that with the second Discovery update, the number of games shown to users via the store's front page increased by 42%, with more conversions into sales from that viewership. In 2016, more games are meeting a rough metric of success defined by Valve as selling more than $200,000 in revenues in its first 90 days of release.[249] Valve added a ""Curator Connect"" program in December 2017. Curators can set up descriptors for the type of games they are interested in, preferred languages, and other tags along with social media profiles, while developers can find and reach out to specific curators from this information, and, after review, provide them directly with access to their game. This step, which eliminates the use of a Steam redemption key, is aimed at reducing the reselling of keys, as well as dissuading users who may be trying to game the curator system to obtain free game keys.[250]

Valve has attempted to deal with ""fake games"", those that are built around reused assets and little other innovation, by adding Steam Explorers atop its existing Steam Curator program. Any Steam user can sign up to be an Explorer and be asked to look at under-performing games on the service to either vouch that the game is truly original or if it is an example of a ""fake game"", at which point Valve can take action to remove the game.[251][252]

In July 2019, the Steam Labs feature was introduced as a means to showcase experimental discovery features Valve considered for including into Steam, to seek public feedback. For example, an initial experiment released at launch was the Interactive Recommender, which uses artificial intelligence algorithms pulling data from the user's past gameplay history to suggest new games that may be of interest to them.[253] As these experiments mature through end-user testing, they have then been brought into the storefront as direct features.[254]

The September 2019 Discovery update, which Valve claimed would improve the visibility of niche and lesser-known games, was met with criticism from some indie game developers, who recorded a significant drop in the exposure of their games, including new wishlist additions and appearances in the ""More Like This"" and ""Discovery queue"" sections of the store.[255][256]

Steam Charts were introduced in September 2022 and publicly track the storefront's best-selling and most-played games, including historically by week and month. Charts replaced a previous statistics page to be more comprehensive, and features content that had previously been part of third-party websites including SteamSpy, SteamDB, and SteamCharts.[257]

In June 2015, Valve created a formal process to allow purchasers to request refunds, with refunds guaranteed within the first two weeks as long as the player had not spent more than two hours in a game.[258] Prior to June 2015, Valve had a no-refunds policy, but allowed them in certain circumstances, such as digital rights management issues or false advertising.[259][260][261]

Games that are no longer available for sale for various reasons can still be downloaded and played by those who have already purchased these.[262]

With the launch of Steam Direct, effectively removing any curation of games by Valve prior to being published on Steam, there have been several incidents of published games that have attempted to mislead Steam users. Starting in June 2018, Valve has taken actions against games and developers that are ""trolling"" the system; in September 2018, Valve explicitly defined that trolls on Steam ""aren't actually interested in good faith efforts to make and sell games to you or anyone"".[263][264] As an example, Valve's Lombardi stated that the game Active Shooter, which would have allowed the player to play as either a SWAT team member tasked to take down the shooter at a school shooting incident or as the shooter themselves, was an example of trolling, as he described it was ""designed to do nothing but generate outrage and cause conflict through its existence"".[265] Within a month of clarifying its definition of trolling, Valve removed approximately 170 games from Steam.[266]

In addition to removing bad actors, Valve has also taken steps to reduce the impact of ""fake games"" which could be used to manipulate the trading card marketplace or artificially boost a user's Steam level, in addition to changes in Steam to prevent such abuse.[267][268][269] Some of these changes have resulted in select false positives for legitimate games with unusual end-user usage patterns, such as Wandersong which was flagged in January 2019 for what the developer believed was related to near-unanimous positive user reviews from the game.[270]

Other actions taken by developers against the terms of service or other policies have prompted Valve to remove games,[271] which has included asset flips, [272] review manipulation,[273] misuse of Steamworks tools,[274][275][276] and hostile activities towards Steam users.[277]

Valve has banned games that incorporate blockchain-type technologies, such as non-fungible tokens (NFTs) since 2022 due to the questionable nature of their markets.[278]
With the rise of generative artificial intelligence in 2023, Valve originally established that games with content generated in this manner could be distributed through Steam, though cautioned developers about assuring that they had the rights for this type of content.[279] As greater concerns about the copyright and ethical nature of generational AI in the latter half of 2023, Valve clarified its stance in January 2024, requiring games that did use content from generational AI to disclose this on the game's store page, including methods that the developers used to assure the AI engines did not generate illegal content.[280] Valve updated policies in February to ban games that incorporated paid advertising as part of the gameplay cycle, such as viewing an ad for virtual rewards.[281]

Valve has also removed or threatened to remove games due to inappropriate or mature content, though there was often confusion as to what material qualified for this. For example, Eek Games' House Party included scenes of nudity and sexual encounters in its original release, which drew criticism from conservative religious organization National Center on Sexual Exploitation, leading Valve to remove the title. Eek Games later included censor bars within the game, allowing the game to be added back to Steam, though they offered a patch on their website to remove the bars.[282] In May 2018, several developers of anime-stylized games that contained some light nudity, such as HuniePop, were told by Valve they had to address sexual content within their games or face removal from Steam, leading to questions of inconsistent application of Valve's policies. The National Center on Sexual Exploitation took credit for convincing Valve to target these games. However, Valve later rescinded its orders, allowing these games to remain.[283]

In June 2018, Valve clarified its policy on content, taking a more hands-off approach outside of illegal material. Rather than trying to make decisions themselves on what content is appropriate, Valve enhanced its filtering system to allow developers and publishers to indicate and justify the types of mature content (violence, nudity, and sexual content) in their games. Users can block games that are marked with this type of content from appearing in the store, and if they have not blocked it, they are presented with the description before they can continue to the store page. Developers and publishers with existing games on Steam have been strongly encouraged to complete these forms for these games, while Valve will use moderators to make sure new games are appropriately marked.[264] Valve also committed to developing anti-harassment tools to support developers who may find their game amid controversy.[263]

""So we ended up going back to one of the principles in the forefront of our minds when we started Steam, and more recently as we worked on Steam Direct to open up the Store to many more developers: Valve shouldn't be the ones deciding this. If you're a player, we shouldn't be choosing for you what content you can or can't buy. If you're a developer, we shouldn't be choosing what content you're allowed to create. Those choices should be yours to make. Our role should be to provide systems and tools to support your efforts to make these choices for yourself, and to help you do it in a way that makes you feel comfortable.""

Until these tools were in place, some adult-themed games were delayed for release.[285][286][287] Negligee: Love Stories developed by Dharker Studios was one of the first sexually explicit games to be offered after the introduction of the tools in September 2018. Dharker noted in discussions with Valve that they would be liable for any content-related fines or penalties that countries may place on Valve, a clause of their publishing contract for Steam, and took steps to restrict sale of the game in over 20 regions.[288] Games that feature mature themes with primary characters that visually appear to be underaged, even if the game's narrative establishes them as adults, have been banned by Valve.[289]

In March 2019, Valve faced pressure over Rape Day, a planned game described as being a dark comedy and power fantasy where the player would control a serial rapist amid a zombie apocalypse. Valve ultimately decided against offering the game on Steam, arguing that while it ""[respects] developers' desire to express themselves"", there were ""costs and risks"" associated with the game, and the developers had ""chosen content matter and a way of representing it that makes it very difficult for us to help them [find an audience]"".[290][291]

In December 2020, following a complaint from Medienanstalt Hamburg/Schleswig-Holstein regarding store page images on Steam; Valve in Germany opted to block access to games with ""Adults Only 18+"" pornographic content.[292] The Anti-defamation League published a report in November 2024 accusing Valve of allowing the proliferation of hate and anti-semitic content generated by users and user groups, with over 40,000 groups identified to have names referring to such extreme views. Senator Mark Warner followed with a letter to Valve, asking the company if they were following their published online content policies and to review the cases identified by the ADL.[293]

Valve introduced the Steam Hardware Survey in 2003 ahead of the release of Half-Life 2. At that time, no information was available as to the distribution of CPU and GPU units among gamers, so Valve used the survey, which automatically collected hardware information with the user's permission through the Steam client, to collect this information and refine the hardware targets for Half-Life 2 to meet the widest possible specifications. Since then, Valve continues to use the Steam Hardware Survey to collect hardware distribution information, sharing the net results with other developers to understand the current market, as well as to make choices on when to discontinue support for older hardware and software.[294]

Steam was originally released exclusively for Microsoft Windows in 2003, but has since been ported to other platforms.[295] More recent Steam client versions use the Chromium Embedded Framework.[296] To take advantage of some of its features for newer interface elements, Steam uses 64-bit versions of Chromium, which makes it unsupported on older operating systems such as Windows XP and Windows Vista. Steam on Windows also relies on some security features built into later versions of Windows. Support for XP and Vista was dropped in 2019. While users still on those operating systems can use the client, they do not have access to newer features. Around 0.2% of Steam users were affected by this when it began.[297] In March 2023, Valve announced that Steam would drop support for Windows 7 and 8 on January 1, 2024.[298]

Valve announced a client for macOS in March 2010.[295] The announcement was preceded by a change in the Steam beta client to support the cross-platform WebKit web browser rendering engine instead of the Trident engine of Internet Explorer.[299][300][301] Valve teased the release by emailing several images to Mac community and gaming websites; the images featured characters from Valve games with Apple logos and parodies of vintage Macintosh advertisements.[302][303] Valve developed a full video homage to Apple's 1984 Macintosh commercial to announce the availability of Half-Life 2 on the service; some concept images for the video had previously been used to tease the Mac Steam client.[304]

Steam for macOS was originally planned for release in April 2010 before being pushed back to May 12, 2010. In addition to the Steam client, several features were made available to developers, allowing them to take advantage of the cross-platform Source engine and Steamworks' platform and network capabilities.[305] Through the Steam Play functionality, the macOS client allows players who have purchased compatible products in the Windows version to download the Mac versions at no cost.[306] The Steam Cloud, along with many multiplayer PC games, also supports cross-platform play.[295]

In July 2012, Valve announced that it was developing a client for Linux based on the Ubuntu distribution.[307] This announcement followed months of speculation, primarily from the website Phoronix that had discovered evidence of Linux developing in recent builds of Steam and other Valve games.[308] Newell stated that getting Steam and games to work on Linux is a key strategy for Valve; Newell called the closed nature of Microsoft Windows 8 ""a catastrophe for everyone in the PC space"", and that Linux would maintain ""the openness of the platform"".[309] Valve is extending support to any developers that want to bring their games to Linux, by ""making it as easy as possible for anybody who's engaged with us—putting their games on Steam and getting those running on Linux"", according to Newell.[309]

The team developing the Linux client had been working for a year before the announcement to validate that such a port would be possible.[310] As of the official announcement, a near-feature-complete Steam client for Linux had been developed and successfully run on Ubuntu.[310] Internal beta testing of the Linux client started in October 2012; external beta testing occurred in early November the same year.[311][312] Open beta clients for Linux were made available in late December 2012,[313] and the client was officially released in mid-February 2013.[314] At the time of announcement, Valve's Linux division assured that its first game on the OS, Left 4 Dead 2, would run at an acceptable frame rate and with a degree of connectivity with the Windows and Mac OS X versions. From there, it began working on porting other games to Ubuntu and expanding to other Linux distributions.[307][315][316] Versions of Steam working under Fedora and Red Hat Enterprise Linux were released by October 2013.[317] There were over 500 Linux-compatible games on Steam in June 2014,[318] and in February 2019, Steam for Linux had 5,800 native games and was described as having ""the power to keep Linux [gaming] alive"" by Engadget.[319]

In August 2018, Valve released a beta version of Proton (named Steam Play), an open-source Windows compatibility layer for Linux, so that Linux users could run Windows games directly through Steam for Linux. Proton comprises a set of open-source tools including Wine and DXVK. The software allows the use of Steam-supported controllers, even those not compatible with Windows.[320] Released in February 2022, Valve's handheld computer, the Steam Deck, runs SteamOS 3.0 which is based on the Arch Linux distribution and uses Proton to support Windows-based games without native Linux ports.[321] Valve worked with various middleware developers to make sure their tools were compatible with Proton on Linux and maximize the number of games that the Steam Deck would support. This included working with various anti-cheat developers such as Easy Anti-Cheat and BattlEye to make sure their solutions worked with Proton.[322][323] To help with compatibility, Valve developed a classification system that they will populate to rank any game as to how well it works as a Linux native solution or through Proton.[324]

Support for Nvidia's proprietary deep learning super sampling (DLSS) on supported video cards and games was added to Proton in June 2021, though this is not available on the Steam Deck which is based on AMD hardware.[325][326]

In March 2022, Google offered a prerelease version of Steam on Chromebooks,[327] and entered public beta in November 2022.[328]

At E3 2010, Newell announced that Steamworks would arrive on the PlayStation 3 with Portal 2.[329] Steamworks made its debut on consoles with Portal 2's PlayStation 3 release. Several features—including cross-platform play and instant messaging, Steam Cloud for saved games, and the ability for PS3 owners to download Portal 2 from Steam (Windows and Mac)—were offered.[330] Valve's Counter-Strike: Global Offensive also supports Steamworks and cross-platform features on the PlayStation 3, including using keyboard and mouse controls as an alternative to the gamepad.[331] Valve said it ""hope[s] to expand upon this foundation with more Steam features and functionality in DLC and future content releases"".[332]

The Xbox 360 does not have support for Steamworks. Newell said that they would have liked to bring the service to the console through the game Counter-Strike: Global Offensive,[333] but later said that cross-platform play would not be present in the final version of the game.[334] Valve attributes the inability to use Steamworks on the Xbox 360 to limitations in the Xbox Live regulations of the ability to deliver patches and new content. Valve's Erik Johnson stated that Microsoft required new content on the console to be certified and validated before distribution, which would limit the usefulness of Steamworks' delivery approach.[335]

Valve released an official Steam client for iOS and Android devices in late January 2012, following a short beta period.[336] The application allows players to log into their accounts to browse the storefront, manage their games, and communicate with friends in the Steam community. The application also incorporates a two-factor authentication system that works with Steam Guard. Newell stated that the application was a strong request from Steam users and sees it as a means ""to make [Steam] richer and more accessible for everyone"".[337] A mobile Steam client for Windows Phone devices was released in June 2016.[338] In May 2019, a mobile chat-only client for Steam was released under the name Steam Chat.[339]

On May 14, 2018, a ""Steam Link"" app with remote play features was released in beta to allow users to stream games to Android phones, named after discontinued set-top box Steam Link.[340] It was also submitted to the iOS App Store, but was denied by Apple Inc., who cited ""business conflicts with app guidelines"".[340][341] Apple later clarified its rule at the following Apple Worldwide Developers Conference in early June, in that iOS apps may not offer an app-like purchasing store, but does not restrict apps that provide remote desktop support.[342] In response, Valve removed the ability to purchase games or other content through the app and resubmitted it for approval in June 2018, where it was accepted by Apple and allowed on their store in May 2019.[343][344]

Before 2013, industry analysts believed that Valve was developing hardware and tuning features of Steam with apparent use on its own hardware. These computers were pre-emptively dubbed as ""Steam Boxes"" by the gaming community and expected to be a dedicated machine focused on Steam functionality and maintaining the core functionality of a traditional video game console.[345] In September 2013, Valve unveiled SteamOS, a custom Linux-based operating system they had developed specifically aimed for running Steam and games, and the final concept of the Steam Machine hardware.[346] Unlike other consoles, the Steam Machine does not have set hardware; its technology is implemented at the discretion of the manufacturer and is fully customizable, much like a personal computer.[347] In 2018 the Steam Machines were removed from the storefront due to low sales and small user traffic.[348]

In November 2015, Valve released the set-top box Steam Link and Steam Controller (which was discontinued in 2019).[349] The Steam Link removed the need for HDMI cables for displaying a PC's screen and allowed for wireless connection when connecting to a TV. That was discontinued in 2018, but now ""Steam Link"" refers to the Remote Play mobile app that allows users to stream content, such as games, from a PC to a mobile device over a network.[350][351][352]

Valve released the Steam Deck, a handheld gaming computer running an updated version of SteamOS, with initial shipments starting on February 25, 2022.[353] The Deck is designed for the play of Steam games, but it can be placed into a separate dock that allows the Deck to output to an external display.[354] The Deck was released on February 25, 2022.[355] Among updates to Steam and SteamOS included better Proton layer support for Windows-based games, improved user interface features in the Steam client for the Steam Deck display, and adding Dynamic Cloud Saves to Steam to allow synchronizing saved games while a game is being played.[40] Valve began marking all games on the service through a Steam Deck Validated program to indicate how compatible they were with the Steam Deck software.[324]

Valve included beta support for Steam Cloud Play in May 2020 for developers to allow users to play games in their library which developers and publishers have opted to allow in a cloud gaming service. At launch, Steam Cloud Play only worked through Nvidia's GeForce Now service and would link up to other cloud services in the future though whether Valve would run its own cloud gaming service was unclear.[356]

China has strict regulations on video games and Internet use; however, access to Steam is allowed through China's governmental firewalls. Currently, a large portion of Steam users are from China. By November 2017, more than half of the Steam userbase was fluent in Chinese, an effect created by the large popularity of Dota 2 and PlayerUnknown's Battlegrounds in the country,[357][358] and several developers have reported that Chinese players make up close to 30% of the total players for their games.[359]

Following a Chinese government-ordered temporary block of many of Steam's functions in December 2017,[360] Valve and Perfect World announced they would help to provide an officially sanctioned version of Steam that meets Chinese Internet requirements. Perfect World has worked with Valve before to help bring Dota 2 and Counter-Strike: Global Offensive to the country through approved government processes.[359][361] All games to be released on Steam China are expected to pass through the government approval process and meet other governmental requirements, such as requiring a Chinese company to run any game with an online presence.[359]

The platform is known locally as ""Steam Platform"" (Chinese: 蒸汽平台; pinyin: Zhēngqì píngtái) and runs independently from the rest of Steam. It was made to comply with China's strict regulations on video games.[362] Valve does not plan to prevent Chinese users from accessing the global Steam platform and will try to assure that a player's cloud data remains usable between the two.[359] The client launched as an open beta on February 9, 2021, with about 40 games available at launch.[363] As of December 2021, only around 100 games that have been reviewed and licensed by the government are available through Steam China.[364]

On December 25, 2021, reports emerged that Steam's global service was the target of a domain name system attack that prevented users in China from accessing its site. The Chinese government Ministry of Industry and Information Technology (MIIT) later confirmed that Chinese gamers would no longer be able to use Steam's global service as its international domain name has been designated as ""illegal"". The block has effectively locked all Chinese users out of games they had purchased through Steam's international service.[364][365] In 2023, reports emerged that the Steam Store could be used as normal in China, while the Steam Community was still blocked.[366]

Steam's success has led to some criticism because it supported DRM and for being an effective monopoly.[367][368] In 2012, Free Software Foundation founder Richard Stallman called DRM using Steam on Linux ""unethical"", but still better than Windows.[369]

Steam's customer service has been highly criticized, with users citing poor response times or lack of response. In March 2015, Valve was given a failing ""F"" grade from the Better Business Bureau due to a large number of complaints about Valve's handling of Steam, leading Valve's Erik Johnson to state that ""we don't feel like our customer service support is where it needs to be right now"".[370] Johnson stated the company plans to better integrate customer support features into the Steam client and be more responsive.[370] In May 2017, in addition to hiring more staff for customer service, Valve publicized pages that show the number and type of customer service requests it was handling over the last 90 days, with an average of 75,000 entered each day. Of those, requests for refunds were the largest segment, and which Valve could resolve within hours, followed by account security and recovery requests. Valve stated at this time that 98% of all service requests were processed within 24 hours of filing.[371]

In August 2011, Valve said Steam's revenue, estimated to be $1 billion in 2010, was comparable to that of its published games and that this makes the company ""tremendously profitable.""[372] Valve reported that there were 125 million active accounts on Steam by the end of 2015.[b] By August 2017, the company reported that there were 27 million new active accounts since January 2016, bringing the total number of active users to at least 150 million.[374] Most accounts were from North America and Western Europe, with there being a significant growth in accounts from Asia around 2017, spurred by their work to help localize the client and make additional currency options available to purchasers.[374] In September 2014, 1.4 million accounts belonged to Australian users; this grew to 2.2 million by October 2015.[375]

Valve also considers concurrent users – how many accounts were logged in at the same time – a key indicator of the success of the platform. By August 2017, Valve reported that they saw a peak of 14 million concurrent players, up from 8.4 million in 2015, with 33 million concurrent players each day and 67 million each month.[374] By January 2018, the peak online count had reached 18.5 million, with over 47 million daily active users.[376][377] During the COVID-19 pandemic in 2020, in which a large proportion of the world's population were at home, Steam saw a concurrent player count of over 23 million in March, along with several games seeing similar record-breaking concurrent counts.[378] The highest concurrent player count reached 39.2 million by December 2024, in part from the combined releases of Marvel Rivals and  Path of Exile 2, and 40 million by February 2025 with the release of Monster Hunter Wilds.[379][380]

Steam has grown from seven games in 2004 to over 30,000 by 2019, with additional non-gaming products, such as creation software, DLC, and videos, numbering over 20,000.[386] More than 50,000 games were on the service as of February 2021.[387] The growth of games on Steam is attributed to changes in Valve's curation approach, which allows publishers to add games without Valve's direct involvement, and games supporting virtual reality technology.[197] The addition of Greenlight and Direct has accelerated the number of games present on the service, with almost 40% of the 19,000 games on Steam by the end of 2017 having been released in 2017.[383] Before Greenlight, Valve saw about five new games published each week. Greenlight expanded this to about seventy, and which doubled to one hundred and eighty week following the introduction of Direct.[388]

Although Steam provides direct sales data to developers and publishers, it does not provide public sales data. In 2011, Valve's Jason Holtman stated that the company felt that such data was outdated for a digital market.[389][390] Data that Valve does provide cannot be released without permission because of a non-disclosure agreement.[391][392]

Developers and publishers have asked for some metrics of sales for games, to allow them to judge the potential success of a title by reviewing how similar games have performed. Algorithms that worked on publicly available data through user profiles to estimate sales data with some accuracy led to the creation of the website Steam Spy in 2015.[393] Steam Spy was credited with being reasonably accurate, but in April 2018, Valve added new privacy settings that defaulted to hiding user game profiles, stating this was part of compliance with the General Data Protection Regulation (GDPR) of the European Union. The change broke the method by which Steam Spy had collected data, rendering it unusable.[394] A few months later, another method had been developed using game achievements to estimate sales with similar accuracy, but Valve shortly changed the Steam API that reduced its functionality. Some have asserted that Valve used the GDPR change as a means to block methods of estimating sales,[395] although Valve subsequently promised to provide tools to developers to help gain such insights that they say will be more accurate.[396] In 2020, Simon Carless revised an approach originally proposed by Mike Boxleiter as early as 2013, with Carless's method used to estimate sales based on the number of reviews it has on Steam based on a modified ""Boxleiter number"" used as a multiplication factor.[397]

The accessibility of publishing games on digital storefronts like Steam has been described as key to the popularity of indie games.[398] As these processes allow developers to publish games on Steam with minimal oversight from Valve, journalists have criticized Valve for lacking curation policies that make it difficult to find quality games among poorly produced games, sometimes called ""shovelware"".[399][400]

Following the launch of Steam Direct, the video game industry was split on Valve's hands-off approach. Some praised Valve for favoring avoiding trying to be a moral adjudicator of content and letting consumers decide what content they want to see, while others felt that this would encourage developers to publish games that are purposely hateful, and that Valve's reliance on user-filters and algorithms may not succeed in blocking undesirable content. Some further criticized the decision based on the financial gain from avoiding blocking any game content, as Valve collects a cut from sales through Steam.[401][402][403][404] The National Center on Sexual Exploitation denounced the policy for avoiding corporate and social responsibility ""in light of the rise of sexual violence and exploitation games being hosted on Steam"".[405]

Steam was estimated to have the largest share of the PC digital distribution market in the 2010s.[406][407] In 2013, sales via the Steam catalog are estimated to be between 50 and 75 percent of the total PC gaming market.[408][2] In 2010 and 2013, with an increase in retail copies of major game publishers integrating or requiring Steam, retailers and journalists referred to the service as a monopoly, which they claimed can be detrimental to the industry and that sector competition would yield positive results for consumers.[409][410] Several developers also noted that Steam's influence on the PC gaming market is powerful and one that smaller developers cannot afford to ignore or not work with, but believe that Valve's corporate practices make it a type of ""benevolent dictator"".[411]

Because of Valve's oversight of sales data, estimates of the market share that Steam has of the videogame market are difficult to compile. Stardock, developer of competing platform Impulse, estimated that Steam had a 70% share in 2009.[408] In February 2011, Forbes reported that Steam sales constituted 50–70% of the US$4 billion market for downloaded PC games and that Steam offered game producers gross margins of 70% of the purchase price, compared with 30% at retail.[412]

Steam has been criticized for its reported 30% cut on revenue with publishers from game sales, a value that is similar to other digital storefronts according to IGN.[413] However, some critics have asserted that the share no longer scales with cheaper costs of serving data. A 2019 Game Developers Conference survey showed only 6% of the 400 respondents deemed the share justified.[414] Epic Games' Tim Sweeney postulated that Valve could reduce its cut to 8%, given that content delivery network costs has dropped significantly.[415] Other services have promoted their sites having a lower cut, including the Epic Games Store[416] and Discord.[417]

In November 2009, online retailers Impulse, Direct2Drive and GamersGate refused to offer Call of Duty: Modern Warfare 2 because it includes mandatory installation of Steamworks.[418] Direct2Drive accused Steamworks of being a ""trojan horse"".[419] Valve's business development director Jason Holtman replied Steamworks' features were chosen by developers and based on consumer wants and that Modern Warfare 2 was one of Steam's ""greatest sellers"".[420] In December 2010, MCV/Develop reported that ""key traditional retailers"" would stop offering games that integrate Steam.[421]

Steam's predominance has led to Valve becoming involved in various legal cases. The lack of a formal refund policy led the Australian Competition and Consumer Commission (ACCC) to sue Valve in September 2014 for violating Australian consumer laws that required stores to offer refunds for faulty or broken products.[422] The ACCC won the lawsuit in March 2016, though recognizing Valve changed its policy in the interim.[423] In December 2016, the court fined Valve A$3 million, as well as requiring Valve to include proper language for Australian consumers outlining their rights when purchasing games off of Steam.[424] In January 2018, Valve filed for special leave to appeal the decision to the High Court of Australia,[425] but the High Court dismissed this request.[426] In September 2018, Valve's Steam refund policy was found to violate France's consumer laws, and it was fined €147,000 and required to modify its refund policy.[427]

In December 2015, the French consumer group UFC-Que Choisir initiated a lawsuit against Valve for several of its Steam policies that conflicted with French law, including the restriction on reselling of purchased games, which is legal within the European Union.[428] In September 2019, the Tribunal de grande instance de Paris found that Valve's practice of preventing resales violated the EU’s Information Society Directive of 2001 and the Computer Programs Directive of 2009, and required them to allow it in the future.[429][430] The Interactive Software Federation of Europe (ISFE) issued a statement that the French court ruling goes against established EU case law related to digital copies and threatened to upend much of the digital distribution systems in Europe should it be upheld.[431]

In August 2016, BT Group filed a lawsuit against Valve, stating that Steam's client infringed on four of its patents, which it said were used within Steam's Library, Chat, Messaging, and Broadcasting services.[432]

In 2017, the European Commission began investigating Valve and five other publishers—Bandai Namco Entertainment, Capcom, Focus Home Interactive, Koch Media, and ZeniMax Media—for anti-competitive practices, specifically the use of geo-blocking to prevent access to software within certain countries within the European Economic Area. Such practices would be against the Digital Single Market initiative set by the European Union.[433] The French gaming trade group, Syndicat National du Jeu Vidéo, noted that geo-blocking was a necessary feature to hinder inappropriate product key reselling.[434] The Commission found, in January 2021, that Valve and co-defendants had violated antitrust rules of the European Union, issued combined fines of €7.8 million, and determined that these companies may be further liable to lawsuits from affected consumers.[435] Valve had chosen ""not to cooperate,"" and was fined €1.6 million, the most of any of the defendants.[436]

A January 2021 class-action lawsuit filed against Valve asserted that it forced developers into entering a ""most favored nation""-type of pricing contract to offer games on their storefront, which required the developers to price their games the same on other platforms as they did on Steam, thus stifling competition.[437] Gamasutra's Simon Carless analyzed the lawsuit and observed that Valve's terms only apply the resale of Steam keys and not games themselves, and thus the lawsuit may be without merit.[438]

A separate class-action lawsuit filed against Valve by Wolfire Games in April 2021 asserted that Steam is essentially a monopoly, since developers must sell to PC users through it and that its 30% cut and ""most favored nation"" pricing practices violate antitrust laws as a result of their position.[439] Valve's response, filed in July 2021, dismissed the complaint stating that it ""has no duty under antitrust law to allow developers to use free Steam Keys to undersell prices for the games they sell on Steam—or to provide Steam Keys at all"". Valve defended its 30% revenue as meeting the current Industry standard and claimed Wolfire's figure Steam's market share to lack evidence.[440] Wolfire's suit was dismissed by the presiding judge in November 2021 after determining that Wolfire had failed to show that Valve had a monopoly on game sales and that the 30% cut has remained unchanged throughout Valve's history.[441] Wolfire refiled its case, narrowing the complaint to Valve's use of its dominance to intimidate developers that sell their game for less on other marketplaces, which the judge allowed to proceed in May 2022.[442] According to discovery, Valve was ordered to have Gabe Newell submit to a deposition for discussion of Valve's business strategy related to Steam.[443]

Valve changed the Steam terms of service in September 2024 to eliminate the forced arbitration clause, such that any disputes with the storefront are to be resolved within courtrooms, and allowing for class-action lawsuits.[444]
"
Gas Installation Services,"The Council for Registered Gas Installers (CORGI) operates a voluntary registration scheme for gas installers in the United Kingdom. From 1991 to 2009 registration with CORGI was a legal requirement for gas operatives and businesses throughout the UK, and before April 2010 in Northern Ireland[1] and Guernsey.[2]

CORGI registration requires (beside payment of fees) that gas operatives hold a certificate of competence under the Accredited Certification Scheme (ACS) demonstrating an appropriate level of competence and experience in particular types of gas work. The ACS replaced a number of different certification schemes in 1998.

CORGI lost its status as official registration body in England, Scotland and Wales on 1 April 2009 and in Northern Ireland and Guernsey[3] in April 2010, with this role being taken on by the Gas Safe Register, run on behalf of the Health and Safety Executive by Capita Group.[4]

CORGI was originally established in 1970 as the Confederation for the Registration of Gas Installers to operate a voluntary register. This followed a gas explosion in 1968 which led to the partial collapse of Ronan Point, a tower block in London.[5]

Notwithstanding gas explosions, the greatest danger to the public in using gas is from carbon monoxide (CO), which is a highly toxic by-product of the combustion process.[6] Most of the concern for gas safety focuses on avoiding excessive production of CO through adequate ventilation and correct combustion, and the safe dispersal of the small amounts produced by correct combustion through effective flue systems. Modern room-sealed gas appliances are much safer in this respect and the number of fatalities from CO poisoning has greatly declined. Each year in the UK around 30 people are killed as a result of CO poisoning.[7]

CORGI was approved by the Health and Safety Executive, in 1991, as a body that could certify operatives for the installation and maintenance of gas equipment in both commercial and domestic properties.  Under these responsibilities CORGI could also inspect registered businesses, and investigate and resolve complaints.[8] Registration with CORGI was a legal requirement under the Gas Safety (Installation and Use) Regulations 1998 for any gas installation business.[6] In 1996 it was estimated there were c. 50,000 CORGI registered businesses in the UK employing c. 120,000 gas operatives.[9] CORGI set the standards for gas installation, however this did not include testing for Carbon Monoxide as this was deemed ""unnecessary"" if the equipment was maintained correctly.[10]

CORGI was answerable to the Health and Safety Executive, which is the Government watchdog on all safety issues, including gas.[6] The HSE has the authority to appoint CORGI and/or other agencies to operate the Register of Gas Installers. In 2006 another body (NAPIT) made an application to the HSE to operate a rival registration scheme. This application was turned down by the Health and Safety Executive. It is the CORGI view - a view that is supported by many other organisations too such as the All Party Parliamentary Gas Safety Group - that introducing another gas registration body would cause confusion. However some registered gas installers believe that CORGI's decision to run schemes for plumbing, electrics and ventilation will itself create public confusion, as well as taking away its focus from the prime consideration of gas safety. However, CORGI has been clear that gas safety is at the core of its business and will remain so. CORGI realises that gas installers often do jobs other than just gas and the decision to offer plumbing, electrical and ventilation schemes was made to help installers that are CORGI registered for gas to comply with Building Regulations in areas closely related to gas work.

Some gas installers, primarily individual, independent installers through associations such as ARGI (Association of Registered Gas Installers) felt that the organisation was overbearing and an excessive financial burden, and that little was being done to stop unregistered installers operating[citation needed] and that they were not being supported by or listened to by the organisation. There were also concerns that it abused its monopolistic position as the sole awarding body for UK gas installers.  The perception was, that CORGI used the customer and installer data to sell products and services (initially by including a sales booklet along with the Installation certificate sent to the customer), which some believed was contrary to their remit for the care of that data.  Others considered that use of this data to promote gas safety, including products and services to installers and public safety information to consumers raised safety standards.  The funds raised through the commercial operations also supported lower gas registration fees for installers - this was demonstrated by the fees barely increasing over the years that CORGI ran the gas scheme.

In 2005, CORGI set up the CORGI Trust, which sees all profits from the commercial side of the business being donated to the Trust. These funds will then be used to help advance gas safety within the UK. Recommendations for the use of these funds has ranged from getting the Government to change the law so that only CORGI registered installers can buy gas appliances, through to doing a national television campaign to increase awareness on the need for customers to only use CORGI registered installers and the dangers of carbon monoxide.

Gas Safe Register is the official gas registration body for the United Kingdom, Isle of Man and Guernsey, appointed by the relevant Health and Safety Authority for each area. By law all gas operatives must be on the Gas Safe Register. Gas Safe Register replaced CORGI as the gas registration body in Great Britain and Isle of Man on 1 April 2009 and Northern Ireland and Guernsey on 1 April 2010.

Gas Safe Register work to protect the public from unsafe gas work through;

Gas Safe Register is run by Capita Gas Registration and Ancillary Services Limited, a division of Capita plc.

As of 2010 the Gas Safe register has c. 137,000 registered installers. Both the Gas Safe and previous CORGI schemes have been seen to increase competence in gas installation, and are ""associated with the declining number of gas accidents and deaths"".[11]
"
Medical Gas Installation Services,"Medical gas supply systems in hospitals and other healthcare facilities are utilized to supply specialized gases and gas mixtures to various parts of the facility. Products handled by such systems typically include:

Source equipment systems are generally required to be monitored by alarm systems at the point of supply for abnormal (high or low) gas pressure in areas such as general ward, operating theatres, intensive care units, recovery rooms, or major treatment rooms. Equipment is connected to the medical gas pipeline system via station outlets (US) or terminal units (ISO). 

Medical gas systems are commonly color coded to identify their contents, but as coding systems and requirements (such as those for bottled gas) vary by jurisdiction, the text or labeling is the most reliable guide to the contents. Emergency shut-off valves, or zone valves, are often installed in order to stop gas flowing to an area in the event of fire or substantial leak, as well as for service. Valves may be positioned at the entrance to departments, with access provided via emergency pull-out windows. 

Oxygen may be used for patients requiring supplemental oxygen via mask. Usually accomplished by a large storage system of liquid oxygen at the hospital which is evaporated into a concentrated oxygen supply, pressures are usually around 345–380 kPa (50.0–55.1 psi),[1][2] or in the UK and Europe, 4–5 bar (400–500 kPa; 58–73 psi).[3] This arrangement is described as a vacuum insulated evaporator or bulk tank.[4] In small medical centers with a low patient capacity, oxygen is usually supplied by a manifold of multiple high-pressure cylinders. In areas where a bulk system or high-pressure cylinder manifold is not suitable, oxygen may be supplied by an oxygen concentrator. However, on site production of oxygen is still a relatively new technology.

Medical air is compressed air supplied by a special air compressor, through a dryer (in order to maintain correct dew point levels), and distributed to patient care areas by half hard BS:EN 13348 copper pipe and also use isolation ball valve for operating the services of compressed air 4 bar. It is also called medical air 4 bar.  In smaller facilities, medical air may also be supplied via high-pressure cylinders. Pressures are maintained around 345–380 kPa (50.0–55.1 psi). If not used correctly it can be harmful to humans. [5]

Nitrous oxide is supplied to various surgical suites for its anaesthetic functions during preoperative procedures. It is delivered to the hospital in high-pressure cylinders and supplied through the Medical Gas system. Some bulk systems exist, but are no longer installed due to environmental concerns and overall reduced consumption of nitrous oxide. System pressures are around 345 kPa (50.0 psi), 4 bar (400 kPa; 58 psi) UK.

Nitrogen is typically used to power pneumatic surgical equipment during various procedures, and is supplied by high-pressure cylinders. Pressures range around 1.2 MPa (170 psi) to various locations.

Like nitrogen, instrument air is used to power surgical equipment. However, it is generated on site by an air compressor (similar to a medical air compressor) rather than high-pressure cylinders. Early air compressors could not offer the purity required to drive surgical equipment. However, this has changed and instrument air is becoming a popular alternative to nitrogen. As with nitrogen, pressures range around 1.2 MPa (170 psi). UK systems are supplied at 11 bar (1.1 MPa; 160 psi) to the local area and regulated down to 7–8 bar (700–800 kPa; 100–120 psi) at point of use.

This gas is typically used pure for insufflation during surgery, but can also be used in its liquid form for cryotherapy or local analgesia. Mixed with other gases, it can be used for sterilisation of equipment, anaesthesia, and stimulation of the respiratory system.[6]

A mixture of 5% carbon dioxide in oxygen is called carbogen and is used in the investigation and treatment of various respiratory conditions, such as to stimulate breathing after a period of apnoea, and  managing chronic respiratory obstruction.[6]

Medical vacuum in a hospital supports suction equipment and evacuation procedures, supplied by vacuum pump systems exhausting to the atmosphere. Vacuum will fluctuate across the pipeline, but is generally maintained around −75 kPa (−560 mmHg; −22 inHg), −450 mmHg (−60 kPa; −18 inHg) UK.

Waste anaesthetic gas disposal, or anaesthetic gas scavenging system, is used in hospital anaesthesia evacuation procedures. Although it is similar to a medical vacuum system, some building codes require anaesthetic gases to be scavenged separately. Scavenging systems do not need to be as powerful as medical vacuum systems, and can be maintained around −50 to −65 kPa (−380 to −490 mmHg; −15 to −19 inHg).

There are many gas mixtures used for clinical and medical applications.  They are often used for patient diagnostics such as lung function testing or blood gas analysis.  Test gases are also used to calibrate and maintain medical devices used for the delivery of anaesthetic gases. In laboratories, culture growth applications include controlled aerobic or anaerobic incubator atmospheres for biological cell culture or tissue growth.  Controlled aerobic conditions are created using mixtures rich in oxygen and anaerobic conditions are created using mixtures rich in hydrogen or carbon dioxide. Supply pressure is 4 bar (400 kPa; 58 psi).

Two common medical gas mixtures are entonox and heliox.
"
Fire Protection System Services,"Fire protection is the study and practice of mitigating the unwanted effects of potentially destructive fires.[1][2] It involves the study of the behaviour, compartmentalisation, suppression and investigation of fire and its related emergencies, as well as the research and development, production, testing and application of mitigating systems. In structures, be they land-based, offshore or even ships, the owners and operators are responsible to maintain their facilities in accordance with a design-basis that is rooted in laws, including the local building code and fire code, which are enforced by the authority having jurisdiction.[3]

Buildings must be maintained in accordance with the current fire code, which is enforced by the fire prevention officers of a local fire department.[4][5] In the event of fire emergencies, Firefighters, fire investigators, and other fire prevention personnel are called to mitigate, investigate and learn from the damage of a fire.[6]

When deciding on what fire protection is appropriate for any given situation, it is important to assess the types of fire hazards that may be faced.[7] Some jurisdictions operate systems of classifying fires using code letters.[8][9] Whilst these may agree on some classifications, they also vary. Below is a table showing the standard operated in Europe and Australia against the system used in the United States.

1 Technically there is no such thing as a ""Class E"" fire, as electricity itself does not burn. However, it is considered a dangerous and very deadly complication to a fire, therefore using the incorrect extinguishing method can result in serious injury or death. Class E, however generally refers to fires involving electricity, therefore a bracketed E, ""(E)"" denoted on various types of extinguishers.[10]

Fires are sometimes categorized as ""one alarm"", ""two alarm"", ""three alarm"" (or higher) fires.[11][12] There is no standard definition for what this means quantifiably, though it always refers to the level response by the local authorities. In some cities, the numeric rating refers to the number of fire stations that have been summoned to the fire. In others, the number counts the number of ""dispatches"" for additional personnel and equipment.[13][14]

Fire protection in land-based buildings, offshore construction or on board ships is typically achieved via all of the following:

Passive fire protection (PFP) in the form of compartmentalisation was developed prior to the invention of or widespread use of active fire protection (AFP), mainly in the form of automatic fire sprinkler systems. During this time, PFP was the dominant mode of protection provided in facility designs. With the widespread installation of fire sprinklers in the past 50 years, the reliance on PFP as the only approach was reduced.

Fire protection within a structure relies on all of its components. The building is designed in compliance with the local building code and fire code by the architect and other consultants.[15] A building permit is issued after review by the Authority Having Jurisdiction (AHJ).

Deviations from that original plan should be made known to the AHJ to make sure that the change is still in compliance with the law to prevent any unsafe conditions that may violate the law and put people at risk. For example, if the firestop systems in a structure were inoperable, a significant part of the fire safety plan might be compromised in the event of a fire because the walls and floors that contain the firestops are intended to have a fire-resistance rating. Likewise, if the sprinkler system or fire alarm system is inoperable for lack of proper maintenance, the likelihood of damage or personal injury is increased.

INDIA

USA

UAE

EUROPE

UK
"
HVAC Installation and Service,"Heating, ventilation, and air conditioning (HVAC) is the use of various technologies to control the temperature, humidity, and purity of the air in an enclosed space. Its goal is to provide thermal comfort and acceptable indoor air quality. HVAC system design is a subdiscipline of mechanical engineering, based on the principles of thermodynamics, fluid mechanics, and heat transfer. ""Refrigeration"" is sometimes added to the field's abbreviation as HVAC&R or HVACR, or ""ventilation"" is dropped, as in HACR (as in the designation of HACR-rated circuit breakers).

HVAC is an important part of residential structures such as single family homes, apartment buildings, hotels, and senior living facilities; medium to large industrial and office buildings such as skyscrapers and hospitals; vehicles such as cars, trains, airplanes, ships and submarines; and in marine environments, where safe and healthy building conditions are regulated with respect to temperature and humidity, using fresh air from outdoors.

Ventilating or ventilation (the ""V"" in HVAC) is the process of exchanging or replacing air in any space to provide high indoor air quality which involves temperature control, oxygen replenishment, and removal of moisture, odors, smoke, heat, dust, airborne bacteria, carbon dioxide, and other gases. Ventilation removes unpleasant smells and excessive moisture, introduces outside air, and keeps interior air circulating. Building ventilation methods are categorized as mechanical (forced) or natural.[1]

The three major functions of heating, ventilation, and air conditioning are interrelated, especially with the need to provide thermal comfort and acceptable indoor air quality within reasonable installation, operation, and maintenance costs. HVAC systems can be used in both domestic and commercial environments. HVAC systems can provide ventilation, and maintain pressure relationships between spaces. The means of air delivery and removal from spaces is known as room air distribution.[2]

In modern buildings, the design, installation, and control systems of these functions are integrated into one or more HVAC systems. For very small buildings, contractors normally estimate the capacity and type of system needed and then design the system, selecting the appropriate refrigerant and various components needed. For larger buildings, building service designers, mechanical engineers, or building services engineers analyze, design, and specify the HVAC systems. Specialty mechanical contractors and suppliers then fabricate, install and commission the systems. Building permits and code-compliance inspections of the installations are normally required for all sizes of buildings

Although HVAC is executed in individual buildings or other enclosed spaces (like NORAD's underground headquarters), the equipment involved is in some cases an extension of a larger district heating (DH) or district cooling (DC) network, or a combined DHC network. In such cases, the operating and maintenance aspects are simplified and metering becomes necessary to bill for the energy that is consumed, and in some cases energy that is returned to the larger system. For example, at a given time one building may be utilizing chilled water for air conditioning and the warm water it returns may be used in another building for heating, or for the overall heating-portion of the DHC network (likely with energy added to boost the temperature).[3][4][5]

Basing HVAC on a larger network helps provide an economy of scale that is often not possible for individual buildings, for utilizing renewable energy sources such as solar heat,[6][7][8] winter's cold,[9][10] the cooling potential in some places of lakes or seawater for free cooling, and the enabling function of seasonal thermal energy storage. Utilizing natural sources for HVAC can significantly benefit the environment and promote awareness of alternative methods.

HVAC is based on inventions and discoveries made by Nikolay Lvov, Michael Faraday, Rolla C. Carpenter, Willis Carrier, Edwin Ruud, Reuben Trane, James Joule, William Rankine, Sadi Carnot, Alice Parker and many others.[11]

Multiple inventions within this time frame preceded the beginnings of the first comfort air conditioning system, which was designed in 1902 by Alfred Wolff (Cooper, 2003) for the New York Stock Exchange, while Willis Carrier equipped the Sacketts-Wilhems Printing Company with the process AC unit the same year. Coyne College was the first school to offer HVAC training in 1899.[12] The first residential AC was installed by 1914, and by the 1950s there was ""widespread adoption of residential AC"".[13]

The invention of the components of HVAC systems went hand-in-hand with the Industrial Revolution, and new methods of modernization, higher efficiency, and system control are constantly being introduced by companies and inventors worldwide.

Heaters are appliances whose purpose is to generate heat (i.e. warmth) for the building. This can be done via central heating. Such a system contains a boiler, furnace, or heat pump to heat water, steam, or air in a central location such as a furnace room in a home, or a mechanical room in a large building. The heat can be transferred by convection, conduction, or radiation. Space heaters are used to heat single rooms and only consist of a single unit.

Heaters exist for various types of fuel, including solid fuels, liquids, and gases. Another type of heat source is electricity, normally heating ribbons composed of high resistance wire (see Nichrome). This principle is also used for baseboard heaters and portable heaters. Electrical heaters are often used as backup or supplemental heat for heat pump systems.

The heat pump gained popularity in the 1950s in Japan and the United States.[14] Heat pumps can extract heat from various sources, such as environmental air, exhaust air from a building, or from the ground. Heat pumps transfer heat from outside the structure into the air inside. Initially, heat pump HVAC systems were only used in moderate climates, but with improvements in low temperature operation and reduced loads due to more efficient homes, they are increasing in popularity in cooler climates. They can also operate in reverse to cool an interior.

In the case of heated water or steam, piping is used to transport the heat to the rooms. Most modern hot water boiler heating systems have a circulator, which is a pump, to move hot water through the distribution system (as opposed to older gravity-fed systems). The heat can be transferred to the surrounding air using radiators, hot water coils (hydro-air), or other heat exchangers. The radiators may be mounted on walls or installed within the floor to produce floor heat.

The use of water as the heat transfer medium is known as hydronics. The heated water can also supply an auxiliary heat exchanger to supply hot water for bathing and washing.

Warm air systems distribute the heated air through ductwork systems of supply and return air through metal or fiberglass ducts. Many systems use the same ducts to distribute air cooled by an evaporator coil for air conditioning. The air supply is normally filtered through air filters[dubious – discuss] to remove dust and pollen particles.[15]

The use of furnaces, space heaters, and boilers as a method of indoor heating could result in incomplete combustion and the emission of carbon monoxide, nitrogen oxides, formaldehyde, volatile organic compounds, and other combustion byproducts. Incomplete combustion occurs when there is insufficient oxygen; the inputs are fuels containing various contaminants and the outputs are harmful byproducts, most dangerously carbon monoxide, which is a tasteless and odorless gas with serious adverse health effects.[16]

Without proper ventilation, carbon monoxide can be lethal at concentrations of 1000 ppm (0.1%). However, at several hundred ppm, carbon monoxide exposure induces headaches, fatigue, nausea, and vomiting. Carbon monoxide binds with hemoglobin in the blood, forming carboxyhemoglobin, reducing the blood's ability to transport oxygen. The primary health concerns associated with carbon monoxide exposure are its cardiovascular and neurobehavioral effects. Carbon monoxide can cause atherosclerosis (the hardening of arteries) and can also trigger heart attacks. Neurologically, carbon monoxide exposure reduces hand to eye coordination, vigilance, and continuous performance. It can also affect time discrimination.[17]

Ventilation is the process of changing or replacing air in any space to control the temperature or remove any combination of moisture, odors, smoke, heat, dust, airborne bacteria, or carbon dioxide, and to replenish oxygen. It plays a critical role in maintaining a healthy indoor environment by preventing the buildup of harmful pollutants and ensuring the circulation of fresh air. Different methods, such as natural ventilation through windows and mechanical ventilation systems, can be used depending on the building design and air quality needs. Ventilation often refers to the intentional delivery of the outside air to the building indoor space. It is one of the most important factors for maintaining acceptable indoor air quality in buildings. 

Although ventilation plays a key role in indoor air quality, it may not be sufficient on its own.[18] A clear understanding of both indoor and outdoor air quality parameters is needed to improve the performance of ventilation in terms of ...[19] In scenarios where outdoor pollution would deteriorate indoor air quality, other treatment devices such as filtration may also be necessary.[20]

Methods for ventilating a building may be divided into mechanical/forced and natural types.[21]

Mechanical, or forced, ventilation is provided by an air handler (AHU) and used to control indoor air quality. Excess humidity, odors, and contaminants can often be controlled via dilution or replacement with outside air. However, in humid climates more energy is required to remove excess moisture from ventilation air.

Kitchens and bathrooms typically have mechanical exhausts to control odors and sometimes humidity. Factors in the design of such systems include the flow rate (which is a function of the fan speed and exhaust vent size) and noise level. Direct drive fans are available for many applications and can reduce maintenance needs.

In summer, ceiling fans and table/floor fans circulate air within a room for the purpose of reducing the perceived temperature by increasing evaporation of perspiration on the skin of the occupants. Because hot air rises, ceiling fans may be used to keep a room warmer in the winter by circulating the warm stratified air from the ceiling to the floor.

Natural ventilation is the ventilation of a building with outside air without using fans or other mechanical systems. It can be via operable windows, louvers, or trickle vents when spaces are small and the architecture permits. ASHRAE defined Natural ventilation as the flow of air through open windows, doors, grilles, and other planned building envelope penetrations, and as being driven by natural and/or artificially produced pressure differentials.[1]

Natural ventilation strategies also include cross ventilation, which relies on wind pressure differences on opposite sides of a building. By strategically placing openings, such as windows or vents, on opposing walls, air is channeled through the space to enhance cooling and ventilation. Cross ventilation is most effective when there are clear, unobstructed paths for airflow within the building.

In more complex schemes, warm air is allowed to rise and flow out high building openings to the outside (stack effect), causing cool outside air to be drawn into low building openings. Natural ventilation schemes can use very little energy, but care must be taken to ensure comfort. In warm or humid climates, maintaining thermal comfort solely via natural ventilation might not be possible. Air conditioning systems are used, either as backups or supplements. Air-side economizers also use outside air to condition spaces, but do so using fans, ducts, dampers, and control systems to introduce and distribute cool outdoor air when appropriate.

An important component of natural ventilation is air change rate or air changes per hour: the hourly rate of ventilation divided by the volume of the space. For example, six air changes per hour means an amount of new air, equal to the volume of the space, is added every ten minutes. For human comfort, a minimum of four air changes per hour is typical, though warehouses might have only two. Too high of an air change rate may be uncomfortable, akin to a wind tunnel which has thousands of changes per hour. The highest air change rates are for crowded spaces, bars, night clubs, commercial kitchens at around 30 to 50 air changes per hour.[22]

Room pressure can be either positive or negative with respect to outside the room. Positive pressure occurs when there is more air being supplied than exhausted, and is common to reduce the infiltration of outside contaminants.[23]

Natural ventilation [24] is a key factor in reducing the spread of airborne illnesses such as tuberculosis, the common cold, influenza, meningitis or COVID-19. Opening doors and windows are good ways to maximize natural ventilation, which would make the risk of airborne contagion much lower than with costly and maintenance-requiring mechanical systems. Old-fashioned clinical areas with high ceilings and large windows provide the greatest protection. Natural ventilation costs little and is maintenance free, and is particularly suited to limited-resource settings and tropical climates, where the burden of TB and institutional TB transmission is highest. In settings where respiratory isolation is difficult and climate permits, windows and doors should be opened to reduce the risk of airborne contagion. Natural ventilation requires little maintenance and is inexpensive.[25]

Natural ventilation is not practical in much of the infrastructure because of climate. This means that the facilities need to have effective mechanical ventilation systems and or use Ceiling Level UV or FAR UV ventilation systems.

Ventilation is measured in terms of Air Changes Per Hour (ACH). As of 2023, the CDC recommends that all spaces have a minimum of 5 ACH.[26] For hospital rooms with airborne contagions the CDC recommends a minimum of 12 ACH.[27] The challenges in facility ventilation are public unawareness,[28][29] ineffective government oversight, poor building codes that are based on comfort levels, poor system operations, poor maintenance, and lack of transparency.[30]

UVC or Ultraviolet Germicidal Irradiation is a function used in modern air conditioners which reduces airborne viruses, bacteria, and fungi, through the use of a built-in LED UV light that emits a gentle glow across the evaporator. As the cross-flow fan circulates the room air, any viruses are guided through the sterilization module’s irradiation range, rendering them instantly inactive.[31]

An air conditioning system, or a standalone air conditioner, provides cooling and/or humidity control for all or part of a building. Air conditioned buildings often have sealed windows, because open windows would work against the system intended to maintain constant indoor air conditions. Outside, fresh air is generally drawn into the system by a vent into a mix air chamber for mixing with the space return air.  Then the mixture air enters an indoor or outdoor heat exchanger section where the air is to be cooled down, then be guided to the space creating positive air pressure. The percentage of return air made up of fresh air can usually be manipulated by adjusting the opening of this vent. Typical fresh air intake is about 10% of the total supply air.[citation needed]

Air conditioning and refrigeration are provided through the removal of heat. Heat can be removed through radiation, convection, or conduction. The heat transfer medium is a refrigeration system, such as water, air, ice, and chemicals are referred to as refrigerants. A refrigerant is employed either in a heat pump system in which a compressor is used to drive thermodynamic refrigeration cycle, or in a free cooling system that uses pumps to circulate a cool refrigerant (typically water or a glycol mix).

It is imperative that the air conditioning horsepower is sufficient for the area being cooled. Underpowered air conditioning systems will lead to power wastage and inefficient usage. Adequate horsepower is required for any air conditioner installed.

The refrigeration cycle uses four essential elements to cool, which are compressor, condenser, metering device, and evaporator.

In variable climates, the system may include a reversing valve that switches from heating in winter to cooling in summer. By reversing the flow of refrigerant, the heat pump refrigeration cycle is changed from cooling to heating or vice versa. This allows a facility to be heated and cooled by a single piece of equipment by the same means, and with the same hardware.

Free cooling systems can have very high efficiencies, and are sometimes combined with seasonal thermal energy storage so that the cold of winter can be used for summer air conditioning. Common storage mediums are deep aquifers or a natural underground rock mass accessed via a cluster of small-diameter, heat-exchanger-equipped boreholes. Some systems with small storages are hybrids, using free cooling early in the cooling season, and later employing a heat pump to chill the circulation coming from the storage. The heat pump is added-in because the storage acts as a heat sink when the system is in cooling (as opposed to charging) mode, causing the temperature to gradually increase during the cooling season.

Some systems include an ""economizer mode"", which is sometimes called a ""free-cooling mode"". When economizing, the control system will open (fully or partially) the outside air damper and close (fully or partially) the return air damper. This will cause fresh, outside air to be supplied to the system. When the outside air is cooler than the demanded cool air, this will allow the demand to be met without using the mechanical supply of cooling (typically chilled water or a direct expansion ""DX"" unit), thus saving energy. The control system can compare the temperature of the outside air vs. return air, or it can compare the enthalpy of the air, as is frequently done in climates where humidity is more of an issue. In both cases, the outside air must be less energetic than the return air for the system to enter the economizer mode.

Central, ""all-air"" air-conditioning systems (or package systems) with a combined outdoor condenser/evaporator unit are often installed in North American residences, offices, and public buildings, but are difficult to retrofit (install in a building that was not designed to receive it) because of the bulky air ducts required.[32] (Minisplit ductless systems are used in these situations.) Outside of North America, packaged systems are only used in limited applications involving large indoor space such as stadiums, theatres or exhibition halls.

An alternative to packaged systems is the use of separate indoor and outdoor coils in split systems. Split systems are preferred and widely used worldwide except in North America. In North America, split systems are most often seen in residential applications, but they are gaining popularity in small commercial buildings.  Split systems are used where ductwork is not feasible or where the space conditioning efficiency is of prime concern.[33] The benefits of ductless air conditioning systems include easy installation, no ductwork, greater zonal control, flexibility of control, and quiet operation.[34] In space conditioning, the duct losses can account for 30% of energy consumption.[35] The use of minisplits can result in energy savings in space conditioning as there are no losses associated with ducting.

With the split system, the evaporator coil is connected to a remote condenser unit using refrigerant piping between an indoor and outdoor unit instead of ducting air directly from the outdoor unit. Indoor units with directional vents mount onto walls, suspended from ceilings, or fit into the ceiling. Other indoor units mount inside the ceiling cavity so that short lengths of duct handle air from the indoor unit to vents or diffusers around the rooms.

Split systems are more efficient and the footprint is typically smaller than the package systems. On the other hand, package systems tend to have a slightly lower indoor noise level compared to split systems since the fan motor is located outside.

Dehumidification (air drying) in an air conditioning system is provided by the evaporator. Since the evaporator operates at a temperature below the dew point, moisture in the air condenses on the evaporator coil tubes. This moisture is collected at the bottom of the evaporator in a pan and removed by piping to a central drain or onto the ground outside.

A dehumidifier is an air-conditioner-like device that controls the humidity of a room or building. It is often employed in basements that have a higher relative humidity because of their lower temperature (and propensity for damp floors and walls). In food retailing establishments, large open chiller cabinets are highly effective at dehumidifying the internal air. Conversely, a humidifier increases the humidity of a building.

The HVAC components that dehumidify the ventilation air deserve careful attention because outdoor air constitutes most of the annual humidity load for nearly all buildings.[36]

All modern air conditioning systems, even small window package units, are equipped with internal air filters.[citation needed] These are generally of a lightweight gauze-like material, and must be replaced or washed as conditions warrant. For example, a building in a high dust environment, or a home with furry pets, will need to have the filters changed more often than buildings without these dirt loads. Failure to replace these filters as needed will contribute to a lower heat exchange rate, resulting in wasted energy, shortened equipment life, and higher energy bills; low air flow can result in iced-over evaporator coils, which can completely stop airflow. Additionally, very dirty or plugged filters can cause overheating during a heating cycle, which can result in damage to the system or even fire.

Because an air conditioner moves heat between the indoor coil and the outdoor coil, both must be kept clean. This means that, in addition to replacing the air filter at the evaporator coil, it is also necessary to regularly clean the condenser coil. Failure to keep the condenser clean will eventually result in harm to the compressor because the condenser coil is responsible for discharging both the indoor heat (as picked up by the evaporator) and the heat generated by the electric motor driving the compressor.

HVAC is significantly responsible for promoting energy efficiency of buildings as the building sector consumes the largest percentage of global energy.[37] Since the 1980s, manufacturers of HVAC equipment have been making an effort to make the systems they manufacture more efficient. This was originally driven by rising energy costs, and has more recently been driven by increased awareness of environmental issues. Additionally, improvements to the HVAC system efficiency can also help increase occupant health and productivity.[38] In the US, the EPA has imposed tighter restrictions over the years. There are several methods for making HVAC systems more efficient.

In the past, water heating was more efficient for heating buildings and was the standard in the United States. Today, forced air systems can double for air conditioning and are more popular.

Some benefits of forced air systems, which are now widely used in churches, schools, and high-end residences, are

A drawback is the installation cost, which can be slightly higher than traditional HVAC systems.

Energy efficiency can be improved even more in central heating systems by introducing zoned heating. This allows a more granular application of heat, similar to non-central heating systems. Zones are controlled by multiple thermostats. In water heating systems the thermostats control zone valves, and in forced air systems they control zone dampers inside the vents which selectively block the flow of air. In this case, the control system is very critical to maintaining a proper temperature.

Forecasting is another method of controlling building heating by calculating the demand for heating energy that should be supplied to the building in each time unit.

Ground source, or geothermal, heat pumps are similar to ordinary heat pumps, but instead of transferring heat to or from outside air, they rely on the stable, even temperature of the earth to provide heating and air conditioning. Many regions experience seasonal temperature extremes, which would require large-capacity heating and cooling equipment to heat or cool buildings. For example, a conventional heat pump system used to heat a building in Montana's −57 °C (−70 °F) low temperature or cool a building in the highest temperature ever recorded in the US—57 °C (134 °F) in Death Valley, California, in 1913 would require a large amount of energy due to the extreme difference between inside and outside air temperatures. A metre below the earth's surface, however, the ground remains at a relatively constant temperature. Utilizing this large source of relatively moderate temperature earth, a heating or cooling system's capacity can often be significantly reduced. Although ground temperatures vary according to latitude, at 1.8 metres (6 ft) underground, temperatures generally only range from 7 to 24 °C (45 to 75 °F).

Photovoltaic solar panels offer a new way to potentially decrease the operating cost of air conditioning.  Traditional air conditioners run using alternating current, and hence, any direct-current solar power needs to be inverted to be compatible with these units.  New variable-speed DC-motor units allow solar power to more easily run them since this conversion is unnecessary, and since the motors are tolerant of voltage fluctuations associated with variance in supplied solar power (e.g., due to cloud cover).

Energy recovery systems sometimes utilize heat recovery ventilation or energy recovery ventilation systems that employ heat exchangers or enthalpy wheels to recover sensible or latent heat from exhausted air. This is done by transfer of energy from the stale air inside the home to the incoming fresh air from outside.

The performance of vapor compression refrigeration cycles is limited by thermodynamics.[39] These air conditioning and heat pump devices move heat rather than convert it from one form to another, so thermal efficiencies do not appropriately describe the performance of these devices. The Coefficient of performance (COP) measures performance, but this dimensionless measure has not been adopted. Instead, the Energy Efficiency Ratio (EER) has traditionally been used to characterize the performance of many HVAC systems. EER is the Energy Efficiency Ratio based on a 35 °C (95 °F) outdoor temperature. To more accurately describe the performance of air conditioning equipment over a typical cooling season a modified version of the EER, the Seasonal Energy Efficiency Ratio (SEER), or in Europe the ESEER, is used. SEER ratings are based on seasonal temperature averages instead of a constant 35 °C (95 °F) outdoor temperature. The current industry minimum SEER rating is 14 SEER. Engineers have pointed out some areas where efficiency of the existing hardware could be improved. For example, the fan blades used to move the air are usually stamped from sheet metal, an economical method of manufacture, but as a result they are not aerodynamically efficient. A well-designed blade could reduce the electrical power required to move the air by a third.[40]

Demand-controlled kitchen ventilation (DCKV) is a building controls approach to controlling the volume of kitchen exhaust and supply air in response to the actual cooking loads in a commercial kitchen. Traditional commercial kitchen ventilation systems operate at 100% fan speed independent of the volume of cooking activity and DCKV technology changes that to provide significant fan energy and conditioned air savings. By deploying smart sensing technology, both the exhaust and supply fans can be controlled to capitalize on the affinity laws for motor energy savings, reduce makeup air heating and cooling energy, increasing safety, and reducing ambient kitchen noise levels.[41]

Air cleaning and filtration removes particles, contaminants, vapors and gases from the air. The filtered and cleaned air then is used in heating, ventilation, and air conditioning. Air cleaning and filtration should be taken in account when protecting our building environments.[42] If present, contaminants can come out from the HVAC systems if not removed or filtered properly.

Clean air delivery rate (CADR) is the amount of clean air an air cleaner provides to a room or space. When determining CADR, the amount of airflow in a space is taken into account. For example, an air cleaner with a flow rate of 30 cubic metres (1,000 cu ft) per minute and an efficiency of 50% has a CADR of 15 cubic metres (500 cu ft) per minute. Along with CADR, filtration performance is very important when it comes to the air in our indoor environment. This depends on the size of the particle or fiber, the filter packing density and depth, and the airflow rate.[42]

The HVAC industry is a worldwide enterprise, with roles including operation and maintenance, system design and construction, equipment manufacturing and sales, and in education and research. The HVAC industry was historically regulated by the manufacturers of HVAC equipment, but regulating and standards organizations such as HARDI (Heating, Air-conditioning and Refrigeration Distributors International), ASHRAE, SMACNA, ACCA (Air Conditioning Contractors of America), Uniform Mechanical Code, International Mechanical Code, and AMCA have been established to support the industry and encourage high standards and achievement. (UL as an omnibus agency is not specific to the HVAC industry.)

The starting point in carrying out an estimate both for cooling and heating depends on the exterior climate and interior specified conditions. However, before taking up the heat load calculation, it is necessary to find fresh air requirements for each area in detail, as pressurization is an important consideration.

ISO 16813:2006 is one of the  ISO building environment standards.[43] It establishes the general principles of building environment design. It takes into account the need to provide a healthy indoor environment for the occupants as well as the need to protect the environment for future generations and promote collaboration among the various parties involved in building environmental design for sustainability. ISO16813 is applicable to new construction and the retrofit of existing buildings.[44]

The building environmental design standard aims to:[44]

In the United States, federal licensure is generally handled by EPA certified (for installation and service of HVAC devices).

Many U.S. states have licensing for boiler operation.  Some of these are listed as follows:

Finally, some U.S. cities may have additional labor laws that apply to HVAC professionals.

Many HVAC engineers are members of the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE). ASHRAE regularly organizes two annual technical committees and publishes recognized standards for HVAC design, which are updated every four years.[55]

Another popular society is AHRI, which provides regular information on new refrigeration technology, and publishes relevant standards and codes.

Codes such as the UMC and IMC do include much detail on installation requirements, however. Other useful reference materials include items from SMACNA, ACGIH, and technical trade journals.

American design standards are legislated in the Uniform Mechanical Code or International Mechanical Code. In certain states, counties, or cities, either of these codes may be adopted and amended via various legislative processes. These codes are updated and published by the International Association of Plumbing and Mechanical Officials (IAPMO) or the International Code Council (ICC) respectively, on a 3-year code development cycle. Typically, local building permit departments are charged with enforcement of these standards on private and certain public properties.

An HVAC technician is a tradesman who specializes in heating, ventilation, air conditioning, and refrigeration. HVAC technicians in the US can receive training through formal training institutions, where most earn associate degrees. Training for HVAC technicians includes classroom lectures and hands-on tasks, and can be followed by an apprenticeship wherein the recent graduate works alongside a professional HVAC technician for a temporary period.[56] HVAC techs who have been trained can also be certified in areas such as air conditioning, heat pumps, gas heating, and commercial refrigeration.

The Chartered Institution of Building Services Engineers is a body that covers the essential Service (systems architecture) that allow buildings to operate. It includes the electrotechnical, heating, ventilating, air conditioning, refrigeration and plumbing industries. To train as a building services engineer, the academic requirements are GCSEs (A-C) / Standard Grades (1-3) in Maths and Science, which are important in measurements, planning and theory. Employers will often want a degree in a branch of engineering, such as building environment engineering, electrical engineering or mechanical engineering. To become a full member of CIBSE, and so also to be registered by the Engineering Council UK as a chartered engineer, engineers must also attain an Honours Degree and a master's degree in a relevant engineering subject.[citation needed] CIBSE publishes several guides to HVAC design relevant to the UK market, and also the Republic of Ireland, Australia, New Zealand and Hong Kong. These guides include various recommended design criteria and standards, some of which are cited within the UK building regulations, and therefore form a legislative requirement for major building services works. The main guides are:

Within the construction sector, it is the job of the building services engineer to design and oversee the installation and maintenance of the essential services such as gas, electricity, water, heating and lighting, as well as many others. These all help to make buildings comfortable and healthy places to live and work in. Building Services is part of a sector that has over 51,000 businesses and employs represents 2–3% of the GDP.

The Air Conditioning and Mechanical Contractors Association of Australia (AMCA), Australian Institute of Refrigeration, Air Conditioning and Heating (AIRAH), Australian Refrigeration Mechanical Association and CIBSE are responsible.

Asian architectural temperature-control have different priorities than European methods. For example, Asian heating traditionally focuses on maintaining temperatures of objects such as the floor or furnishings such as Kotatsu tables and directly warming people, as opposed to the Western focus, in modern periods, on designing air systems.

The Philippine Society of Ventilating, Air Conditioning and Refrigerating Engineers (PSVARE) along with Philippine Society of Mechanical Engineers (PSME) govern on the codes and standards for HVAC / MVAC (MVAC means ""mechanical ventilation and air conditioning"") in the Philippines.

The Indian Society of Heating, Refrigerating and Air Conditioning Engineers (ISHRAE) was established to promote the HVAC industry in India. ISHRAE is an associate of ASHRAE. ISHRAE was founded at New Delhi[57] in 1981 and a chapter was started in Bangalore in 1989. Between 1989 & 1993, ISHRAE chapters were formed in all major cities in India.[citation needed]

 Media related to Climate control at Wikimedia Commons

 Related media at Wikimedia Commons:
"
HVAC Inspections,"Heating, ventilation, and air conditioning (HVAC) is the use of various technologies to control the temperature, humidity, and purity of the air in an enclosed space. Its goal is to provide thermal comfort and acceptable indoor air quality. HVAC system design is a subdiscipline of mechanical engineering, based on the principles of thermodynamics, fluid mechanics, and heat transfer. ""Refrigeration"" is sometimes added to the field's abbreviation as HVAC&R or HVACR, or ""ventilation"" is dropped, as in HACR (as in the designation of HACR-rated circuit breakers).

HVAC is an important part of residential structures such as single family homes, apartment buildings, hotels, and senior living facilities; medium to large industrial and office buildings such as skyscrapers and hospitals; vehicles such as cars, trains, airplanes, ships and submarines; and in marine environments, where safe and healthy building conditions are regulated with respect to temperature and humidity, using fresh air from outdoors.

Ventilating or ventilation (the ""V"" in HVAC) is the process of exchanging or replacing air in any space to provide high indoor air quality which involves temperature control, oxygen replenishment, and removal of moisture, odors, smoke, heat, dust, airborne bacteria, carbon dioxide, and other gases. Ventilation removes unpleasant smells and excessive moisture, introduces outside air, and keeps interior air circulating. Building ventilation methods are categorized as mechanical (forced) or natural.[1]

The three major functions of heating, ventilation, and air conditioning are interrelated, especially with the need to provide thermal comfort and acceptable indoor air quality within reasonable installation, operation, and maintenance costs. HVAC systems can be used in both domestic and commercial environments. HVAC systems can provide ventilation, and maintain pressure relationships between spaces. The means of air delivery and removal from spaces is known as room air distribution.[2]

In modern buildings, the design, installation, and control systems of these functions are integrated into one or more HVAC systems. For very small buildings, contractors normally estimate the capacity and type of system needed and then design the system, selecting the appropriate refrigerant and various components needed. For larger buildings, building service designers, mechanical engineers, or building services engineers analyze, design, and specify the HVAC systems. Specialty mechanical contractors and suppliers then fabricate, install and commission the systems. Building permits and code-compliance inspections of the installations are normally required for all sizes of buildings

Although HVAC is executed in individual buildings or other enclosed spaces (like NORAD's underground headquarters), the equipment involved is in some cases an extension of a larger district heating (DH) or district cooling (DC) network, or a combined DHC network. In such cases, the operating and maintenance aspects are simplified and metering becomes necessary to bill for the energy that is consumed, and in some cases energy that is returned to the larger system. For example, at a given time one building may be utilizing chilled water for air conditioning and the warm water it returns may be used in another building for heating, or for the overall heating-portion of the DHC network (likely with energy added to boost the temperature).[3][4][5]

Basing HVAC on a larger network helps provide an economy of scale that is often not possible for individual buildings, for utilizing renewable energy sources such as solar heat,[6][7][8] winter's cold,[9][10] the cooling potential in some places of lakes or seawater for free cooling, and the enabling function of seasonal thermal energy storage. Utilizing natural sources for HVAC can significantly benefit the environment and promote awareness of alternative methods.

HVAC is based on inventions and discoveries made by Nikolay Lvov, Michael Faraday, Rolla C. Carpenter, Willis Carrier, Edwin Ruud, Reuben Trane, James Joule, William Rankine, Sadi Carnot, Alice Parker and many others.[11]

Multiple inventions within this time frame preceded the beginnings of the first comfort air conditioning system, which was designed in 1902 by Alfred Wolff (Cooper, 2003) for the New York Stock Exchange, while Willis Carrier equipped the Sacketts-Wilhems Printing Company with the process AC unit the same year. Coyne College was the first school to offer HVAC training in 1899.[12] The first residential AC was installed by 1914, and by the 1950s there was ""widespread adoption of residential AC"".[13]

The invention of the components of HVAC systems went hand-in-hand with the Industrial Revolution, and new methods of modernization, higher efficiency, and system control are constantly being introduced by companies and inventors worldwide.

Heaters are appliances whose purpose is to generate heat (i.e. warmth) for the building. This can be done via central heating. Such a system contains a boiler, furnace, or heat pump to heat water, steam, or air in a central location such as a furnace room in a home, or a mechanical room in a large building. The heat can be transferred by convection, conduction, or radiation. Space heaters are used to heat single rooms and only consist of a single unit.

Heaters exist for various types of fuel, including solid fuels, liquids, and gases. Another type of heat source is electricity, normally heating ribbons composed of high resistance wire (see Nichrome). This principle is also used for baseboard heaters and portable heaters. Electrical heaters are often used as backup or supplemental heat for heat pump systems.

The heat pump gained popularity in the 1950s in Japan and the United States.[14] Heat pumps can extract heat from various sources, such as environmental air, exhaust air from a building, or from the ground. Heat pumps transfer heat from outside the structure into the air inside. Initially, heat pump HVAC systems were only used in moderate climates, but with improvements in low temperature operation and reduced loads due to more efficient homes, they are increasing in popularity in cooler climates. They can also operate in reverse to cool an interior.

In the case of heated water or steam, piping is used to transport the heat to the rooms. Most modern hot water boiler heating systems have a circulator, which is a pump, to move hot water through the distribution system (as opposed to older gravity-fed systems). The heat can be transferred to the surrounding air using radiators, hot water coils (hydro-air), or other heat exchangers. The radiators may be mounted on walls or installed within the floor to produce floor heat.

The use of water as the heat transfer medium is known as hydronics. The heated water can also supply an auxiliary heat exchanger to supply hot water for bathing and washing.

Warm air systems distribute the heated air through ductwork systems of supply and return air through metal or fiberglass ducts. Many systems use the same ducts to distribute air cooled by an evaporator coil for air conditioning. The air supply is normally filtered through air filters[dubious – discuss] to remove dust and pollen particles.[15]

The use of furnaces, space heaters, and boilers as a method of indoor heating could result in incomplete combustion and the emission of carbon monoxide, nitrogen oxides, formaldehyde, volatile organic compounds, and other combustion byproducts. Incomplete combustion occurs when there is insufficient oxygen; the inputs are fuels containing various contaminants and the outputs are harmful byproducts, most dangerously carbon monoxide, which is a tasteless and odorless gas with serious adverse health effects.[16]

Without proper ventilation, carbon monoxide can be lethal at concentrations of 1000 ppm (0.1%). However, at several hundred ppm, carbon monoxide exposure induces headaches, fatigue, nausea, and vomiting. Carbon monoxide binds with hemoglobin in the blood, forming carboxyhemoglobin, reducing the blood's ability to transport oxygen. The primary health concerns associated with carbon monoxide exposure are its cardiovascular and neurobehavioral effects. Carbon monoxide can cause atherosclerosis (the hardening of arteries) and can also trigger heart attacks. Neurologically, carbon monoxide exposure reduces hand to eye coordination, vigilance, and continuous performance. It can also affect time discrimination.[17]

Ventilation is the process of changing or replacing air in any space to control the temperature or remove any combination of moisture, odors, smoke, heat, dust, airborne bacteria, or carbon dioxide, and to replenish oxygen. It plays a critical role in maintaining a healthy indoor environment by preventing the buildup of harmful pollutants and ensuring the circulation of fresh air. Different methods, such as natural ventilation through windows and mechanical ventilation systems, can be used depending on the building design and air quality needs. Ventilation often refers to the intentional delivery of the outside air to the building indoor space. It is one of the most important factors for maintaining acceptable indoor air quality in buildings. 

Although ventilation plays a key role in indoor air quality, it may not be sufficient on its own.[18] A clear understanding of both indoor and outdoor air quality parameters is needed to improve the performance of ventilation in terms of ...[19] In scenarios where outdoor pollution would deteriorate indoor air quality, other treatment devices such as filtration may also be necessary.[20]

Methods for ventilating a building may be divided into mechanical/forced and natural types.[21]

Mechanical, or forced, ventilation is provided by an air handler (AHU) and used to control indoor air quality. Excess humidity, odors, and contaminants can often be controlled via dilution or replacement with outside air. However, in humid climates more energy is required to remove excess moisture from ventilation air.

Kitchens and bathrooms typically have mechanical exhausts to control odors and sometimes humidity. Factors in the design of such systems include the flow rate (which is a function of the fan speed and exhaust vent size) and noise level. Direct drive fans are available for many applications and can reduce maintenance needs.

In summer, ceiling fans and table/floor fans circulate air within a room for the purpose of reducing the perceived temperature by increasing evaporation of perspiration on the skin of the occupants. Because hot air rises, ceiling fans may be used to keep a room warmer in the winter by circulating the warm stratified air from the ceiling to the floor.

Natural ventilation is the ventilation of a building with outside air without using fans or other mechanical systems. It can be via operable windows, louvers, or trickle vents when spaces are small and the architecture permits. ASHRAE defined Natural ventilation as the flow of air through open windows, doors, grilles, and other planned building envelope penetrations, and as being driven by natural and/or artificially produced pressure differentials.[1]

Natural ventilation strategies also include cross ventilation, which relies on wind pressure differences on opposite sides of a building. By strategically placing openings, such as windows or vents, on opposing walls, air is channeled through the space to enhance cooling and ventilation. Cross ventilation is most effective when there are clear, unobstructed paths for airflow within the building.

In more complex schemes, warm air is allowed to rise and flow out high building openings to the outside (stack effect), causing cool outside air to be drawn into low building openings. Natural ventilation schemes can use very little energy, but care must be taken to ensure comfort. In warm or humid climates, maintaining thermal comfort solely via natural ventilation might not be possible. Air conditioning systems are used, either as backups or supplements. Air-side economizers also use outside air to condition spaces, but do so using fans, ducts, dampers, and control systems to introduce and distribute cool outdoor air when appropriate.

An important component of natural ventilation is air change rate or air changes per hour: the hourly rate of ventilation divided by the volume of the space. For example, six air changes per hour means an amount of new air, equal to the volume of the space, is added every ten minutes. For human comfort, a minimum of four air changes per hour is typical, though warehouses might have only two. Too high of an air change rate may be uncomfortable, akin to a wind tunnel which has thousands of changes per hour. The highest air change rates are for crowded spaces, bars, night clubs, commercial kitchens at around 30 to 50 air changes per hour.[22]

Room pressure can be either positive or negative with respect to outside the room. Positive pressure occurs when there is more air being supplied than exhausted, and is common to reduce the infiltration of outside contaminants.[23]

Natural ventilation [24] is a key factor in reducing the spread of airborne illnesses such as tuberculosis, the common cold, influenza, meningitis or COVID-19. Opening doors and windows are good ways to maximize natural ventilation, which would make the risk of airborne contagion much lower than with costly and maintenance-requiring mechanical systems. Old-fashioned clinical areas with high ceilings and large windows provide the greatest protection. Natural ventilation costs little and is maintenance free, and is particularly suited to limited-resource settings and tropical climates, where the burden of TB and institutional TB transmission is highest. In settings where respiratory isolation is difficult and climate permits, windows and doors should be opened to reduce the risk of airborne contagion. Natural ventilation requires little maintenance and is inexpensive.[25]

Natural ventilation is not practical in much of the infrastructure because of climate. This means that the facilities need to have effective mechanical ventilation systems and or use Ceiling Level UV or FAR UV ventilation systems.

Ventilation is measured in terms of Air Changes Per Hour (ACH). As of 2023, the CDC recommends that all spaces have a minimum of 5 ACH.[26] For hospital rooms with airborne contagions the CDC recommends a minimum of 12 ACH.[27] The challenges in facility ventilation are public unawareness,[28][29] ineffective government oversight, poor building codes that are based on comfort levels, poor system operations, poor maintenance, and lack of transparency.[30]

UVC or Ultraviolet Germicidal Irradiation is a function used in modern air conditioners which reduces airborne viruses, bacteria, and fungi, through the use of a built-in LED UV light that emits a gentle glow across the evaporator. As the cross-flow fan circulates the room air, any viruses are guided through the sterilization module’s irradiation range, rendering them instantly inactive.[31]

An air conditioning system, or a standalone air conditioner, provides cooling and/or humidity control for all or part of a building. Air conditioned buildings often have sealed windows, because open windows would work against the system intended to maintain constant indoor air conditions. Outside, fresh air is generally drawn into the system by a vent into a mix air chamber for mixing with the space return air.  Then the mixture air enters an indoor or outdoor heat exchanger section where the air is to be cooled down, then be guided to the space creating positive air pressure. The percentage of return air made up of fresh air can usually be manipulated by adjusting the opening of this vent. Typical fresh air intake is about 10% of the total supply air.[citation needed]

Air conditioning and refrigeration are provided through the removal of heat. Heat can be removed through radiation, convection, or conduction. The heat transfer medium is a refrigeration system, such as water, air, ice, and chemicals are referred to as refrigerants. A refrigerant is employed either in a heat pump system in which a compressor is used to drive thermodynamic refrigeration cycle, or in a free cooling system that uses pumps to circulate a cool refrigerant (typically water or a glycol mix).

It is imperative that the air conditioning horsepower is sufficient for the area being cooled. Underpowered air conditioning systems will lead to power wastage and inefficient usage. Adequate horsepower is required for any air conditioner installed.

The refrigeration cycle uses four essential elements to cool, which are compressor, condenser, metering device, and evaporator.

In variable climates, the system may include a reversing valve that switches from heating in winter to cooling in summer. By reversing the flow of refrigerant, the heat pump refrigeration cycle is changed from cooling to heating or vice versa. This allows a facility to be heated and cooled by a single piece of equipment by the same means, and with the same hardware.

Free cooling systems can have very high efficiencies, and are sometimes combined with seasonal thermal energy storage so that the cold of winter can be used for summer air conditioning. Common storage mediums are deep aquifers or a natural underground rock mass accessed via a cluster of small-diameter, heat-exchanger-equipped boreholes. Some systems with small storages are hybrids, using free cooling early in the cooling season, and later employing a heat pump to chill the circulation coming from the storage. The heat pump is added-in because the storage acts as a heat sink when the system is in cooling (as opposed to charging) mode, causing the temperature to gradually increase during the cooling season.

Some systems include an ""economizer mode"", which is sometimes called a ""free-cooling mode"". When economizing, the control system will open (fully or partially) the outside air damper and close (fully or partially) the return air damper. This will cause fresh, outside air to be supplied to the system. When the outside air is cooler than the demanded cool air, this will allow the demand to be met without using the mechanical supply of cooling (typically chilled water or a direct expansion ""DX"" unit), thus saving energy. The control system can compare the temperature of the outside air vs. return air, or it can compare the enthalpy of the air, as is frequently done in climates where humidity is more of an issue. In both cases, the outside air must be less energetic than the return air for the system to enter the economizer mode.

Central, ""all-air"" air-conditioning systems (or package systems) with a combined outdoor condenser/evaporator unit are often installed in North American residences, offices, and public buildings, but are difficult to retrofit (install in a building that was not designed to receive it) because of the bulky air ducts required.[32] (Minisplit ductless systems are used in these situations.) Outside of North America, packaged systems are only used in limited applications involving large indoor space such as stadiums, theatres or exhibition halls.

An alternative to packaged systems is the use of separate indoor and outdoor coils in split systems. Split systems are preferred and widely used worldwide except in North America. In North America, split systems are most often seen in residential applications, but they are gaining popularity in small commercial buildings.  Split systems are used where ductwork is not feasible or where the space conditioning efficiency is of prime concern.[33] The benefits of ductless air conditioning systems include easy installation, no ductwork, greater zonal control, flexibility of control, and quiet operation.[34] In space conditioning, the duct losses can account for 30% of energy consumption.[35] The use of minisplits can result in energy savings in space conditioning as there are no losses associated with ducting.

With the split system, the evaporator coil is connected to a remote condenser unit using refrigerant piping between an indoor and outdoor unit instead of ducting air directly from the outdoor unit. Indoor units with directional vents mount onto walls, suspended from ceilings, or fit into the ceiling. Other indoor units mount inside the ceiling cavity so that short lengths of duct handle air from the indoor unit to vents or diffusers around the rooms.

Split systems are more efficient and the footprint is typically smaller than the package systems. On the other hand, package systems tend to have a slightly lower indoor noise level compared to split systems since the fan motor is located outside.

Dehumidification (air drying) in an air conditioning system is provided by the evaporator. Since the evaporator operates at a temperature below the dew point, moisture in the air condenses on the evaporator coil tubes. This moisture is collected at the bottom of the evaporator in a pan and removed by piping to a central drain or onto the ground outside.

A dehumidifier is an air-conditioner-like device that controls the humidity of a room or building. It is often employed in basements that have a higher relative humidity because of their lower temperature (and propensity for damp floors and walls). In food retailing establishments, large open chiller cabinets are highly effective at dehumidifying the internal air. Conversely, a humidifier increases the humidity of a building.

The HVAC components that dehumidify the ventilation air deserve careful attention because outdoor air constitutes most of the annual humidity load for nearly all buildings.[36]

All modern air conditioning systems, even small window package units, are equipped with internal air filters.[citation needed] These are generally of a lightweight gauze-like material, and must be replaced or washed as conditions warrant. For example, a building in a high dust environment, or a home with furry pets, will need to have the filters changed more often than buildings without these dirt loads. Failure to replace these filters as needed will contribute to a lower heat exchange rate, resulting in wasted energy, shortened equipment life, and higher energy bills; low air flow can result in iced-over evaporator coils, which can completely stop airflow. Additionally, very dirty or plugged filters can cause overheating during a heating cycle, which can result in damage to the system or even fire.

Because an air conditioner moves heat between the indoor coil and the outdoor coil, both must be kept clean. This means that, in addition to replacing the air filter at the evaporator coil, it is also necessary to regularly clean the condenser coil. Failure to keep the condenser clean will eventually result in harm to the compressor because the condenser coil is responsible for discharging both the indoor heat (as picked up by the evaporator) and the heat generated by the electric motor driving the compressor.

HVAC is significantly responsible for promoting energy efficiency of buildings as the building sector consumes the largest percentage of global energy.[37] Since the 1980s, manufacturers of HVAC equipment have been making an effort to make the systems they manufacture more efficient. This was originally driven by rising energy costs, and has more recently been driven by increased awareness of environmental issues. Additionally, improvements to the HVAC system efficiency can also help increase occupant health and productivity.[38] In the US, the EPA has imposed tighter restrictions over the years. There are several methods for making HVAC systems more efficient.

In the past, water heating was more efficient for heating buildings and was the standard in the United States. Today, forced air systems can double for air conditioning and are more popular.

Some benefits of forced air systems, which are now widely used in churches, schools, and high-end residences, are

A drawback is the installation cost, which can be slightly higher than traditional HVAC systems.

Energy efficiency can be improved even more in central heating systems by introducing zoned heating. This allows a more granular application of heat, similar to non-central heating systems. Zones are controlled by multiple thermostats. In water heating systems the thermostats control zone valves, and in forced air systems they control zone dampers inside the vents which selectively block the flow of air. In this case, the control system is very critical to maintaining a proper temperature.

Forecasting is another method of controlling building heating by calculating the demand for heating energy that should be supplied to the building in each time unit.

Ground source, or geothermal, heat pumps are similar to ordinary heat pumps, but instead of transferring heat to or from outside air, they rely on the stable, even temperature of the earth to provide heating and air conditioning. Many regions experience seasonal temperature extremes, which would require large-capacity heating and cooling equipment to heat or cool buildings. For example, a conventional heat pump system used to heat a building in Montana's −57 °C (−70 °F) low temperature or cool a building in the highest temperature ever recorded in the US—57 °C (134 °F) in Death Valley, California, in 1913 would require a large amount of energy due to the extreme difference between inside and outside air temperatures. A metre below the earth's surface, however, the ground remains at a relatively constant temperature. Utilizing this large source of relatively moderate temperature earth, a heating or cooling system's capacity can often be significantly reduced. Although ground temperatures vary according to latitude, at 1.8 metres (6 ft) underground, temperatures generally only range from 7 to 24 °C (45 to 75 °F).

Photovoltaic solar panels offer a new way to potentially decrease the operating cost of air conditioning.  Traditional air conditioners run using alternating current, and hence, any direct-current solar power needs to be inverted to be compatible with these units.  New variable-speed DC-motor units allow solar power to more easily run them since this conversion is unnecessary, and since the motors are tolerant of voltage fluctuations associated with variance in supplied solar power (e.g., due to cloud cover).

Energy recovery systems sometimes utilize heat recovery ventilation or energy recovery ventilation systems that employ heat exchangers or enthalpy wheels to recover sensible or latent heat from exhausted air. This is done by transfer of energy from the stale air inside the home to the incoming fresh air from outside.

The performance of vapor compression refrigeration cycles is limited by thermodynamics.[39] These air conditioning and heat pump devices move heat rather than convert it from one form to another, so thermal efficiencies do not appropriately describe the performance of these devices. The Coefficient of performance (COP) measures performance, but this dimensionless measure has not been adopted. Instead, the Energy Efficiency Ratio (EER) has traditionally been used to characterize the performance of many HVAC systems. EER is the Energy Efficiency Ratio based on a 35 °C (95 °F) outdoor temperature. To more accurately describe the performance of air conditioning equipment over a typical cooling season a modified version of the EER, the Seasonal Energy Efficiency Ratio (SEER), or in Europe the ESEER, is used. SEER ratings are based on seasonal temperature averages instead of a constant 35 °C (95 °F) outdoor temperature. The current industry minimum SEER rating is 14 SEER. Engineers have pointed out some areas where efficiency of the existing hardware could be improved. For example, the fan blades used to move the air are usually stamped from sheet metal, an economical method of manufacture, but as a result they are not aerodynamically efficient. A well-designed blade could reduce the electrical power required to move the air by a third.[40]

Demand-controlled kitchen ventilation (DCKV) is a building controls approach to controlling the volume of kitchen exhaust and supply air in response to the actual cooking loads in a commercial kitchen. Traditional commercial kitchen ventilation systems operate at 100% fan speed independent of the volume of cooking activity and DCKV technology changes that to provide significant fan energy and conditioned air savings. By deploying smart sensing technology, both the exhaust and supply fans can be controlled to capitalize on the affinity laws for motor energy savings, reduce makeup air heating and cooling energy, increasing safety, and reducing ambient kitchen noise levels.[41]

Air cleaning and filtration removes particles, contaminants, vapors and gases from the air. The filtered and cleaned air then is used in heating, ventilation, and air conditioning. Air cleaning and filtration should be taken in account when protecting our building environments.[42] If present, contaminants can come out from the HVAC systems if not removed or filtered properly.

Clean air delivery rate (CADR) is the amount of clean air an air cleaner provides to a room or space. When determining CADR, the amount of airflow in a space is taken into account. For example, an air cleaner with a flow rate of 30 cubic metres (1,000 cu ft) per minute and an efficiency of 50% has a CADR of 15 cubic metres (500 cu ft) per minute. Along with CADR, filtration performance is very important when it comes to the air in our indoor environment. This depends on the size of the particle or fiber, the filter packing density and depth, and the airflow rate.[42]

The HVAC industry is a worldwide enterprise, with roles including operation and maintenance, system design and construction, equipment manufacturing and sales, and in education and research. The HVAC industry was historically regulated by the manufacturers of HVAC equipment, but regulating and standards organizations such as HARDI (Heating, Air-conditioning and Refrigeration Distributors International), ASHRAE, SMACNA, ACCA (Air Conditioning Contractors of America), Uniform Mechanical Code, International Mechanical Code, and AMCA have been established to support the industry and encourage high standards and achievement. (UL as an omnibus agency is not specific to the HVAC industry.)

The starting point in carrying out an estimate both for cooling and heating depends on the exterior climate and interior specified conditions. However, before taking up the heat load calculation, it is necessary to find fresh air requirements for each area in detail, as pressurization is an important consideration.

ISO 16813:2006 is one of the  ISO building environment standards.[43] It establishes the general principles of building environment design. It takes into account the need to provide a healthy indoor environment for the occupants as well as the need to protect the environment for future generations and promote collaboration among the various parties involved in building environmental design for sustainability. ISO16813 is applicable to new construction and the retrofit of existing buildings.[44]

The building environmental design standard aims to:[44]

In the United States, federal licensure is generally handled by EPA certified (for installation and service of HVAC devices).

Many U.S. states have licensing for boiler operation.  Some of these are listed as follows:

Finally, some U.S. cities may have additional labor laws that apply to HVAC professionals.

Many HVAC engineers are members of the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE). ASHRAE regularly organizes two annual technical committees and publishes recognized standards for HVAC design, which are updated every four years.[55]

Another popular society is AHRI, which provides regular information on new refrigeration technology, and publishes relevant standards and codes.

Codes such as the UMC and IMC do include much detail on installation requirements, however. Other useful reference materials include items from SMACNA, ACGIH, and technical trade journals.

American design standards are legislated in the Uniform Mechanical Code or International Mechanical Code. In certain states, counties, or cities, either of these codes may be adopted and amended via various legislative processes. These codes are updated and published by the International Association of Plumbing and Mechanical Officials (IAPMO) or the International Code Council (ICC) respectively, on a 3-year code development cycle. Typically, local building permit departments are charged with enforcement of these standards on private and certain public properties.

An HVAC technician is a tradesman who specializes in heating, ventilation, air conditioning, and refrigeration. HVAC technicians in the US can receive training through formal training institutions, where most earn associate degrees. Training for HVAC technicians includes classroom lectures and hands-on tasks, and can be followed by an apprenticeship wherein the recent graduate works alongside a professional HVAC technician for a temporary period.[56] HVAC techs who have been trained can also be certified in areas such as air conditioning, heat pumps, gas heating, and commercial refrigeration.

The Chartered Institution of Building Services Engineers is a body that covers the essential Service (systems architecture) that allow buildings to operate. It includes the electrotechnical, heating, ventilating, air conditioning, refrigeration and plumbing industries. To train as a building services engineer, the academic requirements are GCSEs (A-C) / Standard Grades (1-3) in Maths and Science, which are important in measurements, planning and theory. Employers will often want a degree in a branch of engineering, such as building environment engineering, electrical engineering or mechanical engineering. To become a full member of CIBSE, and so also to be registered by the Engineering Council UK as a chartered engineer, engineers must also attain an Honours Degree and a master's degree in a relevant engineering subject.[citation needed] CIBSE publishes several guides to HVAC design relevant to the UK market, and also the Republic of Ireland, Australia, New Zealand and Hong Kong. These guides include various recommended design criteria and standards, some of which are cited within the UK building regulations, and therefore form a legislative requirement for major building services works. The main guides are:

Within the construction sector, it is the job of the building services engineer to design and oversee the installation and maintenance of the essential services such as gas, electricity, water, heating and lighting, as well as many others. These all help to make buildings comfortable and healthy places to live and work in. Building Services is part of a sector that has over 51,000 businesses and employs represents 2–3% of the GDP.

The Air Conditioning and Mechanical Contractors Association of Australia (AMCA), Australian Institute of Refrigeration, Air Conditioning and Heating (AIRAH), Australian Refrigeration Mechanical Association and CIBSE are responsible.

Asian architectural temperature-control have different priorities than European methods. For example, Asian heating traditionally focuses on maintaining temperatures of objects such as the floor or furnishings such as Kotatsu tables and directly warming people, as opposed to the Western focus, in modern periods, on designing air systems.

The Philippine Society of Ventilating, Air Conditioning and Refrigerating Engineers (PSVARE) along with Philippine Society of Mechanical Engineers (PSME) govern on the codes and standards for HVAC / MVAC (MVAC means ""mechanical ventilation and air conditioning"") in the Philippines.

The Indian Society of Heating, Refrigerating and Air Conditioning Engineers (ISHRAE) was established to promote the HVAC industry in India. ISHRAE is an associate of ASHRAE. ISHRAE was founded at New Delhi[57] in 1981 and a chapter was started in Bangalore in 1989. Between 1989 & 1993, ISHRAE chapters were formed in all major cities in India.[citation needed]

 Media related to Climate control at Wikimedia Commons

 Related media at Wikimedia Commons:
"
Air Duct Cleaning Services,"Duct cleaning refers to the process of cleaning various heating and cooling system components of forced air systems. This includes the supply and return air ducts and registers, grilles and diffusers, heat exchangers, heating and cooling coils, condensate drain pans (drip pans), fan motor and fan housing, and the air handling unit housing. Proper duct cleaning can help improve indoor air quality and enhance the efficiency of the HVAC system.[1]

Duct cleaning has become more popular as awareness of indoor air quality has increased. The practice began gaining traction in the latter half of the 20th century when studies started showing the potential health effects of poor indoor air quality. Advances in technology and increased focus on health and wellness have further driven the demand for duct cleaning services. As time goes on, scams in the industry increased. To learn more about scams and to avoid them, go here.

Duct trucks, also known as air duct cleaning trucks or vacuum trucks, are specialized vehicles designed for the professional cleaning of HVAC duct systems. These trucks are equipped with powerful vacuum systems and a range of tools necessary for the effective removal of dust, debris, and contaminants from ductwork. The core component of a duct truck is its high-efficiency vacuum, which creates strong suction to draw contaminants out of the ducts and into a secure containment system within the truck.

Source removal is the most effective method of cleaning air ducts. This involves a thorough cleaning of the ductwork to remove dirt and debris. The process usually involves the use of mechanical agitation to loosen dirt and high-powered vacuum systems to extract it.

Mechanical brushes can be used to clean duct surfaces. These brushes are rotated within the ductwork to dislodge debris, which is then vacuumed out.

Air whips are flexible devices that use compressed air to clean ductwork. They are inserted into the ducts and emit bursts of air to dislodge debris, which is then vacuumed out.

HEPA vacuums are used to ensure that any particulates removed from the ducts do not re-enter the indoor environment. These vacuums are capable of trapping very fine particles.

Some duct cleaning processes include the use of chemical biocides designed to kill microbiological contaminants. There are also chemical treatments applied to prevent future microbial growth and to cover or encapsulate inside surfaces of the HVAC system.

Clean ducts can help reduce the presence of dust, allergens, and other particulate matter in the air, contributing to a healthier indoor environment.

A clean HVAC system can operate more efficiently, as the airflow is not impeded by dirt and debris. This can lead to lower energy costs and a longer lifespan for the system.

Removing contaminants from ductwork can alleviate symptoms for people with allergies or respiratory conditions, such as asthma.

Cleaning ducts can help remove odors caused by mold, mildew, pets, and other sources, improving the overall smell of the indoor environment.

There is ongoing debate about the necessity and efficacy of duct cleaning. Some studies and experts argue that duct cleaning has not been shown to prevent health problems definitively. Others believe that much of the dust in ducts adheres to duct surfaces and does not necessarily enter the living space. While still debatable, many people do tend to benefit from the service and see many improved changes such as odors, not as much dust in the house, and more.[2]

The NADCA has established standards for the assessment, cleaning, and restoration of HVAC systems. These standards ensure that duct cleaning is performed to a high standard.

The EPA provides guidelines on when and how to clean air ducts. They recommend cleaning ducts on an as-needed basis, such as when there is visible mold growth, excessive dust, or evidence of rodent or insect infestation.

While some homeowners may attempt to clean their ducts themselves, professional duct cleaning services are generally recommended. Professionals have the appropriate tools, knowledge, and experience to thoroughly clean the ductwork without causing damage.

The cost of duct cleaning can vary based on several factors, including the size of the system, the extent of contamination, and the geographic location. On average, homeowners can expect to pay between $400 and $600 for a professional duct cleaning service depending on the size of the house and equipment the company uses.

Duct cleaning is an important part of maintaining a healthy and efficient HVAC system. While there is debate about its necessity, many homeowners do find value in the service for its potential health and efficiency benefits. To get a find a company to get your ducts cleaned, head to the NADCA website and put in your zip code to find reputable companies near you.
"
Water Treatment Services,"Water treatment is any process that improves the quality of water to make it appropriate for a specific end-use. The end use may be drinking, industrial water supply, irrigation, river flow maintenance, water recreation or many other uses, including being safely returned to the environment.  Water treatment removes contaminants and undesirable components, or reduces their concentration so that the water becomes fit for its desired end-use. This treatment is crucial to human health and allows humans to benefit from both drinking and irrigation use.

Water contamination is primarily caused by the discharge of untreated wastewater from enterprises. The effluent from various enterprises, which contains varying levels of contaminants, is dumped into rivers or other water resources. The wastewater may have a high proportion of organic and inorganic contaminants at the initial discharge. Industries generate wastewater as a result of fabrication processes, processes dealing with paper and pulp, textiles, chemicals, and from various streams such as cooling towers, boilers, and production lines.[1]
Treatment for drinking water production involves the removal of contaminants and/or inactivation of any potentially harmful microbes from raw water to produce water that is pure enough for human consumption without any short term or long term risk of any adverse health effect.  In general terms, the greatest microbial risks are associated with ingestion of water that is contaminated with human or animal (including bird) feces. Feces can be a source of pathogenic bacteria, viruses, protozoa and helminths. The removal or destruction of microbial pathogens is essential, and commonly involves the use of reactive chemical agents such as suspended solids, to remove bacteria, algae, viruses, fungi, and minerals including iron and manganese. Research including Professor Linda Lawton's group at Robert Gordon University, Aberdeen is working to improve detection of cyanobacteria.[2] These substances continue to cause great harm to several less developed countries who do not have access to effective water purification systems.[original research?]

Measures taken to ensure water quality not only relate to the treatment of the water, but to its conveyance and distribution after treatment. It is therefore common practice to keep residual disinfectants in the treated water to kill bacteriological contamination during distribution and to keep the pipes clean.[3]

Water supplied to domestic properties such as for tap water or other uses, may be further treated before use, often using an in-line treatment process. Such treatments can include water softening or ion exchange. [citation needed]

Wastewater treatment is a process which removes and eliminates contaminants from wastewater. It thus converts it into an effluent that can be returned to the water cycle. Once back in the water cycle, the effluent creates an acceptable impact on the environment. It is also possible to reuse it. This process is called water reclamation.[4] The treatment process takes place in a wastewater treatment plant. There are several kinds of wastewater which are treated at the appropriate type of wastewater treatment plant. For domestic wastewater the treatment plant is called a Sewage Treatment. Municipal wastewater or sewage are other names for domestic wastewater. For industrial wastewater, treatment takes place in a separate Industrial wastewater treatment, or in a sewage treatment plant. In the latter case it usually follows pre-treatment. Further types of wastewater treatment plants include Agricultural wastewater treatment and leachate treatment plants.

One common process in wastewater treatment is phase separation, such as sedimentation. Biological and chemical processes such as oxidation are another example. Polishing is also an example. The main by-product from wastewater treatment plants is a type of sludge that is usually treated in the same or another wastewater treatment plant.[5]: Ch.14  Biogas can be another by-product if the process uses anaerobic treatment. Treated wastewater can be reused as reclaimed water.[6] The main purpose of wastewater treatment is for the treated wastewater to be able to be disposed or reused safely. However, before it is treated, the options for disposal or reuse must be considered so the correct treatment process is used on the wastewater.

Water treatment is used to optimize most water-based industrial processes, such as heating, cooling, processing, cleaning, and rinsing so that operating costs and risks are reduced. Poor water treatment lets water interact with the surfaces of pipes and vessels which contain it. Steam boilers can scale up or corrode, and these deposits will mean more fuel is needed to heat the same amount of water. Cooling towers can also scale up and corrode, but left untreated, the warm, dirty water they can contain will encourage bacteria to grow, and Legionnaires' disease can be the fatal consequence. Water treatment is also used to improve the quality of water contacting the manufactured product (e.g., semiconductors) and/or can be part of the product (e.g., beverages, pharmaceuticals).  In these instances, poor water treatment can cause defective products.[citation needed]

For the elimination of hazardous chemicals from the water, many treatment procedures have been applied.[8]

The processes involved in removing the contaminants include physical processes such as settling and filtration,  chemical processes such as disinfection and coagulation, and biological processes such as slow sand filtration.

A combination selected from the following processes (depending on the season and contaminants and chemicals present in the raw water) is used for municipal drinking water treatment worldwide.

Different chemical procedures for the conversion into final products or the removal of pollutants are used for the safe disposal of contaminants.[9]

Physical techniques of water/waste water treatment rely on physical phenomena to complete the removal process, rather than biological or chemical changes.[9]

Most common physical techniques are:

Also referred to as ""Conventional"" Treatment

Chemical precipitation is a common process used to reduce heavy metals concentrations in wastewater. The dissolved metal ions are transformed to an insoluble phase by a chemical interaction with a precipitant agent such as lime. In industrial applications stronger alkalis may be used to effect complete precipitation. In drinking water treatment, the common-ion effect  is often used to help reduce water hardness.[14]

Flotation uses bubble attachment to separate solids or dispersed liquids from a liquid phase.[15]

Membrane filtration can remove suspended solids and organic components, and inorganic pollutants such heavy metals. For heavy metal removal, several forms of membrane filtration, such as ultrafiltration, nanofiltration, and reverse osmosis, can be used depending on the particle size that can be maintained.[16][17] Aminophosphonates can be added for antiscalant properties to maintain filtration.[18]

Ion exchange is a reversible ion exchange process in which an insoluble substance (resin) takes ions from an electrolytic solution and releases additional ions of the same charge in a chemically comparable amount without changing the resin's structure.[19][20]

Adsorption is a mass transfer process in which a substance is transported from the liquid phase to the surface of a solid/liquid (adsorbent) and becomes physically and chemically bonded (adsorbate). Adsorption can be classified into two forms based on the type of attraction between the adsorbate and the adsorbent: physical and chemical adsorption, commonly known as physisorption and chemisorptions.[21][22]

Activated carbons (ACs) or biological-activated carbon (BAC)[23] are effective adsorbents for a wide variety of contaminants. The adsorptive removal of color, aroma, taste, and other harmful organics and inorganics from drinking water and wastewater is one of their industrial applications.[24]

Both a high surface area and a large pore size can improve the efficiency of activated carbon. Activated carbon was utilized by a number of studies to remove heavy metals and other types of contaminants from wastewater. The cost of activated carbon is rising due to a shortage of commercial activated carbon (AC). Because of its high surface area, porosity, and flexibility, activated carbon has a lot of potential in wastewater treatment.[24]

This is the method by which dissolved and suspended organic chemical components are eliminated through biodegradation, in which an optimal amount of microorganism is given to re-enact the same natural self-purification process.[25] Through two distinct biological process, such as biological oxidation and biosynthesis, microorganisms can degrade organic materials in wastewater. Microorganisms involved in wastewater treatment produce end products such as minerals, carbon dioxide, and ammonia during the biological oxidation process. The minerals (products) remained in the wastewater and were discharged with the effluent. Microorganisms use organic materials in wastewater to generate new microbial cells with dense biomass that is eliminated by sedimentation throughout the biosynthesis process.[26]

Many developed countries specify standards to be applied in their own country. In Europe, this includes the European Drinking Water Directive[27] and in the United States the United States Environmental Protection Agency (EPA) establishes standards as required by the Safe Drinking Water Act. For countries without a legislative or administrative framework for such standards, the World Health Organization publishes guidelines on the standards that should be achieved.[28] China adopted its own drinking water standard GB3838-2002 (Type II) enacted by Ministry of Environmental Protection in 2002.[29]

Where drinking water quality standards do exist, most are expressed as guidelines or targets rather than requirements, and very few water standards have any legal basis or, are subject to enforcement.[30] Two exceptions are the European Drinking Water Directive and the Safe Drinking Water Act in the United States, which require legal compliance with specific standards.

Appropriate technology options in water treatment include both community-scale and household-scale point-of-use (POU) or self-supply designs.[31] Such designs may employ solar water disinfection methods, using solar irradiation to inactivate harmful waterborne microorganisms directly, mainly by the UV-A component of the solar spectrum, or indirectly through the presence of an oxide photocatalyst, typically supported TiO2 in its anatase or rutile phases.[32] Despite progress in SODIS technology, military surplus water treatment units like the ERDLator are still frequently used in developing countries. Newer military style Reverse Osmosis Water Purification Units (ROWPU) are portable, self-contained water treatment plants are becoming more available for public use.[33]

For waterborne disease reduction to last, water treatment programs that research and development groups start in developing countries must be sustainable by the citizens of those countries. This can ensure the efficiency of such programs after the departure of the research team, as monitoring is difficult because of the remoteness of many locations.

Energy Consumption: Water treatment plants can be significant consumers of energy. In California, more than 4% of the state's electricity consumption goes towards transporting moderate quality water over long distances, treating that water to a high standard.[34] In areas with high quality water sources which flow by gravity to the point of consumption, costs will be much lower.
Much of the energy requirements are in pumping. Processes that avoid the need for pumping tend to have overall low energy demands. Those water treatment technologies that have very low energy requirements including trickling filters, slow sand filters, gravity aqueducts.

A 2021 study found that a large-scale water chlorination program in urban areas of Mexico massively reduced childhood diarrheal disease mortality rates.[35]

Stainless steels, such as Type 304L and 316L, are used extensively in the fabrication of water treatment plants due to their corrosion resistance to water and to the corrosivity of chlorination used for disinfection.[36][37]
"
Residential Electrical Services,"

An electrician is a tradesperson specializing in electrical wiring of buildings, transmission lines,  stationary machines, and related equipment. Electricians may be employed in the installation of new electrical components or the maintenance and repair of existing electrical infrastructure.[1] Electricians may also specialize in wiring ships, airplanes, and other mobile platforms, as well as data and cable lines.

Electricians were originally people who demonstrated or studied the principles of electricity, often electrostatic generators of one form or another.[2]

In the United States, electricians are divided into two primary categories: lineperson, who work on electric utility company distribution systems at higher voltages, and wiremen, who work with the lower voltages utilized inside buildings.  Wiremen are generally trained in one of five primary specialties: commercial, residential, light industrial, industrial, and low-voltage wiring, more commonly known as Voice-Data-Video, or VDV.  Other sub-specialties such as control wiring and fire-alarm may be performed by specialists trained in the devices being installed, or by inside wiremen.

Electricians are trained to one of three levels: Apprentice, Journeyperson, and Master Electrician.  In the US and Canada, apprentices work and receive a reduced compensation while learning their trade.  They generally take several hundred hours of classroom instruction and are contracted to follow apprenticeship standards for a period of between three and six years, during which time they are paid as a percentage of the Journeyperson's pay.  Journeymen are electricians who have completed their Apprenticeship and who have been found by the local, State, or National licensing body to be competent in the electrical trade.  Master Electricians have performed well in the trade for a period of time, often seven to ten years, and have passed an exam to demonstrate superior knowledge of the National Electrical Code, or NEC.

Service electricians are tasked to respond to requests for isolated repairs and upgrades.  They have skills troubleshooting wiring problems, installing wiring in existing buildings, and making repairs. Construction electricians primarily focus on larger projects, such as installing all new electrical system for an entire building, or upgrading an entire floor of an office building as part of a remodeling process. Other specialty areas are marine electricians, research electricians and hospital electricians. ""Electrician"" is also used as the name of a role in stagecraft, where electricians are tasked primarily with hanging, focusing, and operating stage lighting.  In this context, the Master Electrician is the show's chief electrician.  Although theater electricians routinely perform electrical work on stage lighting instruments and equipment, they are not part of the electrical trade and have a different set of skills and qualifications from the electricians that work on building wiring.

In the film industry and on a television crew the head electrician is referred to as a Gaffer.

Electrical contractors are businesses that employ electricians to design, install, and maintain electrical systems.  Contractors are responsible for generating bids for new jobs, hiring tradespeople for the job, providing material to electricians in a timely manner, and communicating with architects, electrical and building engineers, and the customer to plan and complete the finished product.

Many jurisdictions have regulatory restrictions concerning electrical work for safety reasons due to the many hazards of working with electricity. Such requirements may be testing, registration or licensing. Licensing requirements vary between jurisdictions.

An electrician's license entitles the holder to carry out all types of electrical installation work in Australia without supervision. However, to contract, or offer to contract, to carry out electrical installation work, a licensed electrician must also be registered as an electrical contractor. Under Australian law, electrical work that involves fixed wiring is strictly regulated and must almost always be performed by a licensed electrician or electrical contractor.[3] A local electrician can handle a range of work including air conditioning, light fittings and installation, safety switches, smoke alarm installation, inspection and certification and testing and tagging of electrical appliances.

To provide data, structured cabling systems, home automation & theatre, LAN, WAN and VPN data solutions or phone points, an installer must be licensed as a Telecommunications Cable Provider under a scheme controlled by Australian Communications and Media Authority[4]

Electrical licensing in Australia is regulated by the individual states. In Western Australia, the Department of Commerce tracks licensee's and allows the public to search for individually named/licensed Electricians.[5]

Currently in Victoria the apprenticeship lasts for four years, during three of those years the apprentice attends trade school in either a block release of one week each month or one day each week. At the end of the apprenticeship the apprentice is required to pass three examinations, one of which is theory based with the other two practically based. Upon successful completion of these exams, providing all other components of the apprenticeship are satisfactory, the apprentice is granted an A Class licence on application to Energy Safe Victoria (ESV).

An A Class electrician may perform work unsupervised but is unable to work for profit or gain without having the further qualifications necessary to become a Registered Electrical Contractor (REC) or being in the employment of a person holding REC status. However, some exemptions do exist.[6]

In most cases a certificate of electrical safety must be submitted to the relevant body after any electrical works are performed.

Safety equipment used and worn by electricians in Australia (including insulated rubber gloves and mats) needs to be tested regularly to ensure it is still protecting the worker. Because of the high risk involved in this trade, this testing needs to be performed regularly and regulations vary according to state. Industry best practice is the Queensland Electrical Safety Act 2002, and requires six-monthly testing.

Training of electricians follows an apprenticeship model, taking four or five years to progress to fully qualified journeyperson level.[7] Typical apprenticeship programs consists of 80-90% hands-on work under the supervision of journeymen and 10-20% classroom training.[8]  Training and licensing of electricians is regulated by each province, however professional licenses are valid throughout Canada under Agreement on Internal Trade. An endorsement under the Red Seal Program provides additional competency assurance to industry standards.[9]
In order for individuals to become a licensed electricians, they need to have 9000 hours of practical, on the job training. They also need to attend school for 4 terms and pass a provincial exam. This training enables them to become journeyperson electricians. Furthermore, in British Columbia, an individual can go a step beyond that and become a ""FSR"", or field safety representative. This credential gives the ability to become a licensed electrical contractor and to pull permits. Notwithstanding this, some Canadian provinces only grant ""permit pulling privileges"" to current Master Electricians, that is, a journeyperson who has been engaged in the industry for three years and has passed the Master's examination (i.e. Alberta).  The various levels of field safety representatives are A, B and C. The only difference between each class is that they are able to do increasingly higher voltage and current work.

The two qualification awarding organisations are City and Guilds and EAL. Electrical competence is required at Level 3 to practice as a 'qualified electrician' in the UK. Once qualified and demonstrating the required level of competence an Electrician can apply to register for a Joint Industry Board Electrotechnical Certification Scheme card in order to work on building sites or other controlled areas.

Although partly covered during Level 3 training, more in depth knowledge and qualifications can be obtained covering subjects such as Design and Verification or Testing and Inspection among others. These additional qualifications can be listed on the reverse of the JIB card. Beyond this level is additional training and qualifications such as EV charger installations or training and working in specialist areas such as street furniture or within industry.

The Electricity at Work Regulations are a statutory document that covers the use and proper maintenance of electrical equipment and installations within businesses and other organisations such as charities. Parts of the Building Regulations cover the legal requirements of the installation of electrical technical equipment with Part P outlining most of the regulations covering dwellings

Information regarding design, selection, installation and testing of electrical structures is provided in the non-statutory publication 'Requirements for Electrical Installations, IET Wiring Regulations, Eighteenth Edition, BS 7671:2018' otherwise known as the Wiring Regulations or 'Regs'. Usual amendments are published on an ad hoc bases when minor changes occur. The first major update of the 18th Edition were published during February 2020 mainly covering the section covering Electric vehicles charger installations although an addendum was published during December 2019 correcting some minor mistakes and adding some small changes. The IET also publish a series of 'Guidance Notes' in book form that provide further in-depth knowledge.

With the exception of the work covered by Part P of the Building Regulations, such as installing consumer units, new circuits or work in bathrooms, there are no laws that prevent anyone from carrying out some basic electrical work in the UK.

In British English, an electrician is colloquially known as a ""spark"".[10]

The United States does not offer nationwide licensing and electrical licenses are issued by individual states. There are variations in licensing requirements, however, all states recognize three basic skill categories: level electricians. Journeyperson electricians can work unsupervised provided that they work according to a master's direction. Generally, states do not offer journeyperson permits, and journeyperson electricians and other apprentices can only work under permits issued to a master electrician. Apprentices may not work without direct supervision.[11]

Before electricians can work unsupervised, they are usually required to serve an apprenticeship lasting three to five years under the general supervision of a master electrician and usually the direct supervision of a journeyperson electrician.[11] Schooling in electrical theory and electrical building codes is required to complete the apprenticeship program. Many apprenticeship programs provide a salary to the apprentice during training. A journeyperson electrician is a classification of licensing granted to those who have met the experience requirements for on the job training (usually 4,000 to 6,000 hours) and classroom hours (about 144 hours). Requirements include completion of two to six years of apprenticeship training and passing a licensing exam.[12]

An electrician's license is valid for work in the state where the license was issued. In addition, many states recognize licenses from other states, sometimes called interstate reciprocity participation, although there can be conditions imposed. For example, California reciprocates with Arizona, Nevada, and Utah on the condition that licenses are in good standing and have been held at the other state for five years.[13] Nevada reciprocates with Arizona, California, and Utah.[14] Maine reciprocates with New Hampshire and Vermont at the master level, and the state reciprocates with New Hampshire, North Dakota, Idaho, Oregon, Vermont, and Wyoming at the journeyperson level.[15] Colorado maintains a journeyperson alliance with Alaska, Arkansas, the Dakotas, Idaho, Iowa, Minnesota, Montana, Nebraska, New Hampshire, New Mexico, Oklahoma, Utah, and Wyoming.[16]

Electricians use a range of hand and power tools and instruments.

Some of the more common tools are:

In addition to the workplace hazards generally faced by industrial workers, electricians are also particularly exposed to injury by electricity. An electrician may experience electric shock due to direct contact with energized circuit conductors or due to stray voltage caused by faults in a system. An electric arc exposes eyes and skin to hazardous amounts of heat and light. Faulty switchgear may cause an arc flash incident with a resultant blast. Electricians are trained to work safely and take many measures to minimize the danger of injury. Lockout and tagout procedures are used to make sure that circuits are proven to be de-energized before work is done. Limits of approach to energized equipment protect against arc flash exposure; specially designed flash-resistant clothing provides additional protection; grounding (earthing) clamps and chains are used on line conductors to provide a visible assurance that a conductor is de-energized. Personal protective equipment provides electrical insulation as well as protection from mechanical impact; gloves have insulating rubber liners, and work boots and hard hats are specially rated to provide protection from shock. If a system cannot be de-energized, insulated tools are used; even high-voltage transmission lines can be repaired while energized, when necessary.[17]

Electrical workers, which includes electricians, accounted for 34% of total electrocutions of construction trades workers in the United States between 1992 and 2003.[18]

Working conditions for electricians vary by specialization. Generally an electrician's work is physically demanding such as climbing ladders and lifting tools and supplies. Occasionally an electrician must work in a cramped space or on scaffolding, and may frequently be bending, squatting or kneeling, to make connections in awkward locations. Construction electricians may spend much of their days in outdoor or semi-outdoor loud and dirty work sites. Industrial electricians may be exposed to the heat, dust, and noise of an industrial plant. Power systems electricians may be called to work in all kinds of adverse weather to make emergency repairs.

Some electricians are union members and work under their union's policies.

Electricians can choose to be represented by the Electrical Trade Union (ETU). Electrical Contractors can be represented by the National Electrical & Communications Association or Master Electricians Australia.

Some electricians are union members. Some examples of electricians' unions include the International Brotherhood of Electrical Workers, Canadian Union of Public Employees, and the International Association of Machinists and Aerospace Workers.The International Brotherhood of Electrical Workers provides its own apprenticeships through its National Joint Apprenticeship and Training Committee and the National Electrical Contractors Association.  Many merit shop training and apprenticeship programs also exist, including those offered by such as trade associations as Associated Builders and Contractors and Independent Electrical Contractors. These organizations provide comprehensive training, in accordance with U.S. Department of Labor regulations.

In the United Kingdom, electricians are represented by several unions including Unite the Union

In the Republic of Ireland there are two self-regulation/self certification bodies RECI Register of Electrical Contractors of Ireland and ECSSA.

An auto electrician is a tradesperson specializing in electrical wiring of motor vehicles. Auto electricians may be employed in the installation of new electrical components or the maintenance and repair of existing electrical components. Auto electricians specialize in cars and commercial vehicles. The auto electrical trade is generally more difficult than the electrical trade due to the confined spaces, engineering complexity of modern automotive electrical systems, and working conditions (often roadside breakdowns or on construction sites, mines, quarries to repair machinery etc.) Also the presence of high-current DC electricity makes injury from burns and arc-flash injury possible.


"
Commercial Electrical Services,"An electrical contractor is a business person or firm that performs specialized construction work related to the design, installation, and maintenance of electrical systems.[1]
An electrical contractor is different from an electrician; an electrician is an individual tradesman and an electrical contractor is a business person or company that employs electricians. Both usually hold licenses and insurances to properly and safely operate a business, protecting the employees and home owners/business owners from insurance liabilities. These requirements vary from state to state. Electricians may work for an electrical contractor, or directly for individuals or companies.

Electrical contractors are generally classified by three major types of work performed.

Electrical contractors employ workers in many capacities, determined by their level of training and experience. Some common jobs include:

In the United Kingdom, the two main trade associations are the Electrical Contractors' Association, covering England, Northern Ireland and Wales, and SELECT - the Electrical Contractors' Association for Scotland. The main certification bodies are the National Inspection Council for Electrical Installation Contracting and Elecsa.[3]

The National Electrical Contractors Association (NECA) is the largest trade association in the electrical contracting industry, with about 4500 members. NECA publishes an industry magazine, and sponsors an annual convention and trade show.[4] Independent Electrical Contractors (IEC) is another trade association for electrical contractors with 70 chapters across the U.S. They provide education and training via a U.S. Department of Labor recognized apprenticeship program.

The International Brotherhood of Electrical Workers organizes and represents over 700,000 members, and provides training and apprenticeship programs.

Electrical contractors in the United States are required to follow National Electrical Code (NEC) to ensure systems work in a safe manner.[5][6] The NEC is a widely adopted model code for the installation of electrical components and systems, designed to safeguard persons and property from hazards arising from the use of electricity.[7] While these are the default minimum requirements and guidelines, some states modify selected areas of the NEC code to suit their specific circumstances.[8]
"
Alarm Installation Services,"A security alarm is a system designed to detect intrusions, such as unauthorized entry, into a building or other areas, such as a home or school. Security alarms protect against burglary (theft) or property damage, as well as against intruders. Examples include personal systems, neighborhood security alerts, car alarms, and prison alarms.

Some alarm systems serve a single purpose of burglary protection; combination systems provide fire and intrusion protection. Intrusion-alarm systems are combined with closed-circuit television surveillance (CCTV) systems to record intruders' activities and interface to access control systems for electrically locked doors. There are many types of security systems. Homeowners typically have small, self-contained noisemakers. These devices can also be complicated, multirole systems with computer monitoring and control. It may even include a two-way voice which allows communication between the panel and monitoring station.

The most basic alarm consists of at least one sensor to detect trespassers and an alerting device to indicate the intrusion. However, a typical premises security alarm employs the following components:

In addition to the system itself, security alarms often offer a monitoring service. In the event of an alarm, the premises control unit contacts a central monitoring station. Operators at the station take appropriate action, such as contacting property owners, notifying the police, or dispatching private security forces. Such alerts transmit via dedicated alarm circuits, telephone lines, or the internet.

The hermetically sealed reed switch is a common type of two-piece sensor. This switch operates with an electrically conductive switch that is either normally open or normally closed when under the influence of a magnetic field in respect to proximity to the second piece, which contains a magnet. When the magnet moves away from the reed switch, the reed switch either closes or opens, based on the normally closed or open design. This action, coupled with an electric current, allows an alarm control panel to detect a fault on that zone or circuit. These sensors are common, are found wired directly to an alarm control panel, or are typically found in wireless door or window contacts as sub-components.

The passive infrared (PIR) motion detector is one of the most common sensors found in household and small business environments.  This sensor does not generate or radiate energy; it works entirely by detecting the heat energy given off by other objects.

PIR sensors identify abrupt changes in temperature at a given point. As an intruder walks in front of the sensor, the temperature at that point will rise from room temperature to body temperature and then back again. This quick change triggers the detection.

PIR sensors designed to be wall- or ceiling-mounted come in various fields of view. PIRs require a power supply in addition to the detection signaling circuit.

The infrasound detector works by detecting infrasound, or sound waves at frequencies below 20 Hz. Sounds at those frequencies are inaudible to the human ear.[1] Due to its inherent properties, infrasound can travel distances of many hundreds of kilometers.[2]

The entire infrasound detection system consists of the following components: a speaker (infrasound sensor) as a microphone input, an order-frequency filter, an analog-to-digital (A/D) converter, and an microcomputer to analyze the recorded signal.

If a potential intruder tries to enter into a house, they test whether it is closed and locked, uses tools on openings, or/and applies pressure, creating low-frequency sound vibrations. Before the intruder breaks in, the infrasound detector automatically detects the intruder's actions.

The purpose of such a system is to detect burglars before they enter the house to avoid both theft and vandalism. The sensitivity is dependent on the size of a home and the presence of animals.

These active detectors transmit ultrasonic sound waves that are inaudible to humans using frequencies between 15 kHz and 75 kHz. The Doppler shift principle is the underlying method of operation which detects a change in frequency due to object motion. This detection occurs when the object must cause a change in the ultrasonic frequency to the receiver relative to the transmitting frequency.

The ultrasonic detector operates by the transmitter emitting an ultrasonic signal into the area to be protected. Solid objects (such as the surrounding floor, walls, and ceiling) reflect sound waves, which the receiver will detect. Because ultrasonic waves are transmitted through air, hard-surfaced objects tend to reflect most of the ultrasonic energy, while soft surfaces tend to absorb the most energy.

When the surfaces are stationary, the frequency of the waves detected by the receiver will be equal to the transmitted frequency.  However, a change in frequency will occur as a result of the Doppler principle when a person or object is moving towards or away from the detector. Such an event initiates an alarm signal. This technology is not active in many properties as many consider this obsolete.

This device emits microwaves from a transmitter and detects any reflected microwaves or reduction in beam intensity using a receiver. The transmitter and receiver are usually combined inside a single housing (monostatic) for indoor applications and separate housings (bistatic) for the protection of outdoor perimeters high-risk sites and critical infrastructures such as fuel storage, petrochemical facilities, military sites, civil and military airports, nuclear facilities and more. To reduce false alarms this type of detector is usually combined with a passive infrared detector or similar alarm. Compared to the monostatic, the bistatic units work over longer distances: typical distances for transmitter-receivers up to 200 m for X-band frequencies and up to 500 m for K-band frequencies.[3]

Microwave detectors respond to a Doppler shift in the frequency of the reflected energy, by a phase shift, or by a sudden reduction of the level of received energy. Any of these effects may indicate motion of an intruder. Microwave detectors are low cost, easy to install, have an invisible perimeter barrier. and is not affected by fog, rain, snow, sand storms, or wind. May be affected by the presence of water dripping on the ground. Typically need a sterile clearance area to prevent partial blocking of the detection field.

The microwave generator is equipped with an antenna that allows it to concentrate the beam of electromagnetic waves in one preferred location and the beam is intercepted by the receiver, equipped with a similar antenna to the transmitter.

The graphical representation of the beam is similar to a cigar, and, when not disturbed, it runs between the transmitter and the receiver and generates a continuous signal. When an individual tries to cross this beam, it produces a disturbance that is caught by the receiver as a variation of amplitude of the received signal.

These barriers are immune to harsh weather, such as fog, heavy rain, snow and sandstorms: none of these atmospheric phenomena affect in any way the behaviour and the reliability of the microwave detection. Furthermore, the working temperature range of this technology goes from -35 °C to +70 °C.[4]

The more recent and higher performance models of these detectors generate a detection whether the intruder is rolling, crossing, crawling or moving very slow within the electromagnetic field[5] reducing false alarms. The ellipsoidal shape of the longitudinal section however does not allow a good detection capability close to the receiver or transmitter heads, and those areas are commonly referred to as ""dead zones"". A solution to avoid this problem, when installing 2 or more barriers, is to cross the respective transmitter and receiver heads some meters from the respective heads or to use mono-head sensor to cover the dead zones.[6]

Compact surveillance radar emits microwaves from a transmitter and detects any reflected microwaves. They are similar to microwave detectors but can detect the precise location and a GPS coordinate of intruders in areas extending over hundreds of acres. It has the capability of measuring the range, angle, velocity, direction, and size of the target.  This target information is typically displayed on a map, user interface or situational awareness software that defines geographical alert zones or geofences with different types of actions initiated depending on time of day and other factors. CSR is commonly used to protect outside the fence line of critical facilities such as electrical substations, power plants, dams, and bridges.

Photoelectric beam systems detect the presence of an intruder by transmitting invisible infrared light beams across an area, where these beams may be obstructed. To improve the detection surface area, the beams are often employed in stacks of two or more. However, if an intruder is aware of the technology's presence, it can be avoided. The technology can be an effective long-range detection system, if installed in stacks of three or more where the transmitters and receivers are staggered to create a fence-like barrier. To prevent a clandestine attack using a secondary light source being used to hold the detector in a sealed condition whilst an intruder passes through, most systems use and detect a modulated light source. These sensors are low cost, easy to install, and require very little sterile clearance area to operate. However, it may be affected by fog or very high luminosity, and the position of the transmitter can be located with cameras.

A glass-break detector may be used for internal perimeter building protection. Glass-break acoustic detectors are mounted in close proximity to the glass panes and listen for sound frequencies associated with glass breaking.

Seismic glass-break detectors, generally referred to as shock sensors, are different in that they are installed on the glass pane. When glass breaks it produces specific shock frequencies which travel through the glass and often through the window frame and the surrounding walls and ceiling. Typically, the most intense frequencies generated are between 3 and 5 kHz, depending on the type of glass and the presence of a plastic interlayer. Seismic glass-break detectors feel these shock frequencies and in turn generate an alarm condition.

Window foil is a less advanced detection method that involves gluing a thin strip of conducting foil on the inside of the glass and putting low-power electric current through it. Breaking the glass will tear the foil and break the circuit.

Most systems can also be equipped with smoke, heat, and/or carbon monoxide detectors. These are also known as 24-hour zones (which are on at all times). Smoke and heat detectors protect from the risk of fire using different detection methods. Carbon monoxide detectors help protect from the risk of carbon monoxide poisoning. Although an intruder alarm panel may also have these detectors connected, it may not meet all the local fire code requirements of a fire alarm system.

Traditional smoke detectors are ionization smoke detectors which create an electric current between two metal plates, which sound an alarm when disrupted by smoke entering the chamber. Ionization smoke alarms can quickly detect the small amounts of particles produced by fast-flaming fires, such as cooking fires or those fueled by paper or flammable liquids. A newer type of the smoke detector is the photoelectric smoke detector. It contains a light source, which is positioned indirectly to the light sensitive electric sensor. Normally, light from the light source shoots straight across and misses the sensor. When smoke enters the chamber, it scatters the light, which then hits the sensor and triggers the alarm. Photoelectric smoke detectors typically respond faster to a fire in its early, smoldering stage, before the source of the fire bursts into flames.

Motion sensors are devices that use various forms of technology to detect movement. The technology typically found in motion sensors to trigger an alarm includes infrared, ultrasonic, vibration and contact. Dual technology sensors combine two or more forms of detection in order to reduce false alarms as each method has its advantages and disadvantages. Traditionally motion sensors are an integral part of a home security system. These devices are typically installed to cover a large area as they commonly cover up to 40 ft (12 m), with a 135° field of vision.

A type of motion sensor was used by the Japanese since ancient times.  In the past, ""(m)any people in Japan kept singing crickets and used them like watch dogs.""[7]  Although a dog would bark when it senses an intruder, a cricket stops singing when approached by an intruder. The crickets are kept in decorative cages resembling bird cages, and these cages are placed in contact with the floor.  During the day, the house is busy with normal daytime tasks.  When activity reduces at night, the crickets start singing.  If someone comes into the house at night, the floor starts to vibrate.  ""The vibration frightens the crickets and they stop singing.  Then everyone wakes up --- from the silence.[8]  The family is used to hearing crickets at night and knows something is wrong if the crickets aren't singing.  A similar observation was made in England about millers who lived in their mills.  A mill wheel makes a great deal of noise, but the miller only awakens when the mill wheel stops turning.

Driveway alarm systems can be combined with most security and automation systems. They are designed to alert residents to unexpected visitors, intruders, or deliveries arriving at the property. Types of driveway sensors include magnetic and infrared motion sensors. Driveway alarms can be found in both hard-wired and wireless systems. They are common in rural security systems as well as for commercial applications.

These electro-mechanical devices are mounted on barriers and are used primarily to detect an attack on the structure itself. The technology relies on an unstable mechanical configuration that forms part of the electrical circuit. When movement or vibration occurs, the unstable portion of the circuit moves and breaks the current flow, which produces an alarm.  The medium transmitting the vibration must be correctly selected for the specific sensor as they are best suited to different types of structures and configurations. These systems are low cost and easily installed on existing fences, but can only be fence mounted and are unable to analyze differences in the pattern of vibrations (for example, the difference between gusts of wind and a person climbing the fence). For this reason, this technology is gradually being replaced by digital accelerometer-based systems.

MEMS technology is an electromagnetic device that is created using photolithography, incision and ionian implantation. This produces a very compact and small device. In this device, in addition to the mechanical system, there are electronic circuits for control, acquisition and conditioning of the signal able to sense the environment.[9]

MEMS accelerometer can be divided into two groups, piezoresistive and capacitive-based accelerometers. The former consists of a single-degree-of-freedom system of a mass suspended by a spring. They also have a beam with a proof mass at the beam’s tip and a Piezoresistive patch on the beam web.[10]

On the contrary, capacitive-based accelerometers, also known as vibration sensors, rely on a change in electrical capacitance in response to acceleration.[11]

The current technology allows to realize suspended silicon structures that are attached to the substrate in some points called anchors, and that constitute the sensitive mass of the accelerometer MEMS. These structures are free to move in the direction of the acceleration detected. They constitute the mobile reinforcement of a pair of capacitors connected to the half bridge.

In this way, the acquired signals are amplified, filtered and converted in digital signals with the supervision of specific control circuits. MEMS' incorporations evolved from a single, stand-alone device to the integrated inertial motion units that are available today.[12]

This technology uses a variety of transduction mechanisms to detect the displacement. They include capacitive, piezoresistive, thermal, optical, piezoelectric and tunneling.[13]

In the last decades, many technological progresses have been made in this area and MEMS accelerometers are used in high-reliability environments and are starting to replace other established technologies.

MEMS accelerometer can be applied as a sensor in the earthquake disaster prevention, since one of the main characteristics of MEMS accelerometers is the linear frequency response to DC to about 500 Hz, and this capability offers an improvement in measuring ground motion at lower-frequency band.[14]

Another practical application of MEMS accelerometers is in machine condition monitoring to reduce machines’ maintenance. Wireless and embedded technologies such as Micro-electro Mechanical system sensors offer a wireless smart vibration measurement of machine’s condition.[15]

Moving to the defence field, it can be applied in fence-mounted intrusion detection systems. Since MEMS sensors are able to work in a wide temperature range, they can prevent intrusions in outdoors and very spread-off perimeters.

An advantage offered by MEMS Accelerometers is the ability to measure static accelerations, such as acceleration due to gravity. This enables them to constantly verify that the positioning of the sensor, based on MEMS accelerometer, remains unaltered from the installation one.

MEMS accelerometers’ significant advantages also stem from their small size and high measurement frequency; additionally, they can be integrated with multiple sensors with different functions.[16]

Change in the local magnetic field due to the presence of ferrous metals induces a current in the buried sensors (buried cables or discrete sensors) which are analyzed by the system. If the change exceeds a predetermined threshold, an alarm is generated.[17] This type of sensor can be used to detect intruders carrying substantial amounts of metal, such as a firearm, making it ideally suited for anti-poaching applications.[18]

Sometimes referred to as E-field,  this volumetric sensor uses Electric field proximity sensing and can be installed on buildings, perimeters, fences, and walls. It also has the ability to be installed free-standing on dedicated poles. The system uses an electromagnetic field generator powering one wire, with another sensing wire running parallel to it. The sensing wire is connected to a signal processor that analyses amplitude change (mass of intruder), rate change (movement of intruder), and preset disturbance time (time the intruder is in the pattern). These items define the characteristics of an intruder and when all three are detected simultaneously, an alarm signal is generated.

The barrier can provide vertical protection from the ground to the height of the mounting posts (typically 4–6 meters of height), depending on the number of sensor wires installed.  It is usually configured in zones of about 200 metre lengths. Electrostatic field sensors are high-security and difficult to defeat, and have high vertical detection field. However, these sensors are expensive and have short zones, which contributes to more electronics (and thus a higher cost).

Microphonic systems vary in design (for example, time-domain reflectrometer or piezo-electric) but each is generally based on the detection of an intruder attempting to cut or climb over a fence. Usually the microphonic detection systems are installed as sensor cables attached to rigid chain-wire fences, however, some specialized versions of these systems can also be installed buried underground. Depending on the type, it can be sensitive to different frequencies or levels of noise or vibration. The system is based on coaxial or electro-magnetic sensor cable with the controller having the ability to differentiate between signals from the cable or chain-wire being cut, an intruder climbing the fence, or bad weather conditions.

The systems are designed to detect and analyze incoming electronic signals received from the sensor cable, and then to generate alarms from signals which exceed pre-set conditions. The systems have adjustable electronics to permit installers to change the sensitivity of the alarm detectors to the suit specific environmental conditions. The tuning of the system is usually done during commissioning of the detection devices.

Microphonic systems are relatively inexpensive compared to other systems and easy to install, but older systems may have a high rate of false alarms caused by wind and other distances. Some newer systems use DSP to process the signal and reduce false alarms.

A taut wire perimeter security system is an independent screen of tensioned tripwires usually mounted on a fence or wall. Alternatively, the screen can be made thicker to avoid the need for a supporting chain-wire fence. These systems are designed to detect any physical attempt to penetrate the barrier. Taut wire systems can operate with a variety of switches or detectors that sense movement at each end of the tense wires. These switches or detectors can be a simple mechanical contact, static force transducer or an electronic strain gauge. Unwanted alarms caused by birds and other animals can be avoided by adjusting the sensors to ignore objects that exert small amounts of pressure on the wires. This type of system is vulnerable to intruders digging under the fence. A concrete footing directly below the fence is installed to prevent this type of attack.

Taut wire fence systems have low false alarm rates, reliable sensors, and high detection rates, but is expensive and complicated to install.

A fiber-optic cable can be used to detect intruders by measuring the difference in the amount of light sent through the fiber core. A variety of fiber optic sensing technologies may be used, including Rayleigh scattering or interferometry. If the cable is disturbed, the light will change and the intrusion is detected. The cable can be attached directly to a chain-wire fence or bonded into a barbed steel tape that is used to protect the tops of walls and fences. This type of barbed tape provides a good physical deterrent as well as giving an immediate alarm if the tape is cut or severely distorted.

Being cable-based, fiber optic cables are very similar to the microphonic system and easy to install and can cover large perimeters. However, despite performing in a similar manner to microphonic-based systems, fiber optic cables have higher cost and is more complex due to the use of fiber-optic technology.

This system employs an electro-magnetic field disturbance principle based on two unshielded coaxial cables.[19] The transmitter emits continuous radio frequency (RF) energy along one cable and the energy is received by the other cable. When the change in field strength weakens due to the presence of an object and reaches a pre-set lower threshold, an alarm condition is generated. The system is covert after installation. The surrounding soil must offer good drainage in order to avoid nuisance alarms. Ported coaxial cables are concealed as a buried form but can be affected by RF noise and is difficult to install.

Security electric fences consist of wires that carry pulses of electric current to provide a non-lethal shock to deter potential intruders. Tampering with the fence also results in an alarm that is logged by the security electric fence energiser, and can also trigger a siren, strobe, and/or notifications to a control room or directly to the owner via email or phone. In practical terms, security electric fences are a type of sensor array that acts as a (or part of a) physical barrier, a psychological deterrent to potential intruders, and as part of a security alarm system.

Electric fences are less expensive than many other methods, less likely to give false alarms than many other alternative perimeter security methods, and have highest psychological deterrent of all methods, but there is a potential for unintended shock.

The trigger signal from sensors are transmitted to one or more control units either through wires or wireless means, such as radio, line carrier, and infrared.

Wired systems are convenient when sensors, such as passive infrared motion sensors and smoke detectors require external power to operate correctly; however, they may be more costly to install. Basic wired systems utilize a star network topology, where the panel is at the center logically, and all devices home run their line wires back to the panel. More complex panels use a Bus network topology where the wire basically is a data loop around the perimeter of the facility, and has drops for the sensor devices which must include a unique device identifier integrated into the sensor device itself.  Wired systems also have the advantage, if wired properly for example by dual loop, of being tamper-evident.

Wireless systems, on the other hand, often use battery-powered transmitters which are easier to install and have less expensive start-up costs, but may fail if the batteries are not maintained.  Depending on distance and construction materials, one or more wireless repeaters may be required to bring the signal to the alarm panel reliably.  A wireless system can be moved to a new property easily. An important wireless connection for security is between the control panel and the monitoring station. Wireless monitoring of the alarm system protects against a burglar cutting cables or from failures of an internet provider. This setup is commonly referred to as fully wireless.

Hybrid systems use both wired and wireless sensors to achieve the benefits of both. Transmitters can also be connected through the premises' electrical circuits to transmit coded signals to the control unit (line carrier). The control unit usually has a separate channel or zone for burglar and fire sensors, and more advanced systems have a separate zone for every different sensor, as well as internal trouble indicators, such as mains power loss, low battery, and broken wires.

Depending upon the application, the alarm output may be local, remote or a combination of both.  Local alarms do not include monitoring, but may include indoor and/or outdoor sounders, such as motorized bells or electronic sirens and lights, such strobe lights which may be useful for signaling an evacuation notice during fire emergencies, or to scare off an amateur burglar quickly.  However, with the widespread use of alarm systems, especially in cars, false alarms are very frequent and many urbanites tend to ignore alarms rather than investigating, and not contacting the necessary authorities. In rural areas where not many may hear the fire bell or burglar siren, lights or sounds may not make much difference, as the nearest emergency responders may arrive too late to avoid losses.

Remote alarm systems are used to connect the control unit to a predetermined monitor of some sort, and they are available in many different configurations. Advanced systems connect to a central station or first responder (e.g. police/fire/medical) via a direct phone wire, a cellular network, a radio network, or an IP path. In the case of a dual signaling system two of these options are utilized simultaneously. The alarm monitoring includes not only the sensors, but also the communication transmitter itself. While direct phone circuits are still available in some areas from phone companies, because of their high cost and the advent of dual signaling with its comparatively lower cost, their use is being phased out. Direct connections are now most usually seen only in federal, state, and local government buildings, or on a school campus that has a dedicated security, police, fire, or emergency medical department. In the United Kingdom, communication is only possible to an alarm receiving centre, and communication directly to the emergency services is not permitted.

More typical systems incorporate a digital cellular communication unit that will contact the central station or a monitoring station via the Public Switched Telephone Network (PSTN) and raise the alarm, either with a synthesized voice or increasingly via an encoded message string that the central station decodes. These may connect to the regular phone system on the system side of the demarcation point, but typically connect on the customer side ahead of all phones within the monitored premises so that the alarm system can seize the line by cutting-off any active calls and call the monitoring company if needed. A dual signaling system would raise the alarm wirelessly via a radio path or cellular path using the phone line or broadband line as a backup overcoming any compromise to the phone line. Encoders can be programmed to indicate which specific sensor was triggered, and monitors can show the physical location of the sensor on a list or even a map of the protected premises, which can make the resulting response more effective.

Many alarm panels are equipped with a backup communication path for use when the primary PSTN circuit is not functioning. The redundant dialer may be connected to a second communication path, or a specialized encoded cellular phone, radio, or internet interface device to bypass the PSTN entirely, to thwart intentional tampering with the phone lines. Tampering with the line could trigger a supervisory alarm via the radio network, giving early warning of an imminent problem. In some cases a remote building may not have PSTN phone service, and the cost of trenching and running a direct line may be prohibitive. It is possible to use a wireless cellular or radio device as the primary communication method.

In the UK, the most popular solution of this kind is similar in principle to the above but with the primary and backup paths reversed. Utilizing a radio path as the primary signaling path is not only quicker than PSTN but also allows significant cost savings as unlimited amounts of data can be sent at no extra expense.

Increasing deployment of voice over IP technology (VoIP) is driving the adoption of broadband signaling for alarm reporting. Many sites requiring alarm installations no longer have conventional telephone lines (POTS), and alarm panels with conventional telephone dialer capability do not work reliably over some types of VoIP service.

Dial-up analogue alarm panels or systems with serial/parallel data ports may be migrated to broadband through the addition of an alarm server device which converts telephone signaling signals or data port traffic to IP messages suitable for broadband transmission. However, the direct use of VoIP to transport analogue alarms without an alarm server device is problematic as the audio codecs used throughout the entire network transmission path cannot guarantee a suitable level of reliability or quality of service acceptable for the application.

In response to the changing public communications network, new alarm systems often use broadband signaling as a method of alarm transmission, and manufacturers are including IP reporting capability directly in their alarm panel products.  When the  Internet is used as a primary signaling method for critical security and life safety applications, frequent supervision messages are configured to overcome concerns about backup power for network equipment and signal delivery time. But for typical applications, connectivity concerns are controlled by normal supervision messages.

Dual signaling is a method of alarm transmission that uses a mobile phone network and a telephone and/or IP path to transmit intruder, fire and personal attack signals at high speed from the protected premises to an Alarm Receiving Centre (ARC). It most commonly uses GPRS or GSM, a high-speed signaling technology used to send and receive ‘packets’ of data, with a telephone line in addition. IP is not used as frequently due to issues with installation and configuration as high levels of expertise is often required in addition to alarm installation knowledge.

A dual signaling communication device is attached to a control panel on a security installation and is the component that transmits the alarm to the ARC. It can do this in a number of different ways, via the GPRS radio path, via the GSM radio path or via the telephone line/or IP. These multiple signaling paths are all present and live at the same time backing each other up to minimize exposure of the property to intruders. Should one fail, there is always a back up and depending on the manufacturer chosen up to three paths working simultaneously at any one time.

Dual paths allow distinction between hardware failures and a genuine attack on the alarm. This helps eliminate false alarms and unnecessary responses. Dual signaling has helped considerably with the restoration of police response as in an instance where a phone line is cut as the dual signaling device can continue to send alarm calls via one of its alternative paths either confirming or denying the alarm from the initial path.

In the UK, CSL DualCom Ltd pioneered dual signaling in 1996. The company offered an alternative to existing alarm signaling while setting the current standard for professional dual path security monitoring. Dual signaling is now firmly regarded as the standard format for alarm signaling and is duly specified by all of the leading insurance companies.[20]

Monitored alarms and speaker phones allow for the central station to speak with the homeowner or intruder.  This may be beneficial to the owner for medical emergencies.  For actual break-ins, the speaker phones allow the central station to urge the intruder to cease and desist as response units have been dispatched.  Listen-in alarm monitoring is also known as Immediate Audio-Response monitoring or Speaking Alarm Systems in the UK.

In the United States, police respond to at least 36 million alarm activations each year, at an estimated annual cost of $1.8 billion.[21]

Depending upon the zone triggered, number and sequence of zones, time of day, and other factors, the alarm monitoring center may automatically initiate various actions. Central station operators might be instructed to call emergency services immediately, or to first call the protected premises or property manager to try to determine if  the alarm is genuine. Operators could also start calling a list of phone numbers provided by the customer to contact someone to go check on the protected premises.  Some zones may trigger a call to the local heating oil company to check on the system, or a call to the owner with details of which room may be flooded.  Some alarm systems are tied to video surveillance systems so that current video of the intrusion area can be instantly displayed on a remote monitor and recorded.

Some alarm systems use real-time audio and video monitoring technology to verify the legitimacy of an alarm. In some municipalities around the United States, this type of alarm verification allows the property it is protecting to be placed on a ""verified response"" list, allowing for quicker and safer police responses.

To be useful, an intrusion alarm system is deactivated or reconfigured when authorized personnel are present. Authorization may be indicated in any number of ways, often with keys or codes used at the control panel or a remote panel near an entry. High-security alarms may require multiple codes, or a fingerprint, badge, hand-geometry, retinal scan, encrypted-response generator, and other means that are deemed sufficiently secure for the purpose.

Failed authorizations would result in an alarm or a timed lockout to prevent experimenting with possible codes. Some systems can be configured to permit deactivation of individual sensors or groups. Others can also be programmed to bypass or ignore individual sensors and leave the remainder of the system armed. This feature is useful for permitting a single door to be opened and closed before the alarm is armed, or to permit a person to leave, but not return. High-end systems allow multiple access codes, and may only permit them to be used once, or on particular days, or only in combination with other users' codes (i.e., escorted). In any case, a remote monitoring center should arrange an oral code to be provided by an authorized person in case of false alarms, so the monitoring center can be assured that a further alarm response is unnecessary.  As with access codes, there can also be a hierarchy of oral codes, for example, for furnace repairperson to enter the kitchen and basement sensor areas but not the silver vault in the pantry. There are also systems that permit a duress code to be entered and silence the local alarm, but still trigger the remote alarm to summon the police to a robbery.

Fire sensors can be isolated, meaning that when triggered, they will not trigger the main alarm network. This is important when smoke and heat is intentionally produced. The owners of buildings can be fined for generating false alarms that waste the time of emergency personnel.

The United States Department of Justice estimates that between 94% and 98% of all alarm calls to law enforcement are false alarms.[21]

System reliability and user error are the cause of most false alarms, sometimes called ""nuisance alarms."" False alarms can be very costly to local governments, local law enforcement, security system users and members of local communities. In 2007, the Department of Justice reported that in just one year, false alarms cost local municipalities and their constituents at least $1.8 billion.[21]

In many municipalities across the United States, policies have been adopted to fine home and business owners for multiple false alarm activations from their security system. If multiple false alarms from the same property persist, that property could be added to a ""no response"" list, which prevents police dispatch to the property except in the event of verified emergency.  Approximately 1% of police alarm calls actually involve a crime.[21] Nuisance alarms occur when an unintended event evokes an alarm status by an otherwise properly working alarm system.  A false alarm also occurs when there is an alarm system malfunction that results in an alarm state. In all three circumstances, the source of the problem should be immediately found and fixed, so that responders will not lose confidence in the alarm reports.  It is easier to know when there are false alarms, because the system is designed to react to that condition.  Failure alarms are more troublesome because they usually require periodic testing to make sure the sensors are working and that the correct signals are getting through to the monitor.  Some systems are designed to detect problems internally, such as low or dead batteries, loose connections, phone circuit trouble, etc. While earlier nuisance alarms could be set off by small disturbances, like insects or pets, newer model alarms have technology to measure the size/weight of the object causing the disturbance, and thus are able to decide how serious the threat is, which is especially useful in burglar alarms.

Some municipalities across the United States require alarm verification before police are dispatched. Under this approach, alarm monitoring companies must verify the legitimacy of alarms (except holdup, duress, and panic alarms) before calling the police. Verified response typically involves visual on-scene verification of a break-in, or remote audio or video verification.[21]

Alarms that utilize audio, video, or combination of both audio and video verification technology give security companies, dispatchers, police officers, and property managers more reliable data to assess the threat level of a triggered alarm.[22]

Audio and video verification techniques use microphones and cameras to record audio frequencies, video signals, or image snapshots. The source audio and video streams are sent over a communication link, usually an Internet protocol (IP) network, to the central station where monitors retrieve the images through proprietary software. The information is then relayed to law enforcement and recorded to an event file, which can be used to plan a more strategic and tactical approach of a property, and later as prosecution evidence.

An example of this system is when a passive infrared or other sensor is triggered a designated number of video frames from before and after the event is sent to the central station.

A second video solution can be incorporated into a standard panel, which sends the central station an alarm. When a signal is received, a trained monitoring professional accesses the on-site digital video recorder (DVR) through an IP link to determine the cause of the activation. For this type of system, the camera input to the DVR reflects the alarm panel's zones and partitioning, which allows personnel to look for an alarm source in multiple areas.

The United States Department of Justice states that legislation requiring alarm companies to verify the legitimacy of an alarm, before contacting law enforcement (commonly known as ""verified response"") is the most effective way to reduce false burglar alarms. The Department of Justice considers audio, video, or an eye-witness account as verification for the legitimacy of a burglar alarm.[21]

Cross-zoning is a strategy that uses multiple sensors to monitor activity in one area and software analyses input from all the sources. For example, if a motion detector trips in one area, the signal is recorded and the central-station monitor notifies the customer. A second alarm signal—received in an adjacent zone within a short time—is the confirmation the central-station monitor needs to request a dispatch immediately. This method builds increased protection.

Enhanced call verification (ECV) helps reduce false dispatches while still protecting citizens, and is mandated in several US jurisdictions, although the alarm industry has successfully opposed it in others.[21] ECV requires central station personnel to attempt to verify the alarm activation by making a minimum of two phone calls to two different responsible party telephone numbers before dispatching law enforcement to the scene.

The first alarm-verification call goes to the location the alarm originated. If contact with a person is not made, a second call is placed to a different number. The secondary number, best practices dictate, should be to a telephone that is answered even after hours, preferably a cellular phone of a decision maker authorized to request or bypass emergency response.

ECV, as it cannot confirm an actual intrusion event and will not prompt a priority law enforcement dispatch, is not considered true alarm verification by the security industry.

Some insurance companies and local agencies require that alarm systems be installed to code or be certified by an independent third party. The alarm system is required to have a maintenance check carried out every 6 – 12 months. In the UK, 'Audible Only' intruder alarm systems require a routine service visit once every 12 months and monitored intruder alarm systems require a check twice in every 12-month period. This is to ensure all internal components, sensors and PSUs are functioning correctly. In the past, this would require an alarm service engineer to attend site and carry the checks out. With the use of the Internet or radio path and a compatible IP/radio transmitting device (at the alarmed premises), some checks can now be carried out remotely from the central station.
"
Electric Line Construction,"

Electric power transmission is the bulk movement of electrical energy from a generating site, such as a power plant, to an electrical substation. The interconnected lines that facilitate this movement form a transmission network. This is distinct from the local wiring between high-voltage substations and customers, which is typically referred to as electric power distribution. The combined transmission and distribution network is part of electricity delivery, known as the electrical grid.

Efficient long-distance transmission of electric power requires high voltages. This reduces the losses produced by strong currents. Transmission lines use either alternating current (AC) or direct current (DC). The voltage level is changed with transformers. The voltage is stepped up for transmission, then reduced for local distribution.

A wide area synchronous grid, known as an interconnection in North America, directly connects generators delivering AC power with the same relative frequency to many consumers. North America has four major interconnections: Western, Eastern, Quebec and Texas. One grid connects most of continental Europe.

Historically, transmission and distribution lines were often owned by the same company, but starting in the 1990s, many countries liberalized the regulation of the electricity market in ways that led to separate companies handling transmission and distribution.[2]

Most North American transmission lines are high-voltage three-phase AC, although single phase AC is sometimes used in railway electrification systems. DC technology is used for greater efficiency over longer distances, typically hundreds of miles. High-voltage direct current (HVDC) technology is also used in submarine power cables (typically longer than 30 miles (50 km)), and in the interchange of power between grids that are not mutually synchronized. HVDC links stabilize power distribution networks where sudden new loads, or blackouts, in one part of a network might otherwise result in synchronization problems and cascading failures.

Electricity is transmitted at high voltages to reduce the energy loss due to resistance that occurs over long distances. Power is usually transmitted through overhead power lines. Underground power transmission has a significantly higher installation cost and greater operational limitations, but lowers maintenance costs. Underground transmission is more common in urban areas or environmentally sensitive locations.

Electrical energy must typically be generated at the same rate at which it is consumed. A sophisticated control system is required to ensure that power generation closely matches demand. If demand exceeds supply, the imbalance can cause generation plant(s) and transmission equipment to automatically disconnect or shut down to prevent damage. In the worst case, this may lead to a cascading series of shutdowns and a major regional blackout.

The US Northeast faced blackouts in 1965, 1977, 2003, and major blackouts in other US regions in 1996 and 2011. Electric transmission networks are interconnected into regional, national, and even continent-wide networks to reduce the risk of such a failure by providing multiple redundant, alternative routes for power to flow should such shutdowns occur. Transmission companies determine the maximum reliable capacity of each line (ordinarily less than its physical or thermal limit) to ensure that spare capacity is available in the event of a failure in another part of the network.

High-voltage overhead conductors are not covered by insulation. The conductor material is nearly always an aluminium alloy, formed of several strands and possibly reinforced with steel strands. Copper was sometimes used for overhead transmission, but aluminum is lighter, reduces yields only marginally and costs much less. Overhead conductors are supplied by several companies. Conductor material and shapes are regularly improved to increase capacity.

Conductor sizes range from 12 mm2 (#6 American wire gauge) to 1,092 mm2 (2,156,000 circular mils area), with varying resistance and current-carrying capacity. For large conductors (more than a few centimetres in diameter), much of the current flow is concentrated near the surface due to the skin effect. The center of the conductor carries little current but contributes weight and cost. Thus, multiple parallel cables (called bundle conductors) are used for higher capacity. Bundle conductors are used at high voltages to reduce energy loss caused by corona discharge.

Today, transmission-level voltages are usually considered to be 110 kV and above. Lower voltages, such as 66 kV and 33 kV, are usually considered subtransmission voltages, but are occasionally used on long lines with light loads. Voltages less than 33 kV are usually used for distribution. Voltages above 765 kV are considered extra high voltage and require different designs.

Overhead transmission wires depend on air for insulation, requiring that lines maintain minimum clearances. Adverse weather conditions, such as high winds and low temperatures, interrupt transmission. Wind speeds as low as 23 knots (43 km/h) can permit conductors to encroach operating clearances, resulting in a flashover and loss of supply.[3] Oscillatory motion of the physical line is termed conductor gallop or flutter depending on the frequency and amplitude of oscillation.

Electric power can be transmitted by underground power cables. Underground cables take up no right-of-way, have lower visibility, and are less affected by weather. However, cables must be insulated. Cable and excavation costs are much higher than overhead construction. Faults in buried transmission lines take longer to locate and repair.

In some metropolitan areas, cables are enclosed by metal pipe and insulated with dielectric fluid (usually an oil) that is either static or circulated via pumps. If an electric fault damages the pipe and leaks dielectric, liquid nitrogen is used to freeze portions of the pipe to enable draining and repair. This extends the repair period and increases costs. The temperature of the pipe and surroundings are monitored throughout the repair period.[4][5][6]

Underground lines are limited by their thermal capacity, which permits less overload or re-rating lines. Long underground AC cables have significant capacitance, which reduces their ability to provide useful power beyond 50 miles (80 kilometres). DC cables are not limited in length by their capacitance.

Commercial electric power was initially transmitted at the same voltage used by lighting and mechanical loads. This restricted the distance between generating plant and loads. In 1882, DC voltage could not easily be increased for long-distance transmission. Different classes of loads (for example, lighting, fixed motors, and traction/railway systems) required different voltages, and so used different generators and circuits.[7][8]

Thus, generators were sited near their loads, a practice that later became known as distributed generation using large numbers of small generators.[9]

Transmission of alternating current (AC) became possible after Lucien Gaulard and John Dixon Gibbs built what they called the secondary generator, an early transformer provided with 1:1 turn ratio and open magnetic circuit, in 1881.

The first long distance AC line was 34 kilometres (21 miles) long, built for the 1884 International Exhibition of Electricity in Turin, Italy. It was powered by a 2 kV, 130 Hz Siemens & Halske alternator and featured several Gaulard transformers with primary windings connected in series, which fed incandescent lamps. The system proved the feasibility of AC electric power transmission over long distances.[8]

The first commercial AC distribution system entered service in 1885 in via dei Cerchi, Rome, Italy, for public lighting. It was powered by two Siemens & Halske alternators rated 30 hp (22 kW), 2 kV at 120 Hz and used 19 km of cables and 200 parallel-connected 2 kV to 20 V step-down transformers provided with a closed magnetic circuit, one for each lamp. A few months later it was followed by the first British AC system, serving Grosvenor Gallery. It also featured Siemens alternators and 2.4 kV to 100 V step-down transformers – one per user – with shunt-connected primaries.[10]

Working to improve what he considered an impractical Gaulard-Gibbs design, electrical engineer William Stanley, Jr. developed the first practical series AC transformer in 1885.[11] Working with the support of George Westinghouse, in 1886 he demonstrated a transformer-based AC lighting system in Great Barrington, Massachusetts. It was powered by a steam engine-driven 500 V Siemens generator. Voltage was stepped down to 100 volts using the Stanley transformer to power incandescent lamps at 23 businesses over 4,000 feet (1,200 m).[12] This practical demonstration of a transformer and alternating current lighting system led Westinghouse to begin installing AC systems later that year.[11]

In 1888 the first designs for an AC motor appeared. These were induction motors running on polyphase current, independently invented by Galileo Ferraris and Nikola Tesla. Westinghouse licensed Tesla's design. Practical three-phase motors were designed by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown.[13] Widespread use of such motors were delayed many years by development problems and the scarcity of polyphase power systems needed to power them.[14][15]

In the late 1880s and early 1890s smaller electric companies merged into larger corporations such as Ganz and AEG in Europe and General Electric and Westinghouse Electric in the US. These companies developed AC systems, but the technical difference between direct and alternating current systems required a much longer technical merger.[16] Alternating current's economies of scale with large generating plants and long-distance transmission slowly added the ability to link all the loads. These included single phase AC systems, poly-phase AC systems, low voltage incandescent lighting, high-voltage arc lighting, and existing DC motors in factories and street cars. In what became a universal system, these technological differences were temporarily bridged via the rotary converters and motor-generators that allowed the legacy systems to connect to the AC grid.[16][17] These stopgaps were slowly replaced as older systems were retired or upgraded.

The first transmission of single-phase alternating current using high voltage came in Oregon in 1890 when power was delivered from a hydroelectric plant at Willamette Falls to the city of Portland 14 miles (23 km) down river.[18] The first three-phase alternating current using high voltage took place in 1891 during the international electricity exhibition in Frankfurt. A 15 kV transmission line, approximately 175 km long, connected Lauffen on the Neckar and Frankfurt.[10][19]

Transmission voltages increased throughout the 20th century. By 1914, fifty-five transmission systems operating at more than 70 kV were in service. The highest voltage then used was 150 kV.[20] Interconnecting multiple generating plants over a wide area reduced costs. The most efficient plants could be used to supply varying loads during the day. Reliability was improved and capital costs were reduced, because stand-by generating capacity could be shared over many more customers and a wider area. Remote and low-cost sources of energy, such as hydroelectric power or mine-mouth coal, could be exploited to further lower costs.[7][10]


The 20th century's rapid industrialization made electrical transmission lines and grids critical infrastructure. Interconnection of local generation plants and small distribution networks was spurred by World War I, when large electrical generating plants were built by governments to power munitions factories.[21]
These networks use components such as power lines, cables, circuit breakers, switches and transformers. The transmission network is usually administered on a regional basis by an entity such as a regional transmission organization or transmission system operator.[22]

Transmission efficiency is improved at higher voltage and lower current. The reduced current reduces heating losses. Joule's first law states that energy losses are proportional to the square of the current. Thus, reducing the current by a factor of two lowers the energy lost to conductor resistance by a factor of four for any given size of conductor.

The optimum size of a conductor for a given voltage and current can be estimated by Kelvin's law for conductor size, which states that size is optimal when the annual cost of energy wasted in resistance is equal to the annual capital charges of providing the conductor. At times of lower interest rates and low commodity costs, Kelvin's law indicates that thicker wires are optimal. Otherwise, thinner conductors are indicated. Since power lines are designed for long-term use, Kelvin's law is used in conjunction with long-term estimates of the price of copper and aluminum as well as interest rates.

Higher voltage is achieved in AC circuits by using a step-up transformer. High-voltage direct current (HVDC) systems require relatively costly conversion equipment that may be economically justified for particular projects such as submarine cables and longer distance high capacity point-to-point transmission. HVDC is necessary for sending energy between unsynchronized grids.

A transmission grid is a network of power stations, transmission lines, and substations. Energy is usually transmitted within a grid with three-phase AC. Single-phase AC is used only for distribution to end users since it is not usable for large polyphase induction motors. In the 19th century, two-phase transmission was used but required either four wires or three wires with unequal currents. Higher order phase systems require more than three wires, but deliver little or no benefit.

While the price of generating capacity is high, energy demand is variable, making it often cheaper to import needed power than to generate it locally. Because loads often rise and fall together across large areas, power often comes from distant sources. Because of the economic benefits of load sharing, wide area transmission grids may span countries and even continents. Interconnections between producers and consumers enables power to flow even if some links are inoperative.

The slowly varying portion of demand is known as the base load and is generally served by large facilities with constant operating costs, termed firm power. Such facilities are nuclear, coal or hydroelectric, while other energy sources such as concentrated solar thermal and geothermal power have the potential to provide firm power. Renewable energy sources, such as solar photovoltaics, wind, wave, and tidal, are, due to their intermittency, not considered to be firm. The remaining or peak power demand, is supplied by peaking power plants, which are typically smaller, faster-responding, and higher cost sources, such as combined cycle or combustion turbine plants typically fueled by natural gas.

Long-distance transmission (hundreds of kilometers) is cheap and efficient, with costs of US$0.005–0.02 per kWh, compared to annual averaged large producer costs of US$0.01–0.025 per kWh, retail rates upwards of US$0.10 per kWh, and multiples of retail for instantaneous suppliers at unpredicted high demand moments.[23] New York often buys over 1000 MW of low-cost hydropower from Canada.[24] Local sources (even if more expensive and infrequently used) can protect the power supply from weather and other disasters that can disconnect distant suppliers.

Hydro and wind sources cannot be moved closer to big cities, and solar costs are lowest in remote areas where local power needs are nominal. Connection costs can determine whether any particular renewable alternative is economically realistic. Costs can be prohibitive for transmission lines, but high capacity, long distance super grid transmission network costs could be recovered with modest usage fees.

--- Grid input ---

At power stations, power is produced at a relatively low voltage between about 2.3 kV and 30 kV, depending on the size of the unit. The voltage is then stepped up by the power station transformer to a higher voltage (115 kV to 765 kV AC) for transmission.

In the United States, power transmission is, variously, 230 kV to 500 kV, with less than 230 kV or more than 500 kV as exceptions.

The Western Interconnection has two primary interchange voltages: 500 kV AC at 60 Hz, and ±500 kV (1,000 kV net) DC from North to South (Columbia River to Southern California) and Northeast to Southwest (Utah to Southern California). The 287.5 kV (Hoover Dam to Los Angeles line, via Victorville) and 345 kV (Arizona Public Service (APS) line) are local standards, both of which were implemented before 500 kV became practical.

Transmitting electricity at high voltage reduces the fraction of energy lost to Joule heating, which varies by conductor type, the current, and the transmission distance. For example, a 100 miles (160 km) span at 765 kV carrying 1000 MW of power can have losses of 0.5% to 1.1%. A 345 kV line carrying the same load across the same distance has losses of 4.2%.[25] For a given amount of power, a higher voltage reduces the current and thus the resistive losses. For example, raising the voltage by a factor of 10 reduces the current by a corresponding factor of 10 and therefore the 




I

2


R


{\displaystyle I^{2}R}

 losses by a factor of 100, provided the same sized conductors are used in both cases. Even if the conductor size (cross-sectional area) is decreased ten-fold to match the lower current, the 




I

2


R


{\displaystyle I^{2}R}

 losses are still reduced ten-fold using the higher voltage.

While power loss can also be reduced by increasing the wire's conductance (by increasing its cross-sectional area), larger conductors are heavier and more expensive. And since conductance is proportional to cross-sectional area, resistive power loss is only reduced proportionally with increasing cross-sectional area, providing a much smaller benefit than the squared reduction provided by multiplying the voltage.

Long-distance transmission is typically done with overhead lines at voltages of 115 to 1,200 kV. At higher voltages, where more than 2,000 kV exists between conductor and ground, corona discharge losses are so large that they can offset the lower resistive losses in the line conductors. Measures to reduce corona losses include larger conductor diameter, hollow cores[26] or conductor bundles.

Factors that affect resistance and thus loss include temperature, spiraling, and the skin effect. Resistance increases with temperature. Spiraling, which refers to the way stranded conductors spiral about the center, also contributes to increases in conductor resistance. The skin effect causes the effective resistance to increase at higher AC frequencies. Corona and resistive losses can be estimated using a mathematical model.[27]

US transmission and distribution losses were estimated at 6.6% in 1997,[28] 6.5% in 2007[28] and 5% from 2013 to 2019.[29] In general, losses are estimated from the discrepancy between power produced (as reported by power plants) and power sold; the difference constitutes transmission and distribution losses, assuming no utility theft occurs.

As of 1980, the longest cost-effective distance for DC transmission was 7,000 kilometres (4,300 miles). For AC it was 4,000 kilometres (2,500 miles), though US transmission lines are substantially shorter.[23]

In any AC line, conductor inductance and capacitance can be significant. Currents that flow solely in reaction to these properties, (which together with the resistance define the impedance) constitute reactive power flow, which transmits no power to the load. These reactive currents, however, cause extra heating losses. The ratio of real power transmitted to the load to apparent power (the product of a circuit's voltage and current, without reference to phase angle) is the power factor. As reactive current increases, reactive power increases and power factor decreases.

For transmission systems with low power factor, losses are higher than for systems with high power factor. Utilities add capacitor banks, reactors and other components (such as phase-shifters; static VAR compensators; and flexible AC transmission systems, FACTS) throughout the system help to compensate for the reactive power flow, reduce the losses in power transmission and stabilize system voltages. These measures are collectively called 'reactive support'.

Current flowing through transmission lines induces a magnetic field that surrounds the lines of each phase and affects the inductance of the surrounding conductors of other phases. The conductors' mutual inductance is partially dependent on the physical orientation of the lines with respect to each other. Three-phase lines are conventionally strung with phases separated vertically. The mutual inductance seen by a conductor of the phase in the middle of the other two phases is different from the inductance seen on the top/bottom.

Unbalanced inductance among the three conductors is problematic because it may force the middle line to carry a disproportionate amount of the total power transmitted. Similarly, an unbalanced load may occur if one line is consistently closest to the ground and operates at a lower impedance. Because of this phenomenon, conductors must be periodically transposed along the line so that each phase sees equal time in each relative position to balance out the mutual inductance seen by all three phases. To accomplish this, line position is swapped at specially designed transposition towers at regular intervals along the line using various transposition schemes.

Subtransmission runs at relatively lower voltages. It is uneconomical to connect all distribution substations to the high main transmission voltage, because that equipment is larger and more expensive. Typically, only larger substations connect with this high voltage. Voltage is stepped down before the current is sent to smaller substations. Subtransmission circuits are usually arranged in loops so that a single line failure does not stop service to many customers for more than a short time.

Loops can be normally closed, where loss of one circuit should result in no interruption, or normally open where substations can switch to a backup supply. While subtransmission circuits are usually carried on overhead lines, in urban areas buried cable may be used. The lower-voltage subtransmission lines use less right-of-way and simpler structures; undergrounding is less difficult.

No fixed cutoff separates subtransmission and transmission, or subtransmission and distribution. Their voltage ranges overlap. Voltages of 69 kV, 115 kV, and 138 kV are often used for subtransmission in North America. As power systems evolved, voltages formerly used for transmission were used for subtransmission, and subtransmission voltages became distribution voltages. Like transmission, subtransmission moves relatively large amounts of power, and like distribution, subtransmission covers an area instead of just point-to-point.[30]

Substation transformers reduce the voltage to a lower level for distribution to customers. This distribution is accomplished with a combination of sub-transmission (33 to 138 kV) and distribution (3.3 to 25 kV). Finally, at the point of use, the energy is transformed to end-user voltage (100 to 4160 volts).

High-voltage power transmission allows for lesser resistive losses over long distances. This efficiency delivers a larger proportion of the generated power to the loads.

In a simplified model, the grid delivers electricity from an ideal voltage source with voltage 



V


{\displaystyle V}

, delivering a power 




P

V




{\displaystyle P_{V}}

) to a single point of consumption, modelled by a resistance 



R


{\displaystyle R}

, when the wires are long enough to have a significant resistance 




R

C




{\displaystyle R_{C}}

.

If the resistances are in series with no intervening transformer, the circuit acts as a voltage divider, because the same current 



I
=


V

R
+

R

C







{\displaystyle I={\frac {V}{R+R_{C}}}}

 runs through the wire resistance and the powered device. As a consequence, the useful power (at the point of consumption) is:

Should an ideal transformer convert high-voltage, low-current electricity into low-voltage, high-current electricity with a voltage ratio of 



a


{\displaystyle a}

 (i.e., the voltage is divided by 



a


{\displaystyle a}

 and the current is multiplied by 



a


{\displaystyle a}

 in the secondary branch, compared to the primary branch), then the circuit is again equivalent to a voltage divider, but the wires now have apparent resistance of only 




R

C



/


a

2




{\displaystyle R_{C}/a^{2}}

. The useful power is then:

For 



a
>
1


{\displaystyle a>1}

 (i.e. conversion of high voltage to low voltage near the consumption point), a larger fraction of the generator's power is transmitted to the consumption point and a lesser fraction is lost to Joule heating.

The terminal characteristics of the transmission line are the voltage and current at the sending (S) and receiving (R) ends. The transmission line can be modeled as a black box and a 2 by 2 transmission matrix is used to model its behavior, as follows:

The line is assumed to be a reciprocal, symmetrical network, meaning that the receiving and sending labels can be switched with no consequence. The transmission matrix T has the properties:

The parameters A, B, C, and D differ depending on how the desired model handles the line's resistance (R), inductance (L), capacitance (C), and shunt (parallel, leak) conductance G.

The four main models are the short line approximation, the medium line approximation, the long line approximation (with distributed parameters), and the lossless line. In such models, a capital letter such as R refers to the total quantity summed over the line and a lowercase letter such as c refers to the per-unit-length quantity.

The lossless line approximation is the least accurate; it is typically used on short lines where the inductance is much greater than the resistance. For this approximation, the voltage and current are identical at the sending and receiving ends.

The characteristic impedance is pure real, which means resistive for that impedance, and it is often called surge impedance. When a lossless line is terminated by surge impedance, the voltage does not drop. Though the phase angles of voltage and current are rotated, the magnitudes of voltage and current remain constant along the line. For load > SIL, the voltage drops from sending end and the line consumes VARs. For load < SIL, the voltage increases from the sending end, and the line generates VARs.

The short line approximation is normally used for lines shorter than 80 km (50 mi). There, only a series impedance Z is considered, while C and G are ignored. The final result is that A = D = 1 per unit, B = Z Ohms, and C = 0. The associated transition matrix for this approximation is therefore:

The medium line approximation is used for lines running between 80 and 250 km (50 and 155 mi). The series impedance and the shunt (current leak) conductance are considered, placing half of the shunt conductance at each end of the line. This circuit is often referred to as a nominal π (pi) circuit because of the shape (π) that is taken on when leak conductance is placed on both sides of the circuit diagram. The analysis of the medium line produces:

Counterintuitive behaviors of medium-length transmission lines:

The long line model is used when a higher degree of accuracy is needed or when the line under consideration is more than 250 km (160 mi) long. Series resistance and shunt conductance are considered to be distributed parameters, such that each differential length of the line has a corresponding differential series impedance and shunt admittance. The following result can be applied at any point along the transmission line, where 



γ


{\displaystyle \gamma }

 is the propagation constant.

To find the voltage and current at the end of the long line, 



x


{\displaystyle x}

 should be replaced with 



l


{\displaystyle l}

 (the line length) in all parameters of the transmission matrix. This model applies the Telegrapher's equations.

High-voltage direct current (HVDC) is used to transmit large amounts of power over long distances or for interconnections between asynchronous grids. When electrical energy is transmitted over very long distances, the power lost in AC transmission becomes appreciable and it is less expensive to use direct current instead. For a long transmission line, these lower losses (and reduced construction cost of a DC line) can offset the cost of the required converter stations at each end.

HVDC is used for long submarine cables where AC cannot be used because of cable capacitance.[31] In these cases special high-voltage cables are used. Submarine HVDC systems are often used to interconnect the electricity grids of islands, for example, between Great Britain and continental Europe, between Great Britain and Ireland, between Tasmania and the Australian mainland, between the North and South Islands of New Zealand, between New Jersey and New York City, and between New Jersey and Long Island. Submarine connections up to 600 kilometres (370 mi) in length have been deployed.[32]

HVDC links can be used to control grid problems. The power transmitted by an AC line increases as the phase angle between source end voltage and destination ends increases, but too large a phase angle allows the systems at either end to fall out of step. Since the power flow in a DC link is controlled independently of the phases of the AC networks that it connects, this phase angle limit does not exist, and a DC link is always able to transfer its full rated power. A DC link therefore stabilizes the AC grid at either end, since power flow and phase angle can then be controlled independently.

As an example, to adjust the flow of AC power on a hypothetical line between Seattle and Boston would require adjustment of the relative phase of the two regional electrical grids. This is an everyday occurrence in AC systems, but one that can become disrupted when AC system components fail and place unexpected loads on the grid. With an HVDC line instead, such an interconnection would:

(and possibly in other cooperating cities along the transmission route). Such a system could be less prone to failure if parts of it were suddenly shut down. One example of a long DC transmission line is the Pacific DC Intertie located in the Western United States.

The amount of power that can be sent over a transmission line varies with the length of the line. The heating of short line conductors due to line losses sets a thermal limit. If too much current is drawn, conductors may sag too close to the ground, or conductors and equipment may overheat. For intermediate-length lines on the order of 100 kilometres (62 miles), the limit is set by the voltage drop in the line. For longer AC lines, system stability becomes the limiting factor. Approximately, the power flowing over an AC line is proportional to the cosine of the phase angle of the voltage and current at the ends.

This angle varies depending on system loading. It is undesirable for the angle to approach 90 degrees, as the power flowing decreases while resistive losses remain. The product of line length and maximum load is approximately proportional to the square of the system voltage. Series capacitors or phase-shifting transformers are used on long lines to improve stability. HVDC lines are restricted only by thermal and voltage drop limits, since the phase angle is not material.

Understanding the temperature distribution along the cable route became possible with the introduction of distributed temperature sensing (DTS) systems that measure temperatures all along the cable. Without them maximum current was typically set as a compromise between understanding of operation conditions and risk minimization. This monitoring solution uses passive optical fibers as temperature sensors, either inside a high-voltage cable or externally mounted on the cable insulation.

For overhead cables the fiber is integrated into the core of a phase wire. The integrated Dynamic Cable Rating (DCR)/Real Time Thermal Rating (RTTR) solution makes it possible to run the network to its maximum. It allows the operator to predict the behavior of the transmission system to reflect major changes to its initial operating conditions.

Some utilities have embraced reconductoring to handle the increase in electricity production. Reconductoring is the replacement-in-place of existing transmission lines with higher-capacity lines. Adding transmission lines is difficult due to cost, permit intervals, and local opposition. Reconductoring has the potential to double the amount of electricity that can travel across a transmission line.[33] 
A 2024 report found the United States behind countries like Belgium and the Netherlands in adoption of this technique to accommodate electrification and renewable energy. [34] In April 2022, the Biden Administration streamlined environmental reviews for such projects, and in May 2022 announced competitive grants for them funded by the 2021 Bipartisan Infrastructure Law and 2022 Inflation Reduction Act.[35]

The rate of transmission expansion needs to double to support ongoing electrification and reach emission reduction targets. As of 2022, more than 10,000 power plant and energy storage projects were awaiting permission to connect to the US grid — 95% were zero-carbon resources. New power lines can take 10 years to plan, permit, and build.[33]

Traditional power lines use a steel core surrounded by aluminum strands (Aluminium-conductor steel-reinforced cable). Replacing the steel with a lighter, stronger composite material such as carbon fiber (ACCC conductor) allows lines to operate at higher temperatures, with less sag, and doubled transmission capacity. Lowering line sag at high temperatures can prevent wildfires from starting when power lines touch dry vegetation.[34] Although advanced lines can cost 2-4x more than steel, total reconductoring costs are less than half of a new line, given savings in time, land acquisition, permitting, and construction.[33]

A reconductoring project in southeastern Texas upgraded 240 miles of transmission lines at a cost of $900,000 per mile, versus a 3,600-mile greenfield project that averaged $1.9 million per mile.[33]

To ensure safe and predictable operation, system components are controlled with generators, switches, circuit breakers and loads. The voltage, power, frequency, load factor, and reliability capabilities of the transmission system are designed to provide cost effective performance.

The transmission system provides for base load and peak load capability, with margins for safety and fault tolerance. Peak load times vary by region largely due to the industry mix. In hot and cold climates home air conditioning and heating loads affect the overall load. They are typically highest in the late afternoon in the hottest part of the year and in mid-mornings and mid-evenings in the coldest part of the year. Power requirements vary by season and time of day. Distribution system designs always take the base load and the peak load into consideration.

The transmission system usually does not have a large buffering capability to match loads with generation. Thus generation has to be kept matched to the load, to prevent overloading generation equipment.

Multiple sources and loads can be connected to the transmission system and they must be controlled to provide orderly transfer of power. In centralized power generation, only local control of generation is necessary. This involves synchronization of the generation units.

In distributed power generation the generators are geographically distributed and the process to bring them online and offline must be carefully controlled. The load control signals can either be sent on separate lines or on the power lines themselves. Voltage and frequency can be used as signaling mechanisms to balance the loads.

In voltage signaling, voltage is varied to increase generation. The power added by any system increases as the line voltage decreases. This arrangement is stable in principle. Voltage-based regulation is complex to use in mesh networks, since the individual components and setpoints would need to be reconfigured every time a new generator is added to the mesh.

In frequency signaling, the generating units match the frequency of the power transmission system. In droop speed control, if the frequency decreases, the power is increased. (The drop in line frequency is an indication that the increased load is causing the generators to slow down.)

Wind turbines, vehicle-to-grid, virtual power plants, and other locally distributed storage and generation systems can interact with the grid to improve system operation. Internationally[where?], a slow move from a centralized to decentralized power systems have taken place. The main draw of locally distributed generation systems is that they reduce transmission losses by leading to consumption of electricity closer to where it was produced.[36]

Under excess load conditions, the system can be designed to fail incrementally rather than all at once. Brownouts occur when power supplied drops below the demand. Blackouts occur when the grid fails completely.

Rolling blackouts (also called load shedding) are intentionally engineered electrical power outages, used to distribute insufficient power to various loads in turn.

Grid operators require reliable communications to manage the grid and associated generation and distribution facilities. Fault-sensing protective relays at each end of the line must communicate to monitor the flow of power so that faulted conductors or equipment can be quickly de-energized and the balance of the system restored. Protection of the transmission line from short circuits and other faults is usually so critical that common carrier telecommunications are insufficiently reliable, while in some remote areas no common carrier is available. Communication systems associated with a transmission project may use:

Rarely, and for short distances, pilot-wires are strung along the transmission line path. Leased circuits from common carriers are not preferred since availability is not under control of the operator.

Transmission lines can be used to carry data: this is called power-line carrier, or power-line communication (PLC). PLC signals can be easily received with a radio in the long wave range.

Optical fibers can be included in the stranded conductors of a transmission line, in the overhead shield wires. These cables are known as optical ground wire (OPGW). Sometimes a standalone cable is used, all-dielectric self-supporting (ADSS) cable, attached to the transmission line cross arms.

Some jurisdictions, such as Minnesota, prohibit energy transmission companies from selling surplus communication bandwidth or acting as a telecommunications common carrier. Where the regulatory structure permits, the utility can sell capacity in extra dark fibers to a common carrier.

Electricity transmission is generally considered to be a natural monopoly, but one that is not inherently linked to generation.[37][38][39] Many countries regulate transmission separately from generation.

Spain was the first country to establish a regional transmission organization. In that country, transmission operations and electricity markets are separate. The transmission system operator is Red Eléctrica de España (REE) and the wholesale electricity market operator is Operador del Mercado Ibérico de Energía – Polo Español, S.A. (OMEL) OMEL Holding | Omel Holding. Spain's transmission system is interconnected with those of France, Portugal, and Morocco.

The establishment of RTOs in the United States was spurred by the FERC's Order 888, Promoting Wholesale Competition Through Open Access Non-discriminatory Transmission Services by Public Utilities; Recovery of Stranded Costs by Public Utilities and Transmitting Utilities, issued in 1996.[40] In the United States and parts of Canada, electric transmission companies operate independently of generation companies, but in the Southern United States vertical integration is intact. In regions of separation, transmission owners and generation owners continue to interact with each other as market participants with voting rights within their RTO. RTOs in the United States are regulated by the Federal Energy Regulatory Commission.

Merchant transmission projects in the United States include the Cross Sound Cable from Shoreham, New York to New Haven, Connecticut, Neptune RTS Transmission Line from Sayreville, New Jersey, to New Bridge, New York, and Path 15 in California. Additional projects are in development or have been proposed throughout the United States, including the Lake Erie Connector, an underwater transmission line proposed by ITC Holdings Corp., connecting Ontario to load serving entities in the PJM Interconnection region.[41]

Australia has one unregulated or market interconnector – Basslink – between Tasmania and Victoria. Two DC links originally implemented as market interconnectors, Directlink and Murraylink, were converted to regulated interconnectors.[42]

A major barrier to wider adoption of merchant transmission is the difficulty in identifying who benefits from the facility so that the beneficiaries pay the toll. Also, it is difficult for a merchant transmission line to compete when the alternative transmission lines are subsidized by utilities with a monopolized and regulated rate base.[43] In the United States, the FERC's Order 1000, issued in 2010, attempted to reduce barriers to third party investment and creation of merchant transmission lines where a public policy need is found.[44]

The cost of high voltage transmission is comparatively low, compared to all other costs constituting consumer electricity bills. In the UK, transmission costs are about 0.2 p per kWh compared to a delivered domestic price of around 10 p per kWh.[45]

The level of capital expenditure in the electric power T&D equipment market was estimated to be $128.9 bn in 2011.[46]

Mainstream scientific evidence suggests that low-power, low-frequency, electromagnetic radiation associated with household currents and high transmission power lines does not constitute a short- or long-term health hazard.

Some studies failed to find any link between living near power lines and developing any sickness or diseases, such as cancer. A 1997 study reported no increased risk of cancer or illness from living near a transmission line.[47] Other studies, however, reported statistical correlations between various diseases and living or working near power lines. No adverse health effects have been substantiated for people not living close to power lines.[48]

The New York State Public Service Commission conducted a study[49] to evaluate potential health effects of electric fields. The study measured the electric field strength at the edge of an existing right-of-way on a 765 kV transmission line. The field strength was 1.6 kV/m, and became the interim maximum strength standard for new transmission lines in New York State. The opinion also limited the voltage of new transmission lines built in New York to 345 kV. On September 11, 1990, after a similar study of magnetic field strengths, the NYSPSC issued their Interim Policy Statement on Magnetic Fields. This policy established a magnetic field standard of 200 mG at the edge of the right-of-way using the winter-normal conductor rating. As a comparison with everyday items, a hair dryer or electric blanket produces a 100 mG – 500 mG magnetic field.[50][51]

Applications for a new transmission line typically include an analysis of electric and magnetic field levels at the edge of rights-of-way. Public utility commissions typically do not comment on health impacts.

Biological effects have been established for acute high level exposure to magnetic fields above 100 μT (1 G) (1,000 mG). In a residential setting, one study reported ""limited evidence of carcinogenicity in humans and less than sufficient evidence for carcinogenicity in experimental animals"", in particular, childhood leukemia, associated with average exposure to residential power-frequency magnetic field above 0.3 μT (3 mG) to 0.4 μT (4 mG). These levels exceed average residential power-frequency magnetic fields in homes, which are about 0.07 μT (0.7 mG) in Europe and 0.11 μT (1.1 mG) in North America.[52][53]

The Earth's natural geomagnetic field strength varies over the surface of the planet between 0.035 mT and 0.07 mT (35 μT – 70 μT or 350 mG – 700 mG) while the international standard for continuous exposure is set at 40 mT (400,000 mG or 400 G) for the general public.[52]

Tree growth regulators and herbicides may be used in transmission line right of ways,[54] which may have health effects.

In some countries where electric locomotives or electric multiple units run on low frequency AC power, separate single phase traction power networks are operated by the railways. Prime examples are countries such as Austria, Germany and Switzerland that utilize AC technology based on 16 2/3 Hz. Norway and Sweden also use this frequency but use conversion from the 50 Hz public supply; Sweden has a 16 2/3 Hz traction grid but only for part of the system.

High-temperature superconductors (HTS) promise to revolutionize power distribution by providing lossless transmission. The development of superconductors with transition temperatures higher than the boiling point of liquid nitrogen has made the concept of superconducting power lines commercially feasible, at least for high-load applications.[55] It has been estimated that waste would be halved using this method, since the necessary refrigeration equipment would consume about half the power saved by the elimination of resistive losses. Companies such as Consolidated Edison and American Superconductor began commercial production of such systems in 2007.[56]

Superconducting cables are particularly suited to high load density areas such as the business district of large cities, where purchase of an easement for cables is costly.[57]

Single-wire earth return (SWER) or single-wire ground return is a single-wire transmission line for supplying single-phase electrical power to remote areas at low cost. It is principally used for rural electrification, but also finds use for larger isolated loads such as water pumps. Single-wire earth return is also used for HVDC over submarine power cables.

Both Nikola Tesla and Hidetsugu Yagi attempted to devise systems for large scale wireless power transmission in the late 1800s and early 1900s, without commercial success.

In November 2009, LaserMotive won the NASA 2009 Power Beaming Challenge by powering a cable climber 1 km vertically using a ground-based laser transmitter. The system produced up to 1 kW of power at the receiver end. In August 2010, NASA contracted with private companies to pursue the design of laser power beaming systems to power low earth orbit satellites and to launch rockets using laser power beams.

Wireless power transmission has been studied for transmission of power from solar power satellites to the earth. A high power array of microwave or laser transmitters would beam power to a rectenna. Major engineering and economic challenges face any solar power satellite project.

The federal government of the United States stated that the American power grid was susceptible to cyber-warfare.[63][64] The United States Department of Homeland Security works with industry to identify vulnerabilities and to help industry enhance the security of control system networks.[65]

In June 2019, Russia conceded that it was ""possible"" its electrical grid is under cyber-attack by the United States.[66] The New York Times reported that American hackers from the United States Cyber Command planted malware potentially capable of disrupting the Russian electrical grid.[67]
"
Cable Installation Services,"Cable management refers to management of electrical or optical cable in a cabinet or an installation.  The term is used for products, workmanship or planning.  
Cables can easily become tangled, making them difficult to work with, sometimes resulting in devices accidentally becoming unplugged as one attempts to move a cable. Such cases are known as ""cable spaghetti"", and any kind of problem diagnosis and future updates to such enclosures could be very difficult.

Cable management both supports and contains cables during installation, and makes subsequent maintenance or changes to the cable system easier. Products such as cable trays, cable ladders, and cable baskets are used to support a cable through cabling routes.

The choice of cables is also important; for instance, ribbon cables used to connect Parallel ATA drives to the motherboard can disrupt the airflow inside of computers, making case fans less effective; most SATA cables are more compact and therefore do not have this problem.

Color-coding of cables is sometimes used to keep track of which is which. For instance, the wires coming out of ATX power supplies are color-coded by voltage.[1] Documenting and labeling cable runs, tying related cables together by cable ties, cable lacing, rubber bands or other means, running them through cable guides, and clipping or stapling them to walls are other common methods of keeping them organized. Above drop ceilings, hooks or trays are used to organize cables and protect them from electrical interference

Planning is especially crucial for cables such as thicknet that do not bend around corners easily and fiber optic which is very difficult to splice once cut.

Cable strain relief is a mechanical protection for flexible electrical cables, wires, conduits and pneumatic hoses. It is regulated by the European standard EN 62444 (formerly EN 50262.[2]).

With a strain relief component, the connection between a flexible electrical line and its connection port is protected against mechanical stress. Usually, the lines are fixed by clamping them into single cable clamps made of plastic or metal. Another possibility is to use so called cord grips which consist of weaved wire strands that put a grip around the cables.[3]

A more cable-friendly alternative is attaching the lines to special strain relief plates using common cable ties. In case of industrial applications these strain relief plates are also cost-effective because the packing density (meaning the possible number of lines to be fixed on one plate) is much higher than with common cable clamps which are normally designed for holding one single line.

Furthermore, most of the available cable clamps are not very flexible when it comes to routing lines with varying diameters. That causes higher acquisition and storage costs. The installation of the single cable clamps can take a lot of mounting time, depending on the laying length of the lines. Strain relief plates are therefore a more flexible solution which allows a parallel routing of several lines with varying diameters.

Strain relief is often required for terminated electrical lines that are plugged into sockets or ports to prevent unplugging or accidentally ripping out of the connector. At which point the lines have to be strain relieved depends on the application. For PROFINET, for example, which is used in automation it is recommended to set the strain relief component approx. 1 m / 3.5 ft from the connection point.[4]

Strain relief components are also used in applications where cables, conduits and hoses are exposed to constant dynamic stress (cable carriers / drag chains).

Generally, one end of a cable is terminated in the data cabinet. The other end of a cable ends at the desk. The cable management needs at either end are different (see also: Structured cabling).

Buildings and office furniture are often designed with cable management in mind; for instance, desks sometimes have holes to pass cables, and dropped ceilings, raised floors and in-floor cellular raceway systems provide easy access. Some cables have requirements for minimum bending radius or proximity to other cables, particularly power cables, to avoid crosstalk or interference. Power cables often need to be grouped separately and suitably apart from data cables, and only cross at right angles which minimizes electromagnetic interference.

The organized routing of cables inside the computer case allows for optimal airflow and cooling. Good cable management also makes working inside a computer much easier by providing safer hardware installation, repair, or removal.  Some PC mod enthusiasts showcase the internal components of their systems with a window mod, which displays the aesthetics of internal cabling as well as the skills and wealth of the modder.

The IT industry needs data cables to be added, moved, or removed many times during the life of the installation. It is usual practice to install ""fixed cables"" between cabling closets or cabinets. These cables are contained in cable trays etc., and are terminated at each end onto patch panels in the communications cabinet or outlets at the desktop. The circuits are then interconnected to the final destination using patch cables.

Software, such as data center infrastructure management (DCIM), is sometimes used to manage cabling for large IT infrastructures.

Because large IT infrastructures often encompass vast networks of cables — all of which need to be serviced, removed, added, and so on throughout an installation lifecycle — cable planning is a necessity. 

Different methods of cable planning may be employed, depending upon the level of detail required for proper management. Spreadsheet software can be used for this purpose, but there is often a need to visually organize information that goes beyond the capabilities of such general-purpose software. For proper visualization of cabling, companies may opt to use a cable management software package.

In hospital situations, cable management can be critical to preventing medical mistakes. In these settings, cable management includes tubes and hoses used for liquids and gases used in healthcare, along with electrical and other cables. Emergency room nurse manager Pat Gabriel said, ""My wish is that we could somehow not have spaghetti on the bed. When you look at all those wires and those IVs, it's just spaghetti"".[5]

Cabling in healthcare facilities must be grounded, shielded and routed in accordance with life safety codes to minimize interference with medical equipment.

Cables are managed by five methods in commercial buildings:

Cord concealers (also called cord protectors) are commonly used in offices to prevent accidents while protecting the cord and the appliance its attached to. Office furniture can sometimes have built in cable management solutions.

Cable management is particularly important in powered equipment which must move large distances while tethered to a power source and control cabling. There are several common methods of cable management.

With a suspended sliding coil, the cables are coiled like a spring, with each loop of the coil attached to a sliding shoe on a track. As the cabling is paid out, the shoes slide individually along the track and the coils expand. When sliding the other direction, the coils fold back together into a compact spiral. This is also referred to as a festoon.

Folded linear cable uses either a flexible backbone shell, or a flat cable folded into an arc along its long axis. This style of cabling is very common in computer printers to connect the printhead to the circuitry, but is also used in very large linear moving gantries. The cables are flexed only in a small region in a tight radius and so need to be very flexible.
"
Elevator Installation Services,"

An elevator (American English) or lift (Commonwealth English) is a machine that vertically transports people or freight between levels. They are typically powered by electric motors that drive traction cables and counterweight systems such as a hoist, although some pump hydraulic fluid to raise a cylindrical piston like a jack.


Elevators are used in agriculture and manufacturing to lift materials. There are various types, like chain and bucket elevators, grain augers, and hay elevators. Modern buildings often have elevators to ensure accessibility, especially where ramps aren't feasible. High-speed elevators are common in skyscrapers. Some elevators can even move horizontally.[1]
The earliest known reference to an elevator is in the works of the Roman architect Vitruvius, who reported that Archimedes (c. 287 BC – c. 212 BC) built his first elevator probably in 236 BC.[2] Sources from later periods mention elevators as cabs on a hemp rope, powered by people or animals.

The Roman Colosseum, completed in 80 AD, had roughly 25 elevators that were used for raising animals up to the floor.[3] Each elevator could carry about 600 pounds (270 kg) (roughly the weight of two lions) 23 feet (7.0 m) up when powered by up to eight men.[4] In 1000, the Book of Secrets by Ibn Khalaf al-Muradi in Islamic Spain described the use of an elevator-like lifting device to raise a large battering ram to destroy a fortress.[5]

In the 17th century, prototypes of elevators were installed in the palace buildings of England and France. Louis XV of France had a so-called 'flying chair' built for one of his mistresses at the Château de Versailles in 1743.[6]

Ancient and medieval elevators used drive systems based on hoists and windlasses. The invention of a system based on the screw drive was perhaps the most important step in elevator technology since ancient times, leading to the creation of modern passenger elevators. The first screw-drive elevator was built by Ivan Kulibin and installed in the Winter Palace in 1793, although there may have been an earlier design by Leonardo da Vinci.[7] Several years later, another of Kulibin's elevators was installed in the Arkhangelskoye near Moscow.

The development of elevators was led by the need for movement of raw materials, including coal and lumber, from hillsides. The technology developed by these industries, and the introduction of steel beam construction, worked together to provide the passenger and freight elevators in use today. Starting in coal mines, elevators in the mid-19th century operated with steam power, and were used for moving goods in bulk in mines and factories. These devices were soon applied to a diverse set of purposes. In 1823, Burton and Homer, two architects in London, built and operated a novel tourist attraction which they called the ""ascending room"", which elevated customers to a considerable height in the center of London, providing a panoramic view.[8]

Early, crude steam-driven elevators were refined in the ensuing decade. In 1835, an innovative elevator, the Teagle, was developed by the company Frost and Stutt in England. It was belt-driven and used a counterweight for extra lifting ability.[9]

In 1845, Neapolitan architect Gaetano Genovese installed the ""Flying chair"", an elevator ahead of its time in the Royal Palace of Caserta. It was covered with chestnut wood outside and with maple wood inside. It included a light, two benches, and a hand-operated signal, and could be activated from the outside, without any effort by the occupants. Traction was controlled by a motor mechanic utilizing a system of toothed wheels. A safety system was designed to take effect if the cords broke, consisting of a beam pushed outwards by a steel spring.

The hydraulic crane was invented by Sir William Armstrong in 1846, primarily for use at the Tyneside docks for loading cargo. They quickly supplanted the earlier steam-driven elevators, exploiting Pascal's law to provide much greater force. A water pump supplied a variable level of water pressure to a plunger encased inside a vertical cylinder, allowing the platform, carrying a heavy load, to be raised and lowered. Counterweights and balances were also used to increase lifting power.

Henry Waterman of New York is credited with inventing the ""standing rope control"" for an elevator in 1850.[10]

In 1852, Elisha Otis introduced the safety elevator, which prevented the fall of the cab if the cable broke. He demonstrated it at the New York exposition in the Crystal Palace in a dramatic, death-defying presentation in 1854,[10][11] and the first such passenger elevator was installed at 488 Broadway in New York City on 23 March 1857.

The first elevator shaft preceded the first elevator by four years. Construction for Peter Cooper's Cooper Union Foundation building in New York began in 1853. An elevator shaft was included in the design because Cooper was confident that a safe passenger elevator would soon be invented.[12] The shaft was cylindrical because Cooper thought it was the most efficient design.[13] Otis later designed a special elevator for the building.

Peter Ellis, an English architect, installed the first elevators that could be described as paternoster elevators in Oriel Chambers in Liverpool in 1868.[14]

The Equitable Life Building, completed in 1870 in New York City, is thought to be the first office building with passenger elevators.[15]

In 1872, American inventor James Wayland patented a novel method of securing elevator shafts with doors that are automatically opened and closed as the elevator car approaches and leaves them.[16]

In 1874, J. W. Meaker patented a method permitting elevator doors to open and close safely.[17]

The first electric elevator was built by Werner von Siemens in 1880 in Germany.[18] Inventor Anton Freissler further developed von Siemens' ideas and created a successful elevator enterprise in Austria-Hungary. The safety and speed of electric elevators were significantly enhanced by Frank Sprague, who added floor control, automatic operation, acceleration control, and further safety devices. His elevator ran faster and with larger loads than hydraulic or steam elevators. 584 of Sprague's elevators were installed before he sold his company to the Otis Elevator Company in 1895. Sprague also developed the idea and technology for multiple elevators in a single shaft.

In 1871, when hydraulic power was a well established technology, Edward B. Ellington founded Wharves and Warehouses Steam Power and Hydraulic Pressure Company, which became the London Hydraulic Power Company in 1883. It constructed a network of high-pressure mains on both sides of the Thames which ultimately extended 184 miles (296 km) and powered some 8,000 machines, predominantly elevators and cranes.[19]

Schuyler Wheeler patented his electric elevator design in 1883.[20][21][22]

In 1884, American inventor D. Humphreys of Norfolk, Virginia, patented an elevator with automatic doors that closed off the elevator shaft when the car was not being entered or exited.[23][24]

In 1887, American inventor Alexander Miles of Duluth, Minnesota, patented an elevator with automatic doors that closed off the elevator shaft when the car was not being entered or exited.

In 1891, American inventors Joseph Kelly and William L. Woods co-patented a novel way to guard elevator shafts against accident, by way of hatches that would automatically open and close as the car passed through them.[25]

The first elevator in India was installed at the Raj Bhavan in Kolkata by Otis in 1892.[26]

By 1900, completely automated elevators were available, but passengers were reluctant to use them. Their adoption was aided by a 1945 elevator operator strike in New York City, and the addition of an emergency stop button, emergency telephone, and a soothing explanatory automated voice.[27]

In 2000, the first vacuum elevator was offered commercially in Argentina.[28]

Some people argue that elevators began as simple rope or chain hoists (see Traction elevators below). An elevator is essentially a platform that is either pulled or pushed up by mechanical means. A modern-day elevator consists of a cab (also called a ""cabin"", ""cage"", ""carriage"" or ""car"") mounted on a platform within an enclosed space called a shaft or sometimes a ""hoistway"". In the past, elevator drive mechanisms were powered by steam and water hydraulic pistons or by hand. In a ""traction"" elevator, cars are pulled up by means of rolling steel ropes over a deeply grooved pulley, commonly called a sheave in the industry. The weight of the car is balanced by a counterweight. Oftentimes two elevators (or sometimes three) are built so that their cars always move synchronously in opposite directions, and are each other's counterweight.

The friction between the ropes and the pulley furnishes the traction which gives this type of elevator its name.

Hydraulic elevators use the principles of hydraulics (in the sense of hydraulic power) to pressurize an above-ground or in-ground piston to raise and lower the car (see Hydraulic elevators below). Roped hydraulics use a combination of both ropes and hydraulic power to raise and lower cars. Recent innovations include permanent magnet motors, machine room-less rail mounted gearless machines, and microprocessor controls.

The technology used in new installations depends on a variety of factors. Hydraulic elevators are cheaper, but installing cylinders greater than a certain length becomes impractical for very-high lift hoistways. For buildings of much over seven floors, traction elevators must be employed instead. Hydraulic elevators are usually slower than traction elevators.

Elevators are a candidate for mass customization. There are economies to be made from mass production of the components, but each building comes with its own requirements like different number of floors, dimensions of the well and usage patterns.

Elevator doors prevent riders from falling into, entering, or tampering with anything in the shaft. The most common configuration is to have two panels that meet in the middle and slide open laterally. These are known as ""center-opening"". In a cascading telescopic configuration (potentially allowing wider entryways within limited space), the doors roll on independent tracks so that while open, they are tucked behind one another, and while closed, they form cascading layers on one side. This can be configured so that two sets of such cascading doors operate like the center opening doors described above, allowing for a very wide elevator cab. In less expensive installations the elevator can also use one large ""slab"" door: a single panel door the width of the doorway that opens to the left or right laterally. These are known as ""single slide"" doors. Some buildings have elevators with the single door on the shaftway, and double cascading doors on the cab.

Elevators that do not require separate machine rooms are designed so that most of their power and control components fit within the hoistway (the shaft containing the elevator car), and a small cabinet houses the controller. The equipment is otherwise similar to that of a normal traction or hole-less hydraulic elevator. The world's first machine-room-less elevator, the Kone MonoSpace, was introduced in the year 1996, by Kone. Compared to traditional elevators, it:

Its disadvantage was that it could be harder, and significantly more dangerous, to service and maintain.

Double-decker elevators are traction elevators with cars that have an upper and lower deck. Both decks, which can serve a floor at the same time, are usually driven by the same motor.[31] The system increases efficiency in high-rise buildings, and saves space so additional shafts and cars are not required.

In 2003, TK Elevator invented a system called TWIN, with two elevator cars independently running in one shaft.[32]

In 1901, consulting engineer Charles G. Darrach (1846–1927) proposed the first formula to determine elevator service.[33]

In 1908, Reginald P. Bolton published the first book devoted to this subject, Elevator Service.[34] The summation of his work was a massive fold-out chart (placed at the back of his book) that allowed users to determine the number of express and local elevators needed for a given building to meet a desired interval of service.

In 1912, commercial engineer Edmund F. Tweedy and electrical engineer Arthur Williams co-authored a book titled Commercial Engineering for Central Stations.[35] He followed Bolton's lead and developed a ""Chart for determining the number and size of elevators required for office buildings of a given total occupied floor area"".

In 1920, Howard B. Cook presented a paper titled ""Passenger Elevator Service"".[36] This paper marked the first time a member of the elevator industry offered a mathematical means of determining elevator service. His formula determined the round trip time (RTT) by finding the single trip time, doubling it, and adding 10 seconds.

In 1923, Bassett Jones published an article titled ""The Probable Number of Stops Made by an Elevator"".[37] He based his equations on the theory of probabilities and found a reasonably accurate method of calculating the average stop count. The equation in this article assumed a consistent population on every floor.

He went on to write an updated version of his equations in 1926 which accounted for variable population on each floor.[38] Jones credited David Lindquist for the development of the equation but provides no indication as to when it was first proposed.

Although the equations were there, elevator traffic analysis was still a very specialist task that could only be done by world experts. That was until 1967 when Strakosch wrote an eight step method for finding the efficiency of a system in ""Vertical transportation: Elevators and Escalators"".[39]

In 1975, Barney and Dos Santos developed and published the ""Round Trip Time (RTT) formula"", which followed Strakosch's work.[40] This was the first formulized mathematical model and is the simplest form that is still used by traffic analyzers today.

Modification and improvements have been made to this equation over the years, most significantly in 2000 when Peters published ""Improvements to the Up Peak Round Trip Time Calculation""[41] which improved the accuracy of the flight time calculation, making allowances for short elevator journeys when the car does not reach maximum rated speed or acceleration, and added the functionality of express zones. This equation is now referred to as the 'Up peak Calculation'[42] as it uses the assumption that all the passengers are coming into the building from the ground floor (incoming traffic) and that there are no passengers traveling from a higher floor to the ground floor (outgoing traffic) and no passengers traveling from one internal floor to another (interfloor traffic). This model works well if a building is at its most busy first thing in the morning; however, in more complicated elevator systems, this model does not work.

In 1990, Peters published a paper titled ""Lift Traffic Analysis:  Formulae for the General Case""[43] in which he developed a new formula which would account for mixed traffic patterns as well as accounting for passenger bunching using Poisson approximation. This new General Analysis equation enabled much more complex systems to be analyzed however the equations had now become so complex that it was almost impossible to do manually and it became necessary to use software to run the calculations. The GA formula was extended even further in 1996 to account for double deck elevators.[44]

RTT calculations establish an elevator system's handling capacity by using a set of repeatable calculations which, for a given set of inputs, always produce the same answer. It works well for simple systems; but as systems get more complex, the calculations are harder to develop and implement. For very complex systems, the solution is to simulate the building.[45]

In this method, a virtual version of a building is created on a computer, modeling passengers and elevators as realistically as possible, and random numbers are used to model probability rather than mathematical equations and percentage probability.

Dispatcher-based simulation has had major improvements over the years, but the principle remains the same. The most widely used simulator, Elevate, was first showcased in 1998 as Elevate Lite.[46]

Although it is currently the most accurate method of modeling an elevator system, the method does have drawbacks. Unlike calculations, it does not find a RTT value because it does not run standard round trips; thus it does not conform with standardized elevator-traffic analysis methodology, and cannot be used to find values such as average interval; instead, it is generally used to find the average waiting time.

At the first Elevator and Escalator symposium in 2011, Al-Sharif proposed an alternative form of simulation[47] that modeled a car's single round trip before restarting and running again. This method is still capable of modeling complex systems, and also conforms with standard methodology by producing an RTT value. The model was improved further in 2018 when Al-Sharif demonstrated a way to reintroduce a dispatcher-like function which can model destination control systems.[48]

While this does successfully remove simulation's major drawback, it is not quite as accurate as dispatcher-based simulations given its simplifications and non-continual nature. The Monte Carlo method also requires passenger count as an input, rather than passengers per second, in other methodologies.

Elevators can be rope dependent or rope-free.[49] There are at least four means of moving an elevator:

Geared traction machines are driven by AC or DC electric motors. Geared machines use worm gears to control mechanical movement of elevator cars by ""rolling"" steel hoist ropes over a drive sheave which is attached to a gearbox driven by a high-speed motor. These machines are generally the best option for basement or overhead traction use for speeds up to 3 m/s (500 ft/min).[50]

Historically, AC motors were used for single or double-speed elevator machines on the grounds of cost and lower usage applications where car speed and passenger comfort were less of an issue, but for higher speed, larger capacity elevators, the need for infinitely variable speed control over the traction machine becomes an issue. Therefore, DC machines powered by an AC/DC motor generator were the preferred solution. The MG set also typically powered the relay controller of the elevator, which has the added advantage of electrically isolating the elevators from the rest of a building's electrical system, thus eliminating the transient power spikes in the building's electrical supply caused by the motors starting and stopping (causing lighting to dim every time the elevators are used for example), as well as interference to other electrical equipment caused by the arcing of the relay contactors in the control system.

The widespread availability of variable frequency AC drives has allowed AC motors to be used universally, bringing with it the advantages of the older motor-generator, DC-based systems, without the penalties in terms of efficiency and complexity. The older MG-based installations are gradually being replaced in older buildings due to their poor energy efficiency.

Gearless traction machines are low-speed (low-RPM), high-torque electric motors powered either by AC or DC. In this case, the drive sheave is directly attached to the end of the motor. Gearless traction elevators can reach speeds of up to 20 m/s (4,000 ft/min), A brake is mounted between the motor and gearbox or between the motor and drive sheave or at the end of the drive sheave to hold the elevator stationary at a floor. This brake is usually an external drum type and is actuated by spring force and held open electrically; a power failure will cause the brake to engage and prevent the elevator from falling (see inherent safety and safety engineering). But it can also be some form of disc type like one or more calipers over a disc in one end of the motor shaft or drive sheave which is used in high speed, high rise and large capacity elevators with machine rooms (an exception is the Kone MonoSpace's EcoDisc which is not high speed, high rise and large capacity and is machine room less but it uses the same design as is a thinner version of a conventional gearless traction machine) for braking power, compactness and redundancy (assuming there are at least two calipers on the disc), or one or more disc brakes with a single caliper at one end of the motor shaft or drive sheave which is used in machine room less elevators for compactness, braking power, and redundancy (assuming there are two or more brakes).

In each case, steel or kevlar cables are attached to a hitch plate on top of the cab or may be ""underslung"" below a cab, and then looped over the drive sheave to a counterweight attached to the opposite end of the cables which reduces the amount of power needed to move the cab. The counterweight is located in the hoist-way and is carried along a separate railway system; as the car goes up, the counterweight goes down, and vice versa. This action is powered by the traction machine which is directed by the controller, typically a relay logic or computerized device that directs starting, acceleration, deceleration and stopping of the elevator cab. The weight of the counterweight is typically equal to the weight of the elevator cab plus 40–50% of the capacity of the elevator. The grooves in the drive sheave are specially designed to prevent the cables from slipping. ""Traction"" is provided to the ropes by the grip of the grooves in the sheave, thereby the name. As the ropes age and the traction grooves wear, some traction is lost and the ropes must be replaced and the sheave repaired or replaced. Sheave and rope wear may be significantly reduced by ensuring that all ropes have equal tension, thus sharing the load evenly. Rope tension equalization may be achieved using a rope tension gauge, and is a simple way to extend the lifetime of the sheaves and ropes.

Elevators with more than 30 m (98 ft) of travel have a system called compensation. This is a separate set of cables or a chain attached to the bottom of the counterweight and the bottom of the elevator cab. This makes it easier to control the elevator, as it compensates for the differing weight of cable between the hoist and the cab. If the elevator cab is at the top of the hoist-way, there is a short length of hoist cable above the car and a long length of compensating cable below the car and vice versa for the counterweight. If the compensation system uses cables, there will be an additional sheave in the pit below the elevator, to guide the cables. If the compensation system uses chains, the chain is guided by a bar mounted between the counterweight tracks.

Another energy-saving improvement is the regenerative drive,[51] which works analogously to regenerative braking in vehicles, using the elevator's electric motor as a generator to capture some of the gravitational potential energy of descent of a full cab (heavier than its counterweight) or ascent of an empty cab (lighter than its counterweight) and return it to the building's electrical system.

The low mechanical complexity of hydraulic elevators in comparison to traction elevators makes them ideal for low-rise, low-traffic installations. They are less energy-efficient as the pump works against gravity to push the car and its passengers upwards; this energy is lost when the car descends on its own weight. The high current draw of the pump when starting up also places higher demands on a building's electrical system. There are also environmental concerns should the lifting cylinder leak fluid into the ground,[52] hence the development of holeless hydraulic elevators, which also eliminate the need for a relatively deep hole in the bottom of the elevator shaft.[citation needed]

Cable-free elevators using electromagnetic propulsion, capable of moving both vertically and horizontally, have been developed by German engineering firm Thyssen Krupp for use in high rise, high density buildings.[53][54]

A climbing elevator is a self-ascending elevator with its own propulsion. The propulsion can be done by an electric or a combustion engine. Climbing elevators are used in guyed masts or towers, in order to make easy access to parts of these constructions, such as flight safety lamps for maintenance. An example would be the moonlight towers in Austin, Texas, where the elevator holds only one person and equipment for maintenance. The Glasgow Tower—an observation tower in Glasgow, Scotland—also makes use of two climbing elevators. Temporary climbing elevators are commonly used in the construction of new high-rise buildings to move materials and personnel before the building's permanent elevator system is installed, at which point the climbing elevators are dismantled.

An elevator of this kind uses a vacuum on top of the cab and a valve on the top of the ""shaft"" to move the cab upwards and closes the valve in order to keep the cab at the same level. A diaphragm or a piston is used as a ""brake"", if there is a sudden increase in pressure above the cab. To go down, it opens the valve so that the air can pressurize the top of the ""shaft"", allowing the cab to go down by its own weight. This also means that in case of a power failure, the cab will automatically go down. The ""shaft"" is made of acrylic, and is always round due to the shape of the vacuum pump. To keep the air inside of the cab, rubber seals are used. Due to technical limitations, these elevators have a low capacity, they usually allow 1–3 passengers and up to 525 pounds (238 kg).[55]

In the first half of the twentieth century, almost all elevators had no automatic positioning of the floor on which the cab would stop. Some of the older freight elevators were controlled by switches operated by pulling on adjacent ropes. In general, most elevators before WWII were manually controlled by elevator operators using a rheostat connected to the motor. This rheostat (see picture) was enclosed within a cylindrical container about the size and shape of a cake. This was mounted upright or sideways on the cab wall and operated via a projecting handle, which was able to slide around the top half of the cylinder.

The elevator motor was located at the top of the shaft or beside the bottom of the shaft. Pushing the handle forward would cause the cab to rise; backwards would make it sink. The harder the pressure, the faster the elevator would move. The handle also served as a dead man switch: if the operator let go of the handle, it would return to its upright position, causing the elevator cab to stop. In time, safety interlocks would ensure that the inner and outer doors were closed before the elevator was allowed to move.

This lever would allow some control over the energy supplied to the motor and so enabled the elevator to be accurately positioned—if the operator was sufficiently skilled. More typically, the operator would have to ""jog"" the control, moving the cab in small increments until the elevator was reasonably close to the landing point. Then the operator would direct the outgoing and incoming passengers to ""watch the step"".

The Otis Autotronic system of the early 1950s brought the earliest predictive systems which could anticipate traffic patterns within a building to deploy elevator movement in the most efficient manner. Relay-controlled elevator systems remained common until the 1980s; they were gradually replaced with solid-state systems, and microprocessor-based controls are now the industry standard. Most older, manually-operated elevators have been retrofitted with automatic or semi-automatic controls.

A typical modern passenger elevator will have:

The operation of the door open button is transparent, immediately opening and holding the door, typically until a timeout occurs and the door closes. The operation of the door close button is less transparent, and it often appears to do nothing, leading to frequent but incorrect[56][better source needed] reports that the door close button is a placebo button: either not wired up at all, or inactive in normal service. On many older elevators, if one is present, the door close button is functional because the elevator is not ADA compliant and/or it does not have a fire service mode.[57][58][59][60] Working door open and door close buttons are required by code in many jurisdictions, including the United States, specifically for emergency operation: in independent mode, the door open and door close buttons are used to manually open or close the door.[56][61] Beyond this, programming varies significantly, with some door close buttons immediately closing the door, but in other cases being delayed by an overall timeout, so the door cannot be closed until a few seconds after opening. In this case (hastening normal closure), the door close button has no effect. However, the door close button will cause a hall call to be ignored (so the door will not reopen), and once the timeout has expired, the door close will immediately close the door, for example, to cancel a door open push. The minimum timeout for automatic door closing in the US is 5 seconds,[62] which is a noticeable delay if not over-ridden.

Some elevators may have one or more of the following:

An audible signal button, labeled ""S"": in the US, for elevators installed between 1991 and 2012 (initial passage of ADA and coming into force of 2010 revision), a button which if pushed, sounds an audible signal as each floor is passed, to assist visually impaired passengers. This button is no longer used on new elevators, where the sound is normally obligatory.[64][65]

Other controls, which are not available for the public (either because they are key switches, or because they are kept behind a locked panel), include:

Elevators are typically controlled from the outside by a call box, which has up and down buttons, at each stop. When pressed at a certain floor, the button (also known as a ""hall call"" button) calls the elevator to pick up more passengers. If the particular elevator is currently serving traffic in a certain direction, it will only answer calls in the same direction unless there are no more calls beyond that floor.

In a group of two or more elevators, the call buttons may be linked to a central dispatch computer, such that they illuminate and cancel together. This is done to ensure that only one car is called at one time.

Key switches may be installed on the ground floor so that the elevator can be remotely switched on or off from the outside.

In destination control systems, one selects the intended destination floor (in lieu of pressing ""up"" or ""down"") and is then notified which elevator will serve their request.

To distinguish between floors, the different landings are given numbers and sometimes letters. See the above article for more information.

The elevator algorithm, a simple algorithm by which a single elevator can decide where to stop, is summarized as follows:

The elevator algorithm has found an application in computer operating systems as an algorithm for scheduling hard disk requests. Modern elevators use more complex heuristic algorithms to decide which request to service next. In taller buildings with high traffic, such as the New York Marriott Marquis or the Burj Khalifa, the destination dispatch algorithm is used to group passengers going to similar floors, maximizing load by up to 25%.

Some skyscraper buildings and other types of installation feature a destination operating panel where a passenger registers their floor calls before entering the car. The system lets them know which car to wait for, instead of everyone boarding the next car. In this way, travel time is reduced as the elevator makes fewer stops for individual passengers, and the computer distributes adjacent stops to different cars in the bank. Although travel time is reduced, passenger waiting times may be longer as they will not necessarily be allocated the next car to depart. During the down peak period the benefit of destination control will be limited as passengers have a common destination.

It can also improve accessibility, as a mobility-impaired passenger can move to their designated car in advance.

Inside the elevator there is no call button to push, or the buttons are there but they cannot be pushed—except door opening and alarm button—they only indicate stopping floors.

The idea of destination control was originally conceived by Leo Port from Sydney in 1961,[66] but at that time elevator controllers were implemented in relays and were unable to optimize the performance of destination control allocations.

The system was first pioneered by Schindler Elevator in 1992 as the Miconic 10. Manufacturers of such systems claim that average traveling time can be reduced by up to 30%.[67]

However, performance enhancements cannot be generalized as the benefits and limitations of the system are dependent on many factors.[68] One problem is that the system is subject to gaming. Sometimes, one person enters the destination for a large group of people going to the same floor. The dispatching algorithm is usually unable to completely cater for the variation, and latecomers may find the elevator they are assigned to is already full. Also, occasionally, one person may press the floor multiple times. This is common with up/down buttons when people believe this to be an effective way to hurry elevators. However, this will make the computer think multiple people are waiting and will allocate empty cars to serve this one person.

To prevent this problem, in one implementation of destination control, every user is given an RFID card, for identification and tracking, so that the system knows every user call and can cancel the first call if the passenger decides to travel to another destination, preventing empty calls. The newest invention knows even where people are located and how many on which floor because of their identification, either for the purposes of evacuating the building or for security reasons.[69] Another way to prevent this issue is to treat everyone traveling from one floor to another as one group and to allocate only one car for that group.

The same destination scheduling concept can also be applied to public transit such as in group rapid transit.

The anti-crime protection (ACP) feature will force each car to stop at a pre-defined landing and open its doors. This allows a security guard or a receptionist at the landing to visually inspect the passengers. The car stops at this landing as it passes to serve further demand.

During up-peak mode (also called moderate incoming traffic), elevator cars in a group are recalled to the lobby to provide expeditious service to passengers arriving at the building, most typically in the morning as people arrive for work or at the conclusion of a lunch-time period when people are going back to work. Elevators are dispatched one-by-one when they reach a pre-determined passenger load, or when they have had their doors opened for a certain period of time. The next elevator to be dispatched usually has its hall lantern or a ""this car leaving next"" sign illuminated to encourage passengers to make maximum use of the available elevator system capacity. Some elevator banks are programmed so that at least one car will always return to the lobby floor and park whenever it becomes free.

The commencement of up-peak may be triggered by a time clock, by the departure of a certain number of fully loaded cars leaving the lobby within a given time period, or by a switch manually operated by a building attendant.

During down-peak mode, elevator cars in a group are sent away from the lobby towards the highest floor served, after which they commence running down the floors in response to hall calls placed by passengers wishing to leave the building. This allows the elevator system to provide maximum passenger handling capacity for people leaving the building.

The commencement of down-peak may be triggered by a time clock, by the arrival of a certain number of fully loaded cars at the lobby within a given time period, or by a switch manually operated by a building attendant.

In areas with large populations of observant Jews or in facilities catering to Jews, one may find a ""Sabbath elevator"". In this mode, an elevator will stop automatically at every floor, allowing people to step on and off without having to push any buttons. This prevents violation of the Sabbath prohibition against operating electrical devices when Sabbath is in effect for those who observe this ritual.[70]

However, Sabbath mode has the side effect of using considerable amounts of energy, running the elevator car sequentially up and down every floor of a building, repeatedly servicing floors where it is not needed. For a tall building with many floors, the car must move on a frequent enough basis so as to not cause undue delay for potential users that will not touch the controls as it opens the doors on every floor up the building.

Independent service or car preference is a special mode found on most elevators. It is activated by a key switch either inside the elevator itself or on a centralized control panel in the lobby. When an elevator is placed on this mode, it will no longer respond to hall calls. (In a bank of elevators, traffic is rerouted to the other elevators, while in a single elevator, the hall buttons are disabled). The elevator will remain parked on a floor with its doors open until a floor is selected and the door close button is held until the elevator starts to travel. Independent service is useful when transporting large goods or moving groups of people between certain floors.

Inspection service is designed to provide access to the hoistway and car top for inspection and maintenance purposes by qualified elevator mechanics. It is first activated by a key switch on the car operating panel usually labeled 'Inspection', 'Car Top', 'Access Enable' or 'HWENAB' (short for HoistWay access ENABled). When this switch is activated the elevator will come to a stop if moving, car calls will be canceled (and the buttons disabled), and hall calls will be assigned to other elevator cars in the group (or canceled in a single elevator configuration). The elevator can now only be moved by the corresponding 'Access' key switches, usually located at the highest (to access the top of the car) and lowest (to access the elevator pit) landings. The access key switches will allow the car to move at reduced inspection speed with the hoistway door open. This speed can range from anywhere up to 60% of normal operating speed on most controllers, and is usually defined by local safety codes.

Elevators have a car top inspection station that allows the car to be operated by a mechanic in order to move it through the hoistway. Generally, there are three buttons: UP, RUN, and DOWN. Both the RUN and a direction button must be held to move the car in that direction, and the elevator will stop moving as soon as the buttons are released. Most other elevators have an up/down toggle switch and a RUN button. The inspection panel also has standard power outlets for work lamps and powered tools.

Depending on the location of the elevator, fire service code will vary state to state and country to country. Fire service is usually split up into two modes: phase one and phase two. These are separate modes that the elevator can go into.

Phase one mode is activated by a corresponding smoke sensor, heat sensor, or manual key switch in the building. Once an alarm has been activated, the elevator will automatically go into phase one. The elevator will wait an amount of time, then proceed to go into nudging mode to tell everyone the elevator is leaving the floor. Once the elevator has left the floor, depending on where the alarm was set off, the elevator will go to the fire-recall floor. However, if the alarm was activated on the fire-recall floor, the elevator will have an alternate floor to recall to. When the elevator is recalled, it proceeds to the recall floor and stops with its doors open. The elevator will no longer respond to calls or move in any direction. Located on the fire-recall floor is a fire-service key switch. The fire-service key switch has the ability to turn fire service off, turn fire service on or bypass fire service. The only way to return the elevator to normal service is to switch it to bypass after the alarms have reset.

Phase-two mode can only be activated by a key switch located inside the elevator on the car operating panel. This mode was created for firefighters so that they may rescue people from a burning building. The phase-two key switch has three positions: off, on, and hold. By turning phase two on, the firefighter enables the car to move. However, like independent-service mode, the car will not respond to a car call unless the firefighter manually pushes and holds the door close button. Once the elevator gets to the desired floor it will not open its doors unless the firefighter holds the door open button. This is in case the floor is burning and the firefighter can feel the heat and knows not to open the door. The firefighter must hold the door open button until the door is completely opened. If for any reason the firefighter wishes to leave the elevator, they will use the hold position on the key switch to make sure the elevator remains at that floor. If the firefighter wishes to return to the recall floor, they simply turn the key off and close the doors.

In the UK and Europe the requirements for firefighters lifts are defined in the standard EN81-72.[citation needed]

Commonly found in hospitals, code-blue service allows an elevator to be summoned to any floor for use in an emergency situation. Each floor will have a code-blue recall key switch, and when activated, the elevator system will immediately select the elevator car that can respond the fastest, regardless of direction of travel and passenger load. Passengers inside the elevator will be notified with an alarm and indicator light to exit the elevator when the doors open.

Once the elevator arrives at the floor, it will park with its doors open and the car buttons will be disabled to prevent a passenger from taking control of the elevator. Medical personnel must then activate the code-blue key switch inside the car, select their floor and close the doors with the door close button. The elevator will then travel non-stop to the selected floor, and will remain in code-blue service until switched off in the car. Some hospital elevators will feature a 'hold' position on the code-blue key switch (similar to fire service) which allows the elevator to remain at a floor locked out of service until code blue is deactivated.

In the event of civil disturbance, insurrection, or rioting, management can prevent elevators from stopping at the lobby or parking areas, preventing undesired persons from using the elevators while still allowing the building tenants to use them within the rest of the building.

Many elevator installations now feature emergency power systems such as uninterruptible power supply (UPS) which allow elevator use in blackout situations and prevent people from becoming trapped in elevators. To be compliant with BS 9999 safety standards, a passenger lift being used in an emergency situation must have a secondary source of power.

Where a generator is being used as the secondary power supply in a hospital, a UPS must also be present to meet regulations stating that healthcare facilities must test their emergency generators under load at least once per month. During the test period only one supply of power is feeding the lift, in a blackout situation without a UPS, the lifts would not be operational.

When power is lost in a traction elevator system, all elevators will initially come to a halt. One by one, each car in the group will return to the lobby floor, open its doors, and shut down. People in the remaining elevators may see an indicator light or hear a voice announcement informing them that the elevator will return to the lobby shortly. Once all cars have successfully returned, the system will then automatically select one or more cars to be used for normal operations and these cars will return to service. The car(s) selected to run under emergency power can be manually over-ridden by a key or strip switch in the lobby. To help prevent entrapment, when the system detects that it is running low on power, it will bring the running cars to the lobby or nearest floor, open the doors, and shut down.

In hydraulic elevator systems, emergency power will lower the elevators to the lowest landing and open the doors to allow passengers to exit. The doors then close after an adjustable time period and the car remains unusable until reset, usually by cycling the elevator main power switch. Typically, due to the high current draw when starting the pump motor, hydraulic elevators are not run using standard emergency power systems. Buildings like hospitals and nursing homes usually size their emergency generators to accommodate this draw. However, the increasing use of current-limiting motor starters, commonly known as ""soft-start"" contactors, avoids much of this problem, and the current draw of the pump motor is less of a limiting concern.

Most elevators are built to provide about 30 to 40 years of service, as long as service intervals specified and periodic maintenance/inspections by the manufacturer are followed. As the elevator ages and equipment become increasingly difficult to find or replace, along with code changes and deteriorating ride performance, a complete overhaul of the elevator may be suggested to the building owners.

A typical modernization consists of controller equipment, electrical wiring and buttons, position indicators and direction arrows, hoist machines and motors (including door operators), and sometimes door hanger tracks. Rarely are car slings, rails, or other heavy structures changed. The cost of an elevator modernization can range greatly depending on which type of equipment is to be installed.

Modernization can greatly improve operational reliability by replacing electrical relays and contacts with solid-state electronics. Ride quality can be improved by replacing motor-generator-based drive designs with Variable-Voltage, Variable Frequency (V3F) drives, providing near-seamless acceleration and deceleration. Passenger safety is also improved by updating systems and equipment to conform to current codes.

On 26 February 2014, the European Union released their adoption of safety standards through a directive notification.[71]

Statistically speaking, traction elevators are extremely safe. Of the 20 to 30 elevator-related deaths each year, most of them are maintenance-related—for example, technicians leaning too far into the shaft or getting caught between moving parts, and most of the rest are attributed to other kinds of accidents, such as people stepping blindly through doors that open into empty shafts or being strangled by scarves caught in the doors.[72]  While it is possible (though extraordinarily unlikely) for an elevator's cable to snap, all elevators in the modern era have been fitted with several safety devices which prevent the elevator from simply free-falling and crashing. An elevator cab is typically borne by 2 to 6 (up to 12 or more in high rise installations) redundant hoist cables or belts, each of which is capable on its own of supporting the rated load of the elevator plus twenty-five percent more weight. In addition, there is a device which detects whether the elevator is descending faster than its maximum designed speed; if this happens, the device causes copper (or silicon nitride ceramic in high rise installations) brake shoes to clamp down along the vertical rails in the shaft, stopping the elevator quickly, but not so abruptly as to cause injury.  This device, called the governor, was invented by Elisha Graves Otis.[73] For example, in 2007 an elevator at a Seattle children's hospital experienced cable failure, causing it to free-fall until its governor engaged.[74] In addition, an oil/hydraulic or spring or polyurethane or telescopic oil/hydraulic buffer or a combination (depending on the travel height and travel speed) is installed at the bottom of the shaft (or in the bottom of the cab and sometimes also in the top of the cab or shaft) to somewhat cushion any impact.[72] Lethal accidents  do occur, however: in 1989, seven people were killed at a hospital in L'Hospitalet, Spain, when the pulleys connecting the cables to the elevator cabin broke loose and the safety mechanism failed to activate, causing the elevator to plunge seven stories to the ground.[75] A similar accident occurred in 2019 in Santos, Brazil, killing four.[76]

Past problems with hydraulic elevators include underground electrolytic destruction of the cylinder and bulkhead, pipe failures, and control failures. Single bulkhead cylinders, typically built prior to a 1972 ASME A17.1 Elevator Safety Code change requiring a second dished bulkhead, were subject to possible catastrophic failure. The code previously permitted only single-bottom hydraulic cylinders. In the event of a cylinder breach, the fluid loss results in uncontrolled down movement of the elevator. This creates two significant hazards: being subject to an impact at the bottom when the elevator stops suddenly and being in the entrance for a potential shear if the rider is partly in the elevator. Because it is impossible to verify the system at all times, the code requires periodic testing of the pressure capability. Another solution to protect against a cylinder blowout is to install a plunger gripping device. Two commercially available are known by the marketing names ""LifeJacket"" and ""HydroBrake"". The plunger gripper is a device which, in the event of an uncontrolled downward acceleration, nondestructively grips the plunger and stops the car. A device known as an overspeed or rupture valve is attached to the hydraulic inlet/outlet of the cylinder and is adjusted for a maximum flow rate. If a pipe or hose were to break (rupture), the flow rate of the rupture valve will surpass a set limit and mechanically stop the outlet flow of hydraulic fluid, thus stopping the plunger and the car in the down direction.

In addition to the safety concerns for older hydraulic elevators, there is risk of leaking hydraulic oil into the aquifer and causing potential environmental contamination. This has led to the introduction of PVC liners (casings) around hydraulic cylinders which can be monitored for integrity.

In the past decade, recent innovations in inverted hydraulic jacks have eliminated the costly process of drilling the ground to install a borehole jack. This also eliminates the threat of corrosion to the system and increases safety.

Safety testing of mine shaft elevator rails is routinely undertaken. The method involves destructive testing of a segment of the cable. The ends of the segment are frayed, then set in conical zinc molds. Each end of the segment is then secured in a large, hydraulic stretching machine. The segment is then placed under increasing load to the point of failure. Data about elasticity, load, and other factors is compiled and a report is produced. The report is then analyzed to determine whether or not the entire rail is safe to use.

A passenger elevator is designed to move people between a building's floors.

Passenger elevators capacity is related to the available floor space. Generally passenger elevators in buildings of eight floors or fewer are hydraulic or electric, which can reach speeds up to 1 m/s (200 ft/min) hydraulic and up to 3 m/s (500 ft/min) electric.[citation needed]

Sometimes passenger elevators are used as a city transport along with funiculars. For example, there is a three-station underground public elevator in Yalta, Ukraine, which takes passengers from the top of a hill above the Black Sea on which hotels are perched, to a tunnel located on the beach below. At Casco Viejo station in the Bilbao Metro, the elevator that provides access to the station from a hilltop neighborhood doubles as city transportation: the station's ticket barriers are set up in such a way that passengers can pay to reach the elevator from the entrance in the lower city, or vice versa. See also the Elevators for urban transport section.

Passenger elevators may be specialized for the service they perform, including: hospital emergency (code blue), front and rear entrances, a television in high-rise buildings, double-decker, and other uses. Cars may be ornate in their interior appearance, may have audio visual advertising, and may be provided with specialized recorded voice announcements. Elevators may also have loudspeakers in them to play calm, easy listening music. Such music is often referred to as elevator music.

An express elevator does not serve all floors. For example, it moves between the ground floor and a skylobby, or it moves from the ground floor or a skylobby to a range of floors, skipping floors in between.

Residential elevators may be small enough to only accommodate one person while some are large enough for more than a dozen. Wheelchair, or platform elevators, a specialized type of elevator designed to move a wheelchair 3.7 m (12 ft) or less, can often accommodate just one person in a wheelchair at a time with a load of 340 kg (750 lb).[77]

A freight elevator, or goods lift, is an elevator designed to carry goods, rather than passengers. Freight elevators are generally required to display a written notice in the car that the use by passengers is prohibited (though not necessarily illegal), though certain freight elevators allow dual use through the use of an inconspicuous riser. In order for an elevator to be legal to carry passengers in some jurisdictions it must have a solid inner door. Freight elevators are typically larger and capable of carrying heavier loads than a passenger elevator, generally from 2,300 to 4,500 kg (5,100 to 9,900 lb). Freight elevators may have manually operated doors, and often have rugged interior finishes to prevent damage while loading and unloading.[citation needed] In North America, it is common for freight elevators to have vertically opening doors the entire width of the cab.

A sidewalk elevator is a special type of freight elevator. Sidewalk elevators are used to move materials between a basement and a ground-level area, often the sidewalk just outside the building. They are controlled via an exterior switch and emerge from a metal trap door at ground level. Sidewalk elevator cars feature a uniquely shaped top that allows this door to open and close automatically.[78]

Stage lifts and orchestra lifts are specialized elevators, typically powered by hydraulics, that are used to raise and lower entire sections of a theater stage. For example, Radio City Music Hall has four such elevators: an orchestra lift that covers a large area of the stage, and three smaller lifts near the rear of the stage. In this case, the orchestra lift is powerful enough to raise an entire orchestra, or an entire cast of performers (including live elephants) up to stage level from below. There is a barrel on the background of the image of the left which can be used as a scale to represent the size of the mechanism

Vehicular elevators are used within buildings or areas with limited space (in place of ramps), generally to move cars into the parking garage or manufacturer's storage. Geared hydraulic chains (not unlike bicycle chains) generate lift for the platform and there are no counterweights. To accommodate building designs and improve accessibility, the platform may rotate so that the driver only has to drive forward. Most vehicle elevators have a weight capacity of 2 tons.

Rare examples of extra-heavy elevators for 20-ton lorries, and even for railcars (like one that was used at Dnipro Station of the Kyiv Metro) also occur.

In some smaller canals, boats and small ships can pass between different levels of a canal with a boat elevator rather than through a canal lock.

On aircraft carriers, elevators carry aircraft between the flight deck and the hangar deck for operations or repairs. These elevators are designed for much greater capacity than other elevators, up to 91,000 kg (200,000 lb) of aircraft and equipment. Smaller elevators lift munitions to the flight deck from magazines deep inside the ship.

On some passenger double-deck aircraft such as the Boeing 747 or other widebody aircraft, elevators transport flight attendants and food and beverage trolleys from lower deck galleys to upper passenger carrying decks.[79] Franklin Roosevelt had a retractable elevator installed on a Douglas C-54 Skymaster to allow him to board the aircraft in his wheelchair.[80]

The limited-use, limited-application (LU/LA) elevator is a special purpose passenger elevator used infrequently, and which is exempt from many commercial regulations and accommodations. For example, a LU/LA is primarily meant to be handicapped accessible, and there might only be room for a single wheelchair and a standing passenger. In North America, they are limited to a capacity of 1,400 pounds (640 kg) and travel of 25 feet (7.6 m).[81]

A residential elevator or home lift is often permitted to be of lower cost and complexity than full commercial elevators. They may have unique design characteristics suited for home furnishings, such as hinged wooden shaft-access doors rather than the typical metal sliding doors of commercial elevators. Construction may be less robust than in commercial designs with shorter maintenance periods, but safety systems such as locks on shaft access doors, fall arrestors, and emergency phones must still be present in the event of malfunction.

The American Society of Mechanical Engineers (ASME) has a specific section of Safety Code (ASME A17.1 Section 5.3) which addresses Residential Elevators. This section allows for different parameters to alleviate design complexity based on the limited use of a residential elevator by a specific user or user group. Section 5.3 of the ASME A17.1 Safety Code is for Private Residence Elevators, which does not include multi-family dwellings.[82]

Some types of residential elevators do not use a traditional elevator shaft, machine room, and elevator hoistway. This allows an elevator to be installed where a traditional elevator may not fit, and simplifies installation. The ASME board first approved machine-room-less systems in a revision of the ASME A17.1 in 2007. Machine-room-less elevators have been available commercially since the mid-1990s, however cost and overall size prevented their adoption to the residential elevator market until around 2010.[83]

Also, residential elevators are smaller than commercial elevators. The smallest passenger elevator is pneumatic, and it allows for only 1 person.[84] The smallest traction elevator allows for just 2 persons.[85]

Dumbwaiters are small freight elevators that are intended to carry food, books or other small freight loads rather than passengers. They often connect kitchens to rooms on other floors. They usually do not have the same safety features found in passenger elevators, like various ropes for redundancy. They have a lower capacity, and they can be up to 1 metre (3 ft) tall. Control panels at every stop mimic those found in passenger elevators, allowing calling, door control and floor selection.

A special type of elevator is the paternoster, a constantly moving chain of boxes. A similar concept, called the manlift or humanlift, moves only a small platform, which the rider mounts while using a handhold seen in multi-story industrial plants.

The scissor lift is yet another type of elevator. These are usually mobile work platforms that can be easily moved to where they are needed, but can also be installed where space for counter-weights, machine room and so forth is limited. The mechanism that makes them go up and down is like that of a scissor jack.

Rack-and-pinion elevator are powered by a motor driving a pinion gear. Because they can be installed on a building or structure's exterior and there is no machine room or hoistway required, they are the most used type of elevator for buildings under construction (to move materials and tools up and down).[86][87]

Material transport elevators generally consist of an inclined plane on which a conveyor belt runs. The conveyor often includes partitions to ensure that the material moves forward. These elevators are often used in industrial and agricultural applications. When such mechanisms (or spiral screws or pneumatic transport) are used to elevate grain for storage in large vertical silos, the entire structure is called a grain elevator. Belt elevators are often used in docks for loading loose materials such as coal, iron ore and grain into the holds of bulk carriers

There have occasionally been belt lifts for humans; these typically have steps about every 2 m (6 ft 6.7 in) along the length of the belt, which moves vertically, so that the passenger can stand on one step and hold on to the one above. These belts are sometimes used, for example, to carry the employees of parking garages, but are considered too dangerous for public use.

Before the widespread use of elevators, most residential buildings were limited to about seven stories. The wealthy lived on lower floors, while poorer residents—required to climb many flights of stairs—lived on higher floors. The elevator reversed this social stratification, exemplified by the modern penthouse suite.[88]

Early users of elevators sometimes reported nausea caused by abrupt stops while descending, and some users would use stairs to go down. In 1894, a Chicago physician documented ""elevator sickness"".[88]

Elevators necessitated new social protocols. When Nicholas II of Russia visited the Hotel Adlon in Berlin, his courtiers panicked about who would enter the elevator first, and who would press the buttons.[89] In Lifted: A Cultural History of the Elevator, author Andreas Bernard documents other social impacts caused by the modern elevator, including thriller movies about stuck elevators, casual encounters and sexual tension on elevators, the reduction of personal space and claustrophobia, and concerns about personal hygiene.[90]

Elevators may feature talking devices as an accessibility aid for the blind. Since the early 1980s, some elevators feature voice synthesis to announce floor landings, car direction and special messages to passengers.[91]

In addition to the call buttons, elevators usually have floor indicators and direction lanterns. The former are almost universal in cab interiors with more than two stops and may be found outside the elevators as well on one or more of the floors. Up until the 1980s, floor indicators typically had an analog strip of numbers that would light up individually. Around that time, digital floor indicators began appearing, usually either segmented or dot-matrix displays. More detailed displays first began appearing in the early 1990s with Otis's electroluminescent display, and became more common as LCDs became more viable in the 21st century. Likewise, a change of floors or an arrival at a floor is indicated by a sound, depending on the elevator. Some buildings use proximity technology that recognizes residents and brings the elevator to ground level.[92]

Direction lanterns are also found both inside and outside elevator cars, but they should always be visible from outside because their primary purpose is to help people decide whether or not to get on the elevator. If somebody waiting for the elevator wants to go up, but a car comes first that indicates that it is going down, then the person may decide not to get on the elevator. If the person waits, then one will still stop going up. Direction indicators are sometimes etched with arrows or shaped like arrows and/or use the convention that one that lights up red means ""down"" and green (or white) means ""up"". Since the color convention is often undermined or over-ridden by systems that do not invoke it, it is usually used only in conjunction with other differentiating factors. An example of a place whose elevators use only the color convention to differentiate between directions is the Museum of Contemporary Art in Chicago, where a single circle can be made to light up green for ""up"" and red for ""down"". Sometimes directions must be inferred by the position of the indicators relative to one another.

In addition to lanterns, most elevators have a chime to indicate if the elevator is going up or down either before or after the doors open, usually in conjunction with the lanterns lighting up. For example, one chime can indicate ""up"", two ""down"", and no chimes indicate an elevator that is 'free'.[93][additional citation(s) needed]

Observatory service elevators often convey other facts of interest, including elevator speed, stopwatch, and current position (altitude), as with the case for Taipei 101's service elevators.

There are several technologies aimed to provide better experience to passengers suffering from claustrophobia, anthropophobia or social anxiety. Israeli startup DigiGage uses motion sensors to scroll the pre-rendered images, building and floor-specific content on a screen embedded into the wall as the cab moves up and down.[94] British company LiftEye provides a virtual window technology to turn common elevator into panoramic. It creates 3d video panorama using live feed from cameras placed vertically along the facade and synchronizes it with cab movement. The video is projected on a wall-sized screens making it look like the walls are made of glass.[95]

The primary reason for installing an elevator air conditioner is the comfort that it provides while traveling in the elevator. It stabilizes the condition of the air inside the elevator car. Some elevator air conditioners can be used in countries with cold climates if a thermostat is used to reverse the refrigeration cycle to warm the elevator car.

Heat generated from the cooling process is dissipated into the hoistway. The elevator cab (or car) is ordinarily not air-tight, and some of this heat may reenter the car and reduce the overall cooling effect.

The air from the lobby constantly leaks into the elevator shaft due to elevator movements as well as elevator shaft ventilation requirements. Using this conditioned air in the elevator does not increase energy costs. However, by using an independent elevator air conditioner to achieve better temperature control inside the car, more energy will be used.

Air conditioning poses a problem to elevators because of the condensation that occurs. The condensed water produced has to be disposed of; otherwise, it would create flooding in the elevator car and hoistway.

There are at least four ways to remove condensed water from the air conditioner. However, each solution has its pros and cons.

Atomizing, also known as misting the condensed water, is one way to dispose of the condensed water. Spraying ultra-fine water droplets onto the hot coils of the air conditioner ensures that the condensed water evaporates quickly.

Though this is one of the best methods to dispose of the condensed water, it is also one of the costliest because the nozzle that atomizes the water easily gets choked. The majority of the cost goes to maintaining the entire atomizing system.

Disposing of condensed water works by firstly collecting the condensed water and then heating it to above boiling point. The condensed water is eventually evaporated, thereby disposing of it.

Consumers are reluctant to employ this system because of the high rate of energy used just to dispose of this water.

The cascading method works by flowing the condensed water directly onto the hot coils of the air conditioner. This eventually evaporates the condensed water.

The downside of this technology is that the coils have to be at extremely high temperature for the condensed water to be evaporated. There is a chance that the water might not evaporate entirely and that would cause water to overflow onto the exterior of the car.

Drainage system works by creating a sump to collect the condensed water and using a pump to dispose of it through a drainage system.

It is an efficient method, but it comes at a heavy price because of the cost of building the sump. Moreover, maintaining the pump to make sure it operates is very expensive. Furthermore, the pipes used for drainage would look ugly on the exterior. This system also cannot be implemented on a built project.

The mechanical and electrical design of elevators is dictated according to various standards (aka elevator codes), which may be international, national, state, regional or city based. Whereas once many standards were prescriptive, specifying exact criteria which must be complied with, there has recently been a shift towards more performance-based standards where the onus falls on the designer to ensure that the elevator meets or exceeds the standard.

National elevator standards:

converged in ISO 22559 series, ""Safety requirements for lifts (elevators)"":[97][98]

ISO/TC 178 is the Technical Committee on Lifts, escalators and moving walks.[99][100]

Because an elevator is part of a building, it must also comply with building code standards relating to earthquake resilience, fire standards, electrical wiring rules and so forth.

The American National Elevator Standards Group (ANESG) sets an elevator weight standard to be 1,000 kg (2,200 lb).

Additional requirements relating to access by disabled persons, may be mandated by laws or regulations such as the Americans with Disabilities Act. Elevators marked with a Star of Life are big enough for a stretcher.[101]

In most US and Canadian jurisdictions, passenger elevators are required to conform to the American Society of Mechanical Engineers' Standard A17.1, Safety Code for Elevators and Escalators. As of 2006, all states except Kansas, Mississippi, North Dakota, and South Dakota have adopted some version of ASME codes, though not necessarily the most recent.[102] In Canada the document is the CAN/CSA B44 Safety Standard, which was harmonized with the US version in the 2000 edition.[citation needed] In addition, passenger elevators may be required to conform to the requirements of A17.3 for existing elevators where referenced by the local jurisdiction. Passenger elevators are tested using the ASME A17.2 Standard. The frequency of these tests is mandated by the local jurisdiction, which may be a town, city, state or provincial standard.

Passenger elevators must also conform to many ancillary building codes including the local or state building code, National Fire Protection Association standards for electrical, fire sprinklers and fire alarms, plumbing codes, and HVAC codes. Also, passenger elevators are required to conform to the Americans with Disabilities Act and other state and federal civil rights legislation regarding accessibility.

Residential elevators are required to conform to ASME A17.1. platform and wheelchair elevators are required to comply with ASME A18.1 in most US jurisdictions.

Most elevators have a location in which the permit for the building owner to operate the elevator is displayed. While some jurisdictions require the permit to be displayed in the elevator cab, other jurisdictions allow for the operating permit to be kept on file elsewhere – such as the maintenance office – and to be made available for inspection on demand. In such cases instead of the permit being displayed in the elevator cab, often a notice is posted in its place informing riders of where the actual permits are kept.

700,000 (as of June 2019)

As of January 2008,[update] Spain is the nation with the most elevators installed per capita[104] in the world, with 950,000 elevators installed[105] that run more than one hundred million lifts every day, followed by United States with 700,000 elevators installed and China with 610,000 elevators installed since 1949.[106] In Brazil, it is estimated that there are approximately 300,000 elevators currently in operation.[107][108] The world's largest market for elevators is Italy, with more than 1,629 million euros of sales and 1,224 million euros of internal market.

In Spain, the elevators in maintenance invoice €4 million a year, and €250 million in repairs. In 2012, Spain exported €300 million in elevators.[citation needed]

In South Korea there are 530,000 elevators in operation, with 36,000 added in 2015. Hyundai elevators has 48% market share ThyssenKrupp Elevator Korea (formerly the Dongyang Elevator Co.) 17%, Otis Elevator Korea (formerly the elevator division of LG Industrial Systems) 16%, as of 2015. South Korea record 50,000 elevators sales in 2018 with 700,000 accumulated operation as of June,2019. Korean annual elevator maintenance market is around US$1 billion.[citation needed]

The Eiffel Tower has Otis double-deck elevators built into the legs of the tower, serving the ground level to the first and second levels. Even though the shaft runs diagonally upwards with the contour of the tower, both the upper and lower cars remain horizontally level. The offset distance of the two cars changes throughout the journey.

There are four elevator cars of the traditional design that run from the second level to the third level. The cars are connected to their opposite pairs (opposite in the elevator landing/hall) and use each other as the counterweight. As one car ascends from level 2, the other descends from level 3. The operations of these elevators are synchronized by a light signal in the car.

The Statue of Unity, the world's tallest statue at 182 metres (597 ft) high, has 10 high speed (4-metre-per-second (13 ft/s)) elevators leading up to a viewing gallery 153 metres (502 ft) high.[109]

Double deck elevators, installed by Toshiba using Kone EcoDisc machinery,[citation needed] are used in the Taipei 101 office tower. Tenants of even-numbered floors first take an escalator (or an elevator from the parking garage) to the 2nd level, where they will enter the upper deck and arrive at their floors. The lower deck is turned off during low-volume hours, and the upper deck can act as a single-level elevator stopping at all adjacent floors. For example, the 85th floor restaurants can be accessed from the 60th floor sky-lobby. Restaurant customers must clear their reservations at the reception counter on the 2nd floor. A bank of express elevators stop only on the sky lobby levels (36 and 60, upper-deck car), where tenants can transfer to ""local"" elevators.

The high-speed observation deck elevators accelerate to a former world-record certified speed of 1,010 metres per minute (61 km/h) in 16 seconds, and then it slows down for arrival with subtle air pressure sensations. The door opens after 37 seconds from the 5th floor. Special features include aerodynamic car and counterweights, and cabin pressure control to help passengers adapt smoothly to pressure changes. The downwards journey is completed at a reduced speed of 600 meters per minute, with the doors opening at the 52nd second. Many high speed elevators also have aerodynamic cab exteriors.[110]

The Gateway Arch in St. Louis uses a unique tram system to transport visitors to the observation deck. Passengers enter horizontal compartments that form a train, tilting to maintain level orientation as they ascend the curved tracks within the Arch. Two tramways operate at each end, offering views of the Arch's interior structure through windowed doors. The cars transition from hanging below the cables to resting atop them during the journey.

The elevator in the New City Hall in Hanover, Germany is a technical rarity, and unique in Europe, as the elevator starts straight up but
then changes its angle by 15 degrees to follow the contour of the dome of the hall. The cabin therefore tilts 15 degrees during the ride. The elevator travels a height of 43 meters. The new city hall was built in 1913. The elevator was destroyed in 1943 and rebuilt in 1954.

The Luxor Hotel in Las Vegas, Nevada, United States has inclined elevators. The shape of this casino is a pyramid, and the elevator travels up the side of the pyramid at a 39-degree angle. Other locations with inclined elevators include the Cityplace Station in Dallas, Texas, the Huntington Metro Station in Huntington, Virginia, and the San Diego Convention Center in San Diego, California.

At the Radisson Blu hotel in Berlin, Germany, the main elevator was surrounded by an aquarium; 82 feet tall, the aquarium contained more than a thousand different fish until it shattered in December 2022. The design offered views of the fish to people using the elevator. The special elevator was built by the German company GBH-Design GmbH[111]

The Twilight Zone Tower of Terror is an attraction found in three Disney parks, featuring a simulated free-fall using a high-speed elevator system. Passengers are seated and secured for safety. The unique elevator design allows for rapid descent and ascent, exceeding normal gravity. Passenger cabins are separate from the lift mechanism, enabling continuous operation and movement through show scenes. Automated guided vehicles transport passengers into the elevator shaft. Open doorways at the top provide views from the structure.

Guests ascending to the 67th, 69th, and 70th level observation decks (dubbed ""Top of the Rock"") atop the GE Building at Rockefeller Center in New York City ride a high-speed glass-top elevator. When entering the cab, it appears to be any normal elevator ride. However, once the cab begins moving, the interior lights turn off and a special blue light above the cab turns on. This lights the entire shaft, so riders can see the moving cab through its glass ceiling as it rises and lowers through the shaft. Music plays and various animations are also displayed on the ceiling. The entire ride takes about 60 seconds.

Part of the Haunted Mansion attraction at Disneyland in Anaheim, California, and Disneyland in Paris, France, takes place on an elevator.[112] The ride's ""stretching room"" is actually an elevator that descends while giving the illusion of stretching upwards. This effect is achieved by having an open ceiling and decorated shaft, allowing passengers to see the walls move as they descend.[113][114]

In some towns where terrain is difficult to navigate, elevators are used as part of urban transport systems.

Examples:

Internet of things (IOT) technology application is being used in elevators to improve performance, operations, monitoring, maintenance with help of remote diagnostics, real time notifications and predictive behavioral insights.[116]

The Guangzhou CTF Finance Centre holds the current record of world's fastest elevators with their cars traveling at 75.6 km/h (47.0 mph). The elevator, which was tested out the speed in June 2017, was manufactured by Hitachi, and was confirmed a Guinness World Record Sept. 2019.[117]

However, on the way down, the elevators at Yokohama Landmark Tower, manufactured by Mitsubishi Electric, descend at 45 km/h (28 mph), and still hold the record for the fastest descending elevator in the world.[118]
"
Low-Rise Foundation Construction,"A low-rise is a building that is only a few stories tall or any building that is shorter than a high-rise,[1] though others include the classification of mid-rise.[2][3]

Emporis defines a low-rise as ""an enclosed structure below 35 metres [115 feet] which is divided into regular floor levels"".[4] The city of Toronto defines a mid-rise as a building between four and twelve stories.[5]  They also have elevators and stairs. Shorter structures may only have stairs.

Low-rise apartments sometimes offer more privacy and negotiability of rent and utilities than high-rise apartments, although they may have fewer amenities and less flexibility with leases. It is easier to put fires out in low-rise buildings.[6]

Within the United States, due to the legal-economic and modernist perspectives, low-rises can in some cities be seen as less luxurious than high-rises, whereas within Western Europe (for historical identity and legal reasons) low-rise tends to be more attractive. Some businesses prefer low-rise buildings due to lower costs and more usable space. Having all employees on a single floor may also increase work productivity.[7]



This article about a building or structure type is a stub. You can help Wikipedia by expanding it."
High-Rise Foundation Construction,"In engineering, a foundation is the element of a structure which connects it to the ground or more rarely, water (as with floating structures), transferring loads from the structure to the ground. Foundations are generally considered either shallow or deep.[1]  Foundation engineering is the application of soil mechanics and rock mechanics (geotechnical engineering) in the design of foundation elements of structures.

Foundations provide the structure's stability from the ground:

The design and the construction of a well-performing foundation must possess some basic requirements:[2]

Buildings and structures have a long history of being built with wood in contact with the ground.[3][4] Post in ground construction may technically have no foundation. Timber pilings were used on soft or wet ground even below stone or masonry walls.[5] In marine construction and bridge building a crisscross of timbers or steel beams in concrete is called grillage.[6]

Perhaps the simplest foundation is the padstone, a single stone which both spreads the weight on the ground and raises the timber off the ground.[7] Staddle stones are a specific type of padstone.

Dry stone and stones laid in mortar to build foundations are common in many parts of the world. Dry laid stone foundations may have been painted with mortar after construction. Sometimes the top, visible course of stone is hewn, quarried stones.[8] Besides using mortar, stones can also be put in a gabion.[9] One disadvantage is that if using regular steel rebars, the gabion would last much less long than when using mortar (due to rusting). Using weathering steel rebars could reduce this disadvantage somewhat.

Rubble trench foundations are a shallow trench filled with rubble or stones. These foundations extend below the frost line and may have a drain pipe which helps groundwater drain away. They are suitable for soils with a capacity of more than 10 tonnes/m2 (2,000 pounds per square foot).

Often called footings, are usually embedded about a meter or so into soil. One common type is the spread footing which consists of strips or pads of concrete (or other materials) which extend below the frost line and transfer the weight from walls and columns to the soil or bedrock.

Another common type of shallow foundation is the slab-on-grade foundation where the weight of the structure is transferred to the soil through a concrete slab placed at the surface. Slab-on-grade foundations can be reinforced mat slabs, which range from 25 cm to several meters thick, depending on the size of the building, or post-tensioned slabs, which are typically at least 20 cm for houses, and thicker for heavier structures.

Another way to install ready-to-build foundations that is more environmentally friendly is to use screw piles. Screw pile installations have also extended to residential applications, with many homeowners choosing a screw pile foundation over other options. Some common applications for helical pile foundations include wooden decks, fences, garden houses, pergolas, and carports.

Used to transfer the load of a structure down through the upper weak layer of topsoil to the stronger layer of subsoil below. There are different types of deep footings including impact driven piles, drilled shafts, caissons, screw piles, geo-piers[clarification needed] and earth-stabilized columns[clarification needed]. The naming conventions for different types of footings vary between different engineers. Historically, piles were wood, later steel, reinforced concrete, and pre-tensioned concrete.

A type of deep foundation which uses a single, generally large-diameter, structural element embedded into the earth to support all the loads (weight, wind, etc.) of a large above-surface structure.

Many monopile foundations[10] have been used in recent years for economically constructing fixed-bottom offshore wind farms in shallow-water subsea locations.[11]  
For example, a single wind farm off the coast of England went online in 2008 with over 100 turbines, each mounted on a 4.74-meter-diameter monopile footing in ocean depths up to 16 meters of water.[12]

A floating foundation is one that sits on a body of water, rather than dry land. This type of foundation is used for some bridges and floating buildings.

Foundations are designed to have an adequate load capacity depending on the type of subsoil/rock supporting the foundation by a geotechnical engineer, and the footing itself may be designed structurally by a structural engineer. The primary design concerns are settlement and bearing capacity. When considering settlement, total settlement and differential settlement is normally considered. Differential settlement is when one part of a foundation settles more than another part. This can cause problems to the structure which the foundation is supporting. Expansive clay soils can also cause problems.
"
Precast Concrete Installation,"

Precast concrete is a construction product produced by casting concrete in a reusable mold or ""form"" which is then cured in a controlled environment, transported to the construction site and maneuvered into place; examples include precast beams, and wall panels, floors, roofs, and piles. In contrast, cast-in-place concrete is poured into site-specific forms and cured on site.[1]

Recently lightweight expanded polystyrene foam is being used as the cores of precast wall panels, saving weight and increasing thermal insulation.

Precast stone is distinguished from precast concrete by the finer aggregate used in the mixture, so the result approaches the natural product.

Precast concrete is employed in both interior and exterior applications, from highway, bridge, and high-rise projects to parking structures, K-12 schools, warehouses, mixed-use, and industrial building construction. By producing precast concrete in a controlled environment (typically referred to as a precast plant), the precast concrete is afforded the opportunity to properly cure and be closely monitored by plant employees. Using a precast concrete system offers many potential advantages over onsite casting. Precast concrete production can be performed on ground level, which maximizes safety in its casting. There is greater control over material quality and workmanship in a precast plant compared to a construction site. The forms used in a precast plant can be reused hundreds to thousands of times before they have to be replaced, often making it cheaper than onsite casting in terms of cost per unit of formwork.[2]

Precast concrete forming systems for architectural applications differ in size, function, and cost. Precast architectural panels are also used to clad all or part of a building facade or erect free-standing walls for landscaping, soundproofing, and security.  In appropriate instances precast products – such as beams for bridges, highways, and parking structure decks – can  be prestressed structural elements. Stormwater drainage, water and sewage pipes, and tunnels also make use of precast concrete units.

Precast concrete molds can be made of timber, steel, plastic, rubber, fiberglass, or other synthetic materials, with each giving a unique finish.[3]  In addition, many surface finishes for the four precast wall panel types – sandwich, plastered sandwich, inner layer and cladding panels – are available, including those creating the looks of horizontal boards and ashlar stone. Color may be added to the concrete mix, and the proportions and size aggregate also affect the appearance and texture of finished concrete surfaces.

Ancient Roman builders made use of concrete and soon poured the material into moulds to build their complex network of aqueducts, culverts, and tunnels. Modern uses for pre-cast technology include a variety of architectural and structural applications – including individual parts, or even entire building systems.

In the modern world, precast panelled buildings were pioneered in Liverpool, England, in 1905.[4] The process was invented by city engineer John Alexander Brodie. The tram stables at Walton in Liverpool followed in 1906. The idea was not taken up extensively in Britain. However, it was adopted all over the world, particularly in Central and Eastern Europe[5] as well as in Million Programme in Scandinavia.

In the US, precast concrete has evolved as two sub-industries, each represented by a major association. The precast concrete structures industry, represented primarily by of the Precast/Prestressed Concrete Institute (PCI), focuses on prestressed concrete elements and on other precast concrete elements used in above-ground structures such as buildings, parking structures, and bridges, while the precast concrete products industry produces utility, underground, and other non-prestressed products, and is represented primarily by the National Precast Concrete Association (NPCA).

In Australia, The New South Wales Government Railways made extensive use of precast concrete construction for its stations and similar buildings. Between 1917 and 1932, it erected 145 such buildings.[6]

Beyond cladding panels and structural elements, entire buildings can be assembled from precast concrete. Precast assembly enables fast completion of commercial shops and offices with minimal labor. For example, the Jim Bridger Building in Williston, North Dakota, was precast in Minnesota with air, electrical, water, and fiber utilities preinstalled into the building panels. The panels were transported over 800 miles to the Bakken oilfields, and the commercial building was assembled by three workers in minimal time. The building houses over 40,000 square feet of shops and offices. Virtually the entire building was fabricated in Minnesota.

Reinforcing concrete with steel improves strength and durability. On its own, concrete has good compressive strength, but lacks tensile and shear strength and can be subject to cracking when bearing loads for long periods of time. Steel offers high tensile and shear strength to make up for what concrete lacks. Steel behaves similarly to concrete in changing environments, which means it will shrink and expand with concrete, helping avoid cracking.

Rebar is the most common form of concrete reinforcement. It is typically made from steel, manufactured with ribbing to bond with concrete as it cures. Rebar is versatile enough to be bent or assembled to support the shape of any concrete structure. Carbon steel is the most common rebar material. However, stainless steel, galvanized steel, and epoxy coatings can prevent corrosion.[7]

The following is a sampling of the numerous products that utilize precast/prestressed concrete. While this is not a complete list, the majority of precast/prestressed products typically fall under one or

Since precast concrete products can withstand the most extreme weather conditions and will hold up for many decades of constant usage they have wide applications in agriculture.  These include bunker silos, cattle feed bunks, cattle grid, agricultural fencing, H-bunks, J-bunks, livestock slats, livestock watering trough, feed troughs, concrete panels, slurry channels, and more. Prestressed concrete panels are widely used in the UK for a variety of applications including agricultural buildings, grain stores, silage clamps, slurry stores, livestock walling and general retaining walls. Panels can be used horizontally and placed either inside the webbings of RSJs (I-beam) or in front of them. Alternatively panels can be cast into a concrete foundation and used as a cantilever retaining wall.

Precast concrete building components and site amenities are used architecturally as fireplace mantels, cladding, trim products, accessories and curtain walls. Structural applications of precast concrete include foundations, beams, floors, walls and other structural components. It is essential that each structural component be designed and tested to withstand both the tensile and compressive loads that the member will be subjected to over its lifespan. Expanded polystyrene cores are now in precast concrete panels for structural use, making them lighter and serving as thermal insulation.

Multi-storey car parks are commonly constructed using precast concrete. The constructions involve putting together precast parking parts which are multi-storey structural wall panels, interior and exterior columns, structural floors, girders, wall panels, stairs, and slabs. These parts can be large; for example, double-tee structural floor modules need to be lifted into place with the help of precast concrete lifting anchor systems.[8]

Precast concrete is employed in a wide range of engineered earth retaining systems. Products include commercial and residential retaining walls, sea walls, mechanically stabilized earth panels, and other modular block systems.[9]

Sanitary and stormwater management products are structures designed for underground installation that have been specifically engineered for the treatment and removal of pollutants from sanitary and stormwater run-off. These precast concrete products include stormwater detention vaults, catch basins, and manholes.[10]

For communications, electrical, gas or steam systems, precast concrete utility structures protect the vital connections and controls for utility distribution. Precast concrete is nontoxic and environmentally safe. Products include: hand holes, hollow-core products, light pole bases, meter boxes, panel vaults, pull boxes, telecommunications structures, transformer pads, transformer vaults, trenches, utility buildings, utility vaults, utility poles, controlled environment vaults (CEVs), and other utility structures.

Precast water and wastewater products hold or contain water, oil or other liquids for the purpose of further processing into non-contaminating liquids and soil products. Products include: aeration systems, distribution boxes, dosing tanks, dry wells, grease interceptors, leaching pits, sand-oil/oil-water interceptors, septic tanks, water/sewage storage tanks, wet wells, fire cisterns, and other water and wastewater products.

Precast concrete transportation products are used in the construction, safety, and site protection of roads, airports, and railroad transportation systems. Products include: box culverts, 3-sided culverts, bridge systems, railroad crossings, railroad ties, sound walls/barriers, Jersey barriers, tunnel segments, concrete barriers, TVCBs, central reservation barriers, bollards, and other transportation products. Precast concrete can also be used to make underpasses, surface crossings, and pedestrian subways. Precast concrete is also used for the roll ways of some rubber-tyred metros.

Modular paving is available in a rainbow of colors, shapes, sizes, and textures. These versatile precast concrete pieces can be designed to mimic brick, stone or wood.

Underground vaults or mausoleums require watertight structures that withstand natural forces for extended periods of time.

Storage of hazardous material, whether short-term or long-term, is an increasingly important environmental issue, calling for containers that not only seal in the materials, but are strong enough to stand up to natural disasters or terrorist attacks.

Seawalls, floating docks, underwater infrastructure, decking, railings, and a host of amenities are among the uses of precast along the waterfront. When designed with heavy weight in mind, precast products counteract the buoyant forces of water significantly better than most materials.

Prestressing is a technique of introducing stresses into a structural member during fabrication and/or construction to improve its strength and performance. This technique is often employed in concrete beams, columns, spandrels, single and double tees, wall panels, segmental bridge units, bulb-tee girders, I-beam girders, and others. Many projects find that prestressed concrete provides the lowest overall cost, considering production and lifetime maintenance.[9][11]

The precast concrete double-wall panel has been in use in Europe for decades. The original double-wall design consisted of two wythes of reinforced concrete separated by an interior void, held together with embedded steel trusses. With recent concerns about energy use, it is recognized that using steel trusses creates a ""thermal bridge"" that degrades thermal performance. Also, since steel does not have the same thermal expansion coefficient as concrete, as the wall heats and cools any steel that is not embedded in the concrete can create thermal stresses that cause cracking and spalling.

To achieve better thermal performance, insulation was added in the void, and in many applications today the steel trusses have been replaced by composite (fibreglass, plastic, etc.) connection systems. These systems, which are specially developed for this purpose, also eliminate the differential thermal expansion problem.[citation needed]The best thermal performance is achieved when the insulation is continuous throughout the wall section, i.e., the wythes are thermally separated completely to the ends of the panel. Using continuous insulation and modern composite connection systems, R-values up to R-28.2 can be achieved.

The overall thickness of sandwich wall panels in commercial applications is typically 8 inches, but their designs are often customized to the application. In a typical 8-inch wall panel the concrete wythes are each 2-3/8 inches thick), sandwiching 3-1/4 inches of high R-value insulating foam. The interior and exterior wythes of concrete are held together (through the insulation) with some form of connecting system that is able to provide the needed structural integrity. Sandwich wall panels can be fabricated to the length and width desired, within practical limits dictated by the fabrication system, the stresses of lifting and handling, and shipping constraints. Panels of 9-foot clear height are common, but heights up to 12 feet can be found.

The fabrication process for precast concrete sandwich wall panels allows them to be produced with finished surfaces on both sides. Such finishes can be very smooth, with the surfaces painted, stained, or left natural; for interior surfaces, the finish is comparable to drywall in smoothness and can be finished using the same prime and paint procedure as is common for conventional drywall construction. If desired, the concrete can be given an architectural finish, where the concrete itself is colored and/or textured. Colors and textures can provide the appearance of brick, stone, wood, or other patterns through the use of reusable formliners, or, in the most sophisticated applications, actual brick, stone, glass, or other materials can be cast into the concrete surface.[citation needed]

Window and door openings are cast into the walls at the manufacturing plant as part of the fabrication process. In many applications, electrical and telecommunications conduit and boxes are cast directly into the panels in the specified locations. In some applications, utilities, plumbing and even heating components have been cast into the panels to reduce on-site construction time. The carpenters, electricians and plumbers do need to make some slight adjustments when first becoming familiar with some of the unique aspects of the wall panels. However, they still perform most of their job duties in the manner to which they are accustomed.

Precast concrete sandwich wall panels have been used on virtually every type of building, including schools, office buildings, apartment buildings, townhouses, condominiums, hotels, motels, dormitories, and single-family homes. Although typically considered part of a building's enclosure or ""envelope,"" they can be designed to also serve as part of the building's structural system, eliminating the need for beams and columns on the building perimeter. Besides their energy efficiency and aesthetic versatility, they also provide excellent noise attenuation, outstanding durability (resistant to rot, mold, etc.), and rapid construction.

In addition to the good insulation properties, sandwich panels require fewer work phases to complete. Compared to double-walls, for example, which have to be insulated and filled with concrete on site, sandwich panels require much less labor and scaffolding.[12]

The precast concrete industry is largely dominated by Government initiated projects for infrastructural development. However, these are also being extensively used for residential (low and high rise) and commercial constructions because of their various favourable attributes. The efficiency, durability, ease, cost effectiveness, and sustainable properties[13] of these products have brought a revolutionary shift in the time consumed in construction of any structure. Construction industry is a huge energy consuming industry, and precast concrete products are and will continue to be more energy efficient than its counterparts. The wide range of designs, colours, and structural options that these products provide is also making it a favourable choice for its consumers.

Many state and federal transportation projects in the United States require precast concrete suppliers to be certified by either the Architectural Precast Association, National Precast Concrete Association or Precast Prestressed Concrete Institute.
"
Tilt-Up Concrete Services,"Tilt-up, tilt-slab or tilt-wall is a type of building and a construction technique using concrete. Though it is a cost-effective technique with a shorter completion time,[1] poor performance in earthquakes has mandated significant seismic retrofit requirements in older buildings.[2]

With the tilt-up method, concrete elements (walls, columns, structural supports, etc.) are formed horizontally on a concrete slab; this normally requires the building floor as a building form but may be a temporary concrete casting surface near the building footprint. After the concrete has cured, the elements are ""tilted"" to the vertical position with a crane and braced into position until the remaining building structural components (roofs, intermediate floors and walls) are secured.[3][4]

Tilt-up construction is a common method of construction throughout North America, several Caribbean nations, Australia, and New Zealand. It is not significantly used in Europe or the northern two thirds of Asia. It is gaining popularity in southern Asia, the Middle East, parts of Africa, Central and South America.

Concrete elements can also be formed at factories away from the building site.[5] Tilt-up differs from prefabrication, or plant cast construction, in that all elements are constructed on the job site. This eliminates the size limitation imposed by transporting elements from a factory to the project site.

Tilt-up construction requires significant organization and collaboration on the building site. The chronological steps that need to be taken for a tilt-up project are: site evaluation, engineering, footings and floor slabs, forming tilt-up panels, steel placement, embeds and inserts, concrete placement, panel erection and panel finishing.[1][6] Once the pad (casting surface or floor slab) has cured, forms are built on top. Dimensional lumber, a high quality plywood or fiber board that has at least one smooth face is typically used, although aluminum or steel forms are also common. Carpenters work from engineered drawings designed for each panel or element to construct on site. They incorporate all door and window openings, as well as architectural features and other desired shapes that can be molded into the concrete. Studs, gussets and attachment plates are located within the form for embedding in the concrete. The forms are usually anchored to the casting surface with masonry nails or otherwise adhered to prevent damage to the floor slab.[7]

Next, a chemically reactive bondbreaker is sprayed on the form's surfaces to prevent the cast concrete from bonding with the slab. This allows the cast element to separate from the casting surface once it has cured. This is a critical step, as improper chemical selection or application will prevent the lifting of the panels, and will entail costly demolition and rework.

A rebar grid is constructed inside the forms, after the form release is applied, spaced off the casting surface the desired distance with plastic ""chairs"". The rebar size and spacing is generally specified by the engineer of record.[8]

Concrete is then poured, filling the desired thickness and surrounding all steel inserts, embedded features and rebar. The concrete is then settled through vibration to prevent any voids or honeycomb effects. The forms are removed when the concrete is cured; rigging is attached and a crane tilts the panel or lifts the element into place. In circumstances when space is at a premium, concrete elements can be cast one on top of the other, or stack cast. Quite often a separate casting pad is poured for this purpose and is removed when the panels are erected.[9]

Cranes are used to tilt the concrete elements from the casting slab to a vertical position. The slabs are then most often set onto a foundation and secured with braces until the structural steel and the roof diaphragm is in place.

Concrete tilt-up walls can be very heavy, sometimes over 300,000 pounds (140 t).[10] Most tilt-up wall panels are engineered to work with the roof structure and/or floor structures to resist all forces; that is, to function as load-bearing walls. The connections to the roof and floors are usually steel plates with headed studs that were secured into the forms prior to concrete placement. These attachment points are bolted or welded. The upper attachment points are made to the roof trusses. Interior walls may be present for additional stiffness in the building structure as necessary, known as shear walls.

Insulation can be applied to either side of the panels or cast as an integral part of the panel between two layers of concrete to create sandwich panels. Concrete has the ability to absorb and store energy and is high mass, which regulates interior temperature (thermal mass) and provides soundproofing and durability.[1]

Like all concrete construction, tilt-up buildings are fire-resistant. In addition, wall panels can be designed to sag inward when damaged, which minimizes collapse (this can also be done with prefabricated panels).[1]

Tilt-up was first used in America circa 1905. In 1908 Robert Akin patented the tilt-slab method of concrete construction used in the construction of the Schindler House.[11] Early erection was done using tilt tables, but the development of the mobile crane and truck mixers allowed tilt-up construction to grow. Tilt-up gained widespread popularity in the post World War II construction boom.[12][13] Tilt-up was not used successfully in Australia until 1969.[14][15]

Most early tilt-up buildings were warehouses. Today the method is used in nearly every type of building from schools to office structures, houses to hotels. They range from single story to more than seven and can be more than 29 metres (96 feet) in height.[16]

An early example of this method is found in the innovative Schindler House, built in 1922 in West Hollywood, California. Architect Rudolf Schindler claimed that with the assistance of a small hand-operated crane, just two workmen were needed to raise and attach the tilt-up walls.[citation needed]

The first tilt-up built home in Ontario, Canada was built in 2017.[17]

Early tilt-up architecture was very minimalist and boxy. Recent techniques have expanded the range of appearance and shape.

Many finish options are available to the tilt-up contractor, from paints and stains to pigmented concrete, cast-in features like brick and stone to aggressive erosion finishes like sandblasting and acid-etching. Shapes are also a feature that have become dominant in the tilt-up market, with many panels configured with circular or elliptical openings, panel tops that are pedimented or curved, facades that are curved or segmented and featured with significant areas of glazing or other materials.

The Tilt-Up Concrete Association (TCA) is the international trade association for tilt-up concrete construction. TCA is a membership-based association, with nearly 500 members worldwide.[18] TCA members can be contractors (general contractors or tilt-up subcontractors), engineers, architects, developers, consultants, suppliers, specialty trade firms, educators and students.

TCA offers primarily educational, networking and exposure benefits to its members.  TCA also offers an Achievement Awards program annually, recognizing the best examples of tilt-up construction over a variety of end uses.[19]

In the wake of the 2011 Joplin tornado in which seven people were killed in a Home Depot when the 100,000-pound (45 t) panel walls collapsed after the store was hit by an EF5 tornado, engineers in an article published in The Kansas City Star criticized the practice.  They said that once one wall falls, it creates a domino effect.  Twenty-eight people in an un-reinforced training room in the back of the building survived.  According to a study of the collapse, the tornado hit the south corner of the store and lifted the roof up causing the west walls to collapse into the store.  The walls on the east side (where the people survived) collapsed out.  Only two walls remained standing.  Engineers said that stronger roof-to-wall connections might have tempered the collapse.  Two other big box stores at the corner that had concrete block construction (an Academy Sports + Outdoors and Walmart) lost their roofs but the walls remained intact.  Those buildings were not directly hit by the tornado but the Home Depot building suffered a direct hit.  Three people died in the Walmart, but 200 survived.  The engineers told the Star that when concrete blocks fail they usually break apart, and do not come down in huge slabs.  Home Depot, which has hundreds of stores built with tilt-up, said it disagreed with the finding and that it would use tilt-up when it rebuilt the Joplin store.[20]

Shortly after publication of the Kansas City Star article, the technical committee of the Tilt-Up Concrete Association (TCA) formed a task force to investigate the claims presented in the article. With the cooperation of Home Depot, the task group performed detailed engineering calculations, research and investigation of the claims posed in the article. This task force consisted of a nationwide group of practicing structural engineers with a diverse range of experience in tilt-up construction and ""big box"" buildings. The final report was published on January 12, 2012. ""The information provided in these findings will help Association efforts to promote the benefits of site cast Tilt-Up construction and dispute many of the claims presented in The Kansas City Star article.""[21][non-primary source needed]

""The Task Force's findings to date include:

One of the conclusions of the Task Force's report was ""Recommend to ICC and direct
to building owners the use of storm shelters in lieu of designing buildings for high
winds. The TCA should develop specific Tilt-Up based storm shelter designs for winds
of up to 200 MPH that would compete against alternative masonry, precast, or
cast-in-place designs. Storm shelter design is addressed in 2009 IBC, section 423 and ICC-500.""[22][non-primary source needed]
"
Masonry Construction Services,"Masonry is the craft of building a structure with brick, stone, or similar material, including mortar plastering which are often laid in, bound, and pasted together by mortar. The term masonry can also refer to the building units (stone, brick, etc.) themselves.

The common materials of masonry construction are bricks and building stone, rocks such as marble, granite, and limestone, cast stone, concrete blocks, glass blocks, and adobe. Masonry is generally a highly durable form of construction. However, the materials used, the quality of the mortar and workmanship, and the pattern in which the units are assembled can substantially affect the durability of the overall masonry construction.

A person who constructs masonry is called a mason or bricklayer. These are both classified as construction trades.

Masonry is one of the oldest building crafts in the world. The construction of Egyptian pyramids, Roman aqueducts, and medieval cathedrals are all examples of masonry. Early structures used the weight of the masonry itself to stabilize the structure against lateral movements. The types and techniques of masonry used evolved with architectural needs and cultural norms. Since mid-20th century, masonry has often featured steel-reinforced elements to help carry the tension force present in modern thin, light, tall building systems.[1]

Masonry has both structural and non-structural applications. Structural applications include walls, columns, beams, foundations, load-bearing arches, and others. On the other hand, masonry is also used in non-structural applications such as fireplaces chimneys and veneer systems.[1]

Brick and concrete block are the most common types of masonry in use in industrialized nations and may be either load-bearing or non-load-bearing. Concrete blocks, especially those with hollow cores, offer various possibilities in masonry construction. They generally provide great compressive strength and are best suited to structures with light transverse loading when the cores remain unfilled. Filling some or all of the cores with concrete or concrete with steel reinforcement (typically rebar) offers much greater tensile and lateral strength to structures.

One problem with masonry walls is that they rely mainly on their weight to keep them in place; each block or brick is only loosely connected to the next via a thin layer of mortar. This is why they do not perform well in earthquakes, when entire buildings are shaken horizontally. Many collapses during earthquakes occur in buildings that have load-bearing masonry walls. Besides, heavier buildings having masonry suffer more damage.

The strength of a masonry wall is not entirely dependent on the bond between the building material and the mortar; the friction between the interlocking blocks of masonry is often strong enough to provide a great deal of strength on its own. The blocks sometimes have grooves or other surface features added to enhance this interlocking, and some dry set masonry structures forgo mortar altogether.

Stone blocks used in masonry can be dressed or rough, though in both examples corners, door and window jambs, and similar areas are usually dressed.
Stonemasonry utilizing dressed stones is known as ashlar masonry, whereas masonry using irregularly shaped stones is known as rubble masonry. Both rubble and ashlar masonry can be laid in coursed rows of even height through the careful selection or cutting of stones, but a great deal of stone masonry is uncoursed.

Solid brickwork is made of two or more wythes of bricks with the units running horizontally (called stretcher bricks) bound together with bricks running transverse to the wall (called ""header"" bricks). Each row of bricks is known as a course. The pattern of headers and stretchers employed gives rise to different 'bonds' such as the common bond (with every sixth course composed of headers), the English bond, and the Flemish bond (with alternating stretcher and header bricks present on every course). Bonds can differ in strength and in insulating ability. Vertically staggered bonds tend to be somewhat stronger and less prone to major cracking than a non-staggered bond.

The wide selection of brick styles and types generally available in industrialized nations allow much variety in the appearance of the final product. In buildings built during the 1950s-1970s, a high degree of uniformity of brick and accuracy in masonry was typical. In the period since then this style was thought to be too sterile, so attempts were made to emulate older, rougher work. Some brick surfaces are made to look particularly rustic by including burnt bricks, which have a darker color or an irregular shape. Others may use antique salvage bricks, or new bricks may be artificially aged by applying various surface treatments, such as tumbling. The attempts at rusticity of the late 20th century have been carried forward by masons specializing in a free, artistic style, where the courses are intentionally not straight, instead weaving to form more organic impressions.

A crinkle-crankle wall is a brick wall that follows a serpentine path, rather than a straight line. This type of wall is more resistant to toppling than a straight wall; so much so that it may be made of a single wythe of unreinforced brick and so despite its longer length may be more economical than a straight wall.

Blocks of cinder concrete (cinder blocks or breezeblocks), ordinary concrete (concrete blocks), or hollow tile are generically known as Concrete Masonry Units (CMUs). They usually are much larger than ordinary bricks and so are much faster to lay for a wall of a given size. Furthermore, cinder and concrete blocks typically have much lower water absorption rates than brick. They often are used as the structural core for veneered brick masonry or are used alone for the walls of factories, garages, and other industrial-style buildings where such appearance is acceptable or desirable. Such blocks often receive a stucco surface for decoration. Surface-bonding cement, which contains synthetic fibers for reinforcement, is sometimes used in this application and can impart extra strength to a block wall. Surface-bonding cement is often pre-colored and can be stained or painted thus resulting in a finished stucco-like surface.

The primary structural advantage of concrete blocks in comparison to smaller clay-based bricks is that a CMU wall can be reinforced by filling the block voids with concrete with or without steel rebar. Generally, certain voids are designated for filling and reinforcement, particularly at corners, wall-ends, and openings while other voids are left empty.  This increases wall strength and stability more economically than filling and reinforcing all voids. Typically, structures made of CMUs will have the top course of blocks in the walls filled with concrete and tied together with steel reinforcement to form a bond beam. Bond beams are often a requirement of modern building codes and controls. Another type of steel reinforcement referred to as ladder-reinforcement, can also be embedded in horizontal mortar joints of concrete block walls. The introduction of steel reinforcement generally results in a CMU wall having much greater lateral and tensile strength than unreinforced walls.

""Architectural masonry is the evolvement of standard concrete masonry blocks into aesthetically pleasing concrete masonry units (CMUs)"".[5][failed verification] CMUs can be manufactured to provide a variety of surface appearances. They can be colored during manufacturing or stained or painted after installation. They can be split as part of the manufacturing process, giving the blocks a rough face replicating the appearance of natural stone, such as brownstone. CMUs may also be scored, ribbed, sandblasted, polished, striated (raked or brushed), include decorative aggregates, be allowed to slump in a controlled fashion during curing, or include several of these techniques in their manufacture to provide a decorative appearance.[6]

""Glazed concrete masonry units are manufactured by bonding a permanent colored facing (typically composed of polyester resins, silica sand and various other chemicals) to a concrete masonry unit, providing a smooth impervious surface.""[7]

Glass block or glass brick are blocks made from glass and provide a translucent to clear vision through the block.

A masonry veneer wall consists of masonry units, usually clay-based bricks, installed on one or both sides of a structurally independent wall usually constructed of wood or masonry. In this context, the brick masonry is primarily decorative, not structural. The brick veneer is generally connected to the structural wall by brick ties (metal strips that are attached to the structural wall, as well as the mortar joints of the brick veneer). There is typically an air gap between the brick veneer and the structural wall. As clay-based brick is usually not completely waterproof, the structural wall will often have a water-resistant surface (usually tar paper) and weep holes can be left at the base of the brick veneer to drain moisture that accumulates inside the air gap. Concrete blocks, real and cultured stones, and veneer adobe are sometimes used in a very similar veneer fashion.

Most insulated buildings that use concrete block, brick, adobe, stone, veneers or some combination thereof feature interior insulation in the form of fiberglass batts between wooden wall studs or in the form of rigid insulation boards covered with plaster or drywall. In most climates this insulation is much more effective on the exterior of the wall, allowing the building interior to take advantage of the aforementioned thermal mass of the masonry. This technique does, however, require some sort of weather-resistant exterior surface over the insulation and, consequently, is generally more expensive.

Gabions are baskets, usually now of zinc-protected steel (galvanized steel) that are filled with fractured stone of medium size. These will act as a single unit and are stacked with setbacks to form a revetment or retaining wall. They have the advantage of being well drained, flexible, and resistant to flood, water flow from above, frost damage, and soil flow. Their expected useful life is only as long as the wire they are composed of and if used in severe climates (such as shore-side in a salt water environment) must be made of appropriate corrosion-resistant wire.  Most modern gabions are rectangular. Earlier gabions were often cylindrical wicker baskets, open at both ends, used usually for temporary, often military, construction.

Similar work can be done with finer aggregates using cellular confinement.

Masonry walls have an endothermic effect of its hydrates, as in chemically bound water, unbound moisture from the concrete block, and the poured concrete if the hollow cores inside the blocks are filled. Masonry can withstand temperatures up to 1,000 °F (538 °C) and it can withstand direct exposure to fire for up to 4 hours.[citation needed] In addition to that, concrete masonry keeps fires contained to their room of origin 93% of the time.[citation needed] For those reasons, concrete and masonry units hold the highest flame spread index classification, Class A.[8]

Fire cuts can be used to increase safety and reduce fire damage to masonry buildings.

From the point of view of material modeling, masonry is a special material of extreme mechanical properties (with a very high ratio between strength in compression and in tension), so that the applied loads do not diffuse as they do in elastic bodies, but tend to percolate along lines of high stiffness.[9][10]
"
Drywall Services,"

Drywall (also called plasterboard, dry lining,[1] wallboard, sheet rock, gib board, gypsum board, buster board, turtles board, slap board, custard board, gypsum panel and gyprock) is a panel made of calcium sulfate dihydrate (gypsum), with or without additives, typically extruded between thick sheets of facer and backer paper, used in the construction of interior walls and ceilings.[2] The plaster is mixed with fiber (typically paper, glass wool, or a combination of these materials); plasticizer, foaming agent; and additives that can reduce mildew, flammability, and water absorption.

In the mid-20th century, drywall construction became prevalent in North America as a time- and labor-saving alternative to lath and plaster.[3]

Sackett Board was invented in 1890 by New York Coal Tar Chemical Company employees Augustine Sackett and Fred L. Kane,[4] graduates of Rensselaer Polytechnic Institute.[citation needed][5] It was made by layering plaster within four plies of wool felt paper. Sheets were 36 by 36 by 1⁄4 inch (914 mm × 914 mm × 6 mm) thick with open (untaped) edges.[6]


Gypsum board evolved between 1910 and 1930, beginning with wrapped board edges and the elimination of the two inner layers of felt paper in favor of paper-based facings. In 1910 United States Gypsum Corporation bought Sackett Plaster Board Company and by 1917 introduced Sheetrock.[7] Providing installation efficiency, it was developed additionally as a measure of fire resistance. Later air entrainment technology made boards lighter and less brittle, and joint treatment materials and systems also evolved.[6] Gypsum lath was an early substrate for plaster. An alternative to traditional wood or metal lath was a panel made up of compressed gypsum plaster board that was sometimes grooved or punched with holes to allow wet plaster to key into its surface. As it evolved, it was faced with paper impregnated with gypsum crystals that bonded with the applied facing layer of plaster.[8] In 1936, US Gypsum trademarked ROCKLATH[9] for their gypsum lath product.
In 2002, the European Commission imposed fines totaling €420 million on the companies Lafarge, BPB, Knauf and Gyproc Benelux, which had operated a cartel on the market which affected 80% of consumers in France, the UK, Germany and the Benelux countries.[10]

A wallboard panel consists of a layer of gypsum plaster sandwiched between two layers of paper. The raw gypsum, CaSO4·2H2O, is heated to drive off the water and then slightly rehydrated to produce the hemihydrate of calcium sulfate (CaSO4·⁠1/2⁠H2O). The plaster is mixed with fiber (typically paper and/or glass fiber), plasticizer, foaming agent, finely ground gypsum crystal as an accelerator, EDTA, starch or other chelate as a retarder, and various additives that may increase mildew and fire resistance, lower water absorption (wax emulsion or silanes), reduce creep (tartaric or boric acid).[11] The board is then formed by sandwiching a core of the wet mixture between two sheets of heavy paper or fiberglass mats. When the core sets, it is dried in a large drying chamber, and the sandwich becomes rigid and strong enough for use as a building material.

Drying chambers typically use natural gas today. To dry 1,000 square feet (93 m2) of wallboard, between 1,750,000 and 2,490,000 BTU (1.85–2.63 GJ) is required. Organic dispersants and plasticizers are used so that the slurry will flow during manufacture and to reduce the water and hence the drying time.[12] Coal-fired power stations include devices called scrubbers to remove sulfur from their exhaust emissions. The sulfur is absorbed by powdered limestone in a process called flue-gas desulfurization (FGD), which produces several new substances. One is called ""FGD gypsum"". This is commonly used in drywall construction in the United States and elsewhere.[13][14]

In 2020, 8.4 billion square meters of drywall were sold around the world.[15]

As an alternative to a week-long plaster application, an entire house can be drywalled in one or two days by two experienced drywallers, and drywall is easy enough to be installed by many amateur home carpenters. In large-scale commercial construction, the work of installing and finishing drywall is often split between drywall mechanics, or hangers, who install the wallboard, and tapers (also known as finishers, mud men, or float crew) who finish the joints and cover the fastener heads with drywall compound.[16] Drywall can be finished anywhere from a level 0 to a level 5, where 0 is not finished in any fashion, and five is the most pristine.[17] Depending on how significant the finish is to the customer, the extra steps in the finish may or may not be necessary, though priming and painting of drywall are recommended in any location where it may be exposed to any wear.

Drywall is cut to size by scoring the paper on the finished side (usually white) with a utility knife, breaking the sheet along the cut, and cutting the paper backing. Small features such as holes for outlets and light switches are usually cut using a keyhole saw, oscillating multi-tool or a tiny high-speed bit in a rotary tool. Drywall is then fixed to the structure with nails or drywall screws and often glue. Drywall fasteners, also referred to as drywall clips or stops, are gaining popularity in residential and commercial construction. Drywall fasteners are used for supporting interior drywall corners and replacing the non-structural wood or metal blocking that traditionally was used to install drywall. Their function saves material and labor costs, minimizes call-backs due to truss uplift, increases energy efficiency, and makes plumbing and electrical installation simpler.

When driven fully home, drywall screws countersink their heads slightly into the drywall. They use a 'bugle head', a concave taper, rather than the conventional conical countersunk head; this compresses the drywall surface rather than cutting into it and so avoids tearing the paper. Screws for light-gauge steel framing have a sharp point and finely spaced threads. If the steel framing is heavier than 20-gauge, self-drilling screws with finely spaced threads must be used. In some applications, the drywall may be attached to the wall with adhesives.

After the sheets are secured to the wall studs or ceiling joists, the installer conceals the seams between drywall sheets with joint tape or fiber mesh. Layers of joint compound, sometimes called mud, are typically spread with a drywall trowel or knife. This compound is also applied to any screw holes or defects. The compound is allowed to air dry and then typically sanded smooth before painting. Alternatively, for a better finish, the entire wall may be given a skim coat, a thin layer (about 1 mm or 1⁄32 in) of finishing compound, to minimize the visual differences between the paper and mudded areas after painting.

Another similar skim coating process is called veneer plastering, although it is done slightly thicker (about 2 mm or 3⁄32 in). Veneering uses a slightly different specialized setting compound (""finish plaster"") that contains gypsum and lime putty. This application uses blueboard, which has specially treated paper to accelerate the setting of the gypsum plaster component. This setting has far less shrinkage than the air-dry compounds normally used in drywall, so it only requires one coat. Blueboard also has square edges rather than tapered-edge drywall boards. The tapered drywall boards are used to countersink the tape in taped jointing, whereas the tape in veneer plastering is buried beneath a level surface. One coat veneer plaster over dry board is an intermediate style step between full multi-coat ""wet"" plaster and the limited joint-treatment-only given ""dry"" wall.

The method of installation and type of drywall can reduce sound transmission through walls and ceilings. Several builders' books state that thicker drywall reduces sound transmission, but engineering manuals recommend using multiple layers of drywall, sometimes of different thicknesses and glued together, or special types of drywall designed to reduce noise.[18] Also important are the construction details of the framing with steel studs, wider stud spacing, double studding, insulation, and other details reducing sound transmission. Sound transmission class (STC) ratings can be increased from 33 for an ordinary stud-wall to as high as 59 with double 1⁄2-inch (13 mm) drywall on both sides of a wood stud wall with resilient channels on one side and glass wool batt insulation between the studs.[19]

Sound transmission may be slightly reduced using regular 5⁄8-inch (16 mm) panels (with or without light-gauge resilient metal channels and/or insulation), but it is more effective to use two layers of drywall, sometimes in combination with other factors, or specially designed, sound-resistant drywall.[20]

Drywall is highly vulnerable to moisture due to the inherent properties of the materials that constitute it: gypsum, paper, and organic additives and binders. Gypsum will soften with exposure to moisture and eventually turn into a gooey paste with prolonged immersion, such as during a flood. During such incidents, some, or all, of the drywall in an entire building will need to be removed and replaced. Furthermore, the paper facings and organic additives mixed with the gypsum core are food for mold.

The porosity of the board—introduced during manufacturing to reduce the board's weight, lowering construction time and transportation costs—enables water to rapidly reach the core through capillary action, where mold can grow inside. Water that enters a room from overhead may cause ceiling drywall tape to separate from the ceiling as a result of the grooves immediately behind the tape where the drywall pieces meet becoming saturated. The drywall may also soften around the screws holding the drywall in place, and with the aid of gravity, the weight of the water may cause the drywall to sag and eventually collapse, requiring replacement.

Drywall's paper facings are edible to termites, which can eat the paper if they infest a wall cavity covered with drywall. This causes the painted surface to crumble to the touch, its paper backing material being eaten. In addition to the necessity of patching the damaged surface and repainting, if enough of the paper has been eaten, the gypsum core can easily crack or crumble without it, and the drywall must be removed and replaced.

In many circumstances, especially when the drywall has been exposed to water or moisture for less than 48 hours, professional restoration experts can avoid the cost, inconvenience, and difficulty of removing and replacing the affected drywall. They use rapid drying techniques that eliminate the elements required to support microbial activity while restoring most or all of the drywall.

It is for these reasons that greenboard, a type of drywall with an outer face that is wax- and/or chemically coated to resist mold growth,[21] and ideally cement board are used for rooms expected to have high humidity, primarily kitchens, bathrooms, and laundry rooms.

Foam insulation and the gypsum part of sheetrock are easily chewed out by honeybees when they are setting up a stray nest in a building, and they want to enlarge their nest area.[22]

Some fire barrier walls are constructed of Type X drywall as a passive fire protection item. Gypsum contains the water of crystallization bound in the form of hydrates. When exposed to heat or fire, the resulting decomposition reaction releases water vapor and is endothermic (it absorbs thermal energy), which retards heat transfer until the water in the gypsum is gone. The fire-resistance rating of the fire barrier assembly is increased with additional layers of drywall, up to four hours for walls and three hours for floor/ceiling assemblies.[23] Fire-rated assemblies constructed of drywall are documented in design or certification listing catalogues, including DIN 4102 Part 4 and the Canadian Building Code, Underwriters Laboratories and Underwriters Laboratories of Canada (ULC).

Tests result in code-recognized designs with assigned fire-resistance ratings. The resulting designs become part of the code and are not limited to use by any manufacturer. However, individual manufacturers may also have proprietary designs that they have had third-party tested and approved, provided that the material used in the field configuration can be demonstrated to meet the minimum requirements of Type X drywall and that sufficient layers and thicknesses are used. 

In the Type X gypsum board, special glass fibers are intermixed with the gypsum to reinforce the core of the panels. These fibers reduce the size of the cracks that form as the water is driven off, thereby extending the length of time the gypsum panels resist fire without failure.[24]

Type C gypsum panels provide stronger fire resistance than Type X. The core of Type C panels contains a higher density of glass fibers. The core of Type C panels also contains vermiculite which acts as a shrinkage-compensating additive that expands when exposed to elevated temperatures of a fire. This expansion occurs at roughly the same temperature as the calcination of the gypsum in the core, allowing the core of the Type C panels to remain dimensionally stable in a fire.[24]

Because up to 12% of drywall is wasted during the manufacturing and installation processes and the drywall material is frequently not reused, disposal can become a problem. Some landfill sites have banned the dumping of drywall. Some manufacturers take back waste wallboard from construction sites and recycle them into new wallboard. Recycled paper is typically used during manufacturing. More recently, recycling at the construction site itself has been researched. There is potential for using crushed drywall to amend certain soils at building sites, such as sodic clay and silt mixtures (bay mud), as well as using it in compost.[25] As of 2016, industry standards are being developed to ensure that when and if wallboard is taken back for recycling, quality and composition are maintained.

North America is one of the largest gypsum board users in the world, with a total wallboard plant capacity of 42 billion square feet (3.9 billion square metres) per year, roughly half of the worldwide annual production capacity of 85 billion square feet (7.9 billion square metres).[26] Moreover, the homebuilding and remodeling markets in North America in the late 1990s and early 2000s increased demand. The gypsum board market was one of the biggest beneficiaries of the housing boom as ""an average new American home contains more than 7.31 metric tons of gypsum.""[27]

The introduction in March 2005 of the Clean Air Interstate Rule by the United States Environmental Protection Agency requires fossil-fuel power plants to ""cut sulfur dioxide emissions by 73%"" by 2018.[28] The Clean Air Interstate Rule also requested that the power plants install new scrubbers (industrial pollution control devices) to remove sulfur dioxide present in the output waste gas. Scrubbers use the technique of flue-gas desulfurization (FGD), which produces synthetic gypsum as a usable by-product. In response to the new supply of this raw material, the gypsum board market was predicted to shift significantly. However, issues such as mercury release during calcining need to be resolved.[29]

A substantial amount of defective drywall was imported into the United States from China and incorporated into tens of thousands of homes during rebuilding in 2006 and 2007 following Hurricane Katrina and in other places. Complaints included the structure's foul odour, health effects, and metal corrosion. The emission of sulfurous gases causes this. The same drywall was sold in Asia without problems resulting,[citation needed] but US homes are built much more tightly than homes in China, with less ventilation. Volatile sulfur compounds, including hydrogen sulfide, have been detected as emissions from the imported drywall and may be linked to health problems. These compounds are emitted from many different types of drywall.

Several lawsuits are underway in many jurisdictions, but many of the sheets of drywall are simply marked ""Made in China"", thus making the manufacturer's identification difficult. An investigation by the Consumer Product Safety Commission, CPSC, was underway in 2009.[30] In November 2009, the CPSC reported a ""strong association"" between Chinese drywall and corrosion of pipes and wires reported by thousands of homeowners in the United States. The issue was resolved in 2011, and now all drywall must be tested for volatile sulfur, and any containing more than ten ppm is unable to be sold in the US.

The following variants available in Canada or the United States are listed below:

The term plasterboard is used in Australia and New Zealand. In Australia, the product is often called Gyprock, the name of the largest plasterboard manufacturer.[36] In New Zealand it is also called Gibraltar and Gib board,[37] genericised from the registered trademark (""GIB"") of the locally made product that dominates the local market.[38][39] A specific type of Gibraltar board for use in wet conditions (such as bathrooms and kitchens) is known as AquaGib.

It is made in thicknesses of 10 mm, 13 mm, and 16 mm, and sometimes other thicknesses up to 25 mm. Panels are commonly sold in 1200 mm-wide sheets, which may be 1800, 2400, 3000, 4800, or 6000 mm in length. Sheets are usually secured to either timber or cold-formed steel frames anywhere from 150 to 300 mm centres along the beam and 400 to 600 mm across members.[40]

In both countries, plasterboard has become a widely used replacement for scrim and sarking walls in renovating 19th- and early 20th-century buildings.[citation needed]

Drywall panels in Canada and the United States are made in widths of 48, 54, and 96 inches (1.2, 1.4, and 2.4 m) and varying lengths to suit the application. The most common width is 48 inches; however, 54-inch-wide panels are becoming more popular as 9-foot (2.7 m) ceiling heights become more common. Lengths up to 16 feet (4.9 m) are common; the most common is 8 feet (2.4 m). Common thicknesses are 1⁄2 and 5⁄8 inch (13 and 16 mm);  thicknesses of 1⁄4, 3⁄8, 3⁄4, and 1 inch (6, 10, 19, and 25 mm) are used in specific applications. In many parts of Canada, drywall is commonly referred to as Gyproc.

In Europe, most plasterboard is made in sheets 120 centimetres (47 in) wide; sheets 60 and 90 centimetres (24 and 35 in) wide are also made. Plasterboard 120 centimetres (47 in) wide is most commonly made in 240-centimetre (94 in) lengths; sheets of 250, 260, 270, 280, and 300 centimetres (98, 102, 106, 110, and 118 in) and longer also are common. Thicknesses of plasterboard available are 9.5 to 25 millimetres (3⁄8 to 1 in).[41]

Plasterboard is commonly made with one of three edge treatments: tapered edge, where the long edges of the board are tapered with a wide bevel at the front to allow jointing materials to be finished flush with the main board face; plain edge, used where the whole surface will receive a thin coating (skim coat) of finishing plaster; and beveled on all four sides, used in products specialized for roofing. Major UK manufacturers do not offer four-sided chamfered drywall for general use.
"
Tile Installation Services,"

Tiles are usually thin, square or rectangular coverings manufactured from hard-wearing material such as ceramic, stone, metal, baked clay, or even glass. They are generally fixed in place in an array to cover roofs, floors, walls, edges, or other objects such as tabletops.  Alternatively, tile can sometimes refer to similar units made from lightweight materials such as perlite, wood, and mineral wool, typically used for wall and ceiling applications. In another sense, a tile is a construction tile or similar object, such as rectangular counters used in playing games (see tile-based game). The word is derived from the French word tuile, which is, in turn, from the Latin word tegula, meaning a roof tile composed of fired clay.

Tiles are often used to form wall and floor coverings, and can range from simple square tiles to complex or mosaics. Tiles are most often made of ceramic, typically glazed for internal uses and unglazed for roofing, but other materials are also commonly used, such as glass, cork, concrete and other composite materials, and stone. Tiling stone is typically marble, onyx, granite or slate. Thinner tiles can be used on walls than on floors, which require more durable surfaces that will resist impacts.

Global production of ceramic tiles, excluding roof tiles, was estimated to be 12.7 billion m2 in 2019.[1]

Decorative tilework or tile art should be distinguished from mosaic, where forms are made of great numbers of tiny irregularly positioned tesserae, each of a single color, usually of glass or sometimes ceramic or stone. There are various tile patterns, such as herringbone, staggered, offset, grid, stacked, pinwheel, parquet de Versailles, basket weave, tiles Art, diagonal, chevron, and encaustic which can range in size, shape, thickness, and color.[2]

There are several other types of traditional tiles that remain in manufacture, for example the small, almost mosaic, brightly colored zellij tiles of Morocco and the surrounding countries.

The earliest evidence of glazed brick is the discovery of glazed bricks in the Elamite Temple at Chogha Zanbil, dated to the 13th century BC. Glazed and colored bricks were used to make low reliefs in Ancient Mesopotamia, most famously the Ishtar Gate of Babylon (c. 575 BC), now partly reconstructed in Berlin, with sections elsewhere. Mesopotamian craftsmen were imported for the palaces of the Persian Empire such as Persepolis.

The use of sun-dried bricks or adobe was the main method of building in Mesopotamia where river mud was found in abundance along the Tigris and Euphrates. Here the scarcity of stone may have been an incentive to develop the technology of making kiln-fired bricks to use as an alternative. To strengthen walls made from sun-dried bricks, fired bricks began to be used as an outer protective skin for more important buildings like temples, palaces, city walls, and gates. Making fired bricks is an advanced pottery technique. Fired bricks are solid masses of clay heated in kilns to temperatures of between 950° and 1,150°C, and a well-made fired brick is an extremely durable object. Like sun-dried bricks, they were made in wooden molds but for bricks with relief decorations, special molds had to be made.

Rooms with tiled floors made of clay decorated with geometric circular patterns have been discovered from the ancient remains of Kalibangan, Balakot and Ahladino.[3][4]

Tiling was used in the second century by the Sinhalese kings of ancient Sri Lanka, using smoothed and polished stone laid on floors and in swimming pools. The techniques and tools for tiling is advanced, evidenced by the fine workmanship and close fit of the tiles. Such tiling can be seen in Ruwanwelisaya and Kuttam Pokuna in the city of Anuradhapura. The nine-storied Lovamahapaya (3rd century BC) had copper roof tiles.[5] The roofs were tiled, with red, white, yellow, turquoise and brown tiles. There were also tiles made of bronze. Sigiriya also had an elaborate gatehouse made of timber and brick masonry with multiple tiled roofs. The massive timber doorposts remaining today indicate this.

The Achaemenid Empire decorated buildings with glazed brick tiles, including Darius the Great's palace at Susa, and buildings at Persepolis.[6]  The succeeding Sassanid Empire used tiles patterned with geometric designs, flowers, plants, birds and human beings, glazed up to a centimeter thick.[6]

Early Islamic mosaics in Iran consist mainly of geometric decorations in mosques and mausoleums, made of glazed brick. Typical, turquoise, tiling becomes popular in 10th-11th century and is used mostly for Kufic inscriptions on mosque walls. Seyyed Mosque in Isfahan (AD 1122), Dome of Maraqeh (AD 1147) and the Jame Mosque of Gonabad (1212 AD) are among the finest examples.[6] The dome of Jame' Atiq Mosque of Qazvin is also dated to this period.

The golden age of Persian tilework began during the Timurid Empire. In the moraq technique, single-color tiles were cut into small geometric pieces and assembled by pouring liquid plaster between them. After hardening, these panels were assembled on the walls of buildings. But the mosaic was not limited to flat areas. Tiles were used to cover both the interior and exterior surfaces of domes. Prominent Timurid examples of this technique include the Jame Mosque of Yazd (AD 1324–1365), Goharshad Mosque (AD 1418), the Madrassa of Khan in Shiraz (AD 1615), and the Molana Mosque (AD 1444).[6]

Other important tile techniques of this time include girih tiles, with their characteristic white girih, or straps.

Mihrabs, being the focal points of mosques, were usually the places where most sophisticated tilework was placed. The 14th-century mihrab at Madrasa Imami in Isfahan is an outstanding example of aesthetic union between the Islamic calligrapher's art and abstract ornament. The pointed arch, framing the mihrab's niche, bears an inscription in Kufic script used in 9th-century Qur'an.[7]

One of the best known architectural masterpieces of Iran is the Shah Mosque in Isfahan, from the 17th century. Its dome is a prime example of tile mosaic and its winter praying hall houses one of the finest ensembles of cuerda seca tiles in the world. A wide variety of tiles had to be manufactured in order to cover complex forms of the hall with consistent mosaic patterns. The result was a technological triumph as well as a dazzling display of abstract ornament.[7]

During the Safavid period, mosaic ornaments were often replaced by a haft rang (seven colors) technique. Pictures were painted on plain rectangle tiles, glazed and fired afterwards. Besides economic reasons, the seven colors method gave more freedom to artists and was less time-consuming. It was popular until the Qajar period, when the palette of colors was extended by yellow and orange.[6] The seven colors of Haft Rang tiles were usually black, white, ultramarine, turquoise, red, yellow and fawn.

The Persianate tradition continued and spread to much of the Islamic world, notably the İznik pottery of Turkey under the Ottoman Empire in the 16th and 17th centuries. Palaces, public buildings, mosques and türbe mausoleums were heavily decorated with large brightly colored patterns, typically with floral motifs, and friezes of astonishing complexity, including floral motifs and calligraphy as well as geometric patterns.

Islamic buildings in Bukhara in central Asia (16th-17th century) also exhibit very sophisticated floral ornaments. In South Asia monuments and shrines adorned with Kashi tile work from Persia became a distinct feature of the shrines of Multan and Sindh. The Wazir Khan Mosque in Lahore stands out as one of the masterpieces of Kashi time work from the Mughal period.

The zellige tradition of Arabic North Africa uses small colored tiles of various shapes to make very complex geometric patterns. It is halfway to mosaic, but as the different shapes must be fitted precisely together, it falls under tiling. The use of small coloured glass fields also make it rather like enamelling, but with ceramic rather than metal as the support.

Medieval Europe made considerable use of painted tiles, sometimes producing very elaborate schemes, of which few have survived. Religious and secular stories were depicted. The imaginary tiles with Old Testament scenes shown on the floor in Jan van Eyck's 1434 Annunciation in Washington are an example. The 14th century ""Tring tiles"" in the British Museum show childhood scenes from the Life of Christ, possibly for a wall rather than a floor,[8] while their 13th century ""Chertsey Tiles"", though from an abbey, show scenes of Richard the Lionheart battling with Saladin in very high-quality work.[9] Medieval letter tiles were used to create Christian inscriptions on church floors.

Medieval influences between Middle Eastern tilework and tilework in Europe were mainly through Islamic Iberia and the Byzantine and Ottoman Empires. The Alhambra zellige are said to have inspired the tessellations of M. C. Escher.[citation needed]  Medieval encaustic tiles were made of multiple colours of clay, shaped and baked together to form a pattern that, rather than sitting on the surface, ran right through the thickness of the tile, and thus would not wear away.

Azulejos are derived from zellij, and the name is likewise derived. The term is both a simple Portuguese and Spanish term for zellige, and a term for later tilework following the tradition. Some azujelos are small-scale geometric patterns or vegetative motifs, some are blue monochrome and highly pictorial, and some are neither. The Baroque period produced extremely large painted scenes on tiles, usually in blue and white, for walls. Azulejos were also used in Latin American architecture.

Delftware wall tiles, typically with a painted design covering only one (rather small) blue and white tile, were ubiquitous in Holland and widely exported over Northern Europe from the 16th century on, replacing many local industries. Several 18th century royal palaces had porcelain rooms with the walls entirely covered in porcelain in tiles or panels. Surviving examples include ones at Capodimonte, Naples, the Royal Palace of Madrid and the nearby Royal Palace of Aranjuez.

The Victorian period saw a great revival in tilework, largely as part of the Gothic Revival, but also the Arts and Crafts Movement. Patterned tiles, or tiles making up patterns, were now mass-produced by machine and reliably level for floors and cheap to produce, especially for churches, schools and public buildings, but also for domestic hallways and bathrooms. For many uses the tougher encaustic tile was used. Wall tiles in various styles also revived; the rise of the bathroom contributing greatly to this, as well as greater appreciation of the benefit of hygiene in kitchens. William De Morgan was the leading English designer working in tiles, strongly influenced by Islamic designs.

Since the Victorian period tiles have remained standard for kitchens and bathrooms, and many types of public area.

Panot is a type of outdoor cement tile and the associated paving style, both found in Barcelona. In 2010, around 5,000,000 m2 (54,000,000 sq ft) of Barcelona streets were panot-tiled.[10]

Portugal and São Luís continue their tradition of azulejo tilework today, with tiles used to decorate buildings, ships,[11] and even rocks.

With exceptions, notably the Porcelain Tower of Nanjing, decorated tiles or glazed bricks do not feature largely in East Asian ceramics.

Philippines
In 17th century during the colonialization of Spain in the Philippines, they introduced the Baldozas Mosaicos to describe the Mediterranean cement tiles, but they are now more commonly referred to as Machuca tiles during the 19th AD, named after Don Pepe, the son of the renowned producer of Baldozas Mosaicos in the Philippines, Don Jose Machuca by Romero

Roof tiles are overlapping tiles designed mainly to keep out precipitation such as rain or snow, and are traditionally made from locally available materials such as clay or slate. Later tiles have been made from materials such as concrete, and plastic.

Roof tiles can be affixed by screws or nails, but in some cases historic designs such as Marseilles tiles utilize interlocking systems that can be self-supporting. Tiles typically cover an underlayment system, which seals the roof against water intrusion.[12]

Clay roof tiles historically gained their color purely from the clay that they were composed of, resulting in largely red, orange, and tan colored roofs. Over time some cultures, notably in Asia, began to apply glazes to clay tiles, achieving a wide variety of colors and combinations. Modern clay roof tiles typically source their color from kiln firing conditions, the application of glaze, or the use of a ceramic engobe.[13] Contrary to popular belief a glaze does not weatherproof a tile, the porosity of the clay body is what determines how well a tile will survive harsh weather conditions.[14]

Floor tiles are commonly made of ceramic or stone, although recent technological advances have resulted in rubber or glass tiles for floors as well. Ceramic tiles may be painted and glazed. Small mosaic tiles may be laid in various patterns. Floor tiles are typically set into mortar consisting of sand, Portland cement and often a latex additive. The spaces between the tiles are commonly filled with sanded or unsanded floor grout, but traditionally mortar was used.

Natural stone tiles can be beautiful but as a natural product they are less uniform in color and pattern, and require more planning for use and installation. Mass-produced stone tiles are uniform in width and length. Granite or marble tiles are sawn on both sides and then polished or finished on the top surface so that they have a uniform thickness. Other natural stone tiles such as slate are typically ""riven"" (split) on the top surface so that the thickness of the tile varies slightly from one spot on the tile to another and from one tile to another. Variations in tile thickness can be handled by adjusting the amount of mortar under each part of the tile, by using wide grout lines that ""ramp"" between different thicknesses, or by using a cold chisel to knock off high spots.

Some stone tiles such as polished granite, marble, and travertine are very slippery when wet. Stone tiles with a riven surface such as slate or with a sawn and then sandblasted or honed surface will be more slip-resistant. Ceramic tiles for use in wet areas can be made more slip-resistant by using very small tiles so that the grout lines acts as grooves, by imprinting a contour pattern onto the face of the tile, or by adding a non-slip material, such as sand, to the glazed surface.

The hardness of natural stone tiles varies such that some of the softer stone (e.g. limestone) tiles are not suitable for very heavy-traffic floor areas. On the other hand, ceramic tiles typically have a glazed upper surface and when that becomes scratched or pitted the floor looks worn, whereas the same amount of wear on natural stone tiles will not show, or will be less noticeable.

Natural stone tiles can be stained by spilled liquids; they must be sealed and periodically resealed with a sealant in contrast to ceramic tiles which only need their grout lines sealed. However, because of the complex, nonrepeating patterns in natural stone, small amounts of dirt on many natural stone floor tiles do not show.

The tendency of floor tiles to stain depends not only on a sealant being applied, and periodically reapplied, but also on their porosity or how porous the stone is. Slate is an example of a less porous stone while limestone is an example of a more porous stone. Different granites and marbles have different porosities with the less porous ones being more valued and more expensive.

Most vendors of stone tiles emphasize that there will be variation in color and pattern from one batch of tiles to another of the same description and variation within the same batch. Stone floor tiles tend to be heavier than ceramic tiles and somewhat more prone to breakage during shipment.

Rubber floor tiles have a variety of uses, both in residential and commercial settings. They are especially useful in situations where it is desired to have high-traction floors or protection for an easily breakable floor. Some common uses include flooring of garage, workshops, patios, swimming pool decks, sport courts, gyms, and dance floors.

Plastic floor tiles including interlocking floor tiles that can be installed without adhesive or glue are a recent innovation and are suitable for areas subject to heavy traffic, wet areas and floors that are subject to movement, damp or contamination from oil, grease or other substances that may prevent adhesion to the substrate. Common uses include old factory floors, garages, gyms and sports complexes, schools and shops.

Ceiling tiles are lightweight tiles used inside buildings. They are placed in an aluminium grid; they provide little thermal insulation but are generally designed either to improve the acoustics of a room or to reduce the volume of air being heated or cooled.

Mineral fiber tiles are fabricated from a range of products; wet felt tiles can be manufactured from perlite, mineral wool, and fibers from recycled paper; stone wool tiles are created by combining molten stone and binders which is then spun to create the tile; gypsum tiles are based on the soft mineral and then finished with vinyl, paper or a decorative face.[citation needed]

Ceiling tiles very often have patterns on the front face; these are there in most circumstances to aid with the tiles ability to improve acoustics.[citation needed]

Ceiling tiles also provide a barrier to the spread of smoke and fire. Breaking, displacing, or removing ceiling tiles enables hot gases and smoke from a fire to rise and accumulate above detectors and sprinklers. Doing so delays their activation, enabling fires to grow more rapidly.[15]

Ceiling tiles, especially in old Mediterranean houses, were made of terracotta and were placed on top of the wooden ceiling beams and upon those were placed the roof tiles. They were then plastered or painted, but nowadays are usually left bare for decorative purposes.

Modern-day tile ceilings may be flush mounted (nail up or glue up) or installed as dropped ceilings.

Ceramic materials for tiles include earthenware, stoneware and porcelain.[16] Terracotta is a traditional material used for roof tiles.[17]

This is a US term, and defined in ASTM standard C242 as a ceramic mosaic tile or paver that is generally made by dust-pressing and of a composition yielding a tile that is dense, fine-grained, and smooth, with sharply-formed face, usually impervious. The colours of such tiles are generally clear and bright.[18]

The ISO 13006 defines a ""porcelain tile"" as a ""fully vitrified tile with water absorption less than or equal to 0.5%, belonging to groups AIa and BIa (of ISO 13006)."".[19] The ANSI defines as ""a ceramic tile that has 'a water absorption of 0.5%' or less.” It is made generally by the pressed or extruded method.""[20]

Similar to mosaics or other patterned tiles, pebble tiles are tiles made up of small pebbles attached to a backing. The tile is generally designed in an interlocking pattern so that final installations fit of multiple tiles fit together to have a seamless appearance. A relatively new tile design, pebble tiles were originally developed in Indonesia using pebbles found in various locations in the country. Today, pebble tiles feature all types of stones and pebbles from around the world.

Printing techniques and digital manipulation of art and photography are used in what is known as ""custom tile printing"". Dye sublimation printers, inkjet printers and ceramic inks and toners permit printing on a variety of tile types yielding photographic-quality reproduction.[21] Using digital image capture via scanning or digital cameras, bitmap/raster images can be prepared in photo editing software programs. Specialized custom-tile printing techniques permit transfer under heat and pressure or the use of high temperature kilns to fuse the picture to the tile substrate. This has become a method of producing custom tile murals for kitchens, showers, and commercial decoration in restaurants, hotels, and corporate lobbies.

[22] Recent technology applied to Digital ceramic and porcelain printers allow images to be printed with a wider color gamut and greater color stability even when fired in a kiln up to 2200 °F.

A method for custom tile printing involving a diamond-tipped drill controlled by a computer. Compared with the laser engravings, diamond etching is in almost every circumstance more permanent.[citation needed]

Certain shapes of tiles, most obviously rectangles, can be replicated to cover a surface with no gaps. These shapes are said to tessellate (from the Latin tessella, 'tile') and such a tiling is called a tessellation. Geometric patterns of some Islamic polychrome decorative tilings are rather complicated (see Islamic geometric patterns and, in particular, Girih tiles), even up to supposedly quasiperiodic ones, similar to Penrose tilings.
"
Carpentry Services,"Carpentry is a skilled trade and a craft in which the primary work performed is the cutting, shaping and installation of building materials during the construction of buildings, ships, timber bridges, concrete formwork, etc. Carpenters traditionally worked with natural wood and did rougher work such as framing, but today many other materials are also used[1] and sometimes the finer trades of cabinetmaking and furniture building are considered carpentry. In the United States, 98.5% of carpenters are male, and it was the fourth most male-dominated occupation in the country in 1999. In 2006 in the United States, there were about 1.5 million carpentry positions. Carpenters are usually the first tradesmen on a job and the last to leave.[2] Carpenters normally framed post-and-beam buildings until the end of the 19th century; now this old-fashioned carpentry is called timber framing. Carpenters learn this trade by being employed through an apprenticeship training—normally four years—and qualify by successfully completing that country's competence test in places such as the United Kingdom, the United States, Canada, Switzerland, Australia and South Africa.[3] It is also common that the skill can be learned by gaining work experience other than a formal training program, which may be the case in many places.

Carpentry covers various services, such as furniture design and construction, door and window installation or repair, flooring installation, trim and molding installation, custom woodworking, stair construction, structural framing, wood structure and furniture repair, and restoration.

The word ""carpenter"" is the English rendering of the Old French word carpentier (later, charpentier) which is derived from the Latin carpentarius [artifex], ""(maker) of a carriage.""[4] The Middle English and Scots word (in the sense of ""builder"") was wright (from the Old English wryhta, cognate with work), which could be used in compound forms such as wheelwright or boatwright.[5]

In the UK, carpentry is used to describe the skill involved in first fixing of timber items such as construction of roofs, floors and timber framed buildings, i.e. those areas of construction that are normally hidden in a finished building. An easy way to envisage this is that first fix work is all that is done before plastering takes place. The second fix is done after plastering takes place. Second fix work, the installation of items such as skirting boards, architraves, doors, and windows are generally regarded as carpentry, however, the off-site manufacture and pre-finishing of the items is regarded as joinery.[6][7] Carpentry is also used to construct the formwork into which concrete is poured during the building of structures such as roads and highway overpasses. In the UK, the skill of making timber formwork for poured or in situ concrete is referred to as shuttering.

Carpentry in the United States is historically defined similarly to the United Kingdom as the ""heavier and stronger""[8] work distinguished from a joiner ""...who does lighter and more ornamental work than that of a carpenter..."" although the ""...work of a carpenter and joiner are often combined.""[9] Joiner is less common than the terms finish carpenter or cabinetmaker. The terms housewright and barnwright were used historically and are now occasionally used by carpenters who work using traditional methods and materials. Someone who builds custom concrete formwork is a form carpenter.

Along with stone, wood is among the oldest building materials.  The ability to shape it into tools, shelter, and weapons improved with technological advances from the Stone Age to the Bronze Age to the Iron Age. Some of the oldest archaeological evidence of carpentry are water well casings. These include an oak and hazel structure dating from 5256 BC, found in Ostrov, Czech Republic,[10] and one built using split oak timbers with mortise and tenon and notched corners excavated in eastern Germany, dating from about 7,000 years ago in the early Neolithic period.[11]

Relatively little history of carpentry was preserved before written language.  Knowledge and skills were simply passed down over the generations.  Even the advent of cave painting and writing recorded little. The oldest surviving complete architectural text is Vitruvius' ten books collectively titled De architectura, which discuss some carpentry.[citation needed]  It was only with the invention of the printing press in the 15th century that this began to change, albeit slowly, with builders finally beginning to regularly publish guides and pattern books in the 18th and 19th centuries.

Some of the oldest surviving wooden buildings in the world are temples in China such as the Nanchan Temple built in 782, Greensted Church in England, parts of which are from the 11th century, and the stave churches in Norway from the 12th and 13th centuries.


By the 16th century, sawmills were coming into use in Europe. The founding of America was partly based on a desire to extract resources from the new continent including wood for use in ships and buildings in Europe. In the 18th century part of the Industrial Revolution was the invention of the steam engine and cut nails.[12] These technologies combined with the invention of the circular saw led to the development of balloon framing which was the beginning of the decline of traditional timber framing.
The 19th century saw the development of electrical engineering and distribution which allowed the development of hand-held power tools, wire nails, and machines to mass-produce screws. In the 20th century, portland cement came into common use and concrete foundations allowed carpenters to do away with heavy timber sills. Also, drywall (plasterboard) came into common use replacing lime plaster on wooden lath. Plywood, engineered lumber, and chemically treated lumber also came into use.[13]

For types of carpentry used in America see American historic carpentry.

Carpentry requires training which involves both acquiring knowledge and physical practice. In formal training a carpenter begins as an apprentice, then becomes a journeyman, and with enough experience and competency can eventually attain the status of a master carpenter. Today pre-apprenticeship training may be gained through non-union vocational programs such as high school shop classes and community colleges.

Informally a laborer may simply work alongside carpenters for years learning skills by observation and peripheral assistance. While such an individual may obtain journeyperson status by paying the union entry fee and obtaining a journeyperson's card (which provides the right to work on a union carpentry crew) the carpenter foreperson will, by necessity, dismiss any worker who presents the card but does not demonstrate the expected skill level.

Carpenters may work for an employer or be self-employed. No matter what kind of training a carpenter has had, some U.S. states require contractors to be licensed which requires passing a written test and having minimum levels of insurance.

Formal training in the carpentry trade is available in seminars, certificate programs, high-school programs, online classes, in the new construction, restoration, and preservation carpentry fields.[14] Sometimes these programs are called pre-apprenticeship training.

In the modern British construction industry, carpenters are trained through apprenticeship schemes where general certificates of secondary education (GCSE) in Mathematics, English, and Technology help but are not essential. However, this is deemed the preferred route, as young people can earn and gain field experience whilst training towards a nationally recognized qualification.

There are two main divisions of training: construction-carpentry and cabinetmaking. During pre-apprenticeship, trainees in each of these divisions spend 30 hours a week for 12 weeks in classrooms and indoor workshops learning mathematics, trade terminology, and skill in the use of hand and power tools. Construction-carpentry trainees also participate in calisthenics to prepare for the physical aspect of the work.

Upon completion of pre-apprenticeship, trainees who have passed the graded curriculum (taught by highly experienced journeyperson carpenters) are assigned to a local union and to union carpentry crews at work on construction sites or in cabinet shops as First Year Apprentices. Over the next four years, as they progress in status to Second Year, Third Year, and Fourth Year Apprentice, apprentices periodically return to the training facility every three months for a week of more detailed training in specific aspects of the trade.

In the United States, fewer than 5% of carpenters identify as female. A number of schools in the U.S.
appeal to non-traditional tradespeople by offering carpentry classes for and taught by women, including Hammerstone: Carpentry for Women in Ithaca, NY, Yestermorrow in Waitsfield, VT and Oregon Tradeswomen in Portland, OR.

Tradesmen in countries such as Germany and Australia are required to fulfill formal apprenticeships (usually three to four years) to work as professional carpenters. Upon graduation from the apprenticeship, they are known as journeyperson carpenters.

Up through the 19th and even the early 20th century, the journeyperson traveled to another region of the country to learn the building styles and techniques of that area before (usually) returning home. In modern times, journeypeople are not required to travel, and the term now refers to a level of proficiency and skill. Union carpenters in the United States, that is, members of the United Brotherhood of Carpenters and Joiners of America, are required to pass a skills test to be granted official journeyperson status, but uncertified professional carpenters may also be known as journeypersons based on their skill level, years of experience, or simply because they support themselves in the trade and not due to any certification or formal woodworking education.

Professional status as a journeyperson carpenter in the United States may be obtained in a number of ways. Formal training is acquired in a four-year apprenticeship program administered by the United Brotherhood of Carpenters and Joiners of America, in which journeyperson status is obtained after successful completion of twelve weeks of pre-apprenticeship training, followed by four years of on-the-job field training working alongside journeyperson carpenters. The Timber Framers Guild also has a formal apprenticeship program for traditional timber framing. Training is also available in groups like the Kim Bồng woodworking village in Vietnam where apprentices live and work to learn woodworking and carpentry skills.

In Canada, each province sets its own standards for apprenticeship. The average length of time is four years and includes a minimum number of hours of both on-the-job training and technical instruction at a college or other institution. Depending on the number of hours of instruction an apprentice receives, they can earn a Certificate of Proficiency, making them a journeyperson, or a Certificate of Qualification, which allows them to practice a more limited amount of carpentry. Canadian carpenters also have the option of acquiring an additional Interprovincial Red Seal that allows them to practice anywhere in Canada. The Red Seal requires the completion of an apprenticeship and an additional examination.

After working as a journeyperson for a while, a carpenter may go on to study or test as a master carpenter. In some countries, such as Germany, Iceland and Japan, this is an arduous and expensive process, requiring extensive knowledge (including economic and legal knowledge) and skill to achieve master certification; these countries generally require master status for anyone employing and teaching apprentices in the craft. In others, like the United States, 'master carpenter' can be a loosely used term to describe any skilled carpenter.

Fully trained carpenters and joiners will often move into related trades such as shop fitting, scaffolding, bench joinery, maintenance and system installation.

Carpenters traditionally worked with natural wood which has been prepared by splitting (riving), hewing, or sawing with a pit saw or sawmill called lumber (American English) or timber (British English). Today natural and engineered lumber and many other building materials carpenters may use are typically prepared by others and delivered to the job site. In 2013 the carpenters union in America used the term carpenter for a catch-all position. Tasks performed by union carpenters include installing ""...flooring, windows, doors, interior trim, cabinetry, solid surface, roofing, framing, siding, flooring, insulation, ...acoustical ceilings, computer-access flooring, metal framing, wall partitions, office furniture systems, and both custom or factory-produced materials, ...trim and molding,... ceiling treatments, ... exposed columns and beams, displays, mantels, staircases...metal studs, metal lath, and drywall...""[15]

Carpentry is often hazardous work. Types of woodworking and carpentry hazards include: machine hazards, flying materials, tool projection, fire and explosion, electrocution, noise, vibration, dust, and chemicals.
In the United States the Occupational Safety and Health Administration (OSHA) tries to prevent illness, injury, and fire through regulations. However, self-employed workers are not covered by the OSHA act.[16] OSHA claims that ""Since 1970, workplace fatalities have been reduced by more than 65 percent and occupational injury and illness rates have declined by 67 percent. At the same time, U.S. employment has almost doubled.""[17] The leading cause of overall fatalities, called the ""fatal four,"" are falls, followed by struck by object, electrocution, and caught-in/between. In general construction ""employers must provide working conditions that are free of known dangers. Keep floors in work areas in a clean and, so far as possible, dry condition. Select and provide required personal protective equipment at no cost to workers. Train workers about job hazards in a language that they can understand.""[18] Examples of how to prevent falls includes placing railings and toe-boards at any floor opening which cannot be well covered and elevated platforms and safety harness and lines, safety nets, stair railings, and handrails.

Safety is not just about the workers on the job site. Carpenters' work needs to meet the requirements in the Life Safety Code such as in stair building and building codes to promote long-term quality and safety for the building occupants.

[1]
"
Millwork Services,"Millwork is historically any wood-mill produced decorative material used in building construction.  Stock profiled and patterned millwork building components fabricated by milling at a planing mill can usually be installed with minimal alteration.  Today, millwork may encompass items that are made using alternatives to wood, including synthetics, plastics, and wood-adhesive composites.

Often specified by architects and designers, millwork products are considered a design element within a room or on a building to create a mood or design theme. Millwork products are used in both interior and exterior applications and can serve as either decorative or functional features of a building.

Woodworking skills originally formed around  wood carving, carpentry, parquetry, and cabinet making in ancient China. Historically, the term millwork applied to building elements made specifically from wood.[1]  During the ""Golden Age"" of mill working (1880–1910), virtually everything in the house was made from wood.[2]  During this time, the millwork produced in the United States became standardized nationwide.[3]

Today, the increase in the use of synthetic materials has led many professionals to consider any item that is composed of a combination of wood and synthetic elements to also be properly defined as millwork.  This includes products that make use of pressed-wood chips in the design, such as melamine coated shelving.[3]

Millwork building materials include the ready-made carpentry elements usually installed in any building. Many of the specific features in a space are created using different types of architectural millwork: doors, windows, transoms, sidelights, molding, trim, stair parts, and cabinetry to name just a few. The primary material used in millwork items today are most often produced from soft or hardwood lumber. Other materials used in millwork products include MDF (medium density fiberboard), finger-jointed wood, composite materials, particle board and fiberglass. Some millwork products like doors, windows and stair parts now incorporate the use of steel, stainless steel, aluminum, and glass components.[1]

Most wood products used for millwork require decorative finish coatings. These finishes include stain and semi-transparent finishes or paint.[1] The finishes protect the wood from decay, warping, splitting, and fading.  Millwork building materials can usually be installed with little or no modification as part of the construction process.[3]

There are two types of manufacturers of millwork goods.  In one, referred to as ""stock millwork"", commodity fabricators mass-produce trims and building components—with the end product being low cost, interchangeable items for commercial or home builders.  In another, the product is custom produced for individuals or individual building projects—usually a costlier option[4] which is referred to as ""architectural millwork.[5]

Millwork building materials are used for both decoration and function in buildings. Exterior doors and windows are typically tested by independent agencies and rated for energy efficiency. They can also be impact-rated, fire-rated, and can be specified to reduce sound transference.[citation needed]  Interior millwork products are not rated for energy efficiency. These products are used primarily as a decorative feature, but will often serve functions for privacy, storage, and sound-deadening.[citation needed]


"
General Handyman Services,"

A handyman also known as a fixer,[1] handyperson[2][3] or handyworker,[4][5] maintenance worker, maintenance man, repairman, repair worker, or repair technician,[6]  such as basic carpentry, plumbing, minor electrical wiring and property maintenance, wide range of repairs, typically for keeping buildings, shops or equipment around the home in good condition. These tasks include trade skills, repair work, maintenance work, are both interior and exterior, and are sometimes described as ""side work"", ""odd jobs"" or ""fix-up tasks"". Specifically, these jobs could be light plumbing jobs such as fixing a leaky toilet or light electric jobs such as changing a light fixture or bulb.


The term handyman increasingly describes a paid worker, but it also includes non-paid homeowners or do-it-yourselfers. The term handyman is also occasionally applied as an adjective to describe politicians or business leaders who make substantial organizational changes, such as overhauling a business structure or administrative division.[7][8]
Many people can do common household repairs. There are resources on the Internet, as well as do-it-yourself guide books,[9] with instructions about how to complete a wide range of projects. Sometimes the fix-it skill is seen as genetic, and people lacking such skills are said to ""lack the handy-man gene"".[10] One trend is that fewer homeowners are inclined to do fix-up jobs, perhaps because of time constraints, perhaps because of lack of interest; one reporter commented ""my family's fix-it gene petered out before it reached my generation.""[11]

Historically being a handyman was considered a less prestigious occupation than a specialist such as a plumber, electrician, or carpenter. With the emergence of large national chains, there have been efforts to change that perception by emphasizing the professionalism of the trade and that a handyman is a technician with multiple skills and a wide range of knowledge. Handyman tools sometimes become useful in different places: for example, when a proper cranial drill was not available, an Australian doctor used a handyman's drill in 2009 to open a hole in the head of a 13-year-old boy to relieve pressure after a brain injury; the boy's life was saved.[12]


Tasks range from minor to major, from unskilled to highly skilled, and include painting, drywall repair, remodeling, minor plumbing work, minor electrical work, household carpentry, sheetrock, crown moulding, and furniture assembly (see more complete list below.) 
An estimate was that in 2003, the market for home-maintenance and repair spending was up 14% from 2001 to 2003.[13] Another estimate was that the market in the United States was $126 billion and was increasing by about 4% annually.[11] American homes are aging; one estimate was that in 2007, more than half of all homes are older than 25 years.[13] And, as populations worldwide tend to become older, on average, and since increasingly elderly people will be less inclined and able to maintain their homes, it is likely that demand for handyman services will grow.[original research?]

Many towns have handymen who work part-time, for friends or family or neighbors, who are skilled in a variety of tasks. Sometimes they advertise in newspapers or online. They vary in quality, professionalism, skill level, and price. Contractors often criticize the work of previous contractors, and this practice is not limited to handymen, but to all trades.[14] Handymen have advertised their services through flyers and mailings; in addition, free websites such as Craigslist and SkillSlate help customers and handymen find each other.[15]

In 2009, there were national handyman service firms which handle such nationwide tasks as public relations, marketing, advertising, and signage, but sell specific territories to franchise owners. A franchise contract typically gives a franchise owner the exclusive right to take service calls within a given geographical area. The websites of these firms put possible customers in touch with local owners, which have handymen and trucks. Customers call the local numbers. Typically, these firms charge around $100/hour, although fees vary by locality and time of year. In many parts of the world, there are professional handyworker firms that do small home or commercial projects which claim possible advantages such as having workers who are insured and licensed. Their branch offices schedule service appointments for full-time and part-time handymen to visit and make repairs, and sometimes coordinate with sub-contractors.

One Lehman Brothers executive, after being let go from the Wall Street firm, bought a Union, New Jersey franchise from a national handyman firm.[16] A franchise was approximately $110,000 with a franchise fee of $14,900, according to a spokesperson for a national handyman franchise.[16]

Some see a benefit of franchising as ""entrepreneurship under the safety net of a tried-and-true business umbrella""[16] but forecast a 1.2 percent decrease in franchise businesses during the 2008–2009 recession.[16] In 2005, according to a survey released by the Washington-based International Franchise Association showed 909,000 franchised establishments in the United States employing some 11 million people.[16] Franchises offer training, advertising and information technology support, lower procurement costs and access to a network of established operators.[16]

Franchise handyman firms sometimes pitch clients by asking prospective customers about their unresolved ""to-do lists"".[17] The firm does odd jobs, carpentry, and repairs.[17] Trends such as a ""poverty of time"" and a ""glut of unhandy husbands"" has spurred the business.[17] Technicians do a range of services including tile work, painting, and wallpapering. ""One firm"" charges $88 per hour.[17] The firm targets a work category which full-fledged remodelers and contractors find unprofitable.[17] A consumer was quoted by a reporter explaining the decision to hire one firm: ""'I couldn't find anyone to come in and help me because the jobs were too small', said Meg Beck of Huntington, who needed some painting and carpentry done. She turned to one franchise firm and said she liked the fact that the service has well-marked trucks and uniformed technicians and that a dispatcher called with the names of the crew before they showed up.""[17] There are indications that these businesses are growing.[17] There are different firms operating.[13][18]

Other competitors include online referral services.[11] In addition, some large home centers offer installation services for products such as cabinets and carpet installation.[17] Sometimes homeowners contact a professional service after trying, but failing, to do repair work themselves; in one instance, a Minneapolis homeowner attempted a project but called a technician to finish the project, and the overall cost was substantial.[19]

Generally, in the United States, there are few legal issues if an unpaid homeowner works on a project within their own home, with some exceptions. Some jurisdictions require paid handymen to be licensed and/or insured. New Jersey, for example, requires all handymen who work in for-profit businesses serving residential and commercial customers, to be registered and insured.[20] Often handymen are barred from major plumbing, electrical wiring, or gas-fitting projects for safety reasons, and authorities sometimes require workers to be licensed in particular trades. However, minor plumbing work such as fixing water taps, connecting sinks, fixing leaks, or installing new washing machines, are usually permitted to be done without licensing. Many handymen are insured under a property damage liability policy, so that accidental property damage from negligence or accidents are covered.

The handyman image recurs in popular culture. There have been songs about handymen recorded by Elvis Presley in 1964, Del Shannon in 1964, James Taylor in 1977.[21] There are femme-fatale TV characters who fall for handymen.[22] Handymen have been portrayed in books and films, generally positively, as do-gooder helpful types, but not particularly smart or ambitious. In a book by author Carolyn See called The Handyman, a handyman is really an aspiring but discouraged artist who transforms the lives of people he works for, as well as having sexual encounters with some of his clients, and his experiences improve his artistic output.[23] The book suggests handymen discover ""the appalling loneliness of the women who call him for help"" whose needs are sometimes ""comic"", sometimes ""heartbreaking"", and deep down ""sexual"".[24] A 1980 movie called The Handyman was about a carpenter-plumber who was ""good at what he does"" but is ""too honest and trusting"", and gets taken advantage of by ""women who find him handsome and understanding;"" the movie earned negative reviews from critic Vincent Canby.[25] Other movies have used a formula of sexy-handyman meets bored-housewives, such as The Ups and Downs of a Handyman, a 1975 movie in which ""Handsome Bob also finds he's a fast favorite with the local housewives, who seem to have more than small repairs on their minds.""[26] In Canada, Canada's Worst Handyman a reality show in which handyman contestants try their best on jobs in order to not be labeled ""worst handyman"". Home Improvement is an American television sitcom starring Tim Allen, which aired 1991 to 1999. On the children's television show Mister Rogers' Neighborhood, Handyman Negri was one of the characters residing in The Neighborhood of Make-Believe, as well as the neighborhood Mister Rogers resides in. Handy Manny is an American/Hispanic preschool television show that airs on Playhouse Disney and stars a handyman cartoon character named Manny. The Belgian comics and media franchise The Smurfs depicts Handy Smurf with traditional handyman's accoutrements, such as overalls, carpenter's pencil and work hat. Happy Tree Friends also has an orange beaver named Handy who is a handyman.

Two handymen are also the main characters in the Czechoslovak, later Czech, stop-motion animated series Pat & Mat.[27]
"
Insulation Services,"Building insulation is material used in a building (specifically the building envelope) to reduce the flow of thermal energy. While the majority of insulation in buildings is for thermal purposes, the term also applies to acoustic insulation, fire insulation, and impact insulation (e.g. for vibrations caused by industrial applications). Often an insulation material will be chosen for its ability to perform several of these functions at once.

Since prehistoric times, humans have created thermal insulation with materials such as animal fur and plants. With the agricultural development, earth, stone, and cave shelters arose. In the 19th century, people started to produce insulated panels and other artificial materials. Now, insulation is divided into two main categories: bulk insulation and reflective insulation. Buildings typically use a combination.

Insulation is an important economic and environmental investment for buildings. By installing insulation, buildings  use less energy for heating and cooling and occupants experience less thermal variability. Retrofitting buildings with further insulation is an important climate change mitigation tactic,[1][2] especially when buildings are heated by oil, natural gas, or coal-based electricity. Local and national governments and utilities often have a mix of incentives and regulations to encourage insulation efforts on new and renovated buildings as part of efficiency programs in order to reduce grid energy use and its related environmental impacts and infrastructure costs.

Thermal insulation usually refers to the use of appropriate insulation materials and design adaptations for buildings to slow the transfer of heat through the enclosure to reduce heat loss and gain.[3] The transfer of heat is caused by the temperature difference between indoors and outdoors.[3] Heat may be transferred either by conduction, convection, or radiation. The rate of transmission is closely related to the propagating medium.[3] Heat is lost or gained by transmission through the ceilings, walls, floors, windows, and doors. This heat reduction and acquisition are usually unwelcome. It not only increases the load on the HVAC system resulting in more energy wastes but also reduces the thermal comfort of people in the building. Thermal insulation in buildings is an important factor in achieving thermal comfort for its occupants.[4] Insulation reduces unwanted heat loss or gain and can decrease the energy demands of heating and cooling systems. It does not necessarily deal with issues of adequate ventilation and may or may not affect the level of sound insulation. In a narrow sense, insulation can just refer to the insulation materials employed to slow heat loss, such as: cellulose, glass wool, rock wool, polystyrene, polyurethane foam, vermiculite, perlite, wood fiber, plant fiber (cannabis, flax, cotton, cork, etc.), recycled cotton denim, straw, animal fiber (sheep's wool), cement, and earth or soil, reflective insulation (also known as a radiant barrier) but it can also involve a range of designs and techniques to address the main modes of heat transfer - conduction, radiation, and convection materials.

Most of the materials in the above list only retain a large amount of air or other gases between the molecules of the material. The gas conducts heat much less than the solids. These materials can form gas cavities, which can be used to insulate heat with low heat transfer efficiency. This situation also occurs in the fur of animals and birds feathers, animal hair can employ the low thermal conductivity of small pockets of gas, so as to achieve the purpose of reducing heat loss.

The effectiveness of reflective insulation (radiant barrier) is commonly evaluated by the reflectivity (emittance) of the surface with airspace facing to the heat source.

The effectiveness of bulk insulation is commonly evaluated by its R-value, of which there are two – metric (SI) (with unit K⋅W−1⋅m2) and US customary (with unit °F⋅ft2⋅h/BTU), the former being 0.176 times the latter numerically, or the reciprocal quantity the thermal conductivity or U-value W⋅K−1⋅m−2.
For example, in the US the insulation standard for attics, is recommended to be at least R-38 US units, (equivalent to R-6.7 or a U value of 0.15 in SI units).[5]  The equivalent standard in the UK are technically comparable, the approved document L would normally require an average U value over the roof area of 0.11 to 0.18 depending on the age of the property and the type of roof construction. Newer buildings have to meet a higher standard than those built under previous versions of the regulations.
It is important to realise a single R-value or U-value does not take into account the quality of construction or local environmental factors for each building. Construction quality issues can include inadequate vapor barriers and problems with draft-proofing. In addition, the properties and density of the insulation material itself are critical. Most countries have some regime of either inspections or certification of approved installers to make sure that good standards are maintained.

The history of thermal insulation is not so long compared with other materials, but human beings have been aware of the importance of insulation for a long time.[6] In the prehistoric time, human beings began their activity of making shelters against wild animals and heavy weather, human beings started their exploration of thermal insulation.[6][7] Prehistoric peoples built their dwellings by using the materials of animal skins, fur, and plant materials like reed, flax, and straw, these materials were first used as clothing materials, because their dwellings were temporary, they were more likely to use the materials they used in clothing, which were easy to obtain and process.[6] The materials of animal furs and plant products can hold a large amount of air between molecules which can create an air cavity to reduce the heat exchange.

Later, human beings' long life spans and the development of agriculture determined that they needed a fixed place of residence, earth-sheltered houses, stone houses, and cave dwellings began to emerge.[6][7] The high density of these materials can cause a time lag effect in thermal transfer, which can make the inside temperature change slowly. This effect keep inside of the buildings warm in winter and cool in summer, also because of the materials like earth or stone is easy to get, this design is really popular in many places like Russia, Iceland, Greenland.[6]

Organic materials were the first available to build a shelter for people to protect themselves from bad weather conditions and to help keep them warm.[7] But organic materials like animal and plant fiber cannot exist for a long time, so these natural materials cannot satisfy people's long-term need for thermal insulation. So, people began to search for substitutes which are more durable.[7][8] In the 19th century, people were no longer satisfied with using natural materials for thermal insulation, they processed the organic materials and produced the first insulated panels.[7] At the same time, more and more artificial materials start to emerge, and a large range of artificial thermal insulation materials were developed, e.g. rock wool, fiberglass, foam glass, and hollow bricks.[8]

Thermal insulation can play a significant role in buildings, great demands of thermal comfort result in a large amount of energy consumed for full-heating for all rooms.[9] Around 40% of energy consumption can be attributed to the building, mainly consumed by heating or cooling. Sufficient thermal insulation is the fundamental task that ensures a healthy indoor environment and against structure damages. It is also a key factor in dealing with high energy consumption, it can reduce the heat flow through the building envelope. Good thermal insulation can also bring the following benefits to the building:

How much insulation a house should have depends on building design, climate, energy costs, budget, and personal preference. Regional climates make for different requirements. Building codes often set minimum standards for fire safety and energy efficiency, which can be voluntarily exceeded within the context of sustainable architecture for green certifications such as LEED.

The insulation strategy of a building needs to be based on a careful consideration of the mode of energy transfer and the direction and intensity in which it moves. This may alter throughout the day and from season to season. It is important to choose an appropriate design, the correct combination of materials, and building techniques to suit the particular situation.

The thermal insulation requirements in the USA follow the ASHRAE 90.1 which is the U.S. energy standard for all commercial and some residential buildings.[13] ASHRAE 90.1 standard considers multiple perspectives such as prescriptive, building envelope types and energy cost budget. And the standard has some mandatory thermal insulation requirements.[13] All thermal insulation requirements in ASHRAE 90.1 are divided by the climate zone, it means that the amount of insulation needed for a building is determined by which climate zone the building locates. The thermal insulation requirements are shown as R-value and continuous insulation R-value as the second index.[13] The requirements for different types of walls (wood framed walls, steel framed walls, and mass walls) are shown in the table.[14]

To determine whether you should add insulation, you first need to find out how much insulation you already have in your home and where. A qualified home energy auditor will include an insulation check as a routine part of a whole-house energy audit.[15] However, you can sometimes perform a self-assessment in certain areas of the home, such as attics. Here, a visual inspection, along with use of a ruler, can give you a sense of whether you may benefit from additional insulation.[16] Residential energy audits are often initiated due homeowners being alerted by a gradual increase in their utility bills which often reflects the buildings attic as being poorly insulated.[17]

An initial estimate of insulation needs in the United States can be determined by the US Department of Energy's ZIP code insulation calculator.

In Russia, the availability of abundant and cheap gas has led to poorly insulated, overheated, and inefficient consumption of energy. The Russian Center for Energy Efficiency found that Russian buildings are either over- or under-heated, and often consume up to 50 percent more heat and hot water than needed.[18][19] 53 percent of all carbon dioxide (CO2) emissions in Russia are produced through heating and generating electricity for buildings.[20] However, greenhouse gas emissions from the former Soviet Bloc are still below their 1990 levels.[citation needed]

Energy codes in the Soviet Union start to establish in 1955, norms and rules first mentioned the performance of the building envelope and heat losses, and they formed norms to regulate the energy characteristics of  the building envelope.[21] And the most recent version of Russia energy code (SP 50.13330.2012) was published in 2003.[21] The energy codes of Russia were established by experts of government institutes or nongovernmental organization like ABOK. The energy code of Russia have been revised several times since 1955, the 1995 versions reduced energy depletion per square meter for heating by 20%, and the 2000 version reduced by 40%.[21] The code also has a mandatory requirement on thermal insulation of buildings accompany with some voluntary provisions, mainly focused on heat loss from the building shell.

The thermal insulation requirements of Australia follow the climate of the building location, the table below is the minimum insulation requirements based on climate, which is determined by the Building Code of Australia (BCA).[22] The building in Australia applies insulation in roofs, ceilings, external walls, and various components of the building (such as Veranda roofs in the hot climate, Bulkhead, Floors).[23] Bulkheads (wall section between ceilings which are in different heights) should have the same insulated level as the ceilings since they suffer the same temperature levels.[24] And the external walls of Australia's building should be insulated to decrease all kinds of heat transfer.[25] Besides the walls and ceilings, the Australia energy code also requires insulation for floors (not all floors).[25] Raised timber floors must have around 400mm soil clearance below the lowest timbers to provide sufficient space for insulation, and concrete slab such as suspended slabs and slab-on-ground should be insulated in the same way. 

China has various climatic characters, which are divided by geographical areas.[28] There are five climate zones in China to identify the building design include thermal insulation. (The very cold zone, cold zone, hot summer and cold winter zone, hot summer and warm winter zone and cold winter zone).[29]

Germany established its requirements of building energy efficiency in 1977, and the first energy code-the Energy Saving Ordinance (EnEV) which based on the building performance was introduced in 2002.[30] And the 2009 version of the Energy Saving Ordinance increased the minimum R-values of the thermal insulation of the building shell and introduced requirements for air-tightness tests.[31] The Energy Saving Ordinance (EnEV) 2013 clarified the requirement of thermal insulation of the ceiling. And it mentioned that if the ceiling was not fulfilled, thermal insulation will be needed in accessible ceilings over upper floor's heated rooms. [U-Value must be under 0.24 W/(m2⋅K)][31]

The building decree (Bouwbesluit) of the Netherlands makes a clear distinction between home renovation or newly built houses. New builds count as completely new homes, but also new additions and extensions are considered to be new builds. Furthermore, renovations whereby at least 25% of the surface of the integral building is changed or enlarged is also considered to be a new build. Therefore, during thorough renovations, there's a chance that the new construction must meet the new building requirement for insulation of the Netherlands. If the renovation is of a smaller nature, the renovation directive applies. Examples of renovation are post-insulation of a cavity wall and post-insulation of a sloping roof against the roof boarding or under the tiles. Note that every renovation must meet the minimum Rc value of 1.3 W/(m2⋅K). If the current insulation has a higher insulation value (the legally obtained level), then this value counts as a lower limit.[32]

Insulation requirements for new houses and small buildings in New Zealand are set out in the Building Code and standard NZS 4128:2009.[33][34]

Zones 1 and 2 include most of the North Island, including Waiheke Island and Great Barrier Island. Zone 3 includes the Taupo District, Ruapehu District, and the Rangitikei District north of 39°50′ latitude south (i.e. north of and including Mangaweka) in the North Island, the South Island, Stewart Island, and the Chatham Islands.[34]

Insulation requirements are specified in the Building regulations and in England and Wales the technical content is published as Approved Documents
Document L defines thermal requirements, and while setting minimum standards can allow for the U values for elements such as roofs and walls to be traded off against other factors such as the type of heating system in a whole building energy use assessment.
Scotland and Northern Ireland have similar systems but the detail technical standards are not identical.
The standards have been revised several times in recent years, requiring more efficient use of energy as the UK moves towards a low-carbon economy.

In cold conditions, the main aim is to reduce heat flow out of the building.  The components of the building envelope—windows, doors, roofs, floors/foundations, walls, and air infiltration barriers—are all important sources of heat loss;[35][36] in an otherwise well insulated home, windows will then become an important source of heat transfer.[37]  The resistance to conducted heat loss for standard single glazing corresponds to an R-value of about 0.17 m2⋅K⋅W−1 or more than twice that for typical double glazing (compared to 2–4 m2⋅K⋅W−1 for glass wool batts[38]).  Losses can be reduced by good weatherisation, bulk insulation, and minimising the amount of non-insulative (particularly non-solar facing) glazing.  Indoor thermal radiation can also be a disadvantage with spectrally selective (low-e, low-emissivity) glazing. Some insulated glazing systems can double to triple R values.

The vacuum panels and aerogel wall surface insulation are two technologies that can enhance the energy performance and thermal insulating effectiveness of the residential buildings and commercial buildings in cold climate regions such as New England and Boston.[39] In the past time, the price of thermal insulation materials that displayed high insulated performance was very expensive.[39] With the development of material industry and the booming of science technologies, more and more insulation materials and insulated technologies have emerged during the 20th century, which gives us various options for building insulation. Especially in the cold climate areas, a large amount of thermal insulation is needed to deal with the heat losses caused by cold weather (infiltration, ventilation, and radiation). There are two technologies that are worth discussing:

VIPs are noted for their ultra-high thermal resistance,[40] their ability of thermal resistance is four to eight times more than conventional foam insulation materials which lead to a thinner thickness of thermal insulation to the building shell compared with traditional materials. The VIPs are usually composed of core panels and metallic enclosures.[40] The common materials that used to produce Core panels are fumed and precipitated silica, open-cell polyurethane (PU), and different types of fiberglass. And the core panel is covered by the metallic enclosure to create a vacuum environment, the metallic enclosure can make sure that the core panel is kept in the vacuum environment.[40] Although this material has a high thermal performance, it still maintains a high price in the last twenty years.

Aerogel was first discovered by Samuel Stephens Kistle in 1931.[41] It is a kind of gel of which the liquid component of the material is replaced by a gas, thus creating a material that is 99% air.[41] This material has a relatively high R-value of around R-10 per inch which is considerably higher compared with conventional plastic foam insulation materials, due to their designed high porosity.[42] But the difficulties in processing and low productivity limit the development of Aerogels,[41] the cost price of this material still remains at a high level. Only two companies in the United States offer the commercial Aerogel product for wall insulation purposes.

The DOE estimates thermal losses nearing 30% through windows, and thermal gains from sunlight leading to unwanted heating.[43] Due to the high R associated with aerogels, their use for glazing applications has become an area of interest explored by many research institutions. Their implementation, however, must not hinder the primary function of windows: transparency.[44][45] Typically, aerogels have low transmission and appear hazy, even amongst those considered transparent, which is why they have generally been reserved to wall insulation applications.[46] Eldho Abraham, a researcher at the University of Colorado Boulder, recently demonstrated the capabilities of aerogels by designing a silanized cellulose aerogel (SiCellA) which offers near 99% visible transmission in addition to thermal conductivities which effectively reject or retain heat depending on the interior environment, akin to heating/cooling alterations.[47] This is due to the designed 97.5% porosity of the SiCellA: pores are smaller than the wavelength of visible light, leading to transmission; the pores also minimize contact between the cellulose fibers, leading to lower thermal conductivities.[48] The use of cellulose fibers lends itself to sustainability, as this is a naturally derived fiber sourced from wood pulps. This opens the door not only to aerogels, but also more general wood-based materials implementation in an effort to assist sustainable design alternatives with compounding energy saving effects.[49]

In hot conditions, the greatest source of heat energy is solar radiation.[50] This can enter buildings directly through windows or it can heat the building shell to a higher temperature than the ambient, increasing the heat transfer through the building envelope.[51][52] The Solar Heat Gain Co-efficient (SHGC)[53] (a measure of solar heat transmittance) of standard single glazing can be around 78–85%. Solar gain can be reduced by adequate shading from the sun, light coloured roofing, spectrally selective (heat-reflective) paints and coatings and, various types of insulation for the rest of the envelope. Specially coated glazing can reduce SHGC to around 10%. Radiant barriers are highly
effective for attic spaces in hot climates.[54] In this application, they are much more effective in hot climates than cold climates. For downward heat flow, convection is weak and radiation dominates heat transfer across an air space. Radiant barriers must face an adequate air-gap to be effective.

If refrigerative air-conditioning is employed in a hot, humid climate, then it is particularly important to seal the building envelope. Dehumidification of humid air infiltration can waste significant energy. On the other hand, some building designs are based on effective cross-ventilation instead of refrigerative air-conditioning to provide convective cooling from prevailing breezes.

In hot dry climate regions like Egypt and Africa, thermal comfort in the summer is the main question, nearly half of energy consumption in urban area is depleted by air conditioning systems to satisfy peoples' demand for thermal comfort, many developing countries in hot dry climate region suffer a shortage of electricity in the summer due to the increasing use of cooling machines.[55] A new technology called Cool Roof has been introduced to ameliorate this situation.[56] In the past, architects used thermal mass materials to improve thermal comfort, the heavy thermal insulation could cause the time-lag effect which might slow down the speed of heat transfer during the daytime and keep the indoor temperature in a certain range (Hot and dry climate regions usually have a large temperature difference between the day and night).

The cool roof is low-cost technology based on solar reflectance and thermal emittance, which uses reflective materials and light colors to reflect the solar radiation.[55][56] The solar reflectance and the thermal emittance are two key factors that determine the thermal performance of the roof, and they can also improve the effectiveness of the thermal insulation since around 30% solar radiation is reflected back to the sky.[56] The shape of the roof is also under consideration, the curved roof can receive less solar energy compared with conventional shapes.[55][57] Meanwhile, the drawback of this technology is obvious that the high reflectivity will cause visual discomfort. On the other hand, the high reflectivity and thermal emittance of the roof will increase the heating load of the building.

Optimal placement of building elements (e.g. windows, doors, heaters) can play a significant role in insulation by considering the impact of solar radiation on the building and the prevailing breezes. Reflective laminates can help reduce passive solar heat in pole barns, garages, and metal buildings.

See insulated glass and quadruple glazing for discussion of windows.

The thermal envelope defines the conditioned or living space in a house. The attic or basement may or may not be included in this area. Reducing airflow from inside to outside can help to reduce convective heat transfer significantly.[58]

Ensuring low convective heat transfer also requires attention to building construction (weatherization) and the correct installation of insulative materials.[59][60]

The less natural airflow into a building, the more mechanical ventilation will be required to support human comfort. High humidity can be a significant issue associated with lack of airflow, causing condensation, rotting construction materials, and encouraging microbial growth such as mould and bacteria. Moisture can also drastically reduce the effectiveness of insulation by creating a thermal bridge (see below). Air exchange systems can be actively or passively incorporated to address these problems.

Thermal bridges are points in the building envelope that allow heat conduction to occur. Since heat flows through the path of least resistance, thermal bridges can contribute to poor energy performance. A thermal bridge is created when materials create a continuous path across a temperature difference, in which the heat flow is not interrupted by thermal insulation. Common building materials that are poor insulators include glass and metal.

A building design may have limited capacity for insulation in some areas of the structure. A common construction design is based on stud walls, in which thermal bridges are common in wood or steel studs and joists, which are typically fastened with metal. Notable areas that most commonly lack sufficient insulation are the corners of buildings, and areas where insulation has been removed or displaced to make room for system infrastructure, such as electrical boxes (outlets and light switches), plumbing, fire alarm equipment, etc.

Thermal bridges can also be created by uncoordinated construction, for example by closing off parts of external walls before they are fully insulated.
The existence of inaccessible voids within the wall cavity which are devoid of insulation can be a source of thermal bridging.

Some forms of insulation transfer heat more readily when wet, and can therefore also form a thermal bridge in this state.

The heat conduction can be minimized by any of the following: reducing the cross sectional area of the bridges, increasing the bridge length, or decreasing the number of thermal bridges.

One method of reducing thermal bridge effects is the installation of an insulation board (e.g. foam board EPS XPS, wood fibre board, etc.) over the exterior outside wall. Another method is using insulated lumber framing for a thermal break inside the wall.[61]

Insulating buildings during construction is much easier than retrofitting, as generally the insulation is hidden, and parts of the building need to be deconstructed to reach them.

Depending on the country there are different regulations as to which type of insulation is the best alternative for buildings, considering energy efficiency and environmental factors. Geographical location also affects the type of insulation needed as colder climates will need a bigger investment than warmer ones on installation costs.

There are essentially two types of building insulation - bulk insulation and reflective insulation. Most buildings use a combination of both types to make up a total building insulation system. The type of insulation used is matched to create maximum resistance to each of the three forms of building heat transfer - conduction, convection, and radiation.

According to three ways of heat exchange, most thermal insulation we use in our buildings can be divided into two categories: Conductive and convective insulators and radiant heat barriers. And there are more detailed classifications to distinguish between different materials. Many thermal insulation materials work by creating tiny air cavities between molecules, these air cavities can largely reduce the heat exchange through the materials. But there are two exceptions which don't use air cavities as their functional element to prevent heat transfer. One is reflective thermal insulation, which creates a great airspace by forming a radiation barrier by attaching metal foil on one side or both sides, this thermal insulation mainly reduces the radiation heat transfer. Although the polished metal foil attached on the materials can only prevent the radiation heat transfer, its effect to stop heat transfer can be dramatic. Another thermal insulation that doesn't apply air cavities is vacuum insulation, the vacuum-insulated panels can stop all kinds of convection and conduction and it can also largely mitigate the radiation heat transfer. But the effectiveness of vacuum insulation is also limited by the edge of the material, since the edge of the vacuum panel can form a thermal bridge which leads to a reduction of the effectiveness of the vacuum insulation. The effectiveness of the vacuum insulation is also related to the area of the vacuum panels.

Bulk insulators block conductive heat transfer and convective flow either into or out of a building. Air is a very poor conductor of heat and therefore makes a good insulator. Insulation to resist conductive heat transfer uses air spaces between fibers, inside foam or plastic bubbles and in building cavities like the attic. This is beneficial in an actively cooled or heated building, but can be a liability in a passively cooled building; adequate provisions for cooling by ventilation or radiation[62] are needed.

Fibrous materials are made by tiny diameter fibers which evenly distribute the airspace.[63] The commonly used materials are silica, glass, rock wool, and slag wool. Glass fiber and mineral wool are two insulation materials that are most widely used in this type.

Cellular insulation is composed of small cells which are separated from each other.[63] The commonly cellular materials are glass and foamed plastic like polystyrene, polyolefin, and polyurethane.

Radiant barriers work in conjunction with an air space to reduce radiant heat transfer across the air space. Radiant or reflective insulation reflects heat instead of either absorbing it or letting it pass through. Radiant barriers are often seen used in reducing downward heat flow, because upward heat flow tends to be dominated by convection. This means that for attics, ceilings, and roofs, they are most effective in hot climates.[52]
They also have a role in reducing heat losses in cool climates. However, much greater insulation can be achieved through the addition of bulk insulators (see above).

Some radiant barriers are spectrally selective and will preferentially reduce the flow of infra-red radiation in comparison to other wavelengths. For instance, low-emissivity (low-e) windows will transmit light and short-wave infra-red energy into a building but reflect the long-wave infra-red radiation generated by interior furnishings. Similarly, special heat-reflective paints are able to reflect more heat than visible light, or vice versa.

Thermal emissivity values probably best reflect the effectiveness of radiant barriers. Some manufacturers quote an 'equivalent' R-value for these products but these figures can be difficult to interpret, or even misleading, since R-value testing measures total heat loss in a laboratory setting and does not control the type of heat loss responsible for the net result (radiation, conduction, convection).[citation needed]

A film of dirt or moisture can alter the emissivity and hence the performance of radiant barriers.

Eco-friendly insulation is a term used for insulating products with limited environmental impact. The commonly accepted approach to determine whether or not an insulation product, or in fact any product or service is eco-friendly is by doing a life-cycle assessment (LCA). A number of studies compared the environmental impact of insulation materials in their application. The comparison shows that most important is the insulation value of the product meeting the technical requirements for the application. Only in a second order step, a differentiation between materials becomes relevant. The report commissioned by the Belgian government to VITO[64][65] is a good example of such a study.
"
Painting Services,"A house painter and decorator is a tradesperson responsible for the painting and decorating of buildings, and is also known as a decorator, or house painter.[1][2] The purpose of painting is to improve the appearance of a building and to protect it from damage by water, corrosion, insects and mould. House painting can also be a form of artistic and/or cultural expression such as Ndebele house painting.

In England, little is known of the trade and its structures before the late 13th century, at which point guilds began to form, amongst them the Painters Company and the Stainers Company. These two guilds eventually merged with the consent of the Lord Mayor of the City of London in 1502, forming the Worshipful Company of Painter-Stainers. The guild standardised the craft and acted as a protector of the trade secrets. In 1599, the guild asked Parliament for protection, which was eventually granted in a bill of 1606, which granted the trade protection from outside competition such as plasterers.[2]

The Act legislated for a seven-year apprenticeship, and also barred plasterers from painting, unless apprenticed to a painter, with the penalty for such painting being a fine of £5. The Act also enshrined a maximum daily fee of 16 old pence for their labour.[2]

Enforcement of this Act by the Painter-Stainers Company was sought up until the early 19th century, with master painters gathering irregularly to decide the fees that a journeyman could charge, and also instigating an early version of a job centre in 1769, advertising in the London newspapers a ""house of call"" system to advertise for journeymen and also for journeymen to advertise for work. The guild's power in setting the fee a journeyman could charge was eventually overturned by law in 1827, and the period after this saw the guild's power diminish, along with that of the other guilds; the guilds were superseded by trade unions, with the Operative United Painters' Union forming sometime around 1831.[2]

In 1894, a national association formed, recreating itself in 1918 as the National Federation of Master Painters and Decorators of England and Wales, then changing its name once again to the British Decorators Association before merging, in 2002, with the Painting & Decorating Federation to form the Painting & Decorating Association. The Construction Industry Joint Council, a body formed of both unions and business organizations, today has responsibility for the setting of pay levels.[2]

Historically, the painter was responsible for the mixing of the paint; keeping a ready supply of pigments, oils, thinners and driers. The painter would use his experience to determine a suitable mixture depending on the nature of the job. In modern times, the painter is primarily responsible for preparation of the surface to be painted, such as patching holes in drywall, using masking tape and other protection on surfaces not to be painted, applying the paint and then cleaning up.[2]

Larger firms operating within the trade were generally capable of performing many painting or decoration services, from creating an accent wall to sign writing, to the gilding of objects or the finishing or refinishing of furniture.[2]

Painter-work is described in the 1911 Encyclopædia Britannica Eleventh Edition where the relevant skills include preparing surfaces, mixing paint, gilding, distemper, and faux-finishes including marbleizing and graining.[3]

More recently, professional painters are responsible for all preparation prior to painting. All stucco or popcorn or texture scraping, sanding, wallpaper removal, caulking, drywall or wood repair, patching, stain removal, filling nail holes or any defects with plaster or putty, cleaning, taping, preparation and priming are considered to be done by the professional contracted painter.
"
Spray Painting Services,"Spray painting is a painting technique in which a device sprays coating material (paint, ink, varnish, etc.) through the air onto a surface. The most common types employ compressed gas—usually air—to atomize and direct the paint particles. 

Spray guns evolved from airbrushes, and the two are usually distinguished by their size and the size of the spray pattern they produce. Airbrushes are hand-held and used instead of a brush for detailed work such as photo retouching, painting nails, or fine art. Air gun spraying uses generally larger equipment. It is typically used for covering large surfaces with an even coating of liquid. Spray guns can either be automated or hand-held and have interchangeable heads to allow for different spray patterns. 

Single color aerosol paint cans are portable and easy to store.

Spraying paint with compressed air can be traced back to its use on the Southern Pacific Railroad in the early 1880s[1] In 1887 Joseph Binks, the maintenance supervisor at Chicago's Marshall Field's Wholesale Store developed a hand-pumped cold-water paint spraying machine to apply whitewash to the subbasement walls of the store.[2][3] Francis Davis Millet, the decorations director for the World's Columbian Exposition in Chicago in 1893, used Binks and his spray painting system to apply whitewash consisting of a mix of oil and white lead to the buildings at the Exposition, taking considerably less time than traditional brush painting and turning it into what has been called the White City.[4][1][3] In 1949, Edward Seymour developed a type of spray painting, aerosol paint, that could be delivered via a compressed aerosol in a can.

This process occurs when the paint is applied to an object through the use of an air-pressurized spray gun. The air gun has a nozzle, paint basin, and air compressor. When the trigger is pressed the paint mixes with the compressed air stream and is released in a fine spray.[5]

Due to a wide range of nozzle shapes and sizes, the consistency of the paint can be varied. The shape of the workpiece and the desired paint consistency and pattern are important factors when choosing a nozzle. The three most common nozzles are the full cone, hollow cone, and flat stream.[6]
There are two types of air-gun spraying processes. In a manual operation method the air-gun sprayer is held by a skilled operator, about 6 to 10 inches (15–25 cm) from the object, and moved back and forth over the surface, each stroke overlapping the previous to ensure a continuous coat.[7] In an automatic process the gun head is attached to a mounting block and delivers the stream of paint from that position.
The object being painted is usually placed on rollers or a turntable to ensure overall equal coverage of all sides.

High volume low pressure (HVLP) is similar to a conventional spray gun using a compressor to supply the air, but the spray gun itself requires a lower pressure (LP). A higher volume (HV) of air is used to aerosolize and propel the paint at lower air pressure. The result is a higher proportion of paint reaching the target surface with reduced overspray, materials consumption, and air pollution.

A regulator is often required so that the air pressure from a conventional compressor can be lowered for the HVLP spray gun. Alternatively, a turbine unit (commonly containing a vacuum cleaner type motor, reverse mounted) can be used to propel the air without the need for an airline running to the compressor.

A rule of thumb puts two-thirds of the coating on the substrate and one third in the air. True HVLP guns use 8–20 cfm (13.6–34 m3/h), and an industrial compressor with a minimum of 5 horsepower (3.7 kW) output is required. HVLP spray systems are used in the automotive, aviation, marine, decorative, architectural coating, furniture finishing, scenic painting, and cosmetic industries.

Like HVLP, low volume low pressure (LVLP) spray guns also operate at a lower pressure (LP), but they use a low volume (LV) of air when compared to conventional and HVLP equipment. This is a further effort at increasing the transfer efficiency (amount of coating that ends up on the target surface) of spray guns while decreasing the amount of compressed air consumption. 

Electrostatic painting was first patented in the U.S. by Harold Ransburg in the late 1940s. Harold Ransburg founded Ransburg Electrostatic Equipment and discovered that electrostatic spray painting was an immediate success as manufacturers quickly perceived the substantial materials savings that could be achieved. In electrostatic spray painting or powder coating, the atomized particles are made to be electrically charged, thereby repelling each other and spreading themselves evenly as they exit the spray nozzle. The object being painted is charged oppositely or grounded. The paint is then attracted to the object giving a more even coat than wet spray painting, and also greatly increasing the percentage of paint that sticks to the object. This method also means that paint covers hard to reach areas. The whole may then be baked to properly attach the paint: the powder turns into a type of plastic. Car body panels and bike frames are two examples where electrostatic spray painting is often used.

There are three main technologies for charging the fluid (liquid or powders):

With this method the paint is flung into the air by a spinning metal disc (""bell""). The metal disc also imparts an electrical charge to the coating particle.[8]

There is a variety of hand-held paint sprayers that either combine the paint with air or convert the paint to tiny droplets and accelerate these out of a nozzle.

By heating the full-bodied paint to 60-80 °C, it is possible to apply a thicker coat. Originally the paint was recirculated, but as this caused bodying up, the system was changed to direct heating on line. Hot spraying was also used with Airless and Electrostatic Airless to decrease bounce-back. Two-pack materials usually had premix before tip systems using dual pumps.

These use air pressure and fluid pressure 300 to 3,000 pounds per square inch (2,100–20,700 kPa) to achieve atomization of the coating. This equipment provides high transfer and increased application speed and is most often used with flat-line applications in factory finish shops.

The fluid pressure is provided by an airless pump, which allows much heavier materials to be sprayed than is possible with an air spray gun. Compressed air is introduced into the spray via an air nozzle (sometimes called air cap) similar to a standard conventional spray gun. The addition of compressed air improves the fineness of atomization. Additionally, unlike a pure airless spray gun, an AA gun has some control over fan spray to round spray. Some electric airless sprayers (Wagner and Graco) are fitted with a compressor to allow the use of an air-assisted airless gun in situations where portability is important.

These operate connected to a high-pressure pump commonly found using 300 to 7,500 pounds per square inch (2,100–51,700 kPa) pressure to atomize the coating, using different tip sizes to achieve the desired atomization and spray pattern size. This type of system is used by contract painters to paint heavy duty industrial, chemical, and marine coatings and linings.

Advantages of airless spray are:

Most coatings can be sprayed with very little thinner added, thereby reducing drying time and decreasing the release of solvent into the environment.

Care must be used when operating, as airless spray guns can cause serious injury,[9] such as injection injuries, due to the paint ejecting from the nozzle at high pressure.

Airless pumps can be powered by different types of motor: electric, compressed air (pneumatic), or hydraulic. Most have a paint pump (also called a lower) that is a double-acting piston, in which the piston pumps the paint in both the down and the upstroke. Some airless pumps have a diaphragm instead of a piston, but both types have inlet and outlet valves.

Most electric-powered airless pumps have an electric motor connected through a gear train to the paint piston pump. The pressure is achieved by stopping and starting the motor via a pressure sensor (also called a transducer); in more advanced units, this is done by digital control in which the speed of the motor varies with the demand and the difference from the pressure set-point, resulting in very good pressure control. Some direct drive piston pumps are driven by a gasoline engine with pressure control via an electric clutch. In electric diaphragm pumps, the motor drives a hydraulic piston pump that transmits the oil displaced by the piston, to move the diaphragm.

Hydraulic and air-powered airless pumps have linear motors that require a hydraulic pump or an air compressor, which can be electric or gasoline-powered, although an air compressor is usually diesel-powered for mobile use or electric for fixed installations. Some airless units have the hydraulic pump and its motor, built onto the same chassis as the paint pump.

Hydraulic or air-powered airless provide a more uniform pressure control since the paint piston moves at a constant speed except when it changes direction. In most direct drive piston pumps, the piston is crankshaft driven in which the piston will be constantly changing speed. The linear motors of hydraulic or compressed air drive pumps are more efficient in converting engine power to material power than crankshaft-driven units.
All types of paint can be painted using an airless method.

Manufacturers who mass-produce wood products use automated spray systems, allowing them to paint materials at a very high rate with a minimum of personnel. Automated spray systems usually incorporate a paint-saving system that recovers paint not applied to the products. Commonly, linear spray systems are for products which are lying flat on a conveyor belt and then fed into a linear spray system, where automated spray guns are stationed above. When the material is directly below the guns, the guns begin to paint the material. Materials consist of lineal parts usually less than 12 inches (30 cm) wide, such as window frames, wood molding, baseboard, casing, trim stock, and any other material that is simple in design.
These machines are commonly used to apply the stain, sealer, and lacquer. They can apply water- or solvent-based coatings. In recent years ultraviolet-cured coatings have become commonplace in profile finishing, and there are machines particularly suited to this type of coating.

Mass-produced material is loaded on a conveyor belt where it is fed into one of these flatline machines. Flatline machines are designed to specifically paint material that is less than 4 inches (10 cm) thick and complex in shape, for example, a kitchen cabinet door or drawer front. Spray guns are aligned above the material and the guns are in motion to hit all the grooves of the material. The guns can be moved in a cycle, circle, or can be moved back and forth to apply the paint evenly across the material. Flatline systems are typically large and can paint doors, kitchen cabinets, and other plastic or wooden products.

A spray booth is a pressure-controlled closed environment, originally used to paint vehicles in a body shop. Its effective design promotes efficient paint application, minimizing contamination and maximizing the quality of the finished product.[10] To ensure the ideal working conditions (temperature, airflow, and humidity), these environments are equipped with ventilation, consisting of mechanical fans driven by electric motors, and optionally burners to heat the air to speed paint drying. Toxic solvents and paint particles are exhausted outside, possibly after filtering and treatment to reduce air pollution. Prevention of fires and dust explosions is also a high priority. To assist in the removal of the over sprayed paint from the air and to provide efficient operation of the down-draft, water-washed paint spray booths utilize paint detackifying chemical agents.

Artists may also make use of spray booth facilities to enable them to use spray paints (including automotive finishes) efficiently and safely. They may rent space and time in auto body shops or set up their facilities in association with schools or artist cooperatives.

Spray painting poses health hazards that affect the respiratory, nervous, and circulatory systems. Similarly, using solvents to clean one's hands of paint marks and residue may cause skin irritation or even more serious issues since many are carcinogenic or neurotoxic. There are risks involved in working with substances such as paint and thinner, which contain compounds that are potentially harmful to health, or even fatal.[11]

Appropriate training for personnel who are responsible for conducting the painting procedures is important, which may be from a professional training provider or the product supplier. There are also hazards related to the disposal of wastes and materials that are contaminated with potentially harmful chemicals. Decontamination procedures and Material Safety Data Sheets for various products are important. Safety is improved through:

There are several types of paints in the market which are customized for different applications.  Their components varying according to the application needs but often contain substances that can be toxic when inhaled or absorbed through the skin.[1] [16]  Painters, and other workers in his or her proximity a are exposed primarily to solvents. The mechanical removal of paint can lead to exposure to pigments and fillers.[16]  Two common components include:

Spray paints contain hazardous chemicals that can expose a worker to possible side effects. One of the most commonly used chemicals in spray paints includes acetone. Acetone is a clear, heavily scented liquid that is used in common industrial products but also found organically. The chemical is on the ‘Right to Know Hazardous Substance’ list as well as the ‘Special Health Hazard Substance List.’ It is highly flammable and should be treated with mindful handling.[17]  In the workplace, exposure can come from inhalation or direct contact, like dermal or oral.[18]  There are current regulations regarding the exposure to acetone in the workplace due to the possible health risks. When inhaled, irritation to the nose, throat, and lungs have been found to occur. Dermal contact can also result in skin irritation along with dryness and redness. If the chemical is of high concentration, side effects such as headaches, nausea, vomiting, and loss of consciousness can occur. Long term impacts have been minimally studied with a current consensus of the liver and kidney being affected. Prolonged contact with acetone is discouraged.[19]

In the workplace, prevention of exposure is recommended. When storing acetone, it should be handled in a closed container, stored in a location that maintains safe temperatures, and away from possible flame exposures. For dermal exposure prevention, PPE such as gloves or garments that can act as a barrier is recommended. Inhalation exposure can be prevented by regulating air circulation as well as using protection such as filters or respirators.[20]

Xylene is a solvent that is often used in paints including spray paint and therefore found in many spray-painting environments. Xylene is a colorless and flammable liquid that is also a hazard to the wellbeing of many humans.[21]  Xylene is harmful when exposed to human skin or, in more extreme cases, eyes, nose, and throat. Symptoms of xylene exposure can come in many forms including headaches; dizziness; confusion; loss of muscle coordination; and in high doses, even death. Workers who are exposed can show an array of various symptoms, but the severity of these symptoms ultimately depends on the duration of exposure, consistency, and what protective measures are used to prevent the harmful effects of xylene. This can be a big issue for spray painters who are exposed every day especially if individuals are not wearing proper equipment to combat these fumes. [22] At high levels of exposure, xylene can have effects on the central nervous system. This can cause extreme fatigue and nausea and potentially lead to unconsciousness. Unfortunately for many avid spray painters, the effects of xylene only grow more severe with extended and consistent exposure. [23]

Long term exposure to xylene may contribute to many chronic health issues. It can potentially affect certain organs within the human body including the liver, kidneys, and even cognitive functions relating to the brain. Xylene has shown to have negative effects on memory and concentration. To reduce these risks, it is essential that safety equipment such as good ventilation, protective wear, and safe handling practices are used when handling spray paint. [24]

One application of spray painting is graffiti. The introduction of inexpensive and portable aerosol paint has been a boon to this art form, which has spread all over the world. Spray painting has also been used in fine art. Jules Olitski, Dan Christensen, Peter Reginato, Sir Anthony Caro, and Jean-Michel Basquiat have used airbrushes, for both painting and sculpture.
"
Flooring Installation Services,"Flooring is the general term for a permanent covering of a floor, or for the work of installing such a floor covering. Floor covering is a term to generically describe any finish material applied over a floor structure to provide a walking surface. Both terms are used interchangeably but floor covering refers more to loose-laid materials.

Materials almost always classified as flooring include carpet, laminate, tile, and vinyl.

The floor under the flooring is called the subfloor, which provides the support for the flooring. Special purpose subfloors like floating floors, raised floors or sprung floors may be laid upon another underlying subfloor which provides the structural strength. Subfloors that are below grade (underground) or ground level floors in buildings without basements typically have a concrete subfloor. Subfloors above grade (above ground) typically have a plywood subfloor. [citation needed]

The choice of materials for floor covering is affected by factors such as cost, endurance, noise insulation, comfort, and cleaning effort. Some types of flooring must not be installed below grade, including laminate and hardwood due to potential damage from moisture.

The sub-floor may be finished in a way that makes it usable without any extra work, see:

Carpet is a soft floor covering made of bound carpet fibers or stapled fibers. Carpeting refers to wall-to-wall coverage, whereas a rug is simply used to cover a space. This type of flooring is typically used indoors and can be used in both high and low-traffic areas. It typically lasts for 15–18 years before it needs to be replaced. The quality of a carpet is usually measured in face weight, or how many fibers there are per square inch. The higher the face weight the more plush a carpet will feel.[3]

Carpets come in a variety of materials including wool, nylon, olefin, and polyester.

There are different types of carpet like twists, which is commonly referred to as berber. Twist carpeting is composed of multiple twisted fibers set into the carpet backing. It is typically used in low-traffic areas. Another type of carpeting is looped carpets, which are composed of looped fibers set into the carpet backing. This type of carpeting is typically used in high-traffic areas as it is easy to clean.[citation needed]

There are four common widths for carpeting goods: 6' wide, 12' wide, 13'6"" wide, and 15' wide.[citation needed]

Today, there are two primary forms of carpet installation: tackless installation and direct glue down. Tackless installation occurs when a tack strip is installed around the perimeter of a room and the carpet is stretched over padding onto those tack strips to hold it in place. Before this installation method, the carpet was tacked down with staples throughout the installation which is why the new method is called tackless installation; as you no longer need nails in the middle of the floor. Direct glue down occurs when you spread special carpet adhesive over the substrate and then roll the carpet onto the glue to hold it in place.

Padding can be placed underneath the carpet to add comfort and provide some noise insulation. The level of comfort is determined by the type of material used, which can include memory foam and rubber regrind.[citation needed]

Carpet underlay is rated by density in pounds. For example, you could purchase a carpet pad with an 8-pound density rating, which would be softer than a carpet pad with a 10-pound density rating. Softer carpet pads feel better to walk on but sacrifice durability of the carpet that sits on top of it. The denser the carpet pad, the longer the carpet lasts.[citation needed]

Many different species of wood are fabricated into wood flooring in two primary forms:  plank and parquet. Hardwoods are typically much more durable than softwoods. Reclaimed lumber has a unique appearance and is used in green (environmentally responsible) building. The thermal conductivity of the hardwood flooring is less as compared to laminate wood flooring.[4]

Engineered hardwood has a thin solid wood layer on top with a composite core, which is generally plywood, but can be high density fiberboard, stone polymer composite, or strips of a solid wood such as spruce or birch. The composite core increases the dimensional stability of the planks, which expand and contract less than those of solid wood flooring. As a result, engineered wood planks can be significantly larger than solid wood planks while retaining structural integrity. The thickness of the wood layer atop the core determines its ability to be sanded and refinished; engineered wood floors can generally be sanded and refinished 1-2 times, though some cannot be refinished while others can be refinished as many times as a solid wood floor.

Bamboo flooring is a floor manufactured from the bamboo plant and is a type of hardwood flooring, though technically not a wood. Bamboo is known to be durable and environmentally friendly. It is available in many different patterns, colors, and textures. There are three types of bamboo flooring construction: horizontal cut, vertical cut, and strand woven. Strand woven is the hardest and most durable of the three types.

Cork flooring is a flooring material manufactured from the by-product of the cork oak tree. Cork floors are considered to be eco-friendly since the cork oak tree bark is stripped every nine to ten years and doesn't damage the tree. Cork flooring comes in both tiles and planks, and can have glue or glues-less installation.

Hardwood durability and hardness is determined by a ranking system called the Janka scale. The Janka scale is the force that it takes to embed a steel ball into the hardwood. The more force it takes to do so, the harder the wood. 

Hardwood floors can either be purchased as pre-finished planks or as planks that need to be sanded and finished in the home following installation. Manufacturers of pre-finished hardwood floors use varying materials for the finish, which usually include layers of polyurethane and aluminum oxide. The hardness of aluminum oxide can made these floors particularly difficult and costly to refinish, to the extent that most engineered wood floors do not get refinished, even if they can be.

Hardwood floors can be repaired by spot-sanding and refinishing, plank replacement, or a refinish of the full floor.

Laminate is a floor covering that appears similar to hardwood but is made with a plywood or medium density fiberboard (""MDF"") core with a plastic laminate top layer. HDF laminate consists of high density fiberboard topped by one or more layers of decorative paper and a transparent protective layer. Laminate may be more durable than hardwood, but cannot be refinished like hardwood.  Laminate flooring is available in many different patterns which can resemble different woods or even ceramic tile.  It usually locks or taps together. Underlayment is required for laminate flooring to provide moisture and noise control.

The common installation method for laminate flooring is a floating installation, which means the floor connects to each other to form interlocked flooring system and is not attached to the sub-floor which means it is free to ""float"" over a variety of sub-floors. This includes existing flooring like ceramic tile and hardwood floors. It is the most versatile installation method because it can be put over any substrate that is flat.

Hybrid combines the best attributes of both laminate and vinyl to create a rigid floating floor that can be installed throughout the entire home. Hybrid is made of multiple layers of materials pressed together for an extremely hard wearing floor.

Hard flooring (not to be confused with ""hardwood"") is a family of flooring materials that includes concrete or cement, ceramic tile, glass tiles, and natural stone products.

Ceramic tiles are clay products that are formed into thin tiles and fired. Ceramic tiles are set in beds of mortar or mastic with the joints between tiles grouted. Varieties of ceramic tiles include quarry tile, porcelain, and terracotta.

Many different natural stones are cut into a variety of sizes, shapes, and thicknesses for use as flooring. Stone flooring uses a similar installation method to ceramic tile. Slate and marble are popular types of stone flooring that requires polishing and sealing. Stone aggregates, like Terrazzo, can also be used instead of raw cut stone and are available as either preformed tiles or to be constructed in-place using a cement binder.

Porcelain stoneware can be used instead of natural stone. It is a ceramic material like a tile; however, it is typically 20 mm (0.79 in) thick and often comes in squares of 60 cm (24 in).

Concrete or cement finished floor is also used for its ability to be treated for different feel and its durability, such as polished concrete. Epoxy resurfacing of concrete flooring is used to update or upgrade concrete floor surfaces in commercial and residential applications – see seamless polymer flooring section below.[citation needed]

Floating tile flooring, also called modular tile flooring, includes a range of porcelain and ceramic tile products that can be installed without adhesive or mortar. Generally, the tile is rectified to precise dimensions, and fused to an interlocking base. Some products require use of a flexible grout and others have an integrated grout strip. The advantages include speed of installation, ease of use, reusability, and low cost relative to using traditional tile installation methods.[citation needed]

Unlike ceramic and stone tiles, which are made of minerals, resilient flooring is made of materials that have some elasticity, giving the flooring a degree of flexibility called resilience. Performance surfaces used for dance or athletics are usually made of wood or resilient flooring.

Resilient flooring includes many different manufactured products including linoleum, sheet vinyl, vinyl composition tile (VCT)[dubious – discuss], cork (sheet or tile), and rubber.

Vinyl flooring is available in large sheets or pre-cut tiles; the former is resilient. Some come with a pre-applied adhesive for peel-and-stick installation, others require adhesive to be spread onto the substrate.[5]

The two basic categories of vinyl floor tiles are solid vinyl (products with vinyl on binder content higher than 34%) and vinyl composition (products with a vinyl or binder content lower than 34%),[6] and the three basic categories of vinyl sheet flooring are homogeneous, inlaid, and layered composite. These types of vinyl flooring differ in manufacturing process and content, ranging in vinyl (polyvinyl chloride) content from 11% to 55%.

Resilient flooring products, such as PVC and polypropylene are becoming more popular in specialty applications such as trailer flooring and garage flooring. New applications have also emerged for marine flooring. There are important factors to consider in specialty applications, that may not be present in a typical application. For example, certain tires will leave marks on PVC flooring but those marks will be less prevalent on polypropylene products. Adhesives also change based on application.

Many different seamless flooring materials are available that vary from air-drying latex emulsion polymers to reactive thermoset resins such as waterborne, solvents or solvent-free urethanes, polyaspartics , and epoxies.[7] Applied in liquid form, when allowed to dry and/or cure to provide a completely seamless floor covering.

They find use in situations ranging from the simple protection of domestic garage floors to the restoration and protection of commercial and industrial flooring. They are also used to solve a vast array of problems in industry such as wet areas in laboratories or food processing plants where spillages of oils and fats tend to be easily absorbed and/or difficult to clean. Other reasons for covering concrete with synthetic resin flooring are to improve resistance to chemicals, enhancing resistance to impact and wear, and for aesthetic appearance purposes.

Seamless polymer flooring can take many forms:[8]

They typically have granular or rubberized particles added to give better traction/slip resistance on walkways and steps especially in areas subject to frequent washing, and for better traction/skid resistance in traffic aisles.

Sustainable flooring is produced from more sustainable materials (and by more sustainable processes) that reduces demands on ecosystems during its life-cycle.[9]

There are a number of special features that may be used to ornament a floor or perform a useful service:
"
Interior Design Services,"

Interior design is the art and science of enhancing the interior of a building to achieve a healthier and more aesthetically pleasing environment for the people using the space. With a keen eye for detail and a creative flair, an interior designer is someone who plans, researches, coordinates, and manages such enhancement projects. Interior design is a multifaceted profession that includes conceptual development, space planning, site inspections, programming, research, communicating with the stakeholders of a project, construction management, and execution of the design.

In the past, interiors were put together instinctively as a part of the process of building.[1]

The profession of interior design has been a consequence of the development of society and the complex architecture that has resulted from the development of industrial processes.

The pursuit of effective use of space, user well-being and functional design has contributed to the development of the contemporary interior design profession. The profession of interior design is separate and distinct from the role of interior decorator, a term commonly used in the US; the term is less common in the UK, where the profession of interior design is still unregulated and therefore, strictly speaking, not yet officially a profession.

In ancient India, architects would also function as interior designers. This can be seen from the references of Vishwakarma the architect—one of the gods in Indian mythology. In these architects' design of 17th-century Indian homes, sculptures depicting ancient texts and events are seen inside the palaces, while during the medieval times wall art paintings were a common feature of palace-like mansions in India commonly known as havelis. While most traditional homes have been demolished to make way to modern buildings, there are still around 2000 havelis[2] in the Shekhawati region of Rajashtan that display wall art paintings.

In ancient Egypt, ""soul houses"" (or models of houses) were placed in tombs as receptacles for food offerings. From these, it is possible to discern details about the interior design of different residences throughout the different Egyptian dynasties, such as changes in ventilation, porticoes, columns, loggias, windows, and doors.[3]

Painting interior walls has existed for at least 5,000 years, with examples found as far north as the Ness of Brodgar,[4] as have templated interiors, as seen in the associated Skara Brae settlement.[5] It was the Greeks, and later Romans who added co-ordinated, decorative mosaics floors,[6] and templated bath houses, shops, civil offices, Castra (forts) and temple, interiors, in the first millennia BC. With specialised guilds dedicated to producing interior decoration, and formulaic furniture, in buildings constructed to forms defined by Roman architects, such as Vitruvius: De architectura, libri decem (The Ten Books on Architecture).[7][8]

Throughout the 17th and 18th century and into the early 19th century, interior decoration was the concern of the homemaker, or an employed upholsterer or craftsman who would advise on the artistic style for an interior space. Architects would also employ craftsmen or artisans to complete interior design for their buildings.

In the mid-to-late 19th century, interior design services expanded greatly, as the middle class in industrial countries grew in size and prosperity and began to desire the domestic trappings of wealth to cement their new status. Large furniture firms began to branch out into general interior design and management, offering full house furnishings in a variety of styles. This business model flourished from the mid-century to 1914, when this role was increasingly usurped by independent, often amateur, designers. This paved the way for the emergence of the professional interior design in the mid-20th century.[9]

In the 1950s and 1960s, upholsterers began to expand their business remits. They framed their business more broadly and in artistic terms and began to advertise their furnishings to the public. To meet the growing demand for contract interior work on projects such as offices, hotels, and public buildings, these businesses became much larger and more complex, employing builders, joiners, plasterers, textile designers, artists, and furniture designers, as well as engineers and technicians to fulfil the job. Firms began to publish and circulate catalogs with prints for different lavish styles to attract the attention of expanding middle classes.[9]

As department stores increased in number and size, retail spaces within shops were furnished in different styles as examples for customers. One particularly effective advertising tool was to set up model rooms at national and international exhibitions in showrooms for the public to see. Some of the pioneering firms in this regard were Waring & Gillow, James Shoolbred, Mintons, and Holland & Sons. These traditional high-quality furniture making firms began to play an important role as advisers to unsure middle class customers on taste and style, and began taking out contracts to design and furnish the interiors of many important buildings in Britain.[10]

This type of firm emerged in America after the Civil War. The Herter Brothers, founded by two German émigré brothers, began as an upholstery warehouse and became one of the first firms of furniture makers and interior decorators. With their own design office and cabinet-making and upholstery workshops, Herter Brothers were prepared to accomplish every aspect of interior furnishing including decorative paneling and mantels, wall and ceiling decoration, patterned floors, and carpets and draperies.[11]

A pivotal figure in popularizing theories of interior design to the middle class was the architect Owen Jones, one of the most influential design theorists of the nineteenth century.[12] Jones' first project was his most important—in 1851, he was responsible for not only the decoration of Joseph Paxton's gigantic Crystal Palace for the Great Exhibition but also the arrangement of the exhibits within. He chose a controversial palette of red, yellow, and blue for the interior ironwork and, despite initial negative publicity in the newspapers, was eventually unveiled by Queen Victoria to much critical acclaim. His most significant publication was The Grammar of Ornament (1856),[13] in which Jones formulated 37 key principles of interior design and decoration.

Jones was employed by some of the leading interior design firms of the day; in the 1860s, he worked in collaboration with the London firm Jackson & Graham to produce furniture and other fittings for high-profile clients including art collector Alfred Morrison as well as Ismail Pasha, Khedive of Egypt.

In 1882, the London Directory of the Post Office listed 80 interior decorators. Some of the most distinguished companies of the period were Crace, Waring & Gillowm and Holland & Sons; famous decorators employed by these firms included Thomas Edward Collcutt, Edward William Godwin, Charles Barry, Gottfried Semper, and George Edmund Street.[14]


By the turn of the 20th century, amateur advisors and publications were increasingly challenging the monopoly that the large retail companies had on interior design. English feminist author Mary Haweis wrote a series of widely read essays in the 1880s in which she derided the eagerness with which aspiring middle-class people furnished their houses according to the rigid models offered to them by the retailers.[15] She advocated the individual adoption of a particular style, tailor-made to the individual needs and preferences of the customer:
One of my strongest convictions, and one of the first canons of good taste, is that our houses, like the fish's shell and the bird's nest, ought to represent our individual taste and habits.
The move toward decoration as a separate artistic profession, unrelated to the manufacturers and retailers, received an impetus with the 1899 formation of the Institute of British Decorators; with John Dibblee Crace as its president, it represented almost 200 decorators around the country.[16] By 1915, the London Directory listed 127 individuals trading as interior decorators, of which 10 were women. Rhoda Garrett and Agnes Garrett were the first women to train professionally as home decorators in 1874. The importance of their work on design was regarded at the time as on a par with that of William Morris. In 1876, their work – Suggestions for House Decoration in Painting, Woodwork and Furniture – spread their ideas on artistic interior design to a wide middle-class audience.[17]


By 1900, the situation was described by The Illustrated Carpenter and Builder:[18]
Until recently when a man wanted to furnish he would visit all the dealers and select piece by piece of furniture ....Today he sends for a dealer in art furnishings and fittings who surveys all the rooms in the house and he brings his artistic mind to bear on the subject.
In America, Candace Wheeler was one of the first woman interior designers and helped encourage a new style of American design. She was instrumental in the development of art courses for women in a number of major American cities and was considered a national authority on home design. An important influence on the new profession was The Decoration of Houses, a manual of interior design written by Edith Wharton with architect Ogden Codman in 1897 in America. In the book, the authors denounced Victorian-style interior decoration and interior design, especially those rooms that were decorated with heavy window curtains, Victorian bric-a-brac, and overstuffed furniture. They argued that such rooms emphasized upholstery at the expense of proper space planning and architectural design and were, therefore, uncomfortable and rarely used. The book is considered a seminal work, and its success led to the emergence of professional decorators working in the manner advocated by its authors, most notably Elsie de Wolfe.[19]

Elsie De Wolfe was one of the first interior designers. Rejecting the Victorian style she grew up with, she chose a more vibrant scheme, along with more comfortable furniture in the home. Her designs were light, with fresh colors and delicate Chinoiserie furnishings, as opposed to the Victorian preference of heavy, red drapes and upholstery, dark wood and intensely patterned wallpapers. Her designs were also more practical;[20] she eliminated the clutter that occupied the Victorian home, enabling people to entertain more guests comfortably. In 1905, de Wolfe was commissioned for the interior design of the Colony Club on Madison Avenue; its interiors garnered her recognition almost over night.[21][22] She compiled her ideas into her widely read 1913 book, The House in Good Taste.[23]

In England, Syrie Maugham became a legendary interior designer credited with designing the first all-white room. Starting her career in the early 1910s, her international reputation soon grew; she later expanded her business to New York City and Chicago.[24] Born during the Victorian Era, a time characterized by dark colors and small spaces, she instead designed rooms filled with light and furnished in multiple shades of white and mirrored screens. In addition to mirrored screens, her trademark pieces included: books covered in white vellum, cutlery with white porcelain handles, console tables with plaster palm-frond, shell, or dolphin bases, upholstered and fringed sleigh beds, fur carpets, dining chairs covered in white leather, and lamps of graduated glass balls, and wreaths.[25]

The interior design profession became more established after World War II. From the 1950s onwards, spending on the home increased. Interior design courses were established, requiring the publication of textbooks and reference sources. Historical accounts of interior designers and firms distinct from the decorative arts specialists were made available. Organisations to regulate education, qualifications, standards and practices, etc. were established for the profession.[23]

Interior design was previously seen as playing a secondary role to architecture. It also has many connections to other design disciplines, involving the work of architects, industrial designers, engineers, builders, craftsmen, etc. For these reasons, the government of interior design standards and qualifications was often incorporated into other professional organisations that involved design.[23] Organisations such as the Chartered Society of Designers, established in the UK in 1986, and the American Designers Institute, founded in 1938,[26] governed various areas of design.

It was not until later that specific representation for the interior design profession was developed. The US National Society of Interior Designers was established in 1957, while in the UK the Interior Decorators and Designers Association was established in 1966. Across Europe, other organisations such as The Finnish Association of Interior Architects (1949) were being established and in 1994 the International Interior Design Association was founded.[23]

Ellen Mazur Thomson, author of Origins of Graphic Design in America (1997), determined that professional status is achieved through education, self-imposed standards and professional gate-keeping organizations.[23] Having achieved this, interior design became an accepted profession.

Interior design is the art and science of understanding people's behavior to create functional spaces, that are aesthetically pleasing, within a building. Decoration is the furnishing or adorning of a space with decorative elements, sometimes complemented by advice and practical assistance. In short, interior designers may decorate, but decorators do not design.

Interior designer implies that there is more of an emphasis on planning, functional design and the effective use of space, as compared to interior decorating. An interior designer in fine line design can undertake projects that include arranging the basic layout of spaces within a building as well as projects that require an understanding of technical issues such as window and door positioning, acoustics, and lighting.[1] Although an interior designer may create the layout of a space, they may not alter load-bearing walls without having their designs stamped for approval by a structural engineer. Interior designers often work directly with architects, engineers and contractors.

Interior designers must be highly skilled in order to create interior environments that are functional, safe, and adhere to building codes, regulations and ADA requirements. They go beyond the selection of color palettes and furnishings and apply their knowledge to the development of construction documents, occupancy loads, healthcare regulations and sustainable design principles, as well as the management and coordination of professional services including mechanical, electrical, plumbing, and life safety—all to ensure that people can live, learn or work in an innocuous environment that is also aesthetically pleasing.

Someone may wish to specialize and develop technical knowledge specific to one area or type of interior design, such as residential design, commercial design, hospitality design, healthcare design, universal design, exhibition design, furniture design, and spatial branding. 
Interior design is a creative profession that is relatively new, constantly evolving, and often confusing to the public. It is not always an artistic pursuit and can rely on research from many fields to provide a well-trained understanding of how people are often influenced by their environments.

Color is a powerful design tool in decoration, as well as in interior design, which is the art of composing and coordinating colors together to create a stylish scheme on the interior architecture of the space.[27]

It can be important to interior designers to acquire a deep experience with colors, understand their psychological effects, and understand the meaning of each color in different locations and situations in order to create suitable combinations for each place.[28] Color is something that an interior design needs to understand. Color can affect the way that humans think, feel, or look at space. Color can have a major effect on human behavior through all ages. An interior designer must understand that different colors can easily overstimulate people depending on the environment. Color can also have effects on a room. For example, if someone is claustrophobic then painting a room in darker colors could make the room feel smaller therefore the person could feel trapped.

Combining colors together could result in creating a state of mind as seen by the observer, and could eventually result in positive or negative effects on them. Colors can make the room feel either more calm, cheerful, comfortable, stressful, or dramatic. Color combinations can make a tiny room seem larger or smaller.[29] So it is for the Interior designer to choose appropriate colors for a place towards achieving how clients would want to look at, and feel in, that space.[28]

In 2024, red-colored home accessories were popularized on social media and in several design magazines for claiming to enhance interior design. This was coined the Unexpected Red Theory.

Lighting is very important when designing a space. Lighting in a room can affect the way that a room is shown. By adding natural and artificial lighting a designer can enhance the features in space and make it more pleasing. When an interior designer places lighting in a home it is important to know what lighting to put where and how to use lighting to highlight important places in the room. Lighting can enhance the aesthetic appeal of a place, setting the mood for the room. For example, when putting lighting into an office you want tp make sure there is overhead lighting, task/ desk lighting and natural lighting. Making sure there is enough lighting in a workspace is important so the person using the place does not strain their eyesight.

Residential design is the design of the interior of private residences. As this type of design is specific for individual situations, the needs and wants of the individual are paramount in this area of interior design. The interior designer may work on the project from the initial planning stage or may work on the remodeling of an existing structure. It is often a process that takes months to fine-tune and create a space with the vision of the client.[30]

Commercial design encompasses a wide range of subspecialties.

Other areas of specialization include amusement and theme park design, museum and exhibition design, exhibit design, event design (including ceremonies, weddings, baby and bridal showers, parties, conventions, and concerts), interior and prop styling, craft styling, food styling, product styling, tablescape design, theatre and performance design, stage and set design, scenic design, and production design for film and television. Beyond those, interior designers, particularly those with graduate education, can specialize in healthcare design, gerontological design, educational facility design, and other areas that require specialized knowledge. Some university programs offer graduate studies in theses and other areas. For example, both Cornell University and the University of Florida offer interior design graduate programs in environment and behavior studies.

There are various paths that one can take to become a professional interior designer. All of these paths involve some form of training. Working with a successful professional designer is an informal method of training and has previously been the most common method of education. In many states, however, this path alone cannot lead to licensing as a professional interior designer. Training through an institution such as a college, art or design school or university is a more formal route to professional practice.

In many countries, several university degree courses are now available, including those on interior architecture, taking three or four years to complete.

A formal education program, particularly one accredited by or developed with a professional organization of interior designers, can provide training that meets a minimum standard of excellence and therefore gives a student an education of a high standard. There are also university graduate and Ph.D. programs available for those seeking further training in a specific design specialization (i.e. gerontological or healthcare design) or those wishing to teach interior design at the university level.

There are a wide range of working conditions and employment opportunities within interior design. Large and tiny corporations often hire interior designers as employees on regular working hours. Designers for smaller firms and online renovation platforms usually work on a contract or per-job basis. Self-employed designers, who made up 32% of interior designers in 2020,[31] usually work the most hours. Interior designers often work under stress to meet deadlines, stay on budget, and meet clients' needs and wishes.

In some cases, licensed professionals review the work and sign it before submitting the design for approval by clients or construction permitting. The need for licensed review and signature varies by locality, relevant legislation, and scope of work. Their work can involve significant travel to visit different locations. However, with technology development, the process of contacting clients and communicating design alternatives has become easier and requires less travel.[32]

The Art Deco style began in Europe in the early years of the 20th century, with the waning of Art Nouveau. The term ""Art Deco"" was taken from the Exposition Internationale des Arts Decoratifs et Industriels Modernes, a world's fair held in Paris in 1925.[33] Art Deco rejected many traditional classical influences in favour of more streamlined geometric forms and metallic color. The Art Deco style influenced all areas of design, especially interior design, because it was the first style of interior decoration to spotlight new technologies and materials.[34]

Art Deco style is mainly based on geometric shapes, streamlining, and clean lines.[35][36] The style offered a sharp, cool look of mechanized living utterly at odds with anything that came before.[37]

Art Deco rejected traditional materials of decoration and interior design, opting instead to use more unusual materials such as chrome, glass, stainless steel, shiny fabrics, mirrors, aluminium, lacquer, inlaid wood, sharkskin, and zebra skin.[34] The use of harder, metallic materials was chosen to celebrate the machine age. These materials reflected the dawning modern age that was ushered in after the end of the First World War. The innovative combinations of these materials created contrasts that were very popular at the time – for example the mixing together of highly polished wood and black lacquer with satin and furs.[38] The barber shop in the Austin Reed store in London was designed by P. J. Westwood. It was soon regarded as the trendiest barber shop in Britain due to its use of metallic materials.[37]

The color themes of Art Deco consisted of metallic color, neutral color, bright color, and black and white. In interior design, cool metallic colors including silver, gold, metallic blue, charcoal grey, and platinum tended to predominate.[35][39] Serge Chermayeff, a Russian-born British designer made extensive use of cool metallic colors and luxurious surfaces in his room schemes. His 1930 showroom design for a British dressmaking firm had a silver-grey background and black mirrored-glass wall panels.[37][40]

Black and white was also a very popular color scheme during the 1920s and 1930s. Black and white checkerboard tiles, floors and wallpapers were very trendy at the time.[41] As the style developed, bright vibrant colors became popular as well.[42]

Art Deco furnishings and lighting fixtures had a glossy, luxurious appearance with the use of inlaid wood and reflective finishes. The furniture pieces often had curved edges, geometric shapes, and clean lines.[33][37] Art Deco lighting fixtures tended to make use of stacked geometric patterns.[43]

Modern design grew out of the decorative arts, mostly from the Art Deco, in the early 20th century.[44] One of the first to introduce this modernist style was Frank Lloyd Wright, who had not become hugely popularized until completing the house called Fallingwater in the 1930s. Modern art reached its peak during the 1950s and '60s, which is why designers and decorators today may refer to modern design as being ""mid-century"".[44] Modern art does not refer to the era or age of design and is not the same as contemporary design, a term used by interior designers for a shifting group of recent styles and trends.[44]

""Majlis painting"", also called nagash painting, is the decoration of the majlis, or front parlor of traditional Arabic homes, in the Asir province of Saudi Arabia and adjoining parts of Yemen. These wall paintings, an arabesque form of mural or fresco, show various geometric designs in bright colors: ""Called 'nagash' in Arabic, the wall paintings were a mark of pride for a woman in her house.""[45]

The geometric designs and heavy lines seem to be adapted from the area's textile and weaving patterns. ""In contrast with the sobriety of architecture and decoration in the rest of Arabia, exuberant color and ornamentation characterize those of Asir. The painting extends into the house over the walls and doors, up the staircases, and onto the furniture itself. When a house is being painted, women from the community help each other finish the job. The building then displays their shared taste and knowledge. Mothers pass these on to their daughters. This artwork is based on a geometry of straight lines and suggests the patterns common to textile weaving, with solid bands of different colors. Certain motifs reappear, such as the triangular mihrab or 'niche' and the palmette. In the past, paint was produced from mineral and vegetable pigments. Cloves and alfalfa yielded green. Blue came from the indigo plant. Red came from pomegranates and a certain mud. Paintbrushes were created from the tough hair found in a goat's tail. Today, however, women use modern manufactured paint to create new looks, which have become an indicator of social and economic change.""[46]

Women in the Asir province often complete the decoration and painting of the house interior. ""You could tell a family's wealth by the paintings,"" Um Abdullah says: ""If they didn't have much money, the wife could only paint the motholath, the basic straight, simple lines, in patterns of three to six repetitions in red, green, yellow and brown."" When women did not want to paint the walls themselves, they could barter with other women who would do the work. Several Saudi women have become famous as majlis painters, such as Fatima Abou Gahas.[45]

The interior walls of the home are brightly painted by the women, who work in defined patterns with lines, triangles, squares, diagonals and tree-like patterns. ""Some of the large triangles represent mountains. Zigzag lines stand for water and also for lightning. Small triangles, especially when the widest area is at the top, are found in pre-Islamic representations of female figures. That the small triangles found in the wall paintings in 'Asir are called banat may be a cultural remnant of a long-forgotten past.""[45]

""Courtyards and upper pillared porticoes are principal features of the best Nadjdi architecture, in addition to the fine incised plaster wood (jiss) and painted window shutters, which decorate the reception rooms. Good examples of plasterwork can often be seen in the gaping ruins of torn-down buildings- the effect is light, delicate and airy. It is usually around the majlis, around the coffee hearth and along the walls above where guests sat on rugs, against cushions. Doughty wondered if this ""parquetting of jis"", this ""gypsum fretwork... all adorning and unenclosed"" originated from India. However, the Najd fretwork seems very different from that seen in the Eastern Province and Oman, which are linked to Indian traditions, and rather resembles the motifs and patterns found in ancient Mesopotamia. The rosette, the star, the triangle and the stepped pinnacle pattern of dadoes are all ancient patterns, and can be found all over the Middle East of antiquity. Al-Qassim Province seems to be the home of this art, and there it is normally worked in hard white plaster (though what you see is usually begrimed by the smoke of the coffee hearth). In Riyadh, examples can be seen in unadorned clay.[47]

Sustainable Design is becoming more important today. This type of style includes eco-friendly, energy efficient, and sustainable design while keeping the space functional. Modern design prioritizes energy efficient design styles and eco-friendly design styles.

Interior design has become the subject of television shows. In the United Kingdom, popular interior design and decorating programs include 60 Minute Makeover (ITV), Changing Rooms (BBC), and Selling Houses (Channel 4). Famous interior designers whose work is featured in these programs include Linda Barker and Laurence Llewelyn-Bowen. In the United States, the TLC Network aired a popular program called Trading Spaces, a show based on the UK program Changing Rooms. In addition, both HGTV and the DIY Network also televise many programs about interior design and decorating, featuring the works of a variety of interior designers, decorators, and home improvement experts in a myriad of projects.

Fictional interior decorators include the Sugarbaker sisters on Designing Women and Grace Adler on Will & Grace. There is also another show called Home MADE. There are two teams and two houses and whoever has the designed and made the worst room, according to the judges, is eliminated. Another show on the Style Network, hosted by Niecy Nash, is Clean House where they re-do messy homes into themed rooms that the clients would like. Other shows include Design on a Dime, Designed to Sell, and The Decorating Adventures of Ambrose Price. The show called Design Star has become more popular through the five seasons that have already aired. The winners of this show end up getting their own TV shows, of which are Color Splash hosted by David Bromstad, Myles of Style hosted by Kim Myles, Paint-Over! hosted by Jennifer Bertrand, The Antonio Treatment hosted by Antonio Ballatore, and finally Secrets from a Stylist hosted by Emily Henderson. Bravo also has a variety of shows that explore the lives of interior designers. These include Flipping Out, which explores the life of Jeff Lewis and his team of designers; Million Dollar Decorators explores the lives of interior designers Nathan Turner, Jeffrey Alan Marks, Mary McDonald, Kathryn Ireland, and Martyn Lawrence Bullard.

Interior design has also become the subject of radio shows. In the U.S., popular interior design & lifestyle shows include Martha Stewart Living and Living Large featuring Karen Mills. Famous interior designers whose work is featured on these programs include Bunny Williams, Barbara Barry, and Kathy Ireland, among others.

Many interior design magazines exist to offer advice regarding color palette, furniture, art, and other elements that fall under the umbrella of interior design. These magazine often focus on related subjects to draw a more specific audience. For instance, architecture as a primary aspect of Dwell, while Veranda is well known as a luxury living magazine. Lonny Magazine and the newly relaunched, Domino Magazine, cater to a young, hip, metropolitan audience, and emphasize accessibility and a do-it-yourself (DIY) approach to interior design.

Other early interior decorators:

Many of the most famous designers and decorators during the 20th century had no formal training. Some examples include Sister Parish, Robert Denning and Vincent Fourcade, Kerry Joyce, Kelly Wearstler, Stéphane Boudin, Georges Geffroy, Emilio Terry, Carlos de Beistegui, Nina Petronzio, Lorenzo Mongiardino, Mary Jean Thompson and David Nightingale Hicks.

Notable interior designers in the world today include Scott Salvator, Troy Adams, Jonathan Adler, Michael S. Smith, Martin Brudnizki, Mary Douglas Drysdale, Kelly Hoppen, Kelly Wearstler, Nina Campbell, David Collins, Nate Berkus, Sandra Espinet, Jo Hamilton and Nicky Haslam.
"
Home Staging Services,"Home staging is the preparation of a private residence for sale in the real estate marketplace.[1] The goal of staging is to make a home appealing to the highest number of potential buyers, thereby selling a property more swiftly and for more money. Staging techniques focus on improving a property's appeal by ensuring it is a welcoming, attractive product that any buyer can see themself living in and, thus, desire to purchase.[2]

Staging requires a monetary investment into selling what are some people's greatest investment: their home and property. So it is not just decorating, but also needs to include an understanding of how to find the equity in that home and property, where the real estate market is for that home, in that neighborhood, so the homeowner can choose where to make those investments. Comparable houses for sale on the market in the area also needs to be considered to know how to stage the home so it sells quickly and for more money. Return on investment is the purpose of staging.

As for simple decorating to stage, people often use art, painting, accessories, lights, greenery, and carpet to stage the home,[3] to give potential buyers a more attractive first impression of the property.[4] They also rearrange or ""temporarily replace"" furniture.[5] Properly executed staging leads the eye to attractive features, while down-playing flaws. Different areas and rooms of a home can have varying levels of impact when convincing potential homebuyers, and therefore some rooms can be considered more important than others when it comes to staging.[6]

Home staging is not without controversy: ""rattled"" was a New York Times-cited reaction by a writer with a library room being told that ""More than 50 percent of shelf space devoted to books equals clutter.""[3]

In Sweden, professional home staging, known as homestyling, became popular after 2000. Professional home stagers claim homestyling yields around 10% higher pricing compared to just cleaning the home.

In Britain, home staging is sometimes referred to as property presentation or property styling. The techniques were televised by home stylist Ann Maurice in the show House Doctor on Five for six years.[7] and popularized the alternate description ""house doctoring."" Her follow-up show is named House Doctor: Inside And Out.[7]

The New York Times described the seller's situation as ""... not always happily, in the company of unfamiliar sofas ... tables ..."" while some of what they own is placed in temporary storage.[5] However, U.S. research states that home staging can reduce a listing's time on the market by one third to half, and could fetch as much as 6% to 20% more than an empty home or a home not properly staged.[8]

The pricing is not just for a one time dollar amount; high-end fees can be ""$3,000 a month"" (part of which is for renting replacement furniture).[5]

The practice of home staging has long elicited strong reactions. Agents and professional stagers point to examples like the Sarro-Waite apartment, and say staging can usually help a home sell faster, and for a higher price, offering a larger return on the investment.[9] There is criticism on the fact that buyers will have to pay more for nothing, the furniture is not part of the sales.

One 2018 professional view is that it is five years past simple decluttering and some light painting. Home staging services have sizable inventories of alternative chairs, current accessories, and even pillows, blankets, and towels.[5] A decade prior,[10] the simple view was that sellers believed in paid-for-staging, since it raised property values by reducing the home's perceived flaws. More recent changes now include depersonalizing,[3] improving overall condition and even landscaping. For vacant homes, prop or staging furniture is used to create a living space where the buyer can imagine themselves living and also better understand the sizes of rooms.

Prices for initial consultation and monthly follow-up fees vary by region[11][10] and by the size of the property (including number of rooms).[12]
"
Sheet Metal Services,"Sheet metal is metal formed into thin, flat pieces, usually by an industrial process.

Thicknesses can vary significantly; extremely thin sheets are considered foil or leaf, and pieces thicker than 6 mm (0.25 in) are considered plate, such as plate steel, a class of structural steel.

Sheet metal is available in flat pieces or coiled strips. The coils are formed by running a continuous sheet of metal through a roll slitter.

In most of the world, sheet metal thickness is consistently specified in millimeters. In the U.S., the thickness of sheet metal is commonly specified by a traditional, non-linear measure known as its gauge. The larger the gauge number, the thinner the metal. Commonly used steel sheet metal ranges from 30 gauge to about 7 gauge. Gauge differs between ferrous (iron-based) metals and nonferrous metals such as aluminum or copper. Copper thickness, for example, is measured in ounces, representing the weight of copper contained in an area of one square foot. Parts manufactured from sheet metal must maintain a uniform thickness for ideal results.[1]

There are many different metals that can be made into sheet metal, such as aluminium, brass, copper, steel, tin, nickel and titanium. For decorative uses, some important sheet metals include silver, gold, and platinum (platinum sheet metal is also utilized as a catalyst). These metal sheets are processed through different processing technologies, mainly including cold rolling and hot rolling. Sometimes hot-dip galvanizing process is adopted as needed to prevent it from rusting due to constant exposure to the outdoors. Sometimes a layer of color coating is applied to the surface of the cold-rolled sheet to obtain a decorative and protective metal sheet, generally called a color-coated metal sheet.

Sheet metal is used in automobile and truck (lorry) bodies, major appliances, airplane fuselages and wings, tinplate for tin cans, roofing for buildings (architecture), and many other applications. Sheet metal of iron and other materials with high magnetic permeability, also known as laminated steel cores, has applications in transformers and electric machines. Historically, an important use of sheet metal was in plate armor worn by cavalry, and sheet metal continues to have many decorative uses, including in horse tack. Sheet metal workers are also known as ""tin bashers"" (or ""tin knockers""), a name derived from the hammering of panel seams when installing tin roofs.[2]

Hand-hammered metal sheets have been used since ancient times for architectural purposes. Water-powered rolling mills replaced the manual process in the late 17th century. The process of flattening metal sheets required large rotating iron cylinders which pressed metal pieces into sheets. The metals suited for this were lead, copper, zinc, iron and later steel. Tin was often used to coat iron and steel sheets to prevent it from rusting.[3] This tin-coated sheet metal was called ""tinplate."" Sheet metals appeared in the United States in the 1870s, being used for shingle roofing, stamped ornamental ceilings, and exterior façades. Sheet metal ceilings were only popularly known as ""tin ceilings"" later as manufacturers of the period did not use the term. The popularity of both shingles and ceilings encouraged widespread production. With further advances of steel sheet metal production in the 1890s, the promise of being cheap, durable, easy to install, lightweight and fireproof gave the middle-class a significant appetite for sheet metal products. It was not until the 1930s and WWII that metals became scarce and the sheet metal industry began to collapse.[4] However, some American companies, such as the W.F. Norman Corporation, were able to stay in business by making other products until Historic preservation projects aided the revival of ornamental sheet metal.

Grade 304 is the most common of the three grades. It offers good corrosion resistance while maintaining formability and weldability. Available finishes are #2B, #3, and #4. Grade 303 is not available in sheet form.[5]

Grade 316 possesses more corrosion resistance and strength at elevated temperatures than 304. It is commonly used for pumps, valves, chemical equipment, and marine applications. Available finishes are #2B, #3, and #4.[5]

Grade 410 is a heat treatable stainless steel, but it has a lower corrosion resistance than the other grades. It is commonly used in cutlery. The only available finish is dull.[5]

Grade 430 is a popular grade, low-cost alternative to series 300's grades. This is used when high corrosion resistance is not a primary criterion. Common grade for appliance products, often with a brushed finish.[citation needed]

Aluminium is widely used in sheet metal form due to its flexibility, wide range of options, cost effectiveness, and other properties.[6] The four most common aluminium grades available as sheet metal are 1100-H14, 3003-H14, 5052-H32, and 6061-T6.[5][7]

Grade 1100-H14 is commercially pure aluminium, highly chemical and weather resistant. It is ductile enough for deep drawing and weldable, but has low strength. It is commonly used in chemical processing equipment, light reflectors, and jewelry.[5]

Grade 3003-H14 is stronger than 1100, while maintaining the same formability and low cost. It is corrosion resistant and weldable. It is often used in stampings, spun and drawn parts, mail boxes, cabinets, tanks, and fan blades.[5]

Grade 5052-H32 is much stronger than 3003 while still maintaining good formability. It maintains high corrosion resistance and weldability. Common applications include electronic chassis, tanks, and pressure vessels.[5]

Grade 6061-T6 is a common heat-treated structural aluminium alloy. It is weldable, corrosion resistant, and stronger than 5052, but not as formable. It loses some of its strength when welded.[5] It is used in modern aircraft structures.[8]

Brass is an alloy of copper, which is widely used as a sheet metal. It has more strength, corrosion resistance and formability when compared to copper while retaining its conductivity.

In sheet hydroforming, variation in incoming sheet coil properties is a common problem for forming process, especially with materials for automotive applications. Even though incoming sheet coil may meet tensile test specifications, high rejection rate is often observed in production due to inconsistent material behavior.  Thus there is a strong need for a discriminating method for testing incoming sheet material formability. The hydraulic sheet bulge test emulates biaxial deformation conditions commonly seen in production operations.

For forming limit curves of materials aluminium, mild steel and brass. Theoretical analysis is carried out by deriving governing equations for determining of equivalent stress and equivalent strain based on the bulging to be spherical and Tresca's yield criterion with the associated flow rule. For experimentation circular grid analysis is one of the most effective methods.[9]

Use of gauge numbers to designate sheet metal thickness is discouraged by numerous international standards organizations. For example, ASTM states in specification ASTM A480-10a: ""The use of gauge number is discouraged as being an archaic term of limited usefulness not having general agreement on meaning.""[10]

Manufacturers' Standard Gauge for Sheet Steel is based on an average density of 41.82 lb per square foot per inch thick,[11] equivalent to 501.84 pounds per cubic foot (8,038.7 kg/m3). The older United States Standard Gauge is based upon 40  lb per square foot per inch thick. Gauge is defined differently for ferrous (iron-based) and non-ferrous metals (e.g. aluminium and brass).

The gauge thicknesses shown in column 2 (U.S. standard sheet and plate iron and steel decimal inch (mm)) seem somewhat arbitrary. The progression of thicknesses is clear in column 3 (U.S. standard for sheet and plate iron and steel 64ths inch (delta)). The thicknesses vary first by 1⁄32 inch in higher thicknesses and then step down to increments of 1⁄64 inch, then 1⁄128 inch, with the final increments at decimal fractions of 1⁄64 inch.

Some steel tubes are manufactured by folding a single steel sheet into a square/circle and welding the seam together.[12] Their wall thickness has a similar (but distinct) gauge to the thickness of steel sheets.[13]

During the rolling process the rollers bow slightly, which results in the sheets being thinner on the edges.[5] The tolerances in the table and attachments reflect current manufacturing practices and commercial standards and are not representative of the Manufacturer's Standard Gauge, which has no inherent tolerances.

The equation for estimating the maximum bending force is,






F

max


=
k



T
L

t

2



W




{\displaystyle F_{\text{max}}=k{\frac {TLt^{2}}{W}}}

,

where k is a factor taking into account several parameters including friction. T is the ultimate tensile strength of the metal. L and t are the length and thickness of the sheet metal, respectively. The variable W is the open width of a V-die or wiping die.

The curling process is used to form an edge on a ring. This process is used to remove sharp edges. It also increases the moment of inertia near the curled end.
The flare/burr should be turned away from the die. It is used to curl a material of specific thickness. Tool steel is generally used due to the amount of wear done by operation.

It is a metal working process of removing camber, the horizontal bend, from a strip shaped material. It may be done to a finite length section or coils. It resembles flattening of leveling process, but on a deformed edge.

Drawing is a forming process in which the metal is stretched over a form or die.[19] In deep drawing the depth of the part being made is more than half its diameter. Deep drawing is used for making automotive fuel tanks, kitchen sinks, two-piece aluminum cans, etc. Deep drawing is generally done in multiple steps called draw reductions. The greater the depth, the more reductions are required.  Deep drawing may also be accomplished with fewer reductions by heating the workpiece, for example in sink manufacture.

In many cases, material is rolled at the mill in both directions to aid in deep drawing. This leads to a more uniform grain structure which limits tearing and is referred to as ""draw quality"" material.

Expanding is a process of cutting or stamping slits in alternating pattern much like the stretcher bond in brickwork and then stretching the sheet open in accordion-like fashion. It is used in applications where air and water flow are desired as well as when light weight is desired at cost of a solid flat surface. A similar process is used in other materials such as paper to create a low cost packing paper with better supportive properties than flat paper alone.

Hemming is a process of folding the edge of sheet metal onto itself to reinforce that edge. Seaming is a process of folding two sheets of metal together to form a joint.

Hydroforming is a process that is analogous to deep drawing, in that the part is formed by stretching the blank over a stationary die. The force required is generated by the direct application of extremely high hydrostatic pressure to the workpiece or to a bladder that is in contact with the workpiece, rather than by the movable part of a die in a mechanical or hydraulic press. Unlike deep drawing, hydroforming usually does not involve draw reductions—the piece is formed in a single step.

Incremental sheet forming or ISF forming process is basically sheet metal working or sheet metal forming process. In this case, sheet is formed into final shape by a series of processes in which small incremental deformation can be done in each series.

Ironing is a sheet metal working or sheet metal forming process. It uniformly thins the workpiece in a specific area. This is a very useful process. It is used to produce a uniform wall thickness part with a high height-to-diameter ratio.
It is used in making aluminium beverage cans.

Sheet metal can be cut in various ways, from hand tools called tin snips up to very large powered shears. With the advances in technology, sheet metal cutting has turned to computers for precise cutting. Many sheet metal cutting operations are based on computer numerically controlled (CNC) laser cutting or multi-tool CNC punch press.

CNC laser involves moving a lens assembly carrying a beam of laser light over the surface of the metal. Oxygen, nitrogen or air is fed through the same nozzle from which the laser beam exits. The metal is heated and burnt by the laser beam, cutting the metal sheet.[20] The quality of the edge can be mirror smooth and a precision of around 0.1 mm (0.0039 in) can be obtained. Cutting speeds on thin 1.2 mm (0.047 in) sheet can be as high as 25 m (82 ft) per minute. Most laser cutting systems use a CO2 based laser source with a wavelength of around 10 μm; some more recent systems use a YAG based laser with a wavelength of around 1 μm.

Photochemical machining, also known as photo etching, is a tightly controlled corrosion process which is used to produce complex metal parts from sheet metal with very fine detail. The photo etching process involves photo sensitive polymer being applied to a raw metal sheet. Using CAD designed photo-tools as stencils, the metal is exposed to UV light to leave a design pattern, which is developed and etched from the metal sheet.

Perforating is a cutting process that punches multiple small holes close together in a flat workpiece. Perforated sheet metal is used to make a wide variety of surface cutting tools, such as the surform.

This is a form of bending used to produce long, thin sheet metal parts. The machine that bends the metal is called a press brake. The lower part of the press contains a V-shaped groove called the die. The upper part of the press contains a punch that presses the sheet metal down into the v-shaped die, causing it to bend.[21] There are several techniques used, but the most common modern method is ""air bending"". Here, the die has a sharper angle than the required bend (typically 85 degrees for a 90 degree bend) and the upper tool is precisely controlled in its stroke to push the metal down the required amount to bend it through 90 degrees. Typically, a general purpose machine has an available bending force of around 25 tons per meter of length. The opening width of the lower die is typically 8 to 10 times the thickness of the metal to be bent (for example, 5 mm material could be bent in a 40 mm die). The inner radius of the bend formed in the metal is determined not by the radius of the upper tool, but by the lower die width. Typically, the inner radius is equal to 1/6 of the V-width used in the forming process.

The press usually has some sort of back gauge to position depth of the bend along the workpiece. The backgauge can be computer controlled to allow the operator to make a series of bends in a component to a high degree of accuracy. Simple machines control only the backstop, more advanced machines control the position and angle of the stop, its height and the position of the two reference pegs used to locate the material. The machine can also record the exact position and pressure required for each bending operation to allow the operator to achieve a perfect 90 degree bend across a variety of operations on the part.

Punching is performed by placing the sheet of metal stock between a punch and a die mounted in a press. The punch and die are made of hardened steel and are the same shape. The punch is sized to be a very close fit in the die. The press pushes the punch against and into the die with enough force to cut a hole in the stock. In some cases the punch and die ""nest"" together to create a depression in the stock. In progressive stamping, a coil of stock is fed into a long die/punch set with many stages. Multiple simple shaped holes may be produced in one stage, but complex holes are created in multiple stages. In the final stage, the part is punched free from the ""web"".

A typical CNC turret punch has a choice of up to 60 tools in a ""turret"" that can be rotated to bring any tool to the punching position. A simple shape (e.g. a square, circle, or hexagon) is cut directly from the sheet. A complex shape can be cut out by making many square or rounded cuts around the perimeter. A punch is less flexible than a laser for cutting compound shapes, but faster for repetitive shapes (for example, the grille of an air-conditioning unit). A CNC punch can achieve 600 strokes per minute.

A typical component (such as the side of a computer case) can be cut to high precision from a blank sheet in under 15 seconds by either a press or a laser CNC machine.

A continuous bending operation for producing open profiles or welded tubes with long lengths or in large quantities.

Rolling is metal working or metal forming process. In this method, stock passes through one or more pair of rolls to reduce thickness. It is used to make thickness uniform. It is classified according to its temperature of rolling:[22]

Spinning is used to make tubular (axis-symmetric) parts by fixing a piece of sheet stock to a rotating form (mandrel). Rollers or rigid tools press the stock against the form, stretching it until the stock takes the shape of the form. Spinning is used to make rocket motor casings, missile nose cones, satellite dishes and metal kitchen funnels.

Stamping includes a variety of operations such as punching, blanking, embossing, bending, flanging, and coining; simple or complex shapes can be formed at high production rates; tooling and equipment costs can be high, but labor costs are low.

Alternatively, the related techniques repoussé and chasing have low tooling and equipment costs, but high labor costs.

A water jet cutter, also known as a waterjet, is a tool capable of a controlled erosion into metal or other materials using a jet of water at high velocity and pressure, or a mixture of water and an abrasive substance.

The process of using an English wheel is called wheeling. It is basically a metal working or metal forming process. An English wheel is used by a craftsperson to form compound curves from a flat sheet of metal of aluminium or steel. It is costly, as highly skilled labour is required. It can produce different panels by the same method. A stamping press is used for high numbers in production.[23]

The use of sheet metal, through a comprehensive cold working process, including bending, shearing, punching, laser cutting, water jet cutting, riveting, splicing, etc. to make the final product we want (such as computer chassis, washing machine shells, refrigerator door panels, etc.), we generally called sheet metal fabrication. The academic community currently has no uniform definition, but this process has a common feature of the process is that the material is generally a thin sheet, and will not change the thickness of most of the material of the part.

Fasteners that are commonly used on sheet metal include: clecos,[24] rivets,[25] and sheet metal screws.
"
Welding Services,"

Welding is a fabrication process that joins materials, usually metals or thermoplastics, primarily by using high temperature to melt the parts together and allow them to cool, causing fusion. Common alternative methods include solvent welding (of thermoplastics) using chemicals to melt materials being bonded without heat, and solid-state welding processes which bond without melting, such as pressure, cold welding, and diffusion bonding.

Metal welding is distinct from lower temperature bonding techniques such as brazing and soldering, which do not melt the base metal (parent metal) and instead require flowing a filler metal to solidify their bonds.

In addition to melting the base metal in welding, a filler material is typically added to the joint to form a pool of molten material (the weld pool) that cools to form a joint that can be stronger than the base material. Welding also requires a form of shield to protect the filler metals or melted metals from being contaminated or oxidized.

Many different energy sources can be used for welding, including a gas flame (chemical), an electric arc (electrical), a laser, an electron beam, friction, and ultrasound. While often an industrial process, welding may be performed in many different environments, including in open air, under water, and in outer space. Welding is a hazardous undertaking and precautions are required to avoid burns, electric shock, vision damage, inhalation of poisonous gases and fumes, and exposure to intense ultraviolet radiation.

Until the end of the 19th century, the only welding process was forge welding, which blacksmiths had used for millennia to join iron and steel by heating and hammering. Arc welding and oxy-fuel welding were among the first processes to develop late in the century, and electric resistance welding followed soon after. Welding technology advanced quickly during the early 20th century, as world wars drove the demand for reliable and inexpensive joining methods. Following the wars, several modern welding techniques were developed, including manual methods like shielded metal arc welding, now one of the most popular welding methods, as well as semi-automatic and automatic processes such as gas metal arc welding, submerged arc welding, flux-cored arc welding and electroslag welding. Developments continued with the invention of laser beam welding, electron beam welding, magnetic pulse welding, and friction stir welding in the latter half of the century. Today, as the science continues to advance, robot welding is commonplace in industrial settings, and researchers continue to develop new welding methods and gain greater understanding of weld quality.[1]

The term weld is derived from the Middle English verb well (wæll; plural/present tense: wælle) or welling (wællen), meaning 'to heat' (to the maximum temperature possible); 'to bring to a boil'. The modern word was probably derived from the past-tense participle welled (wællende), with the addition of d for this purpose being common in the Germanic languages of the Angles and Saxons. It was first recorded in English in 1590. A fourteenth century translation of the Christian Bible into English by John Wycliffe translates Isaiah 2:4 as ""...thei shul bete togidere their swerdes into shares..."" (they shall beat together their swords into plowshares). In the 1590 version this was changed to ""...thei shullen welle togidere her swerdes in-to scharris..."" (they shall weld together their swords into plowshares), suggesting this particular use of the word probably became popular in English sometime between these periods.[2]

The Old English word for welding iron was samod ('to bring together') or samodwellung ('to bring together hot').[3]

The word is related to the Old Swedish word valla, meaning 'to boil', which could refer to joining metals, as in valla järn (literally 'to boil iron'). Sweden was a large exporter of iron during the Middle Ages, so the word may have entered English from the Swedish iron trade, or may have been imported with the thousands of Viking settlements that arrived in England before and during the Viking Age, as more than half of the most common English words in everyday use are Scandinavian in origin.[4][5]

The history of joining metals goes back several millennia.[6] Fusion welding processes that join metals by melting them were not widely used in pre-industrial welding. Early welding techniques used pressure to join to the metals, often with heat not sufficient to fully melt the base metals.[7] One notable exception was a technique to join sections of large statues. In Greek and Roman lost-wax casting, the statues were cast as smaller pieces and molten bronze was poured into the joints with temperatures sufficient to create fusion welds.[8]

The earliest known welding dates to the Bronze Age. Gold is soft enough to be pressure welded with little to no heat, and archaeologists have found small boxes made by pressure welding overlapping sheets of gold. In the Iron Age, Mediterranean societies developed forge welding.[9] In forge welding, metal is heated to the point that it becomes soft enough for a blacksmith to hammer separate pieces together.[10] Very early notable examples are the iron objects found with Tutankhamun including an iron headrest and dagger.[11][12] The dagger was forged from meteoric iron at temperatures below 950 °C (1,740 °F).[13] Typically, wrought iron is forged at around 1,350 °C (2,460 °F).[14] The ancient Greek historian Herodotus credits Glaucus of Chios with discovering ""iron welding"".[15] Glaucus is known for an iron pedestal welded to hold a silver krater at Delphi.[16]

The Middle Ages brought advances in forge welding, in which blacksmiths pounded heated metal repeatedly until bonding occurred.[18] In Europe and Africa, forging shifted from open charcoal fires to bloomeries. China developed the blast furnace late in the first millennia.[19] Forge welding was used in the construction of the Iron pillar of Delhi, erected in Delhi, India about 310 AD and weighing 5.4 metric tons.[20] In 1540, Vannoccio Biringuccio published De la pirotechnia, which includes descriptions of the forging operation.[18] Renaissance craftsmen were skilled in the process, and the industry continued to grow during the following centuries.[18]

In 1800, Sir Humphry Davy discovered the short-pulse electrical arc and presented his results in 1801.[21][22][23] In 1802, Russian scientist Vasily Petrov created the continuous electric arc,[23][24][25] and subsequently published ""News of Galvanic-Voltaic Experiments"" in 1803, in which he described experiments carried out in 1802. Of great importance in this work was the description of a stable arc discharge and the indication of its possible use for many applications, one being melting metals.[26] In 1808, Davy, who was unaware of Petrov's work, rediscovered the continuous electric arc.[22][23] In 1881–82 inventors Nikolai Benardos (Russian) and Stanisław Olszewski (Polish)[27] created the first electric arc welding method known as carbon arc welding using carbon electrodes. The advances in arc welding continued with the invention of metal electrodes in the late 1800s by a Russian, Nikolai Slavyanov (1888), and an American, C. L. Coffin (1890). Around 1900, A. P. Strohmenger released a coated metal electrode in Britain, which gave a more stable arc. In 1905, Russian scientist Vladimir Mitkevich proposed using a three-phase electric arc for welding. Alternating current welding was invented by C. J. Holslag in 1919, but did not become popular for another decade.[28]

Resistance welding was also developed during the final decades of the 19th century, with the first patents going to Elihu Thomson in 1885, who produced further advances over the next 15 years. Thermite welding was invented in 1893, and around that time another process, oxyfuel welding, became well established. Acetylene was discovered in 1836 by Edmund Davy, but its use was not practical in welding until about 1900, when a suitable torch was developed.[29] At first, oxyfuel welding was one of the more popular welding methods due to its portability and relatively low cost. As the 20th century progressed, however, it fell out of favor for industrial applications. It was largely replaced with arc welding, as advances in metal coverings (known as flux) were made.[30] Flux covering the electrode primarily shields the base material from impurities, but also stabilizes the arc and can add alloying components to the weld metal.[31]

World War I caused a major surge in the use of welding, with the various military powers attempting to determine which of the several new welding processes would be best.[32] The British primarily used arc welding, even constructing a ship, the ""Fullagar"" with an entirely welded hull.[33][34]: 142  Arc welding was first applied to aircraft during the war as well, as some German airplane fuselages were constructed using the process.[35]

During the middle of the century, many new welding methods were invented, including the introduction of automatic welding in 1920, in which electrode wire was fed continuously.[36] Shielding gas received much attention, as scientists attempted to protect welds from the effects of oxygen and nitrogen in the atmosphere. Porosity and brittleness were the primary problems, and the solutions that developed included the use of hydrogen, argon, and helium as welding atmospheres.[37] Testing methods were introduced for weld integrity. First vibration testing was done using a hammer and stethoscope; later, X-ray tests were developed to see into the weld.[38] During the 1930s, further advances allowed for the welding of reactive metals like aluminum and magnesium.[39] This in conjunction with developments in automatic welding, alternating current, and fluxes fed a major expansion of arc welding during the 1930s.[40] Russian inventor Konstantin Khrenov implemented the first underwater electric arc welding.[41] In 1930, Kyle Taylor was responsible for the release of stud welding, which soon became popular in shipbuilding and construction. Submerged arc welding was invented the same year.[42] During World War II, submerged arc welding was widely used for ship-building because it allowed certain types of welds to be done twenty times faster than earlier techniques.[43]

Improvements to welding processes opened up new possibilities for construction.[44] Previously, large metal structures had been made from metals joined mechanically with rivets, along with bolts, screws, and belts. These connected but unfused metal structures had inherent weaknesses.[45] The steamboat Sultana killed over a thousand passengers when its riveted boiler failed under pressure.[46] The ""unsinkable"" Titanic sank due in part to failures in its riveted hull.[47] In 1930, the first all-welded merchant vessel, M/S Carolinian, was launched.[48] The strength of welded steel also allowed for the creation of entirely new types of ships, notably the liquefied natural gas (LNG) tanker. The ASME Boiler and Pressure Vessel Code, created in response to deadly boiler failures was used to develop the spherical tanks that contain LNG during transport.[49] Also noteworthy is the first welded road bridge in the world, the Maurzyce Bridge in Poland (1928).[33] Early skyscrapers and steel truss bridges were built from riveted steel beams.[50][51] Welding allows for stronger and lighter structures and greater range of shapes.[52] The Sydney Opera House's icon shape is built on a stud-welded steel frame.[53]

Gas tungsten arc welding, after decades of development, was finally perfected in 1941, and gas metal arc welding followed in 1948, allowing for fast welding of non-ferrous materials but requiring expensive shielding gases. Shielded metal arc welding was developed during the 1950s, using a flux-coated consumable electrode, and it quickly became the most popular metal arc welding process. In 1957, the flux-cored arc welding process debuted, in which the self-shielded wire electrode could be used with automatic equipment, resulting in greatly increased welding speeds, and that same year, plasma arc welding was invented by Robert Gage. Electroslag welding was introduced in 1958, and it was followed by its cousin, electrogas welding, in 1961.[54] In 1953, the Soviet scientist N. F. Kazakov proposed the diffusion bonding method.[55]

Other recent developments in welding include the 1958 breakthrough of electron beam welding, making deep and narrow welding possible through the concentrated heat source. Following the invention of the laser in 1960, laser beam welding debuted several decades later, and has proved to be especially useful in high-speed, automated welding. Magnetic pulse welding (MPW) has been industrially used since 1967. Friction stir welding was invented in 1991 by Wayne Thomas at The Welding Institute (TWI, UK) and found high-quality applications all over the world.[56] All of these four new processes continue to be quite expensive due to the high cost of the necessary equipment, and this has limited their applications.[57]

Welding joins two pieces of metal using heat, pressure, or both. The most common modern welding methods use heat sufficient to melt the base metals to be joined and the filler metal.[59] This includes gas welding and all forms of arc welding.[60] The area where the base and filler metals melt is called the weld pool or puddle.[61] The weld pool must be protected from oxygen in the air that will oxidize with the molten metal and from other gases that could contaminate the weld.[62] Most welding methods involve pushing the puddle along a joint to create a weld bead.[63] Overlapping pieces of metal can be joined by forming the weld pool within a hole made in the topmost piece of base metal to form a plug weld.[64]

Solid-state welding processes join two pieces of metal using pressure.[65] Electric resistance welding is a common industrial process that combines heat and pressure to join overlapping base metals without any filler material.[66]

Gas welding, also known as oxyacetylene welding, uses an open flame to generate heat and shield the weld. Compared to arc welding, the flame is less concentrated and lower in temperature, about 3100 °C (5600 °F) near the torch tip. This causes slower weld cooling, which can lead to greater residual stresses and weld distortion, though it eases the welding of high alloy steels. The diffuse outer envelope of the flame consumes oxygen before it can reach the molten weld pool.[30] When working with easily oxidized metals, such as stainless steel, flux can be brushed onto the base metals.[67]

The equipment is relatively inexpensive and simple, consisting of a torch, hoses, pressure regulators, a tank of oxygen, and a tank of fuel (usually acetylene).[68] It is one of the oldest and most versatile welding processes, but it has become less popular in industrial applications. It is still widely used for welding pipes and tubes, as well as repair work.[30] A similar process, generally called oxyfuel cutting, is used to cut metals. Oxyfuel equipment can also be used to heat metal before bending or straightening.[69]

All arc welding processes use a welding power supply to create and maintain an electric arc between an electrode and the base material to melt metals at the welding point. They can use alternating current (AC) or direct current (DC). For DC welding, the electrode can be connected to the machine's positive terminal (DCEP) or negative terminal (DCEN), changing the current's direction. The process and type of electrode used will typically determine the current.[71][72]

Shielding gas prevents oxygen in the atmosphere from entering the molten weld pool. In some processes, the shielding gas is delivered from gas cylinders containing inert or semi-inert gas. In others, a flux coating on a consumable electrode disintegrates to create the gas.[73][74] Filler material is typically added to the molten weld pool and is necessary for processes that use a consumable electrode.[75]

One of the most common types of arc welding is shielded metal arc welding (SMAW);[76] it is also known as manual metal arc welding (MMAW) or stick welding. Electric current is used to strike an arc between the base material and consumable electrode rod, which is made of filler material (typical steel) and is covered with a flux that protects the weld area from oxidation and contamination by producing carbon dioxide (CO2) gas during the welding process. The electrode core itself acts as filler material, making a separate filler unnecessary.[76]

The process is versatile and can be performed with relatively inexpensive equipment, making it well suited to shop jobs and field work.[76][77] An operator can become reasonably proficient with a modest amount of training and can achieve mastery with experience. Weld times are rather slow, since the consumable electrodes must be frequently replaced and because slag, the residue from the flux, must be chipped away after welding.[76] Furthermore, the process is generally limited to welding ferrous materials, though special electrodes have made possible the welding of cast iron, stainless steel, aluminum, and other metals.[77]

Gas metal arc welding (GMAW), also known as metal inert gas or MIG welding, is a semi-automatic or automatic process that uses a continuous wire feed as an electrode and an inert or semi-inert gas mixture to protect the weld from contamination. Since the electrode is continuous, welding speeds are greater for GMAW than for SMAW.[78]

A related process, flux-cored arc welding (FCAW), uses similar equipment but uses wire consisting of a tubular steel electrode surrounding a powder fill material. This cored wire is more expensive than the standard solid wire and can generate fumes and/or slag, but it permits even higher welding speed and greater metal penetration.[79] As the electrode is consumed, the flux disintegrates to create shielding gas and a protective layer of slag similar to stick welding. Some flux-cored machines have a nozzle that uses a shielding gas to supplement the protection from the flux. This is called dual shield welding, and uses a specialized gas shielded flux-core wire.[80]

Gas tungsten arc welding (GTAW), or tungsten inert gas (TIG) welding, is a manual welding process that uses a non-consumable tungsten electrode, an inert or semi-inert gas mixture, and a separate filler material.[81] Especially useful for welding thin materials, this method is characterized by a stable arc and high-quality welds, but it requires significant operator skill and can only be accomplished at relatively low speeds.[81]

GTAW can be used on nearly all weldable metals, though it is most often applied to stainless steel and light metals. It is often used when quality welds are extremely important, such as in bicycle, aircraft and naval applications.[81] A related process, plasma arc welding, also uses a tungsten electrode but uses plasma gas to make the arc. The arc is more concentrated than the GTAW arc, making transverse control more critical and thus generally restricting the technique to a mechanized process. Because of its stable current, the method can be used on a wider range of material thicknesses than can the GTAW process and it is much faster. It can be applied to all of the same materials as GTAW except magnesium, and automated welding of stainless steel is one important application of the process. A variation of the process is plasma cutting, an efficient steel cutting process.[82]

Submerged arc welding (SAW) is a high-productivity welding method in which the arc is struck beneath a covering layer of flux. This increases arc quality since contaminants in the atmosphere are blocked by the flux. The slag that forms on the weld generally comes off by itself, and combined with the use of a continuous wire feed, the weld deposition rate is high. Working conditions are much improved over other arc welding processes, since the flux hides the arc and almost no smoke is produced. The process is commonly used in industry, especially for large products and in the manufacture of welded pressure vessels.[83] Other arc welding processes include atomic hydrogen welding, electroslag welding (ESW), electrogas welding, and stud arc welding.[84] ESW is a highly productive, single-pass welding process for thicker materials between 1 inch (25 mm) and 12 inches (300 mm) in a vertical or close to vertical position.

To supply the electrical power necessary for arc welding processes, a variety of different power supplies can be used. The most common welding power supplies are constant current power supplies and constant voltage power supplies. In arc welding, the length of the arc is directly related to the voltage, and the amount of heat input is related to the current. Constant current power supplies are most often used for manual welding processes such as gas tungsten arc welding and shielded metal arc welding, because they maintain a relatively constant current even as the voltage varies. This is important because in manual welding, it can be difficult to hold the electrode perfectly steady, and as a result, the arc length and thus voltage tend to fluctuate. Constant voltage power supplies hold the voltage constant and vary the current, and as a result, are most often used for automated welding processes such as gas metal arc welding, flux-cored arc welding, and submerged arc welding. In these processes, arc length is kept constant, since any fluctuation in the distance between the wire and the base material is quickly rectified by a large change in current. For example, if the wire and the base material get too close, the current will rapidly increase, which in turn causes the heat to increase and the tip of the wire to melt, returning it to its original separation distance.[85]

The type of current used plays an important role in arc welding. Consumable electrode processes such as shielded metal arc welding and gas metal arc welding generally use direct current, but the electrode can be charged either positively or negatively. In welding, the positively charged anode will have a greater heat concentration, and as a result, changing the polarity of the electrode affects weld properties. If the electrode is positively charged, the base metal will be hotter, increasing weld penetration and welding speed. Alternatively, a negatively charged electrode results in more shallow welds.[86] Non-consumable electrode processes, such as gas tungsten arc welding, can use either type of direct current, as well as alternating current. However, with direct current, because the electrode only creates the arc and does not provide filler material, a positively charged electrode causes shallow welds, while a negatively charged electrode makes deeper welds.[87] Alternating current rapidly moves between these two, resulting in medium-penetration welds. One disadvantage of AC, the fact that the arc must be re-ignited after every zero crossings, has been addressed with the invention of special power units that produce a square wave pattern instead of the normal sine wave, making rapid zero crossings possible and minimizing the effects of the problem.[88]

Resistance welding generates heat from electrical resistance in the base metals. Two electrodes are simultaneously used to press the metal sheets together and to pass current through the sheets. The electrodes are made from highly conductive material, usually copper. The higher resistance in the base metals causes small pools of molten metal to form at the weld area as high current (1,000–100,000 A) is passed through.[89]

Resistance spot welding is a popular method used to join overlapping metal sheets of up to 3 mm thick. The advantages of the method include efficient energy use, limited workpiece deformation, high production rates, easy automation, and no required filler materials. Weld strength is significantly lower than with other welding methods, making the process suitable for only certain applications. It is used extensively in the automotive industry—ordinary cars can have several thousand spot welds made by industrial robots. In general, resistance welding methods are efficient and cause little pollution, but their applications are somewhat limited and the equipment cost can be high. A specialized process called shot welding, can be used to spot weld stainless steel.[89]

Seam welding also relies on two electrodes to apply pressure and current to join metal sheets. However, instead of pointed electrodes, wheel-shaped electrodes roll along and often feed the workpiece, making it possible to make long continuous welds. In the past, this process was used in the manufacture of beverage cans, but now its uses are more limited.[89] Other resistance welding methods include butt welding,[90] flash welding, projection welding, and upset welding.[89]

Energy beam welding methods, namely laser beam welding and electron beam welding, are relatively new processes that have become quite popular in high production applications. The two processes are quite similar, differing most notably in their source of power. Laser beam welding employs a highly focused laser beam, while electron beam welding is done in a vacuum and uses an electron beam. Both have a very high energy density, making deep weld penetration possible and minimizing the size of the weld area. Both processes are extremely fast, and are easily automated, making them highly productive. The primary disadvantages are their very high equipment costs (though these are decreasing) and a susceptibility to thermal cracking. Developments in this area include laser-hybrid welding, which uses principles from both laser beam welding and arc welding for even better weld properties, laser cladding, and x-ray welding.[91]

Like forge welding (the earliest welding process discovered), some modern welding methods do not involve the melting of the materials being joined. One of the most popular, ultrasonic welding, is used to connect thin sheets or wires made of metal or thermoplastic by vibrating them at high frequency and under high pressure.[93] The equipment and methods involved are similar to that of resistance welding, but instead of electric current, vibration provides energy input.  When welding metals, the vibrations are introduced horizontally, and the materials are not melted; with plastics, which should have similar melting temperatures, vertically. Ultrasonic welding is commonly used for making electrical connections out of aluminum or copper, and it is also a very common polymer welding process.[93]

Another common process, explosion welding, involves the joining of materials by pushing them together under extremely high pressure. The energy from the impact plasticizes the materials, forming a weld, even though only a limited amount of heat is generated. The process is commonly used for welding dissimilar materials, including bonding aluminum to carbon steel in ship hulls and stainless steel or titanium to carbon steel in petrochemical pressure vessels.[93]

Other solid-state welding processes include friction welding (including friction stir welding and friction stir spot welding),[94] magnetic pulse welding,[95] co-extrusion welding, cold welding, diffusion bonding, exothermic welding, high frequency welding, hot pressure welding, induction welding, and roll bonding.[93]

Welds can be geometrically prepared in many different ways. The five basic types of weld joints are the butt joint, lap joint, corner joint, edge joint, and T-joint (a variant of this last is the cruciform joint). Other variations exist as well—for example, double-V preparation joints are characterized by the two pieces of material each tapering to a single center point at one-half their height. Single-U and double-U preparation joints are also fairly common—instead of having straight edges like the single-V and double-V preparation joints, they are curved, forming the shape of a U. Lap joints are also commonly more than two pieces thick—depending on the process used and the thickness of the material, many pieces can be welded together in a lap joint geometry.[96]

Many welding processes require the use of a particular joint design; for example, resistance spot welding, laser beam welding, and electron beam welding are most frequently performed on lap joints. Other welding methods, like shielded metal arc welding, are extremely versatile and can weld virtually any type of joint. Some processes can also be used to make multipass welds, in which one weld is allowed to cool, and then another weld is performed on top of it. This allows for the welding of thick sections arranged in a single-V preparation joint, for example.[97]

After welding, a number of distinct regions can be identified in the weld area. The weld itself is called the fusion zone—more specifically, it is where the filler metal was laid during the welding process. The properties of the fusion zone depend primarily on the filler metal used, and its compatibility with the base materials. It is surrounded by the heat-affected zone, the area that had its microstructure and properties altered by the weld. These properties depend on the base material's behavior when subjected to heat. The metal in this area is often weaker than both the base material and the fusion zone, and is also where residual stresses are found.[98]

Many distinct factors influence the strength of welds and the material around them, including the welding method, the amount and concentration of energy input, the weldability of the base material, filler material, and flux material, the design of the joint, and the interactions between all these factors.[99]

For example, the factor of welding position influences weld quality, that welding codes & specifications may require testing—both welding procedures and welders—using specified welding positions: 1G (flat), 2G (horizontal), 3G (vertical), 4G (overhead), 5G (horizontal fixed pipe), or 6G (inclined fixed pipe).

To test the quality of a weld, either destructive or nondestructive testing methods are commonly used to verify that welds are free of defects, have acceptable levels of residual stresses and distortion, and have acceptable heat-affected zone (HAZ) properties. Types of welding defects include cracks, distortion, gas inclusions (porosity), non-metallic inclusions, lack of fusion, incomplete penetration, lamellar tearing, and undercutting.

The metalworking industry has instituted codes and specifications to guide welders, weld inspectors, engineers, managers, and property owners in proper welding technique, design of welds, how to judge the quality of welding procedure specification, how to judge the skill of the person performing the weld, and how to ensure the quality of a welding job.[99] Methods such as visual inspection, radiography, ultrasonic testing, phased-array ultrasonics, dye penetrant inspection, magnetic particle inspection, or industrial computed tomography can help with detection and analysis of certain defects.

The heat-affected zone (HAZ) is a ring surrounding the weld in which the temperature of the welding process, combined with the stresses of uneven heating and cooling, alters the heat-treatment properties of the alloy. The effects of welding on the material surrounding the weld can be detrimental—depending on the materials used and the heat input of the welding process used, the HAZ can be of varying size and strength. The thermal diffusivity of the base material plays a large role—if the diffusivity is high, the material cooling rate is high and the HAZ is relatively small. Conversely, a low diffusivity leads to slower cooling and a larger HAZ. The amount of heat injected by the welding process plays an important role as well, as processes like oxyacetylene welding have an unconcentrated heat input and increase the size of the HAZ. Processes like laser beam welding give a highly concentrated, limited amount of heat, resulting in a small HAZ. Arc welding falls between these two extremes, with the individual processes varying somewhat in heat input.[100][101] To calculate the heat input for arc welding procedures, the following formula can be used:

where Q = heat input (kJ/mm), V = voltage (V), I = current (A), and S = welding speed (mm/min). The efficiency is dependent on the welding process used, with shielded metal arc welding having a value of 0.75, gas metal arc welding and submerged arc welding, 0.9, and gas tungsten arc welding, 0.8.[102] Methods of alleviating the stresses and brittleness created in the HAZ include stress relieving and tempering.[103]

One major defect concerning the HAZ is cracking at the junction of the weld face and the base metal. Due to the rapid expansion (heating) and contraction (cooling), the material may not have the ability to withstand the stress and could crack. One method to control the stress is to control the heating and cooling rate, such as pre-heating and post-heating [104]

The durability and life of dynamically loaded, welded steel structures is determined in many cases by the welds, in particular the weld transitions. Through selective treatment of the transitions by grinding (abrasive cutting), shot peening, High-frequency impact treatment, Ultrasonic impact treatment, etc. the durability of many designs increases significantly.

Most solids used are engineering materials consisting of crystalline solids in which the atoms or ions are arranged in a repetitive geometric pattern which is known as a lattice structure. The only exception is material that is made from glass which is a combination of a supercooled liquid and polymers which are aggregates of large organic molecules.[105]

Crystalline solids cohesion is obtained by a metallic or chemical bond that is formed between the constituent atoms. Chemical bonds can be grouped into two types consisting of ionic and covalent. To form an ionic bond, either a valence or bonding electron separates from one atom and becomes attached to another atom to form oppositely charged ions. The bonding in the static position is when the ions occupy an equilibrium position where the resulting force between them is zero. When the ions are exerted in tension force, the inter-ionic spacing increases creating an electrostatic attractive force, while a repulsing force under compressive force between the atomic nuclei is dominant.[105]

Covalent bonding takes place when one of the constituent atoms loses one or more electrons, with the other atom gaining the electrons, resulting in an electron cloud that is shared by the molecule as a whole. In both ionic and covalent bonding the location of the ions and electrons are constrained relative to each other, thereby resulting in the bond being characteristically brittle.[105]

Metallic bonding can be classified as a type of covalent bonding for which the constituent atoms are of the same type and do not combine with one another to form a chemical bond. Atoms will lose an electron(s) forming an array of positive ions. These electrons are shared by the lattice which makes the electron cluster mobile, as the electrons are free to move as well as the ions. For this, it gives metals their relatively high thermal and electrical conductivity as well as being characteristically ductile.[105]

Three of the most commonly used crystal lattice structures in metals are the body-centred cubic, face-centred cubic and close-packed hexagonal. Ferritic steel has a body-centred cubic structure and austenitic steel, non-ferrous metals like aluminium, copper and nickel have the face-centred cubic structure.[105]

Ductility is an important factor in ensuring the integrity of structures by enabling them to sustain local stress concentrations without fracture. In addition, structures are required to be of an acceptable strength, which is related to a material's yield strength. In general, as the yield strength of a material increases, there is a corresponding reduction in fracture toughness.[105]

A reduction in fracture toughness may also be attributed to the embrittlement effect of impurities, or for body-centred cubic metals, from a reduction in temperature. Metals and in particular steels have a transitional temperature range where above this range the metal has acceptable notch-ductility while below this range the material becomes brittle. Within the range, the materials behavior is unpredictable. The reduction in fracture toughness is accompanied by a change in the fracture appearance. When above the transition, the fracture is primarily due to micro-void coalescence, which results in the fracture appearing fibrous. When the temperatures falls the fracture will show signs of cleavage facets. These two appearances are visible by the naked eye. Brittle fracture in steel plates may appear as chevron markings under the microscope. These arrow-like ridges on the crack surface point towards the origin of the fracture.[105]

Fracture toughness is measured using a notched and pre-cracked rectangular specimen, of which the dimensions are specified in standards, for example ASTM E23. There are other means of estimating or measuring fracture toughness by the following: The Charpy impact test per ASTM A370; The crack-tip opening displacement (CTOD) test per BS 7448–1; The J integral test per ASTM E1820; The Pellini drop-weight test per ASTM E208.[105]

While many welding applications are done in controlled environments such as factories and repair shops, some welding processes are commonly used in a wide variety of conditions, such as open air, underwater, and vacuums (such as space). In open-air applications, such as construction and outdoors repair, shielded metal arc welding is the most common process. Processes that employ inert gases to protect the weld cannot be readily used in such situations, because unpredictable atmospheric movements can result in a faulty weld. Shielded metal arc welding is also often used in underwater welding in the construction and repair of ships, offshore platforms, and pipelines, but others, such as flux cored arc welding and gas tungsten arc welding, are also common. Welding in space is also possible—it was first attempted in 1969 by Russian cosmonauts during the Soyuz 6 mission, when they performed experiments to test shielded metal arc welding, plasma arc welding, and electron beam welding in a depressurized environment. Further testing of these methods was done in the following decades, and today researchers continue to develop methods for using other welding processes in space, such as laser beam welding, resistance welding, and friction welding. Advances in these areas may be useful for future endeavours similar to the construction of the International Space Station, which could rely on welding for joining in space the parts that were manufactured on Earth.[106]

Welding can be dangerous and unhealthy if the proper precautions are not taken.[107] Potential safety risks come from fumes, ultraviolet radiation, heat, electric currents, and vibrations.[108] New technology, safe work practices, and proper protection reduce the risks of injury or death from welding.[109]

Since many common welding procedures involve an open flame or electric arc, the risk of burns and fire is significant; this is why it is classified as a hot work process. To prevent injury, welders wear personal protective equipment in the form of heavy leather gloves and protective long-sleeve jackets to avoid exposure to extreme heat and flames. Synthetic clothing such as polyester should not be worn.[110] Wool is less flammable than cotton, but dense cotton fabrics such as denim are still sufficient for clothing. However, any molten material that splatters onto synthetic material will melt directly through the fabric resulting in severe burns.[111]

Arc welding produces intense visible and ultraviolet light. Typical gas metal arc welding has an irradiance of 5W/m2 for the welder, which is many times brighter than sunlight.[112] This can cause a condition called arc eye or flash burns, in which ultraviolet light causes inflammation of the cornea, and can burn the retinas of the eyes. Welding helmets with dark UV-filtering face plates are worn to prevent this exposure.[113] Many helmets include an auto-darkening face plate, which instantly darkens upon exposure to the intense UV light.[114][115] To protect bystanders, the welding area is often surrounded by translucent welding curtains. These curtains, made of a polyvinyl chloride plastic film, shield people outside the welding area from the UV light of the electric arc, but they cannot replace the filter glass used in helmets.[116] The light can also burn exposed skin.[117] Because of the less intense light produced in oxyfuel welding, goggles that use less UV filtering and do not protect the entire face are sufficient.[118]

Depending on the type of material, welding varieties, and other factors, welding can produce over 100 dB(A) of noise.[119] Above 85 dB(A), earplugs should be worn.[120] Long-term or continuous exposure to higher decibels can lead to noise-induced hearing loss.[121] Processes that produce vibrations sufficient to numb a welder's hands are automated because PPE cannot offer sufficient protection.[122]

Welders are often exposed to dangerous gases and particulate matter. Processes like flux-cored arc welding and shielded metal arc welding produce smoke containing particles of various types of oxides. The size of the particles in question tends to influence the toxicity of the fumes, with smaller particles presenting a greater danger. This is because smaller particles can cross the blood–brain barrier. Fumes and gases, such as carbon dioxide, ozone, and fumes containing heavy metals, can be dangerous to welders lacking proper ventilation and training.[123] Exposure to manganese welding fumes, for example, even at low levels (<0.2 mg/m3), may cause neurological problems or damage to the lungs, liver, kidneys, or central nervous system.[124] Nano particles can become trapped in the alveolar macrophages of the lungs and induce pulmonary fibrosis.[125] The use of compressed gases and flames in many welding processes poses an explosion and fire risk. Some common precautions include limiting the amount of oxygen in the air, and keeping combustible materials away from the workplace.[123]

There are several technologies to mitigate dangers from welding fumes. Local exhaust ventilation (LEV) solutions remove fumes, smoke, and dust directly from the welding area. Forms of LEV include downdraft benches, fume hoods, and fume extraction welding guns. Downdraft benches have exhaust ducts beneath the metal welding table. Fume extraction guns have a vacuum hose that runs down to the welding nozzle. Movable fume hoods can positioned directly over the welding area.[126][127] Even with ventilation, there are still respiratory risks that respirators  can further reduce. Studies have shown that respirators, especially half-mask elastomeric respirators, significantly decrease particulate inhalation.[128]

As an industrial process, the cost of welding plays a crucial role in manufacturing decisions. Many different variables affect the total cost, including equipment cost, labor cost, material cost, and energy cost.[129] Depending on the process, equipment cost can vary, from inexpensive for methods like shielded metal arc welding and oxyfuel welding, to extremely expensive for methods like laser beam welding and electron beam welding. Because of their high cost, they are only used in high production operations. Similarly, because automation and robots increase equipment costs, they are only implemented when high production is necessary. Labor cost depends on the deposition rate (the rate of welding), the hourly wage, and the total operation time, including time spent fitting, welding, and handling the part. The cost of materials includes the cost of the base and filler material, and the cost of shielding gases. Finally, energy cost depends on arc time and welding power demand.[129]

For manual welding methods, labor costs generally make up the vast majority of the total cost. As a result, many cost-saving measures are focused on minimizing operation time. To do this, welding procedures with high deposition rates can be selected, and weld parameters can be fine-tuned to increase welding speed. Mechanization and automation are often implemented to reduce labor costs, but this frequently increases the cost of equipment and creates additional setup time. Material costs tend to increase when special properties are necessary, and energy costs normally do not amount to more than several percent of the total welding cost.[129]

In recent years, in order to minimize labor costs in high production manufacturing, industrial welding has become increasingly more automated, most notably with the use of robots in resistance spot welding (especially in the automotive industry) and in arc welding. In robot welding, mechanized devices both hold the material and perform the weld[130] and at first, spot welding was its most common application, but robotic arc welding increases in popularity as technology advances. Other key areas of research and development include the welding of dissimilar materials (such as steel and aluminum, for example) and new welding processes, such as friction stir, magnetic pulse, conductive heat seam, and laser-hybrid welding. Furthermore, progress is desired in making more specialized methods like laser beam welding practical for more applications, such as in the aerospace and automotive industries. Researchers also hope to better understand the often unpredictable properties of welds, especially microstructure, residual stresses, and a weld's tendency to crack or deform.[131]

The trend of accelerating the speed at which welds are performed in the steel erection industry comes at a risk to the integrity of the connection. Without proper fusion to the base materials provided by sufficient arc time on the weld, a project inspector cannot ensure the effective diameter of the puddle weld therefore he or she cannot guarantee the published load capacities unless they witness the actual installation.[132] This method of puddle welding is common in the United States and Canada for attaching steel sheets to bar joist and structural steel members. Regional agencies are responsible for ensuring the proper installation of puddle welding on steel construction sites. Currently there is no standard or weld procedure which can ensure the published holding capacity of any unwitnessed connection, but this is under review by the American Welding Society.

Glasses and certain types of plastics are commonly welded materials. Unlike metals, which have a specific melting point, glasses and plastics have a melting range, called the glass transition. When heating the solid material past the glass-transition temperature (Tg) into this range, it will generally become softer and more pliable. When it crosses through the range, above the glass-melting temperature (Tm), it will become a very thick, sluggish, viscous liquid, slowly decreasing in viscosity as temperature increases. Typically, this viscous liquid will have very little surface tension compared to metals, becoming a sticky, taffy to honey-like consistency, so welding can usually take place by simply pressing two melted surfaces together. The two liquids will generally mix and join at first contact. Upon cooling through the glass transition, the welded piece will solidify as one solid piece of amorphous material.

Glass welding is a common practice during glassblowing. It is used very often in the construction of lighting, neon signs, flashtubes, scientific equipment, and the manufacture of dishes and other glassware. It is also used during glass casting for joining the halves of glass molds, making items such as bottles and jars. Welding glass is accomplished by heating the glass through the glass transition, turning it into a thick, formable, liquid mass. Heating is usually done with a gas or oxy-gas torch, or a furnace, because the temperatures for melting glass are often quite high. This temperature may vary, depending on the type of glass. For example, lead glass becomes a weldable liquid at around 1,600 °F (870 °C), and can be welded with a simple propane torch. On the other hand, quartz glass (fused silica) must be heated to over 3,000 °F (1,650 °C), but quickly loses its viscosity and formability if overheated, so an oxyhydrogen torch must be used. Sometimes a tube may be attached to the glass, allowing it to be blown into various shapes, such as bulbs, bottles, or tubes. When two pieces of liquid glass are pressed together, they will usually weld very readily. Welding a handle onto a pitcher can usually be done with relative ease. However, when welding a tube to another tube, a combination of blowing and suction, and pressing and pulling is used to ensure a good seal, to shape the glass, and to keep the surface tension from closing the tube in on itself. Sometimes a filler rod may be used, but usually not.

Because glass is very brittle in its solid state, it is often prone to cracking upon heating and cooling, especially if the heating and cooling are uneven. This is because the brittleness of glass does not allow for uneven thermal expansion. Glass that has been welded will usually need to be cooled very slowly and evenly through the glass transition, in a process called annealing, to relieve any internal stresses created by a temperature gradient.

There are many types of glass, and it is most common to weld using the same types. Different glasses often have different rates of thermal expansion, which can cause them to crack upon cooling when they contract differently. For instance, quartz has very low thermal expansion, while soda-lime glass has very high thermal expansion. When welding different glasses to each other, it is usually important to closely match their coefficients of thermal expansion, to ensure that cracking does not occur. Also, some glasses will simply not mix with others, so welding between certain types may not be possible.

Glass can also be welded to metals and ceramics, although with metals the process is usually more adhesion to the surface of the metal rather than a commingling of the two materials. However, certain glasses will typically bond only to certain metals. For example, lead glass bonds readily to copper or molybdenum, but not to aluminum. Tungsten electrodes are often used in lighting but will not bond to quartz glass, so the tungsten is often wetted with molten borosilicate glass, which bonds to both tungsten and quartz. However, care must be taken to ensure that all materials have similar coefficients of thermal expansion to prevent cracking both when the object cools and when it is heated again. Special alloys are often used for this purpose, ensuring that the coefficients of expansion match, and sometimes thin, metallic coatings may be applied to a metal to create a good bond with the glass.[133][134][failed verification]

Plastics are generally divided into two categories, which are ""thermosets"" and ""thermoplastics."" A thermoset is a plastic in which a chemical reaction sets the molecular bonds after first forming the plastic, and then the bonds cannot be broken again without degrading the plastic. Thermosets cannot be melted, therefore, once a thermoset has set it is impossible to weld it. Examples of thermosets include epoxies, silicone, vulcanized rubber, polyester, and polyurethane.

Thermoplastics, by contrast, form long molecular chains, which are often coiled or intertwined, forming an amorphous structure without any long-range, crystalline order. Some thermoplastics may be fully amorphous, while others have a partially crystalline/partially amorphous structure. Both amorphous and semicrystalline thermoplastics have a glass transition, above which welding can occur, but semicrystallines also have a specific melting point which is above the glass transition. Above this melting point, the viscous liquid will become a free-flowing liquid (see rheological weldability for thermoplastics). Examples of thermoplastics include polyethylene, polypropylene, polystyrene, polyvinylchloride (PVC), and fluoroplastics like Teflon and Spectralon.

Welding thermoplastic with heat is very similar to welding glass. The plastic first must be cleaned and then heated through the glass transition, turning the weld-interface into a thick, viscous liquid. Two heated interfaces can then be pressed together, allowing the molecules to mix through intermolecular diffusion, joining them as one. Then the plastic is cooled through the glass transition, allowing the weld to solidify. A filler rod may often be used for certain types of joints. The main differences between welding glass and plastic are the types of heating methods, the much lower melting temperatures, and the fact that plastics will burn if overheated. Many different methods have been devised for heating plastic to a weldable temperature without burning it. Ovens or electric heating tools can be used to melt the plastic. Ultrasonic, laser, or friction heating are other methods. Resistive metals may be implanted in the plastic, which respond to induction heating. Some plastics will begin to burn at temperatures lower than their glass transition, so welding can be performed by blowing a heated, inert gas onto the plastic, melting it while, at the same time, shielding it from oxygen.[135]

Many thermoplastics can also be welded using chemical solvents. When placed in contact with the plastic, the solvent will begin to soften it, bringing the surface into a thick, liquid solution. When two melted surfaces are pressed together, the molecules in the solution mix, joining them as one. Because the solvent can permeate the plastic, the solvent evaporates out through the surface of the plastic, causing the weld to drop out of solution and solidify. A common use for solvent welding is for joining PVC (polyvinyl chloride) or ABS (acrylonitrile butadiene styrene) pipes during plumbing, or for welding styrene and polystyrene plastics in the construction of models. Solvent welding is especially effective on plastics like PVC which burn at or below their glass transition, but may be ineffective on plastics like Teflon or polyethylene that are resistant to chemical decomposition.[136]
"
Structural Steel Erection,"Structural steel is steel used for making construction materials in a variety of shapes. Many structural steel shapes take the form of an elongated beam having a profile of a specific cross section. Structural steel shapes, sizes, chemical composition, mechanical properties such as strengths, storage practices, etc., are regulated by standards in most industrialized countries.

Structural steel shapes, such as I-beams, have high second moments of area, so can support a high load without excessive sagging.[1]

The shapes available are described in published standards worldwide, and specialist, proprietary cross sections are also available.[citation needed]

Sections can be hot or cold rolled, or fabricated by welding together flat or bent plates.[3]


The terms angle iron, channel iron, and sheet iron have been in common use since before wrought iron was replaced by steel for commercial purposes and are still sometimes used informally. In technical writing angle stock, channel stock, and sheet are used instead of those misnomers.[citation needed]

Most steels used in Europe are specified to comply with EN 10025. However, some national standards remain in force.[4] Example grades are S275J2 or S355K2W where S denotes structural steel; 275 or 355 denotes the yield strength in newtons per square millimetre or the equivalent megapascals; J2 or K2 denotes the material's toughness by Charpy impact test values, and the W denotes weathering steel. Further letters can be used to designate fine grain steel (N or NL); quenched and tempered steel (Q or QL); and thermomechanically rolled steel (M or ML).[citation needed]

Common yield strengths available are 195, 235, 275, 355, 420, and 460, although some grades are more commonly used than others. In the UK, almost all structural steel is S275 and S355. Higher grades such as 500, 550, 620, 690, 890 and 960 available in quenched and tempered material although grades above 690 receive little if any use in construction at present.[citation needed]

Euronorms define the shape of standard structural profiles:

Steels used for building construction in the US use standard alloys identified and specified by ASTM International.  These steels have an alloy identification beginning with A and then two, three, or four numbers. The four-number AISI steel grades commonly used for mechanical engineering, machines, and vehicles are a completely different specification series.

The standard commonly used structural steels are:[5]

The concept of CE marking for all construction products and steel products is introduced by the Construction Products Directive (CPD). The CPD is a European Directive that ensures the free movement of all construction products within the European Union.

Because steel components are ""safety critical"", CE Marking is not allowed unless the Factory Production Control (FPC) system under which they are produced has been assessed by a suitable certification body that has been approved to the European Commission.[6]

In the case of steel products such as sections, bolts and fabricated steelwork the CE Marking demonstrates that the product complies with the relevant harmonized standard.[7]

For steel structures the main harmonized standards are:

The standard that covers CE Marking of structural steelwork is EN 1090-1. The standard has come into force in late 2010. After a transition period of two years, CE Marking will become mandatory in most European Countries sometime early in 2012.[8] The official end date of the transition period is July 1, 2014.

Steel is sold by weight so the design must be as light as possible whilst being structurally safe. Utilizing multiple, identical steel members can be cheaper than unique components.[9]

Reinforced concrete and structural steel can be sustainable[10] if used properly. Over 80% of structural steel members are fabricated from recycled metals, called A992 steel. This member material is cheaper and has a higher strength to weight ratio than previously used steel members (A36 grade).[11]

Special considerations must be taken into account with structural steel to ensure it is not under a dangerous fire hazard condition.[12]

Structural steel cannot be exposed to the environment without suitable protection, because any moisture, or contact with water, will cause oxidisation to occur, compromising the structural integrity of the building and endangering occupants and neighbors.[12]

Having high strength, stiffness, toughness, and ductile properties, structural steel is one of the most commonly used materials in commercial and industrial building construction.[13]

Structural steel can be developed into nearly any shape, which are either bolted or welded together in construction. Structural steel can be erected as soon as the materials are delivered on site, whereas concrete must be cured at least 1–2 weeks after pouring before construction can continue, making steel a schedule-friendly construction material.[12]

Steel is inherently a noncombustible material. However, when heated to temperatures seen in a fire, the strength and stiffness of the material is significantly reduced. The International Building Code requires steel be enveloped in sufficient fire-resistant materials, increasing overall cost of steel structure buildings.[13]

Steel, when in contact with water, can corrode, creating a potentially dangerous structure. Measures must be taken in structural steel construction to prevent any lifetime corrosion. The steel can be painted, providing water resistance. Also, the fire resistance material used to envelope steel is commonly water resistant.[12]

Steel provides a less suitable surface environment for mold to grow than wood.[14]

Tall structures are constructed using structural steel due to its constructability, as well as its high strength-to-weight ratio.[15]



Steel loses strength when heated sufficiently. The critical temperature of a steel member is the temperature at which it cannot safely support its load [citation needed]. Building codes and structural engineering standard practice defines different critical temperatures depending on the structural element type, configuration, orientation, and loading characteristics. The critical temperature is often considered the temperature at which its yield stress has been reduced to 60% of the room temperature yield stress.[16] In order to determine the fire resistance rating of a steel member, accepted calculations practice can be used,[17] or a fire test can be performed, the critical temperature of which is set by the standard accepted to the Authority Having Jurisdiction, such as a building code. In Japan, this is below 400 °C.[18] In China, Europe and North America (e.g., ASTM E-119), this is approximately 1000–1300 °F[19] (530–810 °C). The time it takes for the steel element that is being tested to reach the temperature set by the test standard determines the duration of the fire-resistance rating.
Heat transfer to the steel can be slowed by the use of fireproofing materials, thus limiting steel temperature. Common fireproofing methods for structural steel include intumescent, endothermic, and plaster coatings as well as drywall, calcium silicate cladding, and mineral wool insulating blankets.[20]

Structural steel fireproofing materials include intumescent, endothermic and plaster coatings as well as drywall, calcium silicate cladding, and mineral or high temperature insulation wool blankets. Attention is given to connections, as the thermal expansion of structural elements can compromise fire-resistance rated assemblies.

Cutting workpieces to length is usually done with a bandsaw.[citation needed]

A beam drill line drills holes and mills slots into beams, channels and HSS elements. CNC beam drill lines are typically equipped with feed conveyors and position sensors to move the element into position for drilling, plus probing capability to determine the precise location where the hole or slot is to be cut.[citation needed]

For cutting irregular openings or non-uniform ends on dimensional (non-plate) elements, a cutting torch is typically used. Oxy-fuel torches are the most common technology and range from simple hand-held torches to automated CNC coping machines that move the torch head around the structural element in accordance with cutting instructions programmed into the machine.[citation needed]

Fabricating flat plate is performed on a plate processing center where the plate is laid flat on a stationary 'table' and different cutting heads traverse the plate from a gantry-style arm or ""bridge"". The cutting heads can include a punch, drill or torch.[citation needed]
"
Non-Structural Steel Fabrication,"Metal fabrication is the creation of metal structures by cutting, bending and assembling processes. It is a value-added[1] process involving the creation of machines, parts, and structures from various raw materials. 

Typically, a fabrication shop bids on a job, usually based on engineering drawings, and if awarded the contract, builds the product. Large fab shops employ a multitude of value-added processes, including welding, cutting, forming and machining. 

As with other manufacturing processes, both human labor and automation are commonly used. A fabricated product may be called a fabrication, and shops specializing in this type of work are called fab shops. The end products of other common types of metalworking, such as machining, metal stamping, forging, and casting, may be similar in shape and function, but those processes are not classified as fabrication.

Fabrication comprises or overlaps with various metalworking specialties:

Standard metal fabrication materials are:

A variety of tools are used to cut raw material. The most common cutting method is shearing.

Special band saws for cutting metal have hardened blades and feed mechanisms for even cutting. Abrasive cut-off saws, also known as chop saws, are similar to miter saws but have a steel-cutting abrasive disks. Cutting torches can cut large sections of steel with little effort.

Burn tables are CNC (computer-operated) cutting torches, usually powered by natural gas. Plasma and laser cutting tables, and water jet cutters, are also common. Plate steel is loaded on the table and the parts are cut out as programmed. The support table consists of a grid of bars that can be replaced when worn. Higher-end burn tables may include CNC punch capability using a carousel of punches and taps. In fabrication of structural steel by plasma and laser cutting, robots move the cutting head in three dimensions around the cut material.

Forming converts flat sheet metal into 3-D parts[4] by applying force without adding or removing material.[5] The force must be great enough to change the metal's initial shape. Forming can be controlled with tools such as punches and dies. Machinery can regulate force magnitude and direction. Machine-based forming can combine forming and welding to produce lengths of fabricated sheeting (e.g. linear grating for water drainage).[6] Most metallic materials, being at least somewhat ductile and capable of considerable permanent deformation without cracking or breaking, lend themselves particularly well to these techniques.[7]

Proper design and use of tools with machinery creates a repeatable form that can be used to create products for many industries, including jewelry, aerospace, automotive, construction, civil and architectural.

Machining is a specialized trade of removing material from a block of metal to make it a desired shape. Fab shops generally have some machining capability, using metal lathes, mills, drills, and other portable machining tools. Most solid components, such as gears, bolts, screws and nuts, are machined.

Welding is the main focus of steel fabrication.[8] Formed and machined parts are assembled and tack-welded in place, then rechecked for accuracy. If multiple weldments have been ordered, a fixture may be used to locate parts for welding. A welder then finishes the work according to engineering drawings (for detailed welding) or by their own experience and judgement (if no details are provided).

Special measures may be needed to prevent or correct warping of weldments due to heat. These may include redesigning the piece to require less welding, employing staggered welding, using a stout fixture, covering the weldment in sand as it cools, and post-weld straightening.

Straightening of warped steel weldments is done with an oxyacetylene torch. In this highly specialized work, heat is selectively applied to the steel in a slow, linear sweep, causing the steel to contract in the direction of the sweep as it cools. A highly skilled welder can remove significant warpage this way.

Steel weldments are occasionally annealed in a low-temperature oven to relieve residual stresses. Such weldments, particularly those for engine blocks, may be line-bored after heat treatment.

After the weldment has cooled, seams are usually ground clean, and the assembly can be sandblasted, primed and painted. Any additional manufacturing is then performed, and the finished product is inspected and shipped.

Many fabrication shops offer specialty processes, including :
"
Windows and Doors Installation,"

A window is an opening in a wall, door, roof, or vehicle that allows the exchange of light and may also allow the passage of sound and sometimes air. Modern windows are usually glazed or covered in some other transparent or translucent material, a sash set in a frame[1] in the opening; the sash and frame are also referred to as a window.[2] Many glazed windows may be opened, to allow ventilation, or closed to exclude inclement weather. Windows may have a latch or similar mechanism to lock the window shut or to hold it open by various amounts.

Types include the eyebrow window, fixed windows, hexagonal windows, single-hung, and double-hung sash windows, horizontal sliding sash windows, casement windows, awning windows, hopper windows, tilt, and slide windows (often door-sized), tilt and turn windows, transom windows, sidelight windows, jalousie or louvered windows, clerestory windows, lancet windows, skylights, roof windows, roof lanterns, bay windows, oriel windows, thermal, or Diocletian, windows, picture windows, rose windows, emergency exit windows, stained glass windows, French windows, panel windows, double/triple-paned windows, and witch windows.

The English language-word window originates from the Old Norse vindauga, from vindr 'wind' and auga 'eye'.[3] In Norwegian, Nynorsk, and Icelandic, the Old Norse form has survived to this day (in Icelandic only as a less used word for a type of small open ""window"", not strictly a synonym for gluggi, the Icelandic word for 'window'[4]). In Swedish, the word vindöga remains as a term for a hole through the roof of a hut, and in the Danish language vindue and Norwegian Bokmål vindu, the direct link to eye is lost, just as for window. The Danish (but not the Bokmål) word is pronounced fairly similarly to window.

Window is first recorded in the early 13th century, and originally referred to an unglazed hole in a roof. Window replaced the Old English eagþyrl, which literally means 'eye-hole', and eagduru 'eye-door'. Many Germanic languages, however, adopted the Latin word fenestra to describe a window with glass, such as standard Swedish fönster, or German Fenster. The use of window in English is probably because of the Scandinavian influence on the English language by means of loanwords during the Viking Age. In English, the word fenester was used as a parallel until the mid-18th century. Fenestration is still used to describe the arrangement of windows within a façade, as well as defenestration, meaning 'to throw out of a window'.

The Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt, in Alexandria c. 100 AD[citation needed]. Presentations of windows can be seen in ancient Egyptian wall art and sculptures from Assyria. Paper windows were economical and widely used in ancient China, Korea, and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century. In the 19th century American west, greased paper windows came to be used by pioneering settlers. Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were fully perfected.

In the 13th century BC, the earliest windows were unglazed openings in a roof to admit light during the day.[citation needed] Later,[when?] windows were covered with animal hide, cloth, or wood. Shutters that could be opened and closed came next.[when?] Over time, windows were built that both protected the inhabitants from the elements and transmitted light, using multiple small pieces of translucent material, such as flattened pieces of translucent animal horn, paper sheets, thin slices of marble (such as fengite), or pieces of glass, set in frameworks of wood, iron or lead. In the Far East, paper was used to fill windows.[1]
The Romans were the first known users of glass for windows, exploiting a technology likely first developed in Roman Egypt. Specifically, in Alexandria c. 100 CE, cast-glass windows, albeit with poor optical properties, began to appear, but these were small thick productions, little more than blown-glass jars (cylindrical shapes) flattened out into sheets with circular striation patterns throughout. It would be over a millennium before window glass became transparent enough to see through clearly, as we expect now. In 1154, Al-Idrisi described glass windows as a feature of the palace belonging to the king of the Ghana Empire.[5][6]

Over the centuries techniques were developed to shear through one side of a blown glass cylinder and produce thinner rectangular window panes from the same amount of glass material. This gave rise to tall narrow windows, usually separated by a vertical support called a mullion. Mullioned glass windows were the windows of choice[when?] among the European well-to-do, whereas paper windows were economical and widely used in ancient China, Korea, and Japan. In England, glass became common in the windows of ordinary homes only in the early-17th century, whereas windows made up of panes of flattened animal horn were used as early as the 14th century.[7]

Modern-style floor-to-ceiling windows became possible only after the industrial plate glass-making processes were perfected in the late 19th century [8] Modern windows are usually filled using glass, although transparent plastic is also used.[1]

The introduction of lancet windows into Western European church architecture from the 12th century CE built on a tradition of arched windows [9] inserted between columns,[10] and led not only to tracery and elaborate stained-glass windows but also to a long-standing motif of pointed or rounded window-shapes in ecclesiastical buildings, still seen in many churches today.

Peter Smith discusses overall trends in early-modern rural Welsh window architecture:

Up to about 1680 windows tended to be horizontal in proportion, a shape suitable for lighting the low-ceilinged rooms that had resulted from the insertion of the upper floor into the hall-house. After that date vertically proportioned windows came into fashion, partly at least as a response to the Renaissance taste for the high ceiling. Since 1914 the wheel has come full circle and a horizontally proportioned window is again favoured.[11]

The spread of plate-glass technology made possible the introduction of picture windows (in Levittown, Pennsylvania,[12] founded 1951–1952[clarification needed]).

Many modern day windows may have a window screen or mesh, often made of aluminum or fibreglass, to keep bugs out when the window is opened. Windows are primarily designed to facilitate a vital connection with the outdoors, offering those within the confines of the building visual access to the everchanging events occurring outside. The provision of this connection serves as an integral safeguard for the health and well-being of those inhabiting buildings, lest they experience the detrimental effects of enclosed buildings devoid of windows. Among the myriad criteria for the design of windows, several pivotal criteria have emerged in daylight standards: location, time, weather, nature, and people. Of these criteria, windows that are designed to provide views of nature are considered to be the most important by people.[13]

A cross-window is a rectangular window usually divided into four lights by a mullion and transom that form a Latin cross.[14]

The term eyebrow window is used in two ways: a curved top window in a wall or an eyebrow dormer; and a row of small windows usually under the front eaves such as the James-Lorah House in Pennsylvania.[15]

A fixed window is a window that cannot be opened,[16] whose function is limited to allowing light to enter (unlike an unfixed window, which can open and close). Clerestory windows in church architecture are often fixed. Transom windows may be fixed or operable. This type of window is used in situations where light or vision alone is needed as no ventilation is possible in such windows without the use of trickle vents or overglass vents.

A single-hung sash window is a window that has one sash that is movable (usually the bottom one) and the other fixed. This is the earlier form of sliding sash window and is also cheaper.[1]

A sash window is the traditional style of window in the United Kingdom, and many other places that were formerly colonized by the UK, with two parts (sashes) that overlap slightly and slide up and down inside the frame. The two parts are not necessarily the same size; where the upper sash is smaller (shorter) it is termed a cottage window. Currently, most new double-hung sash windows use spring balances to support the sashes, but traditionally, counterweights held in boxes on either side of the window were used. These were and are attached to the sashes using pulleys of either braided cord or, later, purpose-made chain. Three types of spring balances are called a tape or clock spring balance; channel or block-and-tackle balance, and a spiral or tube balance.

Double-hung sash windows were traditionally often fitted with shutters. Sash windows can be fitted with simplex hinges that let the window be locked into hinges on one side, while the rope on the other side is detached—so the window can be opened for fire escape or cleaning.

A foldup has two equal sashes similar to a standard double-hung but folds upward allowing air to pass through nearly the full-frame opening. The window is balanced using either springs or counterbalances, similar to a double-hung. The sashes can be either offset to simulate a double-hung, or in-line. The inline versions can be made to fold inward or outward. The inward swinging foldup windows can have fixed screens, while the outward swinging ones require movable screens. The windows are typically used for screen rooms, kitchen pass-throughs, or egress.

A horizontal sliding sash window has two or more sashes that overlap slightly but slide horizontally within the frame. In the UK, these are sometimes called Yorkshire sash windows, presumably because of their traditional use in that county.

A casement window is a window with a hinged sash that swings in or out like a door comprising either a side-hung, top-hung (also called ""awning window""; see below), or occasionally bottom-hung sash or a combination of these types, sometimes with fixed panels on one or more sides of the sash.[2] In the US, these are usually opened using a crank, but in parts of Europe, they tend to use projection friction stays and espagnolette locking. Formerly, plain hinges were used with a casement stay. Handing applies to casement windows to determine direction of swing; a casement window may be left-handed, right-handed, or double. The casement window is the dominant type now found in modern buildings in the UK and many other parts of Europe.

An awning window is a casement window that is hung horizontally, hinged on top, so that it swings outward like an awning. In addition to being used independently, they can be stacked, several in one opening, or combined with fixed glass. They are particularly useful for ventilation.[17]

A hopper window is a bottom-pivoting casement window that opens by tilting vertically, typically to the inside, resembling a hopper chute.[18]

A pivot window is a window hung on one hinge on each of two opposite sides which allows the window to revolve when opened. The hinges may be mounted top and bottom (Vertically Pivoted) or at each jamb (Horizontally Pivoted). The window will usually open initially to a restricted position for ventilation and, once released, fully reverse and lock again for safe cleaning from inside. Modern pivot hinges incorporate a friction device to hold the window open against its weight and may have restriction and reversed locking built-in. In the UK, where this type of window is most common, they were extensively installed in high-rise social housing.

A tilt and slide window is a window (more usually a door-sized window) where the sash tilts inwards at the top similar to a hopper window and then slides horizontally behind the fixed pane.

A tilt and turn window can both tilt inwards at the top or open inwards from hinges at the side. This is the most common type of window in Germany, its country of origin. It is also widespread in many other European countries. In Europe, it is usual for these to be of the ""turn first"" type. i.e. when the handle is turned to 90 degrees the window opens in the side hung mode. With the handle turned to 180 degrees the window opens in bottom hung mode. Most usually in the UK the windows will be ""tilt first"" i.e. bottom hung at 90 degrees for ventilation and side hung at 180 degrees for cleaning the outer face of the glass from inside the building.[19]

A transom window is a window above a door. In an exterior door the transom window is often fixed, in an interior door, it can open either by hinges at top or bottom, or rotate on hinges. It provided ventilation before forced air heating and cooling. A fan-shaped transom is known as a fanlight, especially in the British Isles.

Windows beside a door or window are called side-, wing-, margen-lights, and flanking windows.[20]

Also known as a louvered window, the jalousie window consists of parallel slats of glass or acrylic that open and close like a Venetian blind, usually using a crank or a lever. They are used extensively in tropical architecture. A jalousie door is a door with a jalousie window.

A clerestory window is a window set in a roof structure or high in a wall, used for daylighting.

A skylight is a window built into a roof structure.[21] This type of window allows for natural daylight and moonlight.

A roof window is a sloped window used for daylighting, built into a roof structure. It is one of the few windows that could be used as an exit. Larger roof windows meet building codes for emergency evacuation.

A roof lantern is a multi-paned glass structure, resembling a small building, built on a roof for day or moon light. Sometimes includes an additional clerestory. May also be called a cupola.

A bay window is a multi-panel window, with at least three panels set at different angles to create a protrusion from the wall line.[2]

An oriel window is a form of bay window. This form most often appears in Tudor-style houses and monasteries. It projects from the wall and does not extend to the ground. Originally a form of porch, they are often supported by brackets or corbels.

Thermal, or Diocletian, windows are large semicircular windows (or niches) which are usually divided into three lights (window compartments) by two mullions. The central compartment is often wider than the two side lights on either side of it.

A picture window is a large fixed window in a wall, typically without glazing bars, or glazed with only perfunctory glazing bars (muntins) near the edge of the window. Picture windows provide an unimpeded view, as if framing a picture.[22]

A multi-lite window is a window glazed with small panes of glass separated by wooden or lead glazing bars, or muntins, arranged in a decorative glazing pattern often dictated by the building's architectural style. Due to the historic unavailability of large panes of glass, the multi-lit (or lattice window) was the most common window style until the beginning of the 20th century, and is still used in traditional architecture.

An emergency exit window is a window big enough and low enough so that occupants can escape through the opening in an emergency, such as a fire. In many countries, exact specifications for emergency windows in bedrooms are given in many building codes. Specifications for such windows may also allow for the entrance of emergency rescuers. Vehicles, such as buses, aircraft, and trains frequently have emergency exit windows as well.[23]

A stained glass window is a window composed of pieces of colored glass, transparent, translucent or opaque, frequently portraying persons or scenes. Typically the glass in these windows is separated by lead glazing bars. Stained glass windows were popular in Victorian houses and some Wrightian houses, and are especially common in churches.[24]

A French door[25] has two columns of upright rectangular glass panes (lights) extending its full length; and two of these doors on an exterior wall and without a mullion separating them, that open outward with opposing hinges to a terrace or porch, are referred to as a French window.[26] Sometimes these are set in pairs or multiples thereof along the exterior wall of a very large room, but often, one French window is placed centrally in a typically sized room, perhaps among other fixed windows flanking the feature. French windows are known as porte-fenêtre in France and portafinestra in Italy, and frequently are used in modern houses.

Double-paned windows have two parallel panes (slabs of glass) with a separation of typically about 1 cm; this space is permanently sealed and filled at the time of manufacture with dry air or other dry nonreactive gas. Such windows provide a marked improvement in thermal insulation (and usually in acoustic insulation as well) and are resistant to fogging and frosting caused by temperature differential. They are widely used for residential and commercial construction in intemperate climates. In the UK, double-paned and triple-paned are referred to as double-glazing and triple-glazing. Triple-paned windows are now a common type of glazing in central to northern Europe. Quadruple glazing is now being introduced in Scandinavia.

A hexagonal window is a hexagon-shaped window, resembling a bee cell or crystal lattice of graphite. The window can be vertically or horizontally oriented, openable or dead. It can also be regular or elongately-shaped and can have a separator (mullion). Typically, the cellular window is used for an attic or as a decorative feature, but it can also be a major architectural element to provide the natural lighting inside buildings.

A guillotine window is a window that opens vertically. Guillotine windows have more than one sliding frame, and open from bottom to top or top to bottom.

EN 12519 is the European standard that describes windows terms officially used in EU Member States. The main terms are:

The United States NFRC Window Label lists the following terms:

The European harmonised standard hEN 14351–1, which deals with doors and windows, defines 23 characteristics (divided into essential and non essential). Two other, preliminary European Norms that are under development deal with internal pedestrian doors (prEN 14351–2), smoke and fire resisting doors, and openable windows (prEN 16034).[29]

Windows can be a significant source of heat transfer.[30] Therefore, insulated glazing units consist of two or more panes to reduce the transfer of heat.

These are the pieces of framing that separate a larger window into smaller panes. In older windows, large panes of glass were quite expensive, so muntins let smaller panes fill a larger space. In modern windows, light-colored muntins still provide a useful function by reflecting some of the light going through the window, making the window itself a source of diffuse light (instead of just the surfaces and objects illuminated within the room). By increasing the indirect illumination of surfaces near the window, muntins tend to brighten the area immediately around a window and reduce the contrast of shadows within the room.

Frames and sashes can be made of the following materials:

Composites (also known as Hybrid Windows) are start since early 1998 and combine materials like aluminium + pvc or wood to obtain aesthetics of one material with the functional benefits of another.

A special class of PVC window frames, uPVC window frames, became widespread since the late 20th century, particularly in Europe: there were 83.5 million installed by 1998[33] with numbers still growing as of 2012.[34]

Low-emissivity coated panes reduce heat transfer by radiation, which, depending on which surface is coated, helps prevent heat loss (in cold climates) or heat gains (in warm climates).

High thermal resistance can be obtained by evacuating or filling the insulated glazing units with gases such as argon or krypton, which reduces conductive heat transfer due to their low thermal conductivity. Performance of such units depends on good window seals and meticulous frame construction to prevent entry of air and loss of efficiency.

Modern double-pane and triple-pane windows often include one or more low-e coatings to reduce the window's U-factor (its insulation value, specifically its rate of heat loss). In general, soft-coat low-e coatings tend to result in a lower solar heat gain coefficient (SHGC) than hard-coat low-e coatings.

Modern windows are usually glazed with one large sheet of glass per sash, while windows in the past were glazed with multiple panes separated by glazing bars, or muntins, due to the unavailability of large sheets of glass. Today, glazing bars tend to be decorative, separating windows into small panes of glass even though larger panes of glass are available, generally in a pattern dictated by the architectural style at use. Glazing bars are typically wooden, but occasionally lead glazing bars soldered in place are used for more intricate glazing patterns.

Many windows have movable window coverings such as blinds or curtains to keep out light, provide additional insulation, or ensure privacy.
Windows allow natural light to enter, but too much can have negative effects such as glare and heat gain. Additionally, while windows let the user see outside, there must be a way to maintain privacy on in the inside.[35] Window coverings are practical accommodations for these issues.

Historically, windows are designed with surfaces parallel to vertical building walls. Such a design allows considerable solar light and heat penetration due to the most commonly occurring incidence of sun angles. In passive solar building design, an extended eave is typically used to control the amount of solar light and heat entering the window(s).

An alternative method is to calculate an optimum window mounting angle that accounts for summer sun load minimization, with consideration of actual latitude of the building. This process has been implemented, for example, in the Dakin Building in Brisbane, California—in which most of the fenestration is designed to reflect summer heat load and help prevent summer interior over-illumination and glare, by canting windows to nearly a 45 degree angle.

Photovoltaic windows not only provide a clear view and illuminate rooms, but also convert sunlight to electricity for the building.[36] In most cases, translucent photovoltaic cells are used.

Passive solar windows allow light and solar energy into a building while minimizing air leakage and heat loss. Properly positioning these windows in relation to sun, wind, and landscape—while properly shading them to limit excess heat gain in summer and shoulder seasons, and providing thermal mass to absorb energy during the day and release it when temperatures cool at night—increases comfort and energy efficiency. Properly designed in climates with adequate solar gain, these can even be a building's primary heating system.

A window covering is a shade or screen that provides multiple functions. Some coverings, such as drapes and blinds provide occupants with privacy. Some window coverings control solar heat gain and glare. There are external shading devices and internal shading devices.[37] Low-e window film is a low-cost alternative to window replacement to transform existing poorly-insulating windows into energy-efficient windows. For high-rise buildings, smart glass can provide an alternative.
"
Well Drilling Services,"

Well drilling is the process of drilling a hole in the ground for the extraction of a natural resource such as ground water, brine, natural gas, or petroleum, for the injection of a fluid from surface to a subsurface reservoir or for subsurface formations evaluation or monitoring. Drilling for the exploration of the nature of the material underground (for instance in search of metallic ore) is best described as borehole drilling.

The earliest wells were water wells, shallow pits dug by hand in regions where the water table approached the surface, usually with masonry or wooden walls lining the interior to prevent collapse. Modern drilling techniques utilize long drill shafts, producing holes much narrower and deeper than could be produced by digging.

Well drilling can be done either manually or mechanically and the nature of required equipment varies from extremely simple and cheap to very sophisticated.

In many jurisdictions, drilling activities are regulated to protect groundwater sources from contamination.[1]

Managed Pressure Drilling (MPD) is defined by the International Association of Drilling Contractors (IADC) as “an adaptive drilling process used to more precisely control the annular pressure profile throughout the wellbore."" The objectives of MPD are “to ascertain the downhole pressure environment limits and to manage the annular hydraulic pressure profile accordingly.""

The earliest record of well drilling dates from 347 AD in China.[2] Petroleum was used in ancient China for ""lighting, as a lubricant for cart axles and the bearings of water-powered drop hammers, as a source of carbon for inksticks, and as a medical remedy for sores on humans and mange in animals.""[3] In ancient China, deep well drilling machines were in the forefront of brine well production by the 1st century BC. The ancient Chinese developed advanced sinking wells and were the first civilization to use a well-drilling machine and to use bamboo well casings to keep the holes open.[4][5]

In the modern era, the first roller cone patent was for the rotary rock bit and was issued to American businessman and inventor Howard Hughes Sr. in 1909. It consisted of two interlocking cones. American businessman Walter Benona Sharp worked very closely with Hughes in developing the rock bit. The success of this bit led to the founding of the Sharp-Hughes Tool Company. In 1933 two Hughes engineers, one of whom was Ralph Neuhaus, invented the tricone bit, which has three cones.  The Hughes patent for the tricone bit lasted until 1951, after which other companies made similar bits. However, Hughes still held 40% of the world's drill bit market in 2000. The superior wear performance of polycrystalline diamond compact (PDC) bits gradually eroded the dominance of roller cone bits and early in this century PDC drill bit revenues overtook those of roller cone bits. The technology of both bit types has advanced significantly to provide improved durability and rate of penetration of the rock. This has been driven by the economics of the industry, and by the change from the empirical approach of Hughes in the 1930s, to modern day domain Finite Element codes for both the hydraulic and cutter placement software.

The factors affecting drill bit selection include the type of geology and the capabilities of the rig. Due to the high number of wells that have been drilled, information from an adjacent well is most often used to make the appropriate selection. Two different types of drill bits exist: fixed cutter and roller cone. A fixed cutter bit is one where there are no moving parts, but drilling occurs due to shearing, scraping or abrasion of the rock. Fixed cutter bits can be either polycrystalline diamond compact (PDC) or grit hot-pressed inserts (GHI) or natural diamond.  Roller cone bits can be either tungsten carbide inserts (TCI) for harder formations or milled tooth (MT) for softer rock. The manufacturing process and composites used in each type of drill bit make them ideal for specific drilling situations. Additional enhancements can be made to any bit to increase the effectiveness for almost any drilling situation.

A major factor in drill bit selection is the type of formation to be drilled. The effectiveness of a drill bit varies by formation type. There are three types of formations: soft, medium and hard. A soft formation includes unconsolidated sands, clays, soft limestones, red beds and shale. Medium formations include dolomites, limestones, and hard shale. Hard formations include hard shale, calcites, mudstones, cherty lime stones and hard and abrasive formations.

Until 2006, market share was divided primarily among Hughes Christensen, Security-DBS (Halliburton Drill Bits and Services), Smith Bits (a subsidiary of Schlumberger), and ReedHycalog (acquired by National Oilwell Varco in 2008).

By 2014, Ulterra (then a subsidiary of ESCO Corp.) and Varel International (a subsidiary of Swedish engineering group Sandvik) had together gained nearly 30% of the U.S. bit market and eroded the historical dominance of the Smith, Halliburton, and Baker Hughes. By 2018, Schlumberger, which acquired Smith in 2010,[6] became dominant in international markets thanks to packaging drill bits with their other tools and services, while Ulterra (owned by private equity firms Blackstone Energy Partners and American Securities) continued a Stark growth trend, becoming the market share leader in drill bits in the US according to Spears Research [7] and Kimberlite Research.[8]

Evaluation of the dull bit grading is done by a uniform system promoted by the International Association of Drilling Contractors (IADC). See Society of Petroleum Engineers / IADC Papers SPE 23938 & 23940. See also PDC Bits

Q* ""Mechanical Mole Bores Crooked Wells."" Popular Science, June 1942, pp. 94–95
"
Directional Drilling Services,"

Directional drilling (or slant drilling) is the practice of drilling non-vertical bores. It can be broken down into four main groups: oilfield directional drilling, utility installation directional drilling, directional boring (horizontal directional drilling - HDD), and surface in seam (SIS), which horizontally intersects a vertical bore target to extract coal bed methane.

Many prerequisites enabled this suite of technologies to become productive. Probably, the first requirement was the realization that oil wells, or water wells, do not necessarily need to be vertical. This realization was quite slow, and did not really grasp the attention of the oil industry until the late 1920s when there were several lawsuits alleging that wells drilled from a rig on one property had crossed the boundary and were penetrating a reservoir on an adjacent property.[citation needed] Initially, proxy evidence such as production changes in other wells was accepted, but such cases fueled the development of small diameter tools capable of surveying wells during drilling. Horizontal directional drill rigs are developing towards large-scale, micro-miniaturization, mechanical automation, hard stratum working, exceeding length and depth oriented monitored drilling.[1]

Measuring the inclination of a wellbore (its deviation from the vertical) is comparatively simple, requiring only a pendulum. Measuring the azimuth (direction with respect to the geographic grid in which the wellbore was running from the vertical), however, was more difficult. In certain circumstances, magnetic fields could be used, but would be influenced by metalwork used inside wellbores, as well as the metalwork used in drilling equipment. The next advance was in the modification of small gyroscopic compasses by the Sperry Corporation, which was making similar compasses for aeronautical navigation. Sperry did this under contract to Sun Oil (which was involved in a lawsuit as described above), and a spin-off company ""Sperry Sun"" was formed, which brand continues to this day,[when?][clarification needed] absorbed into Halliburton. Three components are measured at any given point in a wellbore in order to determine its position: the depth of the point along the course of the borehole (measured depth), the inclination at the point, and the magnetic azimuth at the point. These three components combined are referred to as a ""survey"". A series of consecutive surveys are needed to track the progress and location of a wellbore.

Prior experience with rotary drilling had established several principles for the configuration of drilling equipment down hole (""bottom hole assembly"" or ""BHA"") that would be prone to ""drilling crooked hole"" (i.e., initial accidental deviations from the vertical would be increased). Counter-experience had also given early directional drillers (""DD's"") principles of BHA design and drilling practice that would help bring a crooked hole nearer the vertical.[citation needed]

In 1934, H. John Eastman and Roman W. Hines of Long Beach, California, became pioneers in directional drilling when they and George Failing of Enid, Oklahoma, saved the Conroe, Texas, oil field. Failing had recently patented a portable drilling truck. He had started his company in 1931 when he mated a drilling rig to a truck and a power take-off assembly. The innovation allowed rapid drilling of a series of slanted wells. This capacity to quickly drill multiple relief wells and relieve the enormous gas pressure was critical to extinguishing the Conroe fire.[2] In a May, 1934, Popular Science Monthly article, it was stated that ""Only a handful of men in the world have the strange power to make a bit, rotating a mile below ground at the end of a steel drill pipe, snake its way in a curve or around a dog-leg angle, to reach a desired objective."" Eastman Whipstock, Inc., would become the world's largest directional company in 1973.[citation needed]

Combined, these survey tools and BHA designs made directional drilling possible, but it was perceived as arcane. The next major advance was in the 1970s, when downhole drilling motors (aka mud motors, driven by the hydraulic power of drilling mud circulated down the drill string) became common. These allowed the drill bit to continue rotating at the cutting face at the bottom of the hole, while most of the drill pipe was held stationary. A piece of bent pipe (a ""bent sub"") between the stationary drill pipe and the top of the motor allowed the direction of the wellbore to be changed without needing to pull all the drill pipe out and place another whipstock. Coupled with the development of measurement while drilling tools (using mud pulse telemetry, networked or wired pipe or electromagnetism (EM) telemetry, which allows tools down hole to send directional data back to the surface without disturbing drilling operations), directional drilling became easier.

Certain profiles cannot be easily drilled while the drill pipe is rotating. Drilling directionally with a downhole motor requires occasionally stopping rotation of the drill pipe and ""sliding"" the pipe through the channel as the motor cuts a curved path. ""Sliding"" can be difficult in some formations, and it is almost always slower and therefore more expensive than drilling while the pipe is rotating, so the ability to steer the bit while the drill pipe is rotating is desirable. Several companies have developed tools which allow directional control while rotating. These tools are referred to as rotary steerable systems (RSS). RSS technology has made access and directional control possible in previously inaccessible or uncontrollable formations.  

Wells are drilled directionally for several purposes:

Most directional drillers are given a blue well path to follow that is predetermined by engineers and geologists before the drilling commences. When the directional driller starts the drilling process, periodic surveys are taken with a downhole instrument to provide survey data (inclination and azimuth) of the well bore.[3] These pictures are typically taken at intervals between 10 and 150 meters (33 and 492 feet), with 30 meters (98 feet) common during active changes of angle or direction, and distances of 60–100 meters (200–330 feet) being typical while ""drilling ahead"" (not making active changes to angle and direction). During critical angle and direction changes, especially while using a downhole motor, a measurement while drilling (MWD) tool will be added to the drill string to provide continuously updated measurements that may be used for (near) real-time adjustments.

This data indicates if the well is following the planned path and whether the orientation of the drilling assembly is causing the well to deviate as planned. Corrections are regularly made by techniques as simple as adjusting rotation speed or the drill string weight (weight on bottom) and stiffness, as well as more complicated and time-consuming methods, such as introducing a downhole motor. Such pictures, or surveys, are plotted and maintained as an engineering and legal record describing the path of the well bore. The survey pictures taken while drilling are typically confirmed by a later survey in full of the borehole, typically using a ""multi-shot camera"" device.

The multi-shot camera advances the film at time intervals so that by dropping the camera instrument in a sealed tubular housing inside the drilling string (down to just above the drilling bit) and then withdrawing the drill string at time intervals, the well may be fully surveyed at regular depth intervals (approximately every 30 meters (98 feet) being common, the typical length of 2 or 3 joints of drill pipe, known as a stand, since most drilling rigs ""stand back"" the pipe withdrawn from the hole at such increments, known as ""stands"").

Drilling to targets far laterally from the surface location requires careful planning and design. The current record holders manage wells over 10 km (6.2 mi) away from the surface location at a true vertical depth (TVD) of only 1,600–2,600 m (5,200–8,500 ft).[4]

This form of drilling can also reduce the environmental cost and scarring of the landscape. Previously, long lengths of landscape had to be removed from the surface. This is no longer required with directional drilling.

Until the arrival of modern downhole motors and better tools to measure inclination and azimuth of the hole, directional drilling and horizontal drilling was much slower than vertical drilling due to the need to stop regularly and take time-consuming surveys, and due to slower progress in drilling itself (lower rate of penetration). These disadvantages have shrunk over time as downhole motors became more efficient and semi-continuous surveying became possible.

What remains is a difference in operating costs: for wells with an inclination of less than 40 degrees, tools to carry out adjustments or repair work can be lowered by gravity on cable into the hole. For higher inclinations, more expensive equipment has to be mobilized to push tools down the hole.

Another disadvantage of wells with a high inclination was that prevention of sand influx into the well was less reliable and needed higher effort. Again, this disadvantage has diminished such that, provided sand control is adequately planned, it is possible to carry it out reliably.

In 1990, Iraq accused Kuwait of stealing Iraq's oil through slant drilling.[5]
The United Nations redrew the border after the 1991 Gulf war, which ended the seven-month Iraqi occupation of Kuwait. As part of the reconstruction, 11 new oil wells were placed among the existing 600. Some farms and an old naval base that used to be in the Iraqi side became part of Kuwait.[6]

In the mid-twentieth century, a slant-drilling scandal occurred in the huge East Texas Oil Field.[7]

Between 1985 and 1993, the Naval Civil Engineering Laboratory (NCEL) (now the Naval Facilities Engineering Service Center (NFESC)) of Port Hueneme, California developed controllable horizontal drilling technologies.[8] These technologies are capable of reaching 10,000–15,000 ft (3,000–4,600 m) and may reach 25,000 ft (7,600 m) when used under favorable conditions.[9]

Specialized tools determine the wellbore's deviation from vertical (inclination) and its directional orientation (azimuth). This data is vital for trajectory adjustments. These surveys are taken at regular intervals (e.g., every 30–100 meters) to track the wellbore's progress in real time. In critical sections, measurement while drilling (MWD) tools provide continuous downhole measurements for immediate directional corrections as needed. MWD uses gyroscopes, magnetometers, and accelerometers to determine borehole inclination and azimuth while the drilling is being done.
"
Infrastructure Excavation,"

In archaeology, excavation is the exposure, processing and recording of archaeological remains.[1] An excavation site or ""dig"" is the area being studied. These locations range from one to several areas at a time during a project and can be conducted over a few weeks to several years.

Excavation involves the recovery of several types of data from a site. This data includes artifacts (portable objects made or modified by humans), features (non-portable modifications to the site itself such as post molds, burials, and hearths), ecofacts (evidence of human activity through organic remains such as animal bones, pollen, or charcoal), and archaeological context (relationships among the other types of data).[2][3][4][5]

Before excavating, the presence or absence of archaeological remains can often be suggested by, non-intrusive remote sensing, such as ground-penetrating radar.[6] Basic information about the development of the site may be drawn from this work, but to understand finer details of a site, excavation via augering can be used.

During excavation, archaeologists often use stratigraphic excavation to remove phases of the site one layer at a time. This keeps the timeline of the material remains consistent with one another.[7] This is done usually though mechanical means where artifacts can be spot dated and the soil processed through methods such as mechanical sieving or water flotation. Afterwards, digital methods are then used record the excavation process and its results.  Ideally, data from the excavation should suffice to reconstruct the site completely in three-dimensional space.

One of the first known instances of archaeological excavation took place in the sixth century BC when Nabonidus, the king of Babylon, excavated a temple floor that was thousands of years old.[8] During early Roman periods, Julius Caesar's men looted bronze artifacts, and by the medieval period, Europeans had begun digging up pots that had partially emerged from erosion, and weapons that had turned up on farmlands.[8] Antiquarians excavated burial mounds in North America and North-West Europe, which sometimes involved destroying artifacts and their context, losing information about subjects from the past. Meticulous and methodical archaeological excavation took over from antiquarian barrow-digging around the early to mid-nineteenth century and is still being perfected today.[9][8]

The most dramatic change that occurred over time is the amount of recording and care taken to ensure preservation of artifacts and features.[10] In the past, archaeological excavation involved random digging to unearth artifacts. Exact locations of artifacts were not recorded, and measurements were not taken. Modern archaeological excavation has evolved to include removal of thin layers of sediment sequentially and recording of measurements about artifacts' locations in a site.[citation needed]

There are two basic types of modern archaeological excavation:

There are two main types of trial excavation in professional archaeology both commonly associated with development-led excavation: the test pit or trench and the watching brief. The purpose of trial excavations is to determine the extent and characteristics of archaeological potential in a given area before extensive excavation work is undertaken. This is usually conducted in development-led excavations as part of Project management planning. The main difference between Trial trenching and watching briefs is that trial trenches are actively dug for the purpose of revealing archaeological potential[12] whereas watching briefs are cursory examination of trenches where the primary function of the trench is something other than archaeology, for example a trench cut for a gas pipe in a road. In the US, a method of evaluation called a Shovel test pit is used which is a specified half meter square line of  trial trenches dug by hand.

Archaeological material tends to accumulate in events. A gardener swept a pile of soil into a corner, laid a gravel path or planted a bush in a hole. A builder built a wall and back-filled the trench. Years later, someone built a pigsty onto it and drained the pigsty into the nettle patch. Later still, the original wall blew over and so on. Each event, which may have taken a short or long time to accomplish, leaves a context. This layer cake of events is often referred to as the archaeological sequence or record. It is by analysis of this sequence or record that excavation is intended to permit interpretation, which should lead to discussion and understanding.

The prominent processual archaeologist Lewis Binford highlighted the fact that the archaeological evidence left at a site may not be entirely indicative of the historical events that actually took place there. Using an ethnoarchaeological comparison, he looked at how hunters amongst the Nunamiut Iñupiat of north central Alaska spent a great deal of time in a certain area simply waiting for prey to arrive there, and that during this period, they undertook other tasks to pass the time, such as the carving of various objects, including a wooden mould for a mask, a horn spoon and an ivory needle, as well as repairing a skin pouch and a pair of caribou skin socks. Binford notes that all of these activities would have left evidence in the archaeological record, but that none of them would provide evidence for the primary reason that the hunters were in the area; to wait for prey. As he remarked, waiting for animals to hunt ""represented 24% of the total man-hours of activity recorded; yet there is no recognisable archaeological consequences of this behaviour. No tools left on the site were used, and there were no immediate material ""byproducts"" of the ""primary"" activity. All of the other activities conducted at the site were essentially boredom reducers.""[13]

In archaeology, especially in excavating, stratigraphy involves the study of how deposits occurs layer by layer.[7] It is largely based on the Law of Superposition. The Law of Superposition indicates that layers of sediment further down will contain older artifacts than layers above.[14] When archaeological finds are below the surface of the ground (as is most commonly the case), the identification of the context of each find is vital to enable the archaeologist to draw conclusions about the site and the nature and date of its occupation. It is the archaeologist's role to attempt to discover what contexts exist and how they came to be created.[15] Archaeological stratification or sequence is the dynamic superimposition of single units of stratigraphy or contexts.[16] The context (physical location) of a discovery can be of major significance. Archaeological context refers to where an artifact or feature was found as well as what the artifact or feature was located near.[17] Context is important for determining how long ago the artifact or feature was in use as well as what its function may have been.[17] The cutting of a pit or ditch in the past is a context, whilst the material filling it will be another. Multiple fills seen in section would mean multiple contexts. Structural features, natural deposits and inhumations are also contexts.

By separating a site into these basic, discrete units, archaeologists are able to create a chronology for activity on a site and describe and interpret it. Stratigraphic relationships are the relationships created between contexts in time representing the chronological order they were created. An example would be a ditch and the back-fill of said ditch. The relationship of ""the fill"" context to the ditch ""cut"" context is ""the fill"" occurred later in the sequence, i.e., you have to dig a ditch first before you can back-fill it.[18] A relationship that is later in the sequence is sometimes referred to as ""higher"" in the sequence and a relationship that is earlier ""lower"" though the term higher or lower does not itself imply a context needs to be physically higher or lower. It is more useful to think of this higher or lower term as it relates to the contexts position in a Harris matrix, which is a two-dimensional representation of a site's formation in space and time.

Understanding a site in modern archaeology is a process of grouping single contexts together in ever larger groups by virtue of their relationships. The terminology of these larger clusters varies depending on practitioner, but the terms interface, sub-group, group and land use are common. An example of a sub-group could be the three contexts that make up a burial: the grave cut, the body and the back-filled earth on top of the body. In turn sub-groups can be clustered together with other sub-groups by virtue of their stratigraphic relationship to form groups which in turn form ""phases"". A sub-group burial could cluster with other sub-group burials to form a cemetery or burial group which in turn could be clustered with a building such as church to produce a ""phase."" A less rigorously defined combination of one or more contexts is sometimes called a feature.

Phase is the most easily understood grouping for the layman as it implies a near contemporaneous Archaeological horizon representing ""what you would see if you went back to a specific point in time"". Often but not always a phase implies the identification of an occupation surface ""old ground level"" that existed at some earlier time.
The production of phase interpretations is one of the first goals of stratigraphic interpretation and excavation. Digging ""in phase"" is not quite the same as phasing a site. Phasing a site represents reducing the site either in excavation or post-excavation to contemporaneous horizons whereas ""digging in phase"" is the process of stratigraphic removal of archaeological remains so as not to remove contexts that are earlier in time ""lower in the sequence"" before other contexts that have a latter physical stratigraphic relationship to them as defined by the law of superposition. The process of interpretation in practice will have a bearing on excavation strategies on site so ""phasing"" a site is actively pursued during excavation where at all possible and is considered good practice.

An ""intrusion"" or ""intrusive object"" is something that arrived later to the phase in the strata, for example modern pipework or the 16th-century bottles left by treasure-hunters at Sutton Hoo.

Excavation initially involves the removal of any topsoil. A strategy for sampling the contexts and features is formulated which may involve total excavation of each feature or only portions. 

In stratigraphic excavation, the goal is to remove some or, preferably, all archaeological deposits and features in the reverse order they were created and construct a Harris matrix as a chronological record or ""sequence"" of the site.[15] This Harris matrix is used for interpretation and combining contexts into ever larger units of understanding. This stratigraphic removal of the site is crucial for understanding the chronology of events on site.

Stratigraphic excavation involves a process of cleaning or ""troweling back"" the surface of the site and isolating contexts and edges which are definable as either:

Following this preliminary process of defining the context, it is then recorded and removed.
Often, owing to practical considerations or error, the process of defining the edges of contexts is not followed and contexts are removed out of sequence and un-stratigraphically. This is called ""digging out of phase"". It is not good practice.
After removing a context or if practical a set of contexts such as the case would be for features, the ""isolate and dig"" procedure is repeated until no man made remains are left on site and the site is reduced to natural.

""Strip, map and sample"" is a method of excavation applied in the United Kingdom to preserve archaeological remains by record in the face of development threats. It involves machine stripping an area, plotting observed features onto a site plan, and then partially excavating those features (sampling).[citation needed] The approach is undertaken when a site is to be destroyed by development and no satisfactory method of preserving archaeological remains in situ can be devised or adequate funding and time have not been factored into development project planning to allow for a full archaeological investigation.[citation needed]

This describes the use in excavations of various types and sizes of machines from small backhoes to heavy duty earth-moving machinery. Machines are often used in what is called salvage or rescue archaeology in developer-led excavation when there are financial or time pressures.[20] Using a mechanical excavator is the quickest method to remove soil and debris and to prepare the surface for excavation by hand, taking care to avoid damaging archaeological deposits by accident or to make it difficult to identify later precisely where finds were located.[21] The use of such machinery is often routine (as it is for instance with the British archaeological television series Time Team)[22] but can also be controversial as it can result in less discrimination in how the archaeological sequence on a site is recorded. One of the earliest uses of earth-moving machinery was at Durrington Walls in 1967. An old road through the henge was to be straightened and improved and was going to cause considerable damage to the archaeology. Rosemary Hill describes how Geoffrey Wainwright ""oversaw large, high-speed excavations, taking bulldozers to the site in a manner that shocked some of his colleagues but yielded valuable if tantalising information about what Durrington had looked like and how it might have been used.""[23] Machines are used primarily to remove modern overburden and for the control of spoil. In British archaeology mechanical diggers are sometimes nicknamed ""big yellow trowels"".

Archaeological excavation is an unrepeatable process, since the same area of the ground cannot be excavated twice.[24]
Thus, archaeology is often known as a destructive science, where you must destroy the original evidence in order to make observations.  To mitigate this, highly accurate and precise digital methods can be used to record the excavation process and its results.[25]

Single context recording was developed in the 1970s by the museum of London (as well as earlier in Winchester and York) and has become the de facto recording system in many parts of the world and is especially suited to the complexities of deep urban archaeology and the process of Stratification.
Each excavated context is given a unique ""context number"" and is recorded by type on a context sheet and perhaps being drawn on a plan and/or a section. Depending on time constraints and importance contexts may also be photographed, but in this case a grouping of contexts and their associations are the purpose of the photography. Finds from each context are bagged and labeled with their context number and site code for later cross-reference work carried out post-excavation. The height above sea level of pertinent points on a context, such as the top and bottom of a wall are taken and added to plans sections and context sheets. Heights are recorded with a dumpy level or total station by relation to the site  temporary benchmark (abbr. T.B.M). Samples of deposits from contexts are sometimes also taken, for later environmental analysis or for scientific dating.

Digital tools used by field archaeologists during excavation include GPS, tablet computers, relational databases, digital cameras, 3d laser scanners, and unmanned aerial vehicles. After high quality digital data have been recorded, these data can then be shared over the internet for open access and use by the public and archaeological researchers.
Digital imaging or digital image acquisition is digital photography, such as of a physical scene or of the interior structure of an object.  The term is often used to include the processing, compression, storage, printing, and display of the images.

Finds and artifacts that survive in the archaeological record are retrieved in the main by hand and observation as the context they survive in is excavated. Several other techniques are available depending on suitability and time constraints. Sieving (screening) and flotation are used to maximize the recovery of small items such as small shards of pottery or flint flakes, or bones and seeds.

Flotation is a process of retrieval that works by passing spoil onto the surface of water and separating finds that float from the spoil which sinks. This is especially suited to the recovery of environmental data stored in organic material such as seeds and small bones.[26] Not all finds retrieval is done during excavation and some, especially flotation, may take place post-excavation from samples taken during excavation.

The use of sieving (screening) is more common on research-based excavations where more time is available. Some success has been achieved with the use of cement mixers and bulk sieving. This method allows the quick removal of context by shovel and mattock yet allows for a high retrieval rate. Spoil is shoveled into cement mixers and water added to form a slurry which is then poured through a large screen mesh. The speed of this technique is offset by the damage it does to more fragile artifacts. 

One important role of finds retrieval during excavation is the role of specialists to provide spot dating information on the contexts being removed from the archaeological record. This can provide advance warning of potential discoveries to come by virtue of residual finds redeposited in contexts higher in the sequence (which should be coming offsite earlier than contexts from early eras and phases). Spot dating also forms part of a confirmation process, of assessing the validity of the working hypothesis on the phasing of site during excavation. For example, the presence of an anomalous medieval pottery sherd in what was thought to be an Iron Age ditch feature could radically alter onsite thinking on the correct strategy for digging a site and save a lot of information being lost due to incorrect assumptions about the nature of the deposits which will be destroyed by the excavation process and in turn, limit the site's potential for revealing information for post-excavation specialists. Or anomalous information could show up errors in excavation such as ""undercutting"". Dating methodology in part relies on accurate excavation and in this sense the two activities become interdependent.
"
New Ground Excavation,"

In archaeology, excavation is the exposure, processing and recording of archaeological remains.[1] An excavation site or ""dig"" is the area being studied. These locations range from one to several areas at a time during a project and can be conducted over a few weeks to several years.

Excavation involves the recovery of several types of data from a site. This data includes artifacts (portable objects made or modified by humans), features (non-portable modifications to the site itself such as post molds, burials, and hearths), ecofacts (evidence of human activity through organic remains such as animal bones, pollen, or charcoal), and archaeological context (relationships among the other types of data).[2][3][4][5]

Before excavating, the presence or absence of archaeological remains can often be suggested by, non-intrusive remote sensing, such as ground-penetrating radar.[6] Basic information about the development of the site may be drawn from this work, but to understand finer details of a site, excavation via augering can be used.

During excavation, archaeologists often use stratigraphic excavation to remove phases of the site one layer at a time. This keeps the timeline of the material remains consistent with one another.[7] This is done usually though mechanical means where artifacts can be spot dated and the soil processed through methods such as mechanical sieving or water flotation. Afterwards, digital methods are then used record the excavation process and its results.  Ideally, data from the excavation should suffice to reconstruct the site completely in three-dimensional space.

One of the first known instances of archaeological excavation took place in the sixth century BC when Nabonidus, the king of Babylon, excavated a temple floor that was thousands of years old.[8] During early Roman periods, Julius Caesar's men looted bronze artifacts, and by the medieval period, Europeans had begun digging up pots that had partially emerged from erosion, and weapons that had turned up on farmlands.[8] Antiquarians excavated burial mounds in North America and North-West Europe, which sometimes involved destroying artifacts and their context, losing information about subjects from the past. Meticulous and methodical archaeological excavation took over from antiquarian barrow-digging around the early to mid-nineteenth century and is still being perfected today.[9][8]

The most dramatic change that occurred over time is the amount of recording and care taken to ensure preservation of artifacts and features.[10] In the past, archaeological excavation involved random digging to unearth artifacts. Exact locations of artifacts were not recorded, and measurements were not taken. Modern archaeological excavation has evolved to include removal of thin layers of sediment sequentially and recording of measurements about artifacts' locations in a site.[citation needed]

There are two basic types of modern archaeological excavation:

There are two main types of trial excavation in professional archaeology both commonly associated with development-led excavation: the test pit or trench and the watching brief. The purpose of trial excavations is to determine the extent and characteristics of archaeological potential in a given area before extensive excavation work is undertaken. This is usually conducted in development-led excavations as part of Project management planning. The main difference between Trial trenching and watching briefs is that trial trenches are actively dug for the purpose of revealing archaeological potential[12] whereas watching briefs are cursory examination of trenches where the primary function of the trench is something other than archaeology, for example a trench cut for a gas pipe in a road. In the US, a method of evaluation called a Shovel test pit is used which is a specified half meter square line of  trial trenches dug by hand.

Archaeological material tends to accumulate in events. A gardener swept a pile of soil into a corner, laid a gravel path or planted a bush in a hole. A builder built a wall and back-filled the trench. Years later, someone built a pigsty onto it and drained the pigsty into the nettle patch. Later still, the original wall blew over and so on. Each event, which may have taken a short or long time to accomplish, leaves a context. This layer cake of events is often referred to as the archaeological sequence or record. It is by analysis of this sequence or record that excavation is intended to permit interpretation, which should lead to discussion and understanding.

The prominent processual archaeologist Lewis Binford highlighted the fact that the archaeological evidence left at a site may not be entirely indicative of the historical events that actually took place there. Using an ethnoarchaeological comparison, he looked at how hunters amongst the Nunamiut Iñupiat of north central Alaska spent a great deal of time in a certain area simply waiting for prey to arrive there, and that during this period, they undertook other tasks to pass the time, such as the carving of various objects, including a wooden mould for a mask, a horn spoon and an ivory needle, as well as repairing a skin pouch and a pair of caribou skin socks. Binford notes that all of these activities would have left evidence in the archaeological record, but that none of them would provide evidence for the primary reason that the hunters were in the area; to wait for prey. As he remarked, waiting for animals to hunt ""represented 24% of the total man-hours of activity recorded; yet there is no recognisable archaeological consequences of this behaviour. No tools left on the site were used, and there were no immediate material ""byproducts"" of the ""primary"" activity. All of the other activities conducted at the site were essentially boredom reducers.""[13]

In archaeology, especially in excavating, stratigraphy involves the study of how deposits occurs layer by layer.[7] It is largely based on the Law of Superposition. The Law of Superposition indicates that layers of sediment further down will contain older artifacts than layers above.[14] When archaeological finds are below the surface of the ground (as is most commonly the case), the identification of the context of each find is vital to enable the archaeologist to draw conclusions about the site and the nature and date of its occupation. It is the archaeologist's role to attempt to discover what contexts exist and how they came to be created.[15] Archaeological stratification or sequence is the dynamic superimposition of single units of stratigraphy or contexts.[16] The context (physical location) of a discovery can be of major significance. Archaeological context refers to where an artifact or feature was found as well as what the artifact or feature was located near.[17] Context is important for determining how long ago the artifact or feature was in use as well as what its function may have been.[17] The cutting of a pit or ditch in the past is a context, whilst the material filling it will be another. Multiple fills seen in section would mean multiple contexts. Structural features, natural deposits and inhumations are also contexts.

By separating a site into these basic, discrete units, archaeologists are able to create a chronology for activity on a site and describe and interpret it. Stratigraphic relationships are the relationships created between contexts in time representing the chronological order they were created. An example would be a ditch and the back-fill of said ditch. The relationship of ""the fill"" context to the ditch ""cut"" context is ""the fill"" occurred later in the sequence, i.e., you have to dig a ditch first before you can back-fill it.[18] A relationship that is later in the sequence is sometimes referred to as ""higher"" in the sequence and a relationship that is earlier ""lower"" though the term higher or lower does not itself imply a context needs to be physically higher or lower. It is more useful to think of this higher or lower term as it relates to the contexts position in a Harris matrix, which is a two-dimensional representation of a site's formation in space and time.

Understanding a site in modern archaeology is a process of grouping single contexts together in ever larger groups by virtue of their relationships. The terminology of these larger clusters varies depending on practitioner, but the terms interface, sub-group, group and land use are common. An example of a sub-group could be the three contexts that make up a burial: the grave cut, the body and the back-filled earth on top of the body. In turn sub-groups can be clustered together with other sub-groups by virtue of their stratigraphic relationship to form groups which in turn form ""phases"". A sub-group burial could cluster with other sub-group burials to form a cemetery or burial group which in turn could be clustered with a building such as church to produce a ""phase."" A less rigorously defined combination of one or more contexts is sometimes called a feature.

Phase is the most easily understood grouping for the layman as it implies a near contemporaneous Archaeological horizon representing ""what you would see if you went back to a specific point in time"". Often but not always a phase implies the identification of an occupation surface ""old ground level"" that existed at some earlier time.
The production of phase interpretations is one of the first goals of stratigraphic interpretation and excavation. Digging ""in phase"" is not quite the same as phasing a site. Phasing a site represents reducing the site either in excavation or post-excavation to contemporaneous horizons whereas ""digging in phase"" is the process of stratigraphic removal of archaeological remains so as not to remove contexts that are earlier in time ""lower in the sequence"" before other contexts that have a latter physical stratigraphic relationship to them as defined by the law of superposition. The process of interpretation in practice will have a bearing on excavation strategies on site so ""phasing"" a site is actively pursued during excavation where at all possible and is considered good practice.

An ""intrusion"" or ""intrusive object"" is something that arrived later to the phase in the strata, for example modern pipework or the 16th-century bottles left by treasure-hunters at Sutton Hoo.

Excavation initially involves the removal of any topsoil. A strategy for sampling the contexts and features is formulated which may involve total excavation of each feature or only portions. 

In stratigraphic excavation, the goal is to remove some or, preferably, all archaeological deposits and features in the reverse order they were created and construct a Harris matrix as a chronological record or ""sequence"" of the site.[15] This Harris matrix is used for interpretation and combining contexts into ever larger units of understanding. This stratigraphic removal of the site is crucial for understanding the chronology of events on site.

Stratigraphic excavation involves a process of cleaning or ""troweling back"" the surface of the site and isolating contexts and edges which are definable as either:

Following this preliminary process of defining the context, it is then recorded and removed.
Often, owing to practical considerations or error, the process of defining the edges of contexts is not followed and contexts are removed out of sequence and un-stratigraphically. This is called ""digging out of phase"". It is not good practice.
After removing a context or if practical a set of contexts such as the case would be for features, the ""isolate and dig"" procedure is repeated until no man made remains are left on site and the site is reduced to natural.

""Strip, map and sample"" is a method of excavation applied in the United Kingdom to preserve archaeological remains by record in the face of development threats. It involves machine stripping an area, plotting observed features onto a site plan, and then partially excavating those features (sampling).[citation needed] The approach is undertaken when a site is to be destroyed by development and no satisfactory method of preserving archaeological remains in situ can be devised or adequate funding and time have not been factored into development project planning to allow for a full archaeological investigation.[citation needed]

This describes the use in excavations of various types and sizes of machines from small backhoes to heavy duty earth-moving machinery. Machines are often used in what is called salvage or rescue archaeology in developer-led excavation when there are financial or time pressures.[20] Using a mechanical excavator is the quickest method to remove soil and debris and to prepare the surface for excavation by hand, taking care to avoid damaging archaeological deposits by accident or to make it difficult to identify later precisely where finds were located.[21] The use of such machinery is often routine (as it is for instance with the British archaeological television series Time Team)[22] but can also be controversial as it can result in less discrimination in how the archaeological sequence on a site is recorded. One of the earliest uses of earth-moving machinery was at Durrington Walls in 1967. An old road through the henge was to be straightened and improved and was going to cause considerable damage to the archaeology. Rosemary Hill describes how Geoffrey Wainwright ""oversaw large, high-speed excavations, taking bulldozers to the site in a manner that shocked some of his colleagues but yielded valuable if tantalising information about what Durrington had looked like and how it might have been used.""[23] Machines are used primarily to remove modern overburden and for the control of spoil. In British archaeology mechanical diggers are sometimes nicknamed ""big yellow trowels"".

Archaeological excavation is an unrepeatable process, since the same area of the ground cannot be excavated twice.[24]
Thus, archaeology is often known as a destructive science, where you must destroy the original evidence in order to make observations.  To mitigate this, highly accurate and precise digital methods can be used to record the excavation process and its results.[25]

Single context recording was developed in the 1970s by the museum of London (as well as earlier in Winchester and York) and has become the de facto recording system in many parts of the world and is especially suited to the complexities of deep urban archaeology and the process of Stratification.
Each excavated context is given a unique ""context number"" and is recorded by type on a context sheet and perhaps being drawn on a plan and/or a section. Depending on time constraints and importance contexts may also be photographed, but in this case a grouping of contexts and their associations are the purpose of the photography. Finds from each context are bagged and labeled with their context number and site code for later cross-reference work carried out post-excavation. The height above sea level of pertinent points on a context, such as the top and bottom of a wall are taken and added to plans sections and context sheets. Heights are recorded with a dumpy level or total station by relation to the site  temporary benchmark (abbr. T.B.M). Samples of deposits from contexts are sometimes also taken, for later environmental analysis or for scientific dating.

Digital tools used by field archaeologists during excavation include GPS, tablet computers, relational databases, digital cameras, 3d laser scanners, and unmanned aerial vehicles. After high quality digital data have been recorded, these data can then be shared over the internet for open access and use by the public and archaeological researchers.
Digital imaging or digital image acquisition is digital photography, such as of a physical scene or of the interior structure of an object.  The term is often used to include the processing, compression, storage, printing, and display of the images.

Finds and artifacts that survive in the archaeological record are retrieved in the main by hand and observation as the context they survive in is excavated. Several other techniques are available depending on suitability and time constraints. Sieving (screening) and flotation are used to maximize the recovery of small items such as small shards of pottery or flint flakes, or bones and seeds.

Flotation is a process of retrieval that works by passing spoil onto the surface of water and separating finds that float from the spoil which sinks. This is especially suited to the recovery of environmental data stored in organic material such as seeds and small bones.[26] Not all finds retrieval is done during excavation and some, especially flotation, may take place post-excavation from samples taken during excavation.

The use of sieving (screening) is more common on research-based excavations where more time is available. Some success has been achieved with the use of cement mixers and bulk sieving. This method allows the quick removal of context by shovel and mattock yet allows for a high retrieval rate. Spoil is shoveled into cement mixers and water added to form a slurry which is then poured through a large screen mesh. The speed of this technique is offset by the damage it does to more fragile artifacts. 

One important role of finds retrieval during excavation is the role of specialists to provide spot dating information on the contexts being removed from the archaeological record. This can provide advance warning of potential discoveries to come by virtue of residual finds redeposited in contexts higher in the sequence (which should be coming offsite earlier than contexts from early eras and phases). Spot dating also forms part of a confirmation process, of assessing the validity of the working hypothesis on the phasing of site during excavation. For example, the presence of an anomalous medieval pottery sherd in what was thought to be an Iron Age ditch feature could radically alter onsite thinking on the correct strategy for digging a site and save a lot of information being lost due to incorrect assumptions about the nature of the deposits which will be destroyed by the excavation process and in turn, limit the site's potential for revealing information for post-excavation specialists. Or anomalous information could show up errors in excavation such as ""undercutting"". Dating methodology in part relies on accurate excavation and in this sense the two activities become interdependent.
"
Residential Roofing Services,"Domestic roof construction is the framing and roof covering which is found on most detached houses in cold and temperate climates.[1] Such roofs are built with mostly timber, take a number of different shapes, and are covered with a variety of materials.

Modern timber roofs are mostly framed with pairs of common rafters or prefabricated wooden trusses fastened together with truss connector plates. Timber framed and historic buildings may be framed with principal rafters or timber roof trusses. Roofs are also designated as warm or cold roof depending on the way they are designed and built with regard to thermal building insulation and ventilation. The steepness or roof pitch of a sloped roof is determined primarily by the roof covering material and aesthetic design. Flat roofs actually slope up to approximately ten degrees to shed water. Flat roofs on houses are primarily found in arid regions.[2]

In high-wind areas, such as where a cyclone or hurricane may make landfall, the main engineering consideration is to hold the roof down during severe storms. Every component of the roof, as of course the rest of the structure, has to withstand the uplift forces of high wind speeds. This is accomplished by using metal ties fastened to each rafter or truss. This is not normally a problem in areas not prone to high wind or extreme weather conditions.

In the UK, a concrete tiled roof would normally have rafters at 600 mm (24 in) centers, roof battens at 300 mm (12 in) centers and ceiling joists  at 400 mm (16 in) centers. The United States still uses imperial units of measurement and framing members are typically spaced sixteen or twenty-four inches apart.

The roof framing may be interrupted for openings such as a chimney or skylight. Chimneys are typically built with a water diverter known as a cricket or saddle above the chimney. Flashing is used to seal the gap between the chimney and roofing material.

A simple rafter roof consists of rafters that the rafter foot rest on horizontal wall plates on top of each wall.[3] The top ends of the rafters often meet at a ridge beam, but may butt directly to another rafter to form a pair of rafters called a couple. Depending on the roof covering material, either horizontal laths, battens, or purlins are fixed to the rafters; or boards, plywood, or oriented strand board form the roof deck (also called the sheeting or sheathing) to support the roof covering. Heavier under purlins or purlin plates are used to support longer rafter spans. Tie beams, which may also serve as ceiling joists, are typically connected between the lower ends of opposite rafters to prevent them from spreading and forcing the walls apart. Collar beams or collar ties may be fixed higher up between opposite rafters for extra strength. The rafters, tie beams and plates serve to transmit the weight of the roof to the walls of the building

Pre-manufactured roof trusses come in a wide variety of styles. They are designed by the manufacturer for each specific building.

Timber trusses also are built in a variety of styles using wood or metal joints. Heavy timber rafters typically spaced 240 cm (8 ft) to 370 cm (12 ft) apart are called principal rafters. Principal rafters may be mixed with common rafters or carry common purlins.[citation needed]

Roof framing must be designed to hold up a structural load including what is called dead load, its own weight and the weight of the roof covering, and additional loading called the environmental load such as snow and wind. Flat roofs may also need to be designed for live loads if people can walk on them. In the United States, building codes specify the loads in pounds per square foot which vary by region. The load and span (distance between supports) defines the size and spacing of the rafters and trusses.[4]

The roofing material, including underlayment and roof covering, is primarily designed to shed water. The covering is also a major element of the architecture, so roofing materials come in a wide variety of colors and textures. The primary roof covering[5] on houses in North America are asphalt shingles, but some have other types of roof shingles or metal roofs. Tile and thatch roofs are more common in Europe than North America. Some roofing materials help reduce air conditioning costs in hot climates by being designed to reflect light.

Asphalt shingles is the most used roofing material in North America, making up as much as 75% of all steep slope roofs. This type of material is also gaining popularity in Europe due to lower installation costs. Asphalt shingles dominate the North American residential roofing market, because they are typically less expensive[6] compared to other materials.

In the southern US and Mexico, clay tile roofs are also very popular, due to their longevity, and ability to withstand hurricane winds with minimal or no damage.

In Europe, slate and tile roofs are very popular. Many slate roofs in Europe are over 100 years old, and typically require minimal maintenance / repairs.

Roof space ventilation is needed to regulate the temperature within the roof space. Without proper ventilation, humidity can cause interstitial condensation within the roof fabric; this can lead to serious structural damage, wet or dry rot, and can ruin insulation.

Condensation within the roof space is much more of a problem today due to: much less fortuitous ventilation due to tighter building envelopes with high performance windows and door and no chimneys leading. This tighter envelope means the air temperature in buildings has risen, the warmer the air in the building is, the more water vapour the air can carry.

As the occupied part of the building has become warmer, the roof space has become colder, with high performance insulation and roofing membranes leading to a cold roof space.

When the warm, moist air from below rises into the cold roof space, condensation begins as the air temperature drops to the ‘dew point’ or as the warm air comes into contact with any of the cold surfaces in the roof.


Most building materials are permeable to water vapour; brick, concrete, plaster, wood and insulation all can fall victim to interstitial condensation. This is why UK Building Regulations require roofs to be ventilated, either by the use of soffit vents, ridge vents, or replacement ventilation slates or tiles.[7]
Ventilation of the roof deck speeds the evaporation of water from leakage or condensation and removes heat which helps prevent ice dams and helps asphalt shingles last longer. Building codes in the U.S. specify ventilation rates as a minimum of 1 sq ft (0.093 m2) of opening per 150 sq ft (14 m2) (1:150) with a ratio of 1:300 in some conditions.[8] Warm air rises, so ceiling insulation is designed to have a higher r-value and the insulation is often installed between the ceiling joists or rafters. A properly insulated and ventilated roof is called a cold roof. A warm roof is a roof that is not ventilated,[9] where the insulation is placed in line with the roof pitch.[10] A hot roof is a roof designed not to have any ventilation and has enough air-impermeable insulation in contact with the sheathing to prevent condensation[11] such as when spray foam insulation is applied directly to the under-side or top-side of the roof deck or in some cathedral ceilings.[12]

A more recent design is the installation of a roof deck with foil-backed foam along with a second deck that is air-gapped away from the foil-backed foam to allow air to flow vertically to a ventilation outlet at the peak of the roof—it is a double-deck design with an air gap. This design improves efficiency.[13]

In the Near East, from Roman times, throughout the Middle Ages and up to the start of the 20th-century, roofs of houses were constructed in the following manner: They first arranged the wooden rafters over the walls, at a distance of about 60 cm between each rafter; over which rafters they laid down horizontally thin strips of wood, or boards (purlins), in close proximity to each other, each board having a thickness of 3 finger-breadths (6.75 cm), or a little less.[14] Over these boards they then firmly secured in place a matting material made of natural plant fibers woven together, such as cane reeds, leaves of rush or papyrus, palm fronds and tree bark.[15] Over this matting material they first placed a thick layer of clay, followed by a layer of soil having a thickness of 20 centimetres (7.9 in), after which they daubed the top with a clay aggregate consisting of a cement mixture of clay, rubble, crushed potsherds and straw.[15][14][16] This type of roofing material is well-equipped to withstand wetness and rain downpours.[14]

In Sanaa, Yemen, in the early 20th-century, twisted mats of palm-leaves were laid over wooden beams used to support the ceilings.[17] These mats were held in place by wooden sticks laid at right angles to the beams.[17] Above these crossing sticks, bushes were spread, and these were again covered with mats. The wooden beams and the mats were overlaid with a very hard lime or mortar coating.[17] The thickness of the ceiling-wall seldom exceeded 30 centimetres (12 in), excluding the beams.[17]

In ancient Gaul, Spain, Portugal and Aquitaine, houses were constructed with peaked roofs daubed with lumps of dried mud and covered with reeds and leaves, while other roofs made with oak shingles or thatched.[18] The Colchians in Pontus made their roofs by ""cutting away the ends of crossbeams, and [by] making them converge gradually as they lay them across, they bring them up to the top from the four sides in the shape of a pyramid. They [then] cover it with leaves and mud.""[18] In Marseilles, during classical antiquity, tiles were not used on the roofs of houses, but rather only earth mixed with straw.[18] The ancient Phrygians built a pyramidal roof of logs fastened together, and covering them over with reeds and brushwood, upon which was heaped up very high mounds of earth.[18]
"
Roofing Services with Heat Application,"A roofer, roof mechanic, or roofing contractor is a tradesman who specializes in roof construction. Roofers replace, repair, and install the roofs of buildings, using a variety of materials, including shingles, single-ply, bitumen, and metal. Roofing work includes the hoisting, storage, application, and removal of roofing materials and equipment, including related insulation, sheet metal, vapor barrier work, and green technologies rooftop jobs such as vegetative roofs, rainwater harvesting systems, and photovoltaic products, such as solar shingles and solar tiles.[1][2]

Roofing work can be physically demanding because it may involve heavy lifting, climbing, bending, and kneeling, often in extreme weather conditions.[1] Roofers are also vulnerable to falls from heights due to working at elevated heights. Various protective measures are required in many countries.  In the United States these requirement are established by the Occupational Safety and Health Administration (OSHA) to address this concern.[3][4][5] Several resources from occupational health agencies are available on implementing the required and other recommended interventions.[6][7][8]

According to data from the U.S. Bureau of Labor Statistics (BLS), as of May 2022[update], there were 129,300 individuals working as roofers in the construction industry. Among that population, a majority of roofers (93%; 119,800) were contractors for Foundation, Structure, and Building Exterior projects.[9][10] In terms of jobs outlook, it is predicted that there will only be a 2% increase in job growth from 2022 to 2032 in the United States.   Approximately 12,200 openings are expected each year in this decade. Most of the new jobs are likely to be offered to replace roofers who retire or transition out of the trade.[1]

In Australia, this type of carpenter is called a roof carpenter and the term roofer refers to someone who installs the roof cladding (tiles, tin, etc.). The number of roofers in Australia was estimated to be approximately 15,000. New South Wales is the largest province with an 29% market share in the Australian Roofers industry (4,425 companies). Second is Victoria with 3,206 Roofers (21%).[11]

In the United States and Canada, they're often referred to as roofing contractors or roofing professionals. The most common roofing material in the United States is asphalt shingles. In the past, 3-tab shingles were used, but recent trends show ""architectural"" or ""dimensional"" shingles  becoming very popular.[12]

Depending on the region, other commonly applied roofing materials installed by roofers include concrete tiles, clay tiles, natural or synthetic slate, single-ply (primarily EPDM rubber, PVC, or TPO), rubber shingles (made from recycled tires), glass, metal panels or shingles, wood shakes or shingles, liquid-applied, hot asphalt/rubber, foam, thatch, and solar tiles. ""Living roof"" systems, or rooftop landscapes, have become increasingly common in recent years in both residential and commercial applications.[13][14]

Roles and responsibilities of roofing professionals include:[1]

Beyond having common duties such as replacing, repairing, or installing roofs for buildings, roofers can also be involved in other tasks, including but is not limited to:

Roofing is one of the most dangerous professions among construction occupations since it involves working at heights and exposes workers to dangerous weather conditions such as extreme heat.[15]  In the United States as of 2017, the rate of fatalities from falls among roofers is 36 deaths per 100,000 full-time employees, ten times greater than all construction-related professions combined.[16] In the United States, the fatal injury rate in 2021 was 59.0 per 100,000 full-time roofers, compared to the national average of 3.6 per 100,000 full-time employees.[17]  According to the U.S. Bureau of Labor Statistics, roofing has been within the top 5 highest death rates of any profession for over 10 years in a row.[18]  For Hispanic roofers, data from 2001–2008 show fatal injuries from falls account for nearly 80% of deaths in this population, the highest cause of death among Hispanics of any construction trade.[19][20]

A major contributing factor to the high fatality rates among roofers in the United States is the nature of the craft which requires roofers to work on elevated, slanted roof surfaces. Findings from qualitative interviews with Michigan roofing contractors also found hand and finger injuries from handling heavy material and back injuries to be some of the more common task/injury combinations.[21]

Ladder falls contribute to the rates of injury and mortality.  More than half a million people per year are treated for fall from ladder and over 3000 people die as a result.[22]  In 2014 the estimated cost annual cost of ladder injuries, including time away from work, medical, legal, liability expenses was estimated to reach $24 billion.[22] Male, Hispanic, older, self-employed workers and those who work in smaller establishments, and work doing construction, maintenance, and repair experience higher ladder fall injury rates when compared with women and non-Hispanic whites and persons of other races/ethnicities.[23]

Ladders allow for roofers to access upper level work surfaces. For safe use, ladder must be inspected for damage by a competent person and must be used on stable and level surfaces unless they are secured to prevent displacement.[3]

Nearly every industrialized country has established specific safety regulations for work on the roof, ranging from the use of conventional fall protection systems including personal fall arrest systems, guardrail systems, and safety nets.

The European Agency for Safety and Health at Work describes scenarios of risk (fall prevention, falling materials, types of roofs), precautions, training needed and European legislation focused on roof work.[6] European directives set minimum standards for health and safety and are transposed into law in all Member States.

In the United States, OSHA standards require employers to have several means of fall protection available to ensure the safety of workers. In construction, this applies to workers who are exposed to falls of 6 feet or more above lower levels.[3][24] In the United States, regulation of the roofing trade is left up to individual states. Some states leave roofing regulation up to city-level, county-level, and municipal-level jurisdictions. Unlicensed contracting of projects worth over a set threshold may result in stiff fines or even time in prison. In some states, roofers are required to meet insurance and roofing license guidelines. Roofers are also required to display their license number on their marketing material.

Canada's rules are very similar to those from the U.S., and regulatory authority depends on where the business is located and fall under the authority of their local province.

In 2009, in response to high rates of falls in constructions the Japanese Occupational Safety and Health Regulations and Guidelines amended their specific regulations. In 2013 compliance was low and the need for further research and countermeasures for preventing falls and ensuring fall protection from heights was identified.[25]

The United Kingdom has no legislation in place that requires a roofer to have a license to trade, although some do belong to recognized trade organizations.[26]

The purpose of a PFAS is to halt a fall and prevent the worker from making bodily contact with a surface below.  The PFAS consists of an anchorage, connectors, body harness and may include a lanyard, deceleration device, lifeline or suitable combination of these.

Beyond these mandatory components of the PFAS, there are also specific fall distances associated with the functioning of the arrest system. Specifically, there is a total fall distance that the PFAS must allow for to assist the worker in avoiding contact with the ground or other surface below. The total fall distance consists of free fall distance, deceleration distance, D-ring shift, Back D-ring height, and Safety margin. In addition to the fall distance requirements for each component of the PFAS, the anchorage of the PFAS must also be able to support a minimum 5,000 pounds per worker.[4]

OSHA regulations have several requirements.  The free fall distance, to the distance that the worker drops before the PFAS begins to work and slows the speed of the fall,  must be 6 feet or less, nor contact any lower level.  The deceleration, the length that the lanyard must stretch in order to arrest the fall must be no more than 3.5 feet.[4]  The D-ring shift, the distance that the harness stretches and how far the D-ring itself moves when it encounters the full weight of the worker during a fall, is generally assumed to be 1 foot, depending on the equipment design and the manufacturer of the harness.  For the back D-ring height, the distance between the D-ring and the sole of the worker's footwear, employers often use 5 feet as the standard height with the assumption that the worker will be 6 feet in height, but because the D-ring height variability can affect the safety of the system, the back D-ring height must be calculated based on the actual height of the worker.  The safety margin, the additional distance that is needed to ensure sufficient clearance between the worker and the surface beneath the worker after a fall occurs, is generally considered to be a minimum of 2 feet.[3]

A fall restraint system is a type of fall protection system where, the goal is to stop workers from reaching the unprotected sides or edges of a working area in which a fall can subsequently occur. This system is useful where a worker may lose their footing near an unprotected edge or begin sliding. In such a case, the fall restraint system will restrain further movement of the worker toward the unprotected side or edge and prevent a serious fall. Although fall restraint systems are not explicitly defined or mentioned in OSHA's fall protection standards for construction,[24][4] they are allowed by OSHA as specified in an OSHA letter of interpretation last updated in 2004.[27] OSHA does not have any specific requirements for fall restraint systems, but recommends that any fall restraint system be capable of withstanding 3,000 pounds or at least twice the maximum predicted force necessary to save the worker from falling to the lower surface.[3] There are no OSHA specifications on the distance from the edge the restraint system must allow for a falling worker, and although a likely very dangerous practice, the OSHA letter of interpretation states that as long as the restraint system prevents the employee from falling off an edge, the employee can be restrained to ""within inches of the edge.""[27]

Guardrail systems serve as an alternative to PFAS and fall restraint systems by having permanent or temporary guardrails around the perimeter of the roof and any roof openings. OSHA requires the height of the top of the rail to be 39-45 inches above the working surface. Mid-rails must be installed midway between the top of the top rail and the walking/working surface when there is no parapet wall at least 21 inches high. Guardrail systems must be capable of withstanding 200-pounds of force in any outward or downward direction applied within 2 inches of the top edge of the rail.[3][24]

Safety net systems use a tested safety net adjacent to and below the edge of the walking/working surface to catch a worker who may fall off the roof. Safety nets must be installed as close as practicable under the surface where the work is being performed and shall extend outward from the outermost projection of the work surface as follows:[4]

[4]

Safety nets must be drop-tested with a 400-pound bag of sand, or submit a certification record prior to its initial use.[4]

Warning lines systems consist of ropes, wires, or chains which are marked every 6 feet with high-visibility material, and must be supported in such a way so that it is between 34 and 39 inches above the walking/working surface.[4] Warning lines are passive systems that allow for a perimeter to be formed around the working area so that workers are aware of dangerous edges. Warning lines are only permitted on roofs with a low slope (having a slope of less than or equal to 4 inches of vertical rise for every 12 inches horizontal length (4:12)).[28] In the context of roofing fall protection, warning line systems may only be used in combination with a guardrail system, a safety net system, a personal fall arrest system, or a safety monitoring system. The warning line system must be erected around all sides of the roof work area.[4]

Safety monitoring systems use safety monitors to monitor the safety of other workers on the roof. Safety monitors must be competent to recognize fall hazards. The safety monitor is tasked to ensure the safety of other workers on the roof and must be able to orally warn an employee when they are in an unsafe situation.[4]

Multi-layered approaches to fall prevention and protection that use the hierarchy of controls can help to prevent fall injuries, incidents, and fatalities in the roofing industry.[7][8] The hierarchy of controls is a way of determining which actions will best control exposures. The hierarchy of controls has five levels of actions to reduce or remove hazards – elimination, substitution, and engineering controls are among the preferred preventive actions based on general effectiveness.

Resources are available to assist with the implementation of fall safety measures in the roofing industry such as fall prevention plans,[23][29] a ladder safety mobile application,[30] infographics and tipsheets,[31] toolbox talks,[32] videos and webinars,[1] and safety leadership training.[2] Many of these resources are available in Spanish and additional languages other than English.  The recommended safety measures are described next.

In terms of job outlooks, it is predicted that there will only be an 1% increase in job growth from 2021 to 2032. The job openings (15,000) are expected to replace roofers who will retire or transition out of the trade.[9]

Solar Roof installation is one of the fastest growing trends in the roofing industry due to the nature of solar roofs being environmentally friendly and a worthwhile economic investment. Specifically, solar roofs have been found to allow homeowners to potentially save 40-70% on electric bills depending on the number of tiles installed.[33] The US federal government has also begun incentivizing homeowners to install solar roofs with potential eligibility for 30% tax credit on the cost of a solar system based on federal income taxes.[34]

Across 14 researched markets, roofing contracting companies have reported that they have received more frequent calls regarding potential metal roof installations. For instance, one company used to receive 5-6 calls in total regarding metal installations but recently, they have received 5-6 calls weekly for inquiries regarding metal roof installations.[35]
"
Waterproofing Services,"Waterproofing is the process of making an object, person or structure waterproof or water-resistant so that it remains relatively unaffected by water or resists the ingress of water under specified conditions. Such items may be used in wet environments or underwater to specified depths.

Water-resistant and waterproof often refer to resistance to penetration of water in its liquid state and possibly under pressure, whereas damp proof refers to resistance to humidity or dampness. Permeation of water vapour through a material or structure is reported as a moisture vapor transmission rate (MVTR).

The hulls of boats and ships were once waterproofed by applying tar or pitch. Modern items may be waterproofed by applying water-repellent coatings or by sealing seams with gaskets or o-rings.

Waterproofing is used in reference to building structures (such as basements, decks, or wet areas), watercraft, canvas, clothing (raincoats or waders), electronic devices and paper packaging (such as cartons for liquids).

In construction, a building or structure is waterproofed with the use of membranes and coatings to protect contents and structural integrity. The waterproofing of the building envelope in construction specifications is listed under 07 - Thermal and Moisture Protection within MasterFormat 2004, by the Construction Specifications Institute, and includes roofing and waterproofing materials.[citation needed]

In building construction, waterproofing is a fundamental aspect of creating a building envelope, which is a controlled environment. The roof covering materials, siding, foundations, and all of the various penetrations through these surfaces must be water-resistant and sometimes waterproof. Roofing materials are generally designed to be water-resistant and shed water from a sloping roof, but in some conditions, such as ice damming and on flat roofs, the roofing must be waterproof. Many types of waterproof membrane systems are available, including felt paper or tar paper with asphalt or tar to make a built-up roof, other bituminous waterproofing, ethylene propylene diene monomer EPDM rubber, hypalon, polyvinyl chloride, liquid roofing, and more.

Walls are not subjected to standing water, and the water-resistant membranes used as housewraps are designed to be porous enough to let moisture escape. Walls also have vapor barriers or air barriers. Damp proofing is another aspect of waterproofing. Masonry walls are built with a damp-proof course to prevent rising damp, and the concrete in foundations needs to be damp-proofed or waterproofed with a liquid coating, basement waterproofing membrane (even under the concrete slab floor where polyethylene sheeting is commonly used), or an additive to the concrete.

Within the waterproofing industry, below-ground waterproofing is generally divided into two areas:

In buildings using earth sheltering, too much humidity can be a potential problem, so waterproofing is critical. Water seepage can lead to mold growth, causing significant damage and air quality issues. Properly waterproofing foundation walls is required to prevent deterioration and seepage.

Another specialized area of waterproofing is rooftop decks and balconies. Waterproofing systems have become quite sophisticated and are a very specialized area. Failed waterproof decks, whether made of polymer or tile, are one of the leading causes of water damage to building structures and personal injury when they fail. Major problems occur in the construction industry when improper products are used for the wrong application. While the term waterproof is used for many products, each of them has a very specific area of application, and when manufacturer specifications and installation procedures are not followed, the consequences can be severe. Another factor is the impact of expansion and contraction on waterproofing systems for decks. Decks constantly move with changes in temperatures, putting stress on the waterproofing systems. One of the leading causes of waterproof deck system failures is the movement of underlying substrates (plywood) that causes too much stress on the membranes, failing the system. While beyond the scope of this reference document, waterproofing of decks and balconies is a complex of many complimentary elements. These include the waterproofing membrane used, adequate slope-drainage, proper flashing details, and proper construction materials.

The penetrations through a building envelope must be built in a way such that water does not enter the building, such as using flashing and special fittings for pipes, vents, wires, etc. Some caulkings are durable, but many are unreliable for waterproofing.

Also, many types of geomembranes are available to control water, gases, or pollution.

From the late 1990s to the 2010s, the construction industry has had technological advances in waterproofing materials, including integral waterproofing systems and more advanced membrane materials. Integral systems such as hycrete work within the matrix of a concrete structure, giving the concrete itself a waterproof quality. There are two main types of integral waterproofing systems: the hydrophilic and the hydrophobic systems. A hydrophilic system typically uses a crystallization technology that replaces the water in the concrete with insoluble crystals. Various brands available in the market claim similar properties, but not all can react with a wide range of cement hydration by-products and thus require caution. Hydrophobic systems use concrete sealers or even fatty acids to block pores within the concrete, preventing water passage.

Sometimes, the same materials used to keep water out of buildings are used to keep water in, such as a pool or pond liners.

New membrane materials seek to overcome shortcomings in older methods like polyvinyl chloride (PVC) and high-density polyethylene (HDPE). Generally, new technology in waterproof membranes relies on polymer-based materials that are very adhesive to create a seamless barrier around the outside of a structure.

Waterproofing should not be confused with roofing, since roofing cannot necessarily withstand hydrostatic head while waterproofing can.

The standards for waterproofing bathrooms in domestic construction have improved over the years, due in large part to the general tightening of building codes.

Some garments, and tents, are designed to give greater or lesser protection against rain. For urban use, raincoats and jackets are used; for outdoor activities in rough weather, there is a range of hiking apparel. Typical descriptions are ""showerproof"", ""water resistant"", and ""waterproof"".[1] These terms are not precisely defined. A showerproof garment will usually be treated with a water-resisting coating but is not rated to resist a specific hydrostatic head. This is suitable for protection against light rain, but after a short time, water will penetrate. A water-resistant garment is similar, perhaps slightly more resistant to waste,r but also not rated to resist a specific hydrostatic head. A garment described as waterproof will have a water-repellent coating, with the seams also taped to prevent water ingress there. Better waterproof garments have a membrane lining designed to keep water out but allow trapped moisture to escape (""breathability"")—a totally waterproof garment would retain body sweat and become clammy. Waterproof garments specify their hydrostatic rating, ranging from 1,500 for light rain to 20,000 for heavy rain.

Waterproof garments are intended for use in weather conditions which are often windy as well as wet and are usually also wind resistant.

Footwear can also be made waterproof by using a variety of methods, including but not limited to, the application of beeswax, waterproofing spray, or mink oil.[2]

Waterproofing methods have been implemented in many types of objects, including paper packaging, cosmetics, and, more recently, consumer electronics. Electronic devices used in military and severe commercial environments are routinely conformally coated in accordance with IPC-CC-830 to resist moisture and corrosion, but encapsulation is needed to become truly waterproof. Even though it is possible to find waterproof wrapping or other types of protective cases for electronic devices, a new technology enabled the release of diverse waterproof smartphones and tablets in 2013.[3] This method is based on a special nanotechnology coating a thousand times thinner than a human hair which protects electronic equipment from damage due to the penetration of water. Several manufacturers use the nano coating method on their smartphones, tablets, and digital cameras.

A 2013 study found that nanotextured surfaces using cone forms produce highly water-repellent surfaces. These nanocone textures are superhydrophobic (extremely water-hating).[4][5]

Waterproof packaging or other types of protective cases for electronic devices can be found. A new technology enabled the release of various waterproof smartphones and tablets in 2013.[6]
A study from 2013 found that nano-textured surfaces using cone shapes produce highly water-repellent surfaces. These ""nanocone"" textures are superhydrophobic.[7][8]

 Media related to Waterproofing at Wikimedia Commons
"
Septic System Services,"A septic tank is an underground chamber made of concrete, fiberglass, or plastic through which domestic wastewater (sewage) flows for basic sewage treatment.[2] Settling and anaerobic digestion processes reduce solids and organics, but the treatment efficiency is only moderate (referred to as ""primary treatment"").[2] Septic tank systems are a type of simple onsite sewage facility. They can be used in areas that are not connected to a sewerage system, such as rural areas. The treated liquid effluent is commonly disposed in a septic drain field, which provides further treatment. Nonetheless, groundwater pollution may occur and is a problem.

The term ""septic"" refers to the anaerobic bacterial environment that develops in the tank that decomposes or mineralizes the waste discharged into the tank. Septic tanks can be coupled with other onsite wastewater treatment units such as biofilters or aerobic systems involving artificially forced aeration.[3]

The rate of accumulation of sludge—also called septage or fecal sludge—is faster than the rate of decomposition.[2] Therefore, the accumulated fecal sludge must be periodically removed, which is commonly done with a vacuum truck.[4]

A septic tank consists of one or more concrete or plastic tanks of between 4,500 and 7,500 litres (1,000 and 2,000 gallons); one end is connected to an inlet wastewater pipe and the other to a septic drain field. Generally these pipe connections are made with a T pipe, allowing liquid to enter and exit without disturbing any crust on the surface.[citation needed] Today, the design of the tank usually incorporates two chambers, each equipped with an access opening and cover, and separated by a dividing wall with openings located about midway between the floor and roof of the tank.

Wastewater enters the first chamber of the tank, allowing solids to settle and scum to float. The settled solids are anaerobically digested, reducing the volume of solids. The liquid component flows through the dividing wall into the second chamber, where further settlement takes place. One option for the effluent is the draining into the septic drain field, also referred to as a leach field, drain field or seepage field, depending upon locality. A percolation test is required prior to installation to ensure the porosity of the soil is adequate to serve as a drain field.[5][6]

Septic tank effluent can also be conveyed to a secondary treatment, typically constructed wetlands. Constructed wetlands benefit from the good performance of septic tanks at removing solids, which avoids them getting clogged quickly.  

Septic tank effluent can also be conveyed to a centralized treatment facility.  

The remaining impurities are trapped and eliminated in the soil, with the excess water eliminated through percolation into the soil, through evaporation, and by uptake through the root system of plants and eventual transpiration or entering groundwater or surface water. A piping network, often laid in a stone-filled trench (see weeping tile), distributes the wastewater throughout the field with multiple drainage holes in the network.  The size of the drain field is proportional to the volume of wastewater and inversely proportional to the porosity of the drainage field. The entire septic system can operate by gravity alone or, where topographic considerations require, with inclusion of a lift pump.  

Certain septic tank designs include siphons or other devices to increase the volume and velocity of outflow to the drainage field.  These help to fill the drainage pipe more evenly and extend the drainage field life by preventing premature clogging or bioclogging.

An Imhoff tank is a two-stage septic system where the sludge is digested in a separate tank.  This avoids mixing digested sludge with incoming sewage. Also, some septic tank designs have a second stage where the effluent from the anaerobic first stage is aerated before it drains into the seepage field.

A properly designed and normally operating septic system is odour-free. Besides periodic inspection and emptying, a septic tank should last for decades with minimal maintenance, with concrete, fibreglass, or plastic tanks lasting about 50 years.[7]

Waste that is not decomposed by the anaerobic digestion must eventually be removed from the septic tank. Otherwise the septic tank fills up and wastewater containing undecomposed material discharges directly to the drainage field.  Not only is this detrimental for the environment but, if the sludge overflows the septic tank into the leach field, it may clog the leach field piping or decrease the soil porosity itself, requiring expensive repairs.

When a septic tank is emptied, the accumulated sludge (septage, also known as fecal sludge[8]) is pumped out of the tank by a vacuum truck. How often the septic tank must be emptied depends on the volume of the tank relative to the input of solids, the amount of indigestible solids, and the ambient temperature (because anaerobic digestion occurs more efficiently at higher temperatures), as well as usage, system characteristics and the requirements of the relevant authority.

Some health authorities require tanks to be emptied at prescribed intervals, while others leave it up to the decision of an inspector. Some systems require pumping every few years or sooner, while others may be able to go 10–20 years between pumpings. An older system with an undersize tank that is being used by a large family will require much more frequent pumping than a new system used by only a few people. Anaerobic decomposition is rapidly restarted when the tank is refilled.[citation needed]

An empty tank may be damaged by hydrostatic pressure causing the tank to partially ""float"" out of the ground, especially in flood situations or very wet ground conditions.[9]

Another option is ""scheduled desludging"" of septic tanks which has been initiated in several Asian countries including the Philippines, Malaysia, Vietnam, Indonesia, and India.[10] In this process, every property is covered along a defined route and the property occupiers are informed in advance about desludging that will take place.

The maintenance of a septic system is often the responsibility of the resident or property owner. Some forms of abuse or neglect include the following:

Septic tank additives have been promoted by some manufacturers with the aim to improve the effluent quality from septic tanks, reduce sludge build-up and to reduce odors. These additives—which are commonly based on ""effective microorganisms""—are usually costly in the longer term and fail to live up to expectations.[14] It has been estimated that in the U.S. more than 1,200 septic system additives were available on the market in 2011.[15] Very little peer-reviewed and replicated field research exists regarding the efficacy of these biological septic tank additives.[15]

While a properly maintained and located septic tank poses no higher amount of environmental problems than centralized municipal sewage treatment,[16] certain problems could arise with a septic tank in an unsuitable location, and septic tank failures are typically more expensive to fix or replace than municipal sewer.[16] Since septic systems require large drainfields, they are unsuitable for densely built areas.

Some constituents of wastewater, especially sulfates, under the anaerobic conditions of septic tanks, are reduced to hydrogen sulfide, a pungent and toxic gas. Nitrates and organic nitrogen compounds can be reduced to ammonia. Because of the anaerobic conditions, fermentation and methanogenesis processes take place, which may generate carbon dioxide and/or methane. Both carbon dioxide and methane are greenhouse gases, with methane having a global warming potential about 25 times larger than carbon dioxide. This makes septic tanks potential greenhouse gas emitters. The same methane can be burnt to produce energy for local usage.[17]

Septic tanks by themselves are ineffective at removing nitrogen compounds that have potential to cause algal blooms in waterways into which affected water from a septic system finds its way. This can be remedied by using a nitrogen-reducing technology,[18] such as hybrid constructed wetlands, or by simply ensuring that the leach field is properly sited to prevent direct entry of effluent into bodies of water.[citation needed]

The fermentation processes cause the contents of a septic tank to be anaerobic with a low redox potential, which keeps phosphates in a soluble and, thus, mobilized form. Phosphates discharged from a septic tank into the environment can trigger prolific plant growth including algal blooms, which can also include blooms of potentially toxic cyanobacteria.

The soil's capacity to retain phosphorus is usually large enough to handle the load through a normal residential septic tank. An exception occurs when septic drain fields are located in sandy or coarser soils on property adjacent to a water body. Because of limited particle surface area, these soils can become saturated with phosphates. Phosphates will progress beyond the treatment area, posing a threat of eutrophication to surface waters.[19]

Diseases extremely dangerous to human contact such as E. coli and other coliform bacteria are often reported following failures of septic tanks.[20]

A properly functioning septic system, on the other hand, provides significant reduction of pathogens compared to direct discharge due to settling (in the tank) and soil absorption (in the drain field). Log reductions of 4–8 for coliform bacteria, 0–2 for viruses are achieved in the effluent. Parasitic worm eggs are also removed. Additional filters may be added to improve removal performance although they will need to be replaced periodically.[21]

In areas with high population density, groundwater pollution beyond acceptable limits may occur. Some small towns experience the costs of building very expensive centralized wastewater treatment systems because of this problem, due to the high cost of extended collection systems. To reduce residential development that might increase the demand to construct an expensive centralized sewerage system, building moratoriums and limitations on the subdivision of property are often imposed. Ensuring existing septic tanks are functioning properly can also be helpful for a limited time, but becomes less effective as a primary remediation strategy as population density increases.

In areas adjacent to water bodies with fish or shellfish intended for human consumption, improperly maintained and failing septic systems contribute to pollution levels that can force harvest restrictions and/or commercial or recreational harvest closures.

In the United States, the 2008 American Housing Survey indicated that about 20 percent of all households rely on septic tanks,[22] and that the overwhelming majority of systems are located in rural (50%) and suburban (47%) areas.[22] Indianapolis is one example of a large city where many of the city's neighborhoods still rely on separate septic systems.[23] In Europe, septic systems are generally limited to rural areas.[citation needed]

In the European Union the EN 12566 standard provides the general requirements for packaged and site assembled treatment plants used for domestic wastewater treatment.

Part 1 (EN 12566-1) is for septic tanks that are prefabricated or factory manufactured and made of polyethylene, glass reinforced polyester, polypropylene, PVC-U, steel or concrete. Part 4 (EN 12566-4) regulates septic tanks that are assembled on site from prefabricated kits, generally of concrete construction. Certified septic tanks of both types must pass a standardized hydraulic test to assess their ability to retain suspended solids within the system. Additionally, their structural adequacy in relevant ground conditions is assessed in terms of water-tightness, treatment efficiency, and structural behaviour.[24]

In France, about 4 million households (or 20% of the population) are using on-site wastewater disposal systems (l’assainissement non collectif),[25] including septic tanks (fosse septique). The legal framework for regulating the construction and maintenance of septic systems was introduced in 1992 and updated in 2009 and 2012 with the intent to establish the technical requirements applicable to individual sewerage systems.[26] Septic tanks in France are subject to inspection by SPANC (Service Public d’Assainissement Non Collectif), a professional body appointed by the respective local authorities to enforce wastewater collection laws, at least once in four years. Following the introduction of EN 12566, the discharge of effluent directly into ditches or watercourses is prohibited, unless the effluent meets prescribed standards.[27]

According to the Census of Ireland 2011, 27.5% of Irish households (i.e. about 440,000 households), with the majority in rural areas, use an individual septic tank.[28]

Following a European Court of Justice judgment made against Ireland in 2009 that deemed the country non-compliant with the Waste Framework Directive in relation to domestic wastewaters disposed of in the countryside, the Water Services (Amendment) Act 2012 was passed in order to regulate wastewater discharges from domestic sources that are not connected to the public sewer network and to provide arrangements for registration and inspection of existing individual domestic wastewater treatment systems.[29][30]

Additionally, a code of practice has been developed by the Environmental Protection Agency to regulate the planning and construction of new septic tanks, secondary treatment systems, septic drain fields and filter systems.[31] Direct discharge of septic tank effluent into groundwater is prohibited in Ireland, while the indirect discharge via unsaturated subsoil into groundwater, e.g. by means of a septic drain field, or the direct discharge into surface water is permissible in accordance with a Water Pollution Act license.[31] Registered septic tanks must be desludged by an authorized contractor at least once a year; the removed fecal sludge is disposed of, either to a managed municipal wastewater treatment facility or to agriculture provided that nutrient management regulations are met.[31]

Since 2015, only certain property owners in England and Wales with septic tanks or small packaged sewage treatment systems need to register their systems, and either apply for a permit or qualify for an exemption with the Environment Agency.[32] Permits need to be granted to systems that discharge more than a certain volume of effluent in a given time or that discharge effluent directly into sensitive areas (e.g., some groundwater protection zones).[33] In general, permits are not granted for new septic tanks that discharge directly into surface waters. A septic tank discharging into a watercourse must be replaced or upgraded by 1 January 2020 to a Sewage Treatment Plant (also called an Onsite sewage facility), or sooner if the property is sold before this date, or if the Environment Agency (EA) finds that it is causing pollution.

In Northern Ireland, the Department of the Environment must give permission for all wastewater discharges where it is proposed that the discharge will go to a waterway or soil infiltration system. The discharge consent will outline conditions relating to the quality and quantity of the discharge in order to ensure the receiving waterway or the underground aquifer can absorb the discharge.[34]

The Water Environment Regulations 2011 regulate the registration of septic tank systems in Scotland. Proof of registration is required when new properties are being developed or existing properties change ownership.[35]

In Australia, septic tank design and installation requirements are regulated by State Governments, through Departments of Health and Environmental Protection Agencies. Regulation may include Codes of Practice[36][37] and Legislation.[38] Regulatory requirements for the design and installation of septic tanks commonly references Australian Standards (1547 and 1546). Capacity requirements for septic tanks may be outlined within Codes of Practice, and can vary between states.

Mainly because of water leaching from the effluent drains of a lot of closely spaced septic systems,[39] many council districts (e.g. Sunshine Coast, Queensland) have banned septic systems, and require them to be replaced with much more expensive small-scale sewage treatment systems that actively pump air into the tank, producing an aerobic environment.[citation needed] Septic systems have to be replaced as part of any new building applications, regardless of how well the old system performed.[citation needed]

According to the US Environmental Protection Agency, in the United States it is the home owners' responsibility to maintain their septic systems.[40] Anyone who ignores this requirement will eventually experience costly repairs when solids escape the tank and clog the clarified liquid effluent disposal system.

In Washington, for example, a ""shellfish protection district"" or ""clean water district"" is a geographic service area designated by a county to protect water quality and tideland resources. The district provides a mechanism to generate local funds for water quality services to control non-point sources of pollution, such as septic system maintenance. The district also serves as an educational resource, calling attention to the pollution sources that threaten shellfish growing waters.[41]

The term ""septic tank"", or more usually ""septic"", is used in some parts of Britain as a slang term to refer to Americans,[42] from Cockney rhyming slang septic tank equalling yank.[43]  This is sometimes further shortened to ""seppo"" by Australians .[44]
"
Building Cleaning Services,"

Commercial cleaning companies are contracted to carry out cleaning jobs in a variety of premises. Commercial cleaners, often referred to as custodians or janitors, work in a many different types for building such as schools, banks, offices, etc. As a janitor or custodian, the main job duty is to disinfect/ pick up in the fast-paced environments that come with working in commercial buildings.
[1]

Commercial office cleaning companies use a wide variety of cleaning methods, chemicals, and equipment to facilitate and expedite the cleaning process.  The scope of work may include all internal, general and routine cleaning - including floors, tiles, partition walls, internal walls, suspended ceilings, lighting, furniture and cleaning, window cleaning, deep cleans of sanitary conveniences and washing facilities, kitchens and dining areas, consumables and feminine hygiene facilities as well as cleaning of telephones, IT, and other periodic cleaning as required. Essentially, everything involved with a commercial business, be it cleaning a property for a real estate agent, or cleaning the aftermath of a building project. Carpet cleaning though, even with regular vacuuming, needs hot water extraction applied every 18 to 24 months. External cleaning, litter picking, and removal of graffiti may also be incorporated.[2]

The two global cleaning industry associations, the British Institute of Cleaning Science (BICSc) and the International Sanitary Supply Association (ISSA), both publish standards for managers and operatives engaged in cleaning activities.[3]

Contracts often require the cleaning companies to provide consumables such as paper towels, toilet rolls, liquid soap, bin liners, etc.[citation needed]

The commercial cleaning industry is extremely competitive and employees tend to be at the lower end of the pay scale.The salary for working in the commercial cleaning industry rage from approximately $31,000 to $48,000, depending on the job title.[4] However, unionized workers may earn higher wages. Many commercial cleaning companies provide on-the-job training for all new employees due to the nonexistence of tertiary based courses for the cleaning industry. A trend in the cleaning industry is the elimination of the usage of more hazardous chemicals such as drain cleaners due to liability and environmental concerns. Individuals employed in commercial cleaning typically hold the job title of janitor, custodian, or day porter.[5] A study was conducted on hazardous chemicals and the effect they have on employees in the commercial cleaning field. Results from this study showed that the chemicals being used by these employees had respiratory, dermatological and many other negative issues associated with them.[6]

In Australia, the United States, and Europe[clarification needed], commercial cleaning companies are encouraged to screen all employees for evidence of a criminal background. In some countries, such as the United Kingdom, cleaners working in schools, children's care homes and childcare premises are required by law to undergo a criminal record check [7]
"
Fire Safety Equipment Services,"Fire safety is the set of practices intended to reduce destruction caused by fire. Fire safety measures include those that are intended to prevent the ignition of an uncontrolled fire and those that are used to limit the spread and impact of a fire.

Fire safety measures include those that are planned during the construction of a building or implemented in structures that are already standing and those that are taught or provided to occupants of the building.[citation needed]

Threats to fire safety are commonly referred to as fire hazards. A fire hazard may include a situation that increases the likelihood of a fire or may impede escape in the event a fire occurs.[citation needed]

Fire safety is often a component of building safety. Those who inspect buildings for violations of the Fire Code and go into schools to educate children on fire safety topics are Fire Department members known as Fire Prevention Officers. The Chief Fire Prevention Officer or Chief of Fire Prevention will normally train newcomers to the Fire Prevention Division and may also conduct inspections or make presentations.[1]

Fire safety policies apply at the construction of a building and throughout its operating life. Building codes are enacted by local, sub-national, or national governments to ensure such features as adequate fire exits, signage, and construction details such as fire stops and fire rated doors, windows, and walls. Fire safety is also an objective of electrical codes to prevent overheating of wiring or equipment, and to protect from ignition by electrical faults.[2]

Fire codes regulate such requirements as the maximum occupancy for buildings such as theatres or restaurants, for example. Fire codes may require portable fire extinguishers within a building, or may require permanently installed fire detection and suppression equipment such as a fire sprinkler system and a fire alarm system.

Local authorities charged with fire safety may conduct regular inspections for such items as usable fire exit and proper exit signage, functional fire extinguishers of the correct type in accessible places, and proper storage and handling of flammable materials. Depending on local regulations, a fire inspection may result in a notice of required action, or closing of a building until it can be put into compliance with fire code requirements.[citation needed]

Owners and managers of a building may implement additional fire policies.  For example, an industrial site may designate and train particular employees as a fire fighting force. Managers must ensure buildings comply with fire evacuation regulations, and that building features such as spray fireproofing remains undamaged. Fire policies may be in place to dictate training and awareness of occupants and users of the building to avoid obvious mistakes, such as the propping open of fire doors. Buildings, especially institutions such as schools, may conduct fire drills at regular intervals throughout the year.[citation needed]

Beyond individual buildings, other elements of fire safety policies may include technologies such as wood coatings,[3][4] education and prevention, preparedness measures, wildfire detection and suppression, and ensuring geographic coverage of local and sufficient fire extinguishing capacities.

Some common fire hazards are:[5]

In the United States, the fire code (also fire prevention code or fire safety code) is a model code adopted by the state or local jurisdiction and enforced by fire prevention officers within municipal fire departments. It is a set of rules prescribing minimum requirements to prevent fire and explosion hazards arising from storage, handling, or use of dangerous materials, or from other specific hazardous conditions.[6] It complements the building code. The fire code is aimed primarily at preventing fires, ensuring that necessary training and equipment will be on hand, and that the original design basis of the building, including the basic plan set out by the architect, is not compromised. The fire code also addresses inspection and maintenance requirements of various fire protection equipment in order to maintain optimal active fire protection and passive fire protection measures.

A typical fire safety code includes administrative sections about the rule-making and enforcement process, and substantive sections dealing with fire suppression equipment, particular hazards such as containers and transportation for combustible materials, and specific rules for hazardous occupancies, industrial processes, and exhibitions.[citation needed]

Sections may establish the requirements for obtaining permits and specific precautions required to remain in compliance with a permit. For example, a fireworks exhibition may require an application to be filed by a licensed pyrotechnician, providing the information necessary for the issuing authority to determine whether safety requirements can be met. Once a permit is issued, the same authority (or another delegated authority) may inspect the site and monitor safety during the exhibition, with the power to halt operations, when unapproved practices are seen or when unforeseen hazards arise.[citation needed]

Most U.S. fire departments have fire safety education programs.

Fire prevention programs may include distribution of smoke detectors, visiting schools to review key topics with the students and implementing nationally recognized programs such as NFPAS ""Risk Watch"" and ""Learn not to burn"".[7]

Other programs or props can be purchased by fire departments or community organizations. These are usually entertaining and designed to capture children's attention and relay important messages. Props include those that are mostly auditory, such as puppets and robots. The prop is visually stimulating but the safety message is only transmitted orally. Other props are more elaborate, access more senses and increase the learning factor. They mix audio messages and visual cues with hands-on interaction. Examples of these include mobile trailer safety houses and tabletop hazard house simulators. Some fire prevention software is also being developed to identify hazards in a home.[8]

All programs tend to mix messages of general injury prevention, safety, fire prevention, and escape in case of fire. In most cases the fire department representative is regarded as the expert and is expected to present information in a manner that is appropriate for each age group.

The US industry standard that outlines the recommended qualifications for fire safety educators is NFPA 1035: Standard for Professional Qualifications for Public Fire and Life Safety Educator, which includes the requirements for Fire and Life Safety Educator Levels I, II, and III; Public Information Officer; and Juvenile Firesetter Intervention Specialist Levels I and II.[9]

According to the United States Fire Administration, the very young and the elderly are considered to be ""at risk"" populations. These groups represent approximately 33% of the population.

Fire safety has been highlighted in relation to global supply chain management. Sedex, the Supplier Ethical Data Exchange, a collaborative platform for sharing ethical supply chain data,[10] and Verité, Inc., a Massachusetts-based supply chain investigatory NGO, issued a briefing in August 2013 which highlighted the significance of this issue.[11] The briefing referred to several major factory fires, including the 2012 Dhaka garment factory fire in the Tazreen Fashion factory and other examples of fires in Bangladesh, Pakistan[12] and elsewhere, compared the incidence of fire safety issues in a manufacturing context, and highlighted the need for buyers, suppliers and local fire safety enforcement agencies all to take action to improve fire safety within the supply chains for ready-made garments and other products. The briefing recommended that buyers seek greater visibility of fire safety and other risks across the supply chain and identify opportunities to improve standards: ""buyers can encourage change through more responsible and consistent practices"".[11]

A fire safety plan is required by all North American national, state and provincial fire codes based on building use or occupancy types. Generally, the owner of the building is responsible for the preparation of a fire safety plan. Buildings with elaborate emergency systems may require the assistance of a fire protection consultant. After the plan has been prepared, it must be submitted to the Chief Fire Official or authority having jurisdiction for approval. Once approved, the owner is responsible for implementing the fire safety plan and training all staff in their duties. It is also the owner's responsibility to ensure that all visitors and staff are informed of what to do in case of fire. During a fire emergency, a copy of the approved fire safety plan must be available for the responding fire department's use.

In the United Kingdom, a fire safety plan is called a fire risk assessment.[13]

Fire safety plans are a useful tool for fire fighters to have because they allow them to know critical information about a building that they may have to go into. Using this, fire fighters can locate and avoid potential dangers such as hazardous material (hazmat) storage areas and flammable chemicals. In addition to this, fire safety plans can also provide specialized information that, in the case of a hospital fire, can provide information about the location of things like the nuclear medicine ward.[14] In addition to this, fire safety plans also greatly improve the safety of fire fighters. According to FEMA, 16 percent of all fire fighter deaths in 2002 occurred due to a structural collapse or because the fire fighter got lost.[15] Fire safety plans can outline any possible structural hazards, as well as give the fire fighter knowledge of where he is in the building.

In North America alone, there are around 8 million buildings that legally require a fire safety plan, be it due to provincial or state law.[16] Not having a fire safety plan for buildings which fit the fire code occupancy type can result in a fine, and they are required for all buildings, such as commercial, industrial, assembly, etc.

As previously stated, a copy of the approved fire safety plan shall be available for the responding fire department. This, however, is not always the case. Up until now, all fire plans were stored in paper form in the fire department. The problem with this is that sorting and storing these plans is a challenge, and it is difficult for people to update their fire plans. As a result, only half of the required buildings have fire plans, and of those, only around 10 percent are up-to-date.[16] This problem has been solved through the introduction of digital fire plans. These fire plans are stored in a database and can be accessed wirelessly on site by firefighters and are much simpler for building owners to update.

Fire is one of the biggest threats to property with losses adding up to billions of dollars in damages every year. In 2019 alone, the total amount of property damage resulting from fire was $14.8 billion in the United States.[17] Insurance companies in the United States are not only responsible for financially covering fire loss but are also responsible for managing risk associated with it. Most commercial insurance companies hire a risk control specialist whose primary job is to survey property to ensure compliance with NFPA standards, assess the current risk level of the property, and make recommendations to reduce the probability of fire loss. Careers in property risk management continue to grow and have been projected to grow 4 to 8% from 2018 to 2028 in the United States.[18]
"
Testing and Inspection Services,"The testing, inspection and certification (TIC) sector consists of conformity assessment bodies who provide services ranging from auditing and inspection, to testing, verification, quality assurance and certification.  The sector consists of both in-house and outsourced services.

The International Organization for Standardization and the International Electrotechnical Commission, international standard-setting bodies composed of representatives from various national standards organizations, define the different testing, inspection and certification services in the international standard ISO 17000 series which includes ISO/IEC 17000:2020 conformity assessment -- vocabulary and general principles.[1]

The process of conformity assessment demonstrates whether a product, service, process, claim, system or person meets the relevant requirements.[2]

The history of testing, inspection and certification services spans back several centuries. In the late 19th century, following the advent of the Industrial Revolution and the considerable risks involved with high-pressure steam boilers, specialized institutions emerged across Europe which carried out periodical inspections of such vessels to assess their overall condition as a precautionary measure to avoid large and often deadly damages.[3] Likewise, several of today's leading TIC companies started as classification societies in the 18th and 19th century to provide information to shipping underwriters on the condition of ships and equipment.[4]

As a result of globalization, supply chains are becoming increasingly more complex. Outsourcing and rising end user quality expectations have resulted in a higher demand for independent TIC services. Businesses are aiming to ensure that products, infrastructures and processes meet the required standards and regulations in terms of quality, health and safety, environmental protection and social responsibility, reducing the risk of failure, accidents and disruption.[5] This includes such services as the testing and inspection of bulk carriers and their cargos possibly carrying commodities such as petroleum, grains or livestock.

Independent TIC companies also play a key role in aiding governments fulfill their mandate in protecting consumers against hazardous products. In the European Union, for example, under the ""new approach"" directives, certain product categories require assessment by accredited independent TIC companies, known as notified bodies.[6] In the United States, the Consumer Product Safety Improvement Act of 2008 stipulates that third-party testing and certification of certain products is mandatory prior to being placed on the market.
"
Single Family Residential Construction,"Home construction or residential construction is the process of constructing a house, apartment building, or similar residential building[1] generally referred to as a 'home' when giving consideration to the people who might now or someday reside there. Beginning with simple pre-historic shelters, home construction techniques have evolved to produce the vast multitude of living accommodations available today.  Different levels of wealth and power have warranted various sizes, luxuries, and even defenses in a ""home"".  Environmental considerations and cultural influences have created an immensely diverse collection of architectural styles, creating a wide array of possible structures for homes.

The cost of housing and access to it is often controlled by the modern realty trade, which frequently has a certain level of market force speculation. The level of economic activity in the home-construction section is reported as housing starts, though this is contrarily denominated in terms of distinct habitation units, rather than distinct construction efforts. 'Housing' is also the chosen term in the related concepts of housing tenure, affordable housing, and housing unit (aka dwelling). Four of the primary trades involved in home construction are carpenters, masons, electricians and plumbers, but there are many others as well.

Global access to homes is not consistent around the world, with many economies not providing adequate support for the right to housing. Sustainable Development Goal 11 includes a goal to create ""Adequate, safe, and affordable housing and basic services and upgrade slums"".[2] Based on current and  expected global population growth, UN habitat projects needing 96,000 new dwelling units built each day to meet global demands.[3] An important part of housing construction to meet this global demand, is upgrading and retrofitting existing buildings to provide adequate housing.

While homes may have originated in pre-history, there are many notable stages through which cultures pass to reach the current level of modernization.  Countries and communities throughout the world currently exhibit very diverse concepts of housing, at many different stages of home development.

Two methods for constructing a home can be distinguished: the method in which architects simply assume free choice of materials and parts, and the method in which reclaimed materials are used, and the house is thus during its entire construction a ""work in progress"" (meaning every single aspect of it is subject to change at any given time, depending on what materials are found).

The second method has been used throughout history, as materials have always been scarce.

In Britain, there is comparatively little demand for innovative homes produced through radically different production methods, materials, and components. Over the years, a combination of trade protectionism and technical-product conservatism all round has also stymied the growth of indigenous producers of housing products such as aluminum cladding and curtain walling, wall tiles, advanced specialist ironmongery, and structural steel.[4]

Civil Site Plans, Architectural Drawings and Specifications comprise the document set needed to construct a new home.  Specifications consist of a precise description of the materials to be used in construction.  Specifications are typically organized by each trade required to construct a home.

The modern family home has many more systems and facets of construction than one might initially believe.  With sufficient study, an average person can understand everything there is to know about any given phase of home construction.  The do it yourself (DIY) boom of the late twentieth century was due, in large part, to this fact. And an international proliferation of kitset home and prefabricated home suppliers, often consisting of components of Chinese origin has further increased supply and made DIY home building more prevalent.[5]

The process often starts with a planning stage in which plans are prepared by an architect and approved by the client and any regulatory authority.[6] Then the site is cleared, foundations are laid and trenches for connection to services such as sewerage, water, and electricity are established.  If the house is wooden-framed, a framework is constructed to support the boards, siding and roof. If the house is of brick construction, then courses of bricks are laid to construct the walls. Floors, beams and internal walls are constructed as the building develops, with plumbing and wiring for water and electricity being installed as appropriate. Once the main structure is complete, internal fitting with lights and other fitments is done, Decorate home and furnished with furniture, cupboards, carpets, curtains and other fittings.[7][8][better source needed]

To avoid running out of money, consider building your house in phases.[9] This phased approach allows homeowners to prioritize essential components of the house, such as the foundation, structure, and basic utilities, while deferring less critical elements to later phases. It provides the flexibility to pause construction temporarily, if necessary, and resume when funds become available.

The cost of building a house varies by country widely. According to data from the National Association of Realtors, the median cost of buying an existing single-family house in the United States is $274,600, whereas the average cost to build is $296,652.[10][11][when?]
Several different factors can impact the cost of building a house, including the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent.[12]
Some of the typical expenses involved in a site cost can be connections to services such as water, sewer, electricity, and gas; fences; retaining walls; site clearance (trees, roots, bushes); site survey; soil tests.[13]

According to  data from the U.S. Census and Bureau of Labor Statistics found the average floor area of a home in the United States has steadily increased over the past one hundred years, with an estimated 18.5 square foot increase in the average floor area per year. In 1920, the average floor area was 1,048 square feet (97.4 m2), which rose to 1,500 square feet (140 m2) by 1970 and today sits at around 2,261 square feet (210.1 m2).[14][15]

Some have criticized the housebuilding industry. Mass housebuilders can be risk averse, preferring cost-efficient building methods rather than adopting new technologies for improved building performance.[16] Traditional vernacular building methods that suit local conditions and climates can be dispensed with in favour of a generic 'cookie-cutter' housing type.[16]
"
Single Family Renovation Services,"
The concept of home improvement, home renovation or remodeling is the process of renovating, making improvements or making additions to one's home.[1] Home improvement can consist of projects that upgrade an existing home interior (such as electrical and plumbing), exterior (masonry, concrete, siding, roofing) or other improvements to the property (i.e. garden work or garage maintenance/additions). Home improvement projects can be carried out for a number of different reasons; personal preference and comfort, maintenance or repair work, making a home bigger by adding rooms/spaces, as a means of saving energy, or to improve safety.[2]

While ""home improvement"" often refers to building projects that alter the structure of an existing home, it can also include improvements to lawns, gardens, and outdoor structures, such as gazebos and garages. It also encompasses maintenance, repair, and general servicing tasks. Home improvement projects generally have one or more of the following goals:[citation needed]

Maintenance projects can include:

Additional living space may be added by:

Homeowners may reduce utility costs with:

The need to be safer or for better privacy or emergency management can be fulfilled with diversified measures which can be improved, maintained or added. Secret compartments and passages can also be conceived for privacy and security.

Home or residential renovation is an almost $300 billion industry in the United States,[5] and a $48 billion industry in Canada.[6][full citation needed] The average cost per project is $3,000 in the United States and $11,000–15,000 in Canada.

Professional home improvement is ancient and goes back to the beginning of recorded civilization. One example is Sergius Orata, who in the 1st century B.C. is said by the writer Vitruvius (in his famous book De architectura) to have invented the hypocaust. The hypocaust is an underfloor heating system that was used throughout the Roman Empire in villas of the wealthy. He is said to have become wealthy himself by buying villas at a low price, adding spas and his newly invented hypocaust, and reselling them at higher prices.[7]

Perhaps the most important or visible professionals in the renovation industry are renovation contractors or skilled trades. These are the builders that have specialized credentials, licensing and experience to perform renovation services in specific municipalities.

While there is a fairly large ""grey market"" of unlicensed companies, there are those that have membership in a reputable association and/or are accredited by a professional organization. Homeowners are recommended to perform checks such as verifying license and insurance and checking business references prior to hiring a contractor to work on their house.

Because interior renovation will touch the change of the internal structure of the house, ceiling construction, circuit configuration and partition walls, etc., such work related to the structure of the house, of course, also includes renovation of wallpaper posting, furniture settings, lighting, etc.

Aggregators are companies that bundle home improvement service offers and act as intermediary agency between service providers and customers.

Home improvement was popularized on television in 1979 with the premiere of This Old House starring Bob Vila on PBS. American cable channel HGTV features many do-it-yourself shows, as does sister channel DIY Network.[8]
Danny Lipford hosts and produces the nationally syndicated Today's Homeowner with Danny Lipford. Tom Kraeutler and Leslie Segrete co-host the nationally syndicated The Money Pit Home Improvement Radio Show.

Movies that poked fun at the difficulties involved include: Mr. Blandings Builds His Dream House (1948), starring Cary Grant and Myrna Loy; George Washington Slept Here (1942), featuring Jack Benny and Ann Sheridan; and The Money Pit (1986), with Tom Hanks and Shelley Long. The sitcom Home Improvement used the home improvement theme for comedic purposes.
"
Multi-Family Construction Services,"Multifamily residential, also known as multidwelling unit (MDU), is a classification of housing where multiple separate housing units for residential inhabitants are contained within one building or several buildings within one complex.[1] Units can be next to each other (side-by-side units), or stacked on top of each other (top and bottom units). Common forms include apartment building and condominium, where typically the units are owned individually rather than leased from a single building owner. Many intentional communities incorporate multifamily residences, such as in cohousing projects.[2]

Housing units in multifamily housing have greater per capita value than single family homes.[3] Multifamily housing has beneficial fiscal externalities, as their presence reduces property tax rates in the community.[3]

Before the Industrial Revolution, such examples were rare, existing only in historical urban centers. In Ancient Rome, these were called insulae, skyscrapers in Shibam,[4] malice houses in Madrid, and casbah in the Casbah of Algiers.
"
Apartment Renovation Services,"Renovation (also called remodeling) is the process of improving broken, damaged, or outdated structures. Renovations are typically done on either commercial or residential buildings. Additionally, renovation can refer to making something new, or bringing something back to life and can apply in social contexts. For example, a community can be renovated if it is strengthened and revived. It can also be restoring something to a former better state (as by cleaning, repairing, or rebuilding). Renovation is very common in some places. For example, there are more than twenty thousand home improvement projects every year in Hong Kong, affecting more than a million residents[1] (population of HK is around 7.5 million in 2023).[2]

The building renovation process can usually, depending on the extents of the renovation, be broken down into several phases. The phases are as follow.[3]

Projects involving renovation require not just flexibility, but a plan that had been agreed upon by multiple parties. The planning process will involve feedback from financial investors of the project, and from the designer. Part of planning will also entail the collection of data for the completion of the project and then the project plan will be revised and given consent before continuing with renovations.[6]

Technology has had a meaningful effect on the renovation process, increasing the significance and strength of the planning stage. The availability of free online design tools has improved visualization of the changes, at a fraction of the cost of hiring a professional designer. The decision regarding changes is also influenced by the purpose of basement renovation [1]. Depending on the significance of these changes a professional may be required, especially if any changes other than cosmetic work (paint or finishes) is required. Many local codes require a professional to complete work in the built environment such as structural changes, new walls, new plumbing, or many others. Doing these changes without hiring a professional can result in health effects, safety concerns, damages, fines, and increased cost due to having to hire a professional after self-work.[7] Most builders focus on building new homes, so renovating is typically a part-time activity for such tradespeople. The processes and services required for renovations are quite specific and, once plans are signed off, building a new home is relatively predictable. However, during renovation projects, flexibility is often required from renovation companies to respond to unexpected issues that arise. Renovations usually require all of the sub-trades that are needed for the construction of a new building.

In case of a so-called ""fix-and-flip"" (repair and resell) objective, an ROI (return on investment) can result from changes to fix a structural issue, to fix design flow yield,[8] or to use light and color to make rooms appear more spacious. Because interior renovation could change of the internal structure of the house, ceiling construction, circuit configuration and partition walls, etc., such work related to the structure of the house, of course, also includes renovation of wallpaper posting, furniture settings, lighting, etc often times an interior designer is required as well.[9]

Many people renovate homes to create a new appearance. Builders may renovate homes to enhance the home's value as a stable source of income.[11] Homeowners often renovate their homes to increase the re-sale value and to turn a profit when selling. Homeowners may also want to add renovations that make their home more energy efficient, green or sustainable.[12] Also, over time, a homeowner’s personal preferences and needs may change, and the home will be renovated for improved aesthetics, comfort, or functionality.

Other types of renovations also can be initiated for similar reasons. The user or owner of a building can change which can effect the needs or wants for the space prompting a renovation. This is becoming more popular as buildings owners are renting or leasing floors or sections of the buildings to companies which have different spacial requirements than the previous users causing needed renovation. Renovations can also occur as companies increase size which could lead to needed additional retail, office, or other types of spaces. Similarly to homes other building owners could also want renovations to increased value, make the building more energy efficient, green or sustainable, or to update the building.[13] Sometimes shopping centres or shops are renovated to raise rent later.

Wood is versatile and flexible, making it the easiest construction material for renovations, and wood buildings can be redesigned to suit changing needs. Few homeowners or professional remodelers possess the skill and equipment that is required to alter steel-frame or concrete structures.

When looking at embodied carbon in building materials wood is often labeled as the most sustainable. This is because it sequesters carbon which if certified sustainably sourced will significantly reduce embodied carbon of buildings. This makes it a low emitting choice for a building material for an overall building and for renovations.[14]

Forest certification verifies that wood products have been sourced from well-managed forests. Most certification programs provide online search options so that consumers can find certified products—the Certification Canada program includes a search option for all of the certification programs that are in use in Canada.[15]

In North America, most structures are demolished because of external forces such as zoning changes and rising land values. Additionally, buildings that cannot be modified to serve the functional needs of the occupants are subject to demolition. Very few buildings on the continent are demolished due to structural degradation.[16]

The Athena Institute surveyed 227 commercial and residential buildings that were demolished in St. Paul, Minnesota, between 2000 and mid-2003. Thirty percent of the buildings were less than 50 years old, and 6% were less than 25 years old. The four top reasons for demolition were ""area redevelopment"" (35%), ""building’s physical condition"" (31%), ""not suitable for anticipated use"" (22%), and ""fire damage"" (7%). Lack of maintenance was cited as the specific problem for 54 of the 70 buildings where physical condition was given as the reason for demolition.[16]

Plastics are extensively used in the construction and renovation industry.[17] Airborne microplastic dust is produced during renovation, building, bridge and  road reconstruction projects[18] and the use of power tools.[19] It is also generated by deterioration of building materials[20]

Materials containing polyvinyl chloride (PVC), polycarbonate, polypropylene, and acrylic, can degrade overtime releasing microplastics.[17] During the construction process single use plastic containers and wrappers are discarded adding to plastic waste.[21] These plastics are difficult to recycle and end up in landfills where they break down over a long period of time causing potential leaching into the soil and the release of airborne microplastics.[22][23] Efforts have been made to reduce plastic waste by adding it to concrete as agglomerates. However, one solution for resolving the problem from the large amount of plastic wastes generated could bring another serious problem of leaching of microplastics. The unknown part of this area is huge and needs prompt investigation.[24]

Around twenty percent of all plastics and seventy percent of all polyvinyl chloride (PVC) produced in the world each year are used by the construction industry.[25][26] It is predicted that much more will be produced and used in the future.[25] ""In Europe, approximately 20% of all plastics produced are used in the construction sector including different classes of plastics, waste and nanomaterials.""[26]

While the type of material used will determine the composition of the dust generated, the size and amount of particulates produced are mainly determined by the type of tool used. Implementation of effective dust control measures may also play a role.

Use of angle grinder is not preferred as large amounts of harmful sparks and fumes (and particulates) are generated when compared with using reciprocating saw or band saw.[27] Angle grinders produce sparks when cutting ferrous metals. They also produce shards cutting other materials. The blades themselves may also break. This is a great hazard to the face and eyes especially, as well as other parts of the body.[28]

Use of power tools can cause adverse effects on people living nearby. Power tools can produce large amounts of particulates including ultrafine particles.[29]

Particulates are the most harmful form (other than ultra-fines) of air pollution[30] There is no safe level of particulates.[31]

Many tasks create dust. High dust levels are caused by one of more the following:[32]

Examples of high dust level tasks include:[32]

Some power tools are equipped with dust collection system (e.g. HEPA vacuum cleaner) or integrated water delivery system which extract the dust after emission.[33][34]

In the US, “About 75% of homes built before 1978 contain some lead-based paint. The older the home the more likely it is to contain lead-based paint. You should assume that any home built before 1978 contains some lead.”[35]

In April 2010 the U.S. Environmental Protection Agency (EPA) required that all renovators working in homes built before 1978 and disturbing more than 6 square feet (0.56 m2) of lead paint inside the home or 20 square feet (1.9 m2) outside the home be certified. EPA's Lead Renovation, Repair and Painting Rule (RRP Rule) lowers the risk of lead contamination from home renovation activities.[36] It requires that firms performing renovation, repair, and painting projects that disturb lead-based paint in homes, child care facilities and pre-schools (any child occupied facility) built before 1978 be certified by EPA and use certified renovators who are trained by EPA-approved training providers to follow lead-safe work practices.[37]

Careful stabilization of any deteriorated (peeling, chipping, cracking, etc.) paint in a lead-safe manner is also encouraged. Through authority vested in the United States Department of Housing and Urban Development (HUD), lead-based-paint removal by dry scraping, dry sanding, torching and burning, the use of heat guns over 1100°F, and machine-sanding / grinding without a HEPA-filtered vacuum or a HEPA filtered dust collection system, is prohibited, as these methods have been proven to produce significant amount of lead dust during renovation, remodeling and painting.[38]

At the end of any remodeling or repainting job, a dust test performed by an independent third-party professional is also required by HUD for ""clearance"". Lead evaluations are done using a method called X-Ray fluorescence (XRF), which gives a result in 4–8 seconds with a 95% accuracy at the 2-sigma level.

As of 2018[update], there are an estimated 37 million homes and apartments with lead paint in the United States.[39]

Currently, worldwide 38% of emissions and 35% of energy use come from the building sector, including building construction and operation. This means renovations contribute to emissions and energy use of the building sector. These percentages are the largest portion of the total emissions and energy use globally.[40] This makes buildings have the highest potential for decreasing these percentages as well as the largest need to decrease them. Renovations are also one way to do this. 

Renovations decrease emissions as instead of demolishing a building just to build a new one the building is reused. Reuse of buildings is not always desirable as it is often pursued to have a building designed for the many individual and unique needs building owners have but it is not always a necessity. Renovations can take a building and make it completely different from the old building just reusing the structure, which is often the largest contributor of embodied carbon to a building. However, in order to be able to do this buildings need to be design durably and re-use. Designing for durability and reuse is designing for new buildings to be ""long lasting, use-adaptable, and culturally valuable""[14] to allow for the building to be kept for longer to minimize emissions from a complete rebuild. 

Having these ideas in mind while designing new buildings significantly increases the likelihood for renovations to happen.[14] Buildings are more likely to be torn down because they can not accommodate the new desired use then because the structure is failing.[16] Renovations allow old buildings to fit new needs in a way that outputs less emissions than a complete tear down and construction of a new building which is often a feasible option.

Renovation has several effects on economies, including:[41]
"
Restoration Services,"Disaster restoration refers to the process of repairing and restoring property damaged by natural disasters such as floods, hurricanes, wildfires, or earthquakes. It typically involves various services such as structural repairs and water damage restoration, fire damage restoration, mold remediation, and content restoration.

Water damage restoration begins with a preliminary inspection of the building to determine the safety of the structure, severity of the damage, and source of the water. Any standing water must then be pumped out of the structure so that the affected areas can be properly dried. Due to the threat of mold, items and surfaces have to be thoroughly sanitized, after which repairs can take place.[1] The process of disinfection is especially important here as all items involved can be affected. Therefore, proper protective equipment that covers your entire body is strongly recommended throughout the whole process. Other possible threats include household utilities like electricity and gas that can pose a serious threat in a flooded structure.[2]

Before entering any building exposed to fire damage, it is recommended to consult local officials such as the fire department or building inspectors to determine if it is safe. Fire damage in buildings is often accompanied by extensive water damage that occurs from the extinguishing process.[3] Aside from those relevant to water damage, smoke and soot are the primary concerns with fire damage restoration. These both pose a serious health risk so full body protective equipment is advised when working around it.[4] Assuming they are salvageable, any items damaged in a fire or exposed to the aftermath need to be thoroughly cleaned to avoid health hazards and further contamination with other objects.[3] Removing smoke odor can prove to be challenging and will often involve the use of chemicals such as detergents, bleach, and TSP.[4]

Mold poses a serious threat to anyone working around it due to its ability to spread in the air, with the skin, eyes, mouth, and lungs being most susceptible. As such, full body protective equipment is recommended when cleaning it up.[5] Additionally, those with preexisting respiratory conditions such as asthma or COPD should take extra precautions to avoid mold exposure.[6][7] Mold growth occurs most commonly due to water damage in buildings and can grow on any surface, including the backside of walls and ceiling tiles. Whether or not a material can be salvaged is largely determined by how porous it is. Non-porous materials such as glass are able to be fully cleaned while something such as drywall may prove impossible to salvage depending on exposure time. Semi-porous materials like wood can often be saved if properly dried and disinfected in a reasonable amount of time. When used safely, chemicals such as bleach and detergent are effective in removing mold. Extra safety precautions when cleaning up mold may include opening windows to increase ventilation, misting surfaces with water to prevent airborne spores, or storing contaminated items in an airtight container.[8]

The disaster restoration industry, encompassing services such as fire damage repair and mold remediation,[9] has experienced significant growth in recent decades due to a confluence of factors. Severe natural disasters, coupled with increasing development in disaster-prone areas, have created a steady demand for restoration services. While historically dominated by local family-owned businesses, the industry has witnessed a notable consolidation trend driven by private equity firms seeking to capitalize on its recession-proof nature.[10]

The global post-storm remediation market is projected to expand from $70 billion in 2024 to $92 billion by 2029, reflecting the enduring demand for restoration services in the face of climate change and other environmental challenges.[11]
"
Mobile Home Construction Services,"A mobile home (also known as a house trailer, park home, trailer, or trailer home) is a prefabricated structure, built in a factory on a permanently attached chassis before being transported to site (either by being towed or on a trailer). Used as permanent homes, or for holiday or temporary accommodation, they are often left permanently or semi-permanently in one place, but can be moved, and may be required to move from time to time for legal reasons.

Mobile homes share the same historic origins as travel trailers, but today the two are very different, with travel trailers being used primarily as temporary or vacation homes. Behind the cosmetic work fitted at installation to hide the base, mobile homes have strong trailer frames, axles, wheels, and tow-hitches.

In the United States, this form of housing goes back to the early years of cars and motorized highway travel.[1] It was derived from the travel trailer (often referred to during the early years as ""house trailers"" or ""trailer coaches""), a small unit with wheels attached permanently, often used for camping or extended travel. 
The original rationale for this type of housing was its mobility. Units were initially marketed primarily to people whose lifestyle required mobility. However, in the 1950s, the homes began to be marketed primarily as an inexpensive form of housing designed to be set up and left in a location for long periods of time or even permanently installed with a masonry foundation. Previously, units had been eight feet or fewer in width, but in 1956, the 10-foot (3.0 m) wide home (""ten-wide"") was introduced, along with the new term ""mobile home"".[2]

The homes were given a rectangular shape, made from pre-painted aluminum panels, rather than the streamlined shape of travel trailers, which were usually painted after assembly. All of this helped increase the difference between these homes and home/travel trailers. The smaller, ""eight-wide"" units could be moved simply with a car, but the larger, wider units (""ten-wide"", and, later, ""twelve-wide"") usually required the services of a professional trucking company, and, often, a special moving permit from a state highway department. During the late 1960s and early 1970s, the homes were made even longer and wider, making the mobility of the units more difficult. Nowadays, when a factory-built home is moved to a location, it is usually kept there permanently and the mobility of the units has considerably decreased. In some states, mobile homes have been taxed as personal property if the wheels remain attached, but as real estate if the wheels are removed. Removal of the tongue and axles may also be a requirement for real estate classification.

Mobile homes built in the United States since June 1976, legally referred to as manufactured homes, are required to meet FHA certification requirements and come with attached metal certification tags. Mobile homes permanently installed on owned land are rarely mortgageable, whereas FHA code manufactured homes are mortgageable through VA, FHA, and Fannie Mae.

Many people who could not afford a traditional site-built home, or did not desire to commit to spending a large sum of money on housing, began to see factory-built homes as a viable alternative for long-term housing needs. The units were often marketed as an alternative to apartment rental. However, the tendency of the units of this era to depreciate rapidly in resale value[citation needed] made using them as collateral for loans much riskier than traditional home loans. Terms were usually limited to less than the thirty-year term typical of the general home-loan market, and interest rates were considerably higher.[citation needed] In that way, mobile home loans resembled motor vehicle loans more than traditional home mortgage loans.

Mobile homes come in two major sizes, single-wides and double-wides. Single-wides are 18 feet (5.5 m) or less in width and 90 feet (27 m) or less in length and can be towed to their site as a single unit. Double-wides are 20 feet (6.1 m) or more wide and are 90 feet (27 m) in length or less and are towed to their site in two separate units, which are then joined. Triple-wides and even homes with four, five, or more units are also built but less frequently.

While site-built homes are rarely moved, single-wide owners often ""trade"" or sell their home to a dealer in the form of the reduction of the purchase of a new home. These ""used"" homes are either re-sold to new owners or to park owners who use them as inexpensive rental units. Single-wides are more likely to be traded than double-wides because removing them from the site is easier. In fact, only about 5% of all double-wides will ever be moved.[citation needed]

While an EF1 tornado might cause minor damage to a site-built home, it could do significant damage to a factory-built home, especially an older model or one that is not properly secured. Also, structural components (such as windows) are typically weaker than those in site-built homes.[3] 70 miles per hour (110 km/h) winds can destroy a mobile home in a matter of minutes. Many brands offer optional hurricane straps, which can be used to tie the home to anchors embedded in the ground.

In the United States, mobile homes are regulated by the US Department of Housing and Urban Development (HUD), via the Federal National Manufactured Housing Construction and Safety Standards Act of 1974. This national regulation has allowed many manufacturers to distribute nationwide because they are immune to the jurisdiction of local building authorities.[4]
[5]: 1 
By contrast, producers of modular homes must abide by state and local building codes. There are, however, wind zones adopted by HUD that home builders must follow. For example, statewide, Florida is at least wind zone 2. South Florida is wind zone 3, the strongest wind zone. After Hurricane Andrew in 1992, new standards were adopted for home construction. The codes for building within these wind zones were significantly amended, which has greatly increased their durability. During the 2004 hurricanes in Florida, these standards were put to the test, with great success. Yet, older models continue to face the exposed risk to high winds because of the attachments applied such as carports, porch and screen room additions. Such areas are exposed to ""wind capture"" which apply extreme force to the underside of the integrated roof panel systems, ripping the fasteners through the roof pan causing a series of events which destroys the main roof system and the home.

The popularity of the factory-built homes caused complications the legal system was not prepared to handle. Originally, factory-built homes tended to be taxed as vehicles rather than real estate, which resulted in very low property tax rates for their inhabitants. That caused local governments to reclassify them for taxation purposes.

However, even with that change, rapid depreciation often resulted in the home occupants paying far less in property taxes than had been anticipated and budgeted. The ability to move many factory-built homes rapidly into a relatively small area resulted in strains to the infrastructure and governmental services of the affected areas, such as inadequate water pressure and sewage disposal, and highway congestion. That led jurisdictions to begin placing limitations on the size and density of developments.

Early homes, even those that were well-maintained, tended to depreciate over time, much like motor vehicles. That is in contrast to site-built homes which include the land they are built on and tend to appreciate in value. The arrival of mobile homes in an area tended to be regarded with alarm, in part because of the devaluation of the housing potentially spreading to preexisting structures.

This combination of factors has caused most jurisdictions to place zoning regulations on the areas in which factory-built homes are placed, and limitations on the number and density of homes permitted on any given site. Other restrictions, such as minimum size requirements, limitations on exterior colors and finishes, and foundation mandates have also been enacted. There are many jurisdictions that will not allow the placement of any additional factory-built homes. Others have strongly limited or forbidden all single-wide models, which tend to depreciate more rapidly than modern double-wide models.

Apart from all the practical issues described above, there is also the constant discussion about legal fixture and chattels and so the legal status of a trailer is or could be affected by its incorporation to the land or not. This sometimes involves such factors as whether or not the wheels have been removed.

The North Carolina Board of Transportation allowed 14-foot-wide homes on the state's roads, but until January 1997, 16-foot-wide homes were not allowed. 41 states allowed 16-foot-wide homes, but they were not sold in North Carolina. Under a trial program approved January 10, 1997, the wider homes could be delivered on specific roads at certain times of day and travel 10 mph below the speed limit, with escort vehicles in front and behind.[6][7] Eventually, all homes had to leave the state on interstate highways.[8]

In December 1997, a study showed that the wider homes could be delivered safely, but some opponents still wanted the program to end.[9] On December 2, 1999, the NC Manufactured Housing Institute asked the state Board of Transportation to expand the program to allow deliveries of 16-foot-wide homes within North Carolina.[8] A month later, the board extended the pilot program by three months but did not vote to allow shipments within the state.[10] In June 2000, the board voted to allow 16-foot-side homes to be shipped to other states on more two-lane roads, and to allow shipments in the state east of US 220. A third escort was required, including a law enforcement officer on two-lane roads.[11]

In New York State, the Homes and Community Renewal agency tracks mobile home parks and provides regulations concerning them. For example, the agency requires park owners to provide residents with a $15,000 grant if residents are forced to move when the land is transferred to a new owner. Residents are also granted the right of first refusal for a sale of the park, however, if the owner does not evict tenants for five years, the land sale can go ahead. State law also restricts the annual increase in land lot fee to a cap of 3 percent, unless the landowner demonstrates hardship in a local court, and can then raise the land lot fee by up to 6 percent in a year.[12]

Mobile homes are often sited in land lease communities known as trailer parks (also 'trailer courts', 'mobile home parks', 'mobile home communities', 'manufactured home communities', 'factory-built home communities' etc.); these communities allow homeowners to rent space on which to place a home. In addition to providing space, the site often provides basic utilities such as water, sewer, electricity, or natural gas and other amenities such as mowing, garbage removal, community rooms, pools, and playgrounds.

There are over 38,000[13] trailer parks in the United States ranging in size from 5 to over 1,000 home sites. Although most parks appeal to meeting basic housing needs, some communities specialize towards certain segments of the market. One subset of mobile home parks, retirement communities, restrict residents to those age 55 and older. Another subset of mobile home parks, seasonal communities, are located in popular vacation destinations or are used as a location for summer homes. In New York State, as of 2019, there were 1,811 parks with 83,929 homes.[12]

Newer homes, particularly double-wides, tend to be built to much higher standards than their predecessors and meet the building codes applicable to most areas. That has led to a reduction in the rate of value depreciation of most used units.[14]

Additionally, modern homes tend to be built from materials similar to those used in site-built homes rather than inferior, lighter-weight materials. They are also more likely to physically resemble site-built homes. Often, the primary differentiation in appearance is that factory-built homes tend to have less of a roof slope so that they can be readily transported underneath bridges and overpasses.[citation needed]

The number of double-wide units sold exceeds the number of single-wides, which is due in part to the aforementioned zoning restrictions. Another reason for higher sales is the spaciousness of double-wide units, which are now comparable to site-built homes. Single-wide units are still popular primarily in rural areas, where there are fewer restrictions. They are frequently used as temporary housing in areas affected by natural disasters when restrictions are temporarily waived.[citation needed]

Another recent trend has been parks in which the owner of the mobile home owns the lot on which their unit is parked. Some of these communities simply provide land in a homogeneous neighborhood, but others are operated more like condominiums with club homes complete with swimming pools and meeting rooms which are shared by all of the residents, who are required to pay membership fees and dues.

Mobile home (or mobile-homes) are used in many European campgrounds to refer to fixed caravans, purpose-built cabins, and even large tents, which are rented by the week or even year-round as cheap accommodation, similar to the US concept of a trailer park. Like many other US loanwords, the term is not used widely in Britain.[citation needed]

 Mobile Homes or Static Caravans are popular across the United Kingdom. They are more commonly referred to as Park Homes or Leisure Lodges, depending on if they are marketed as a residential dwelling or as a second holiday home residence.

Residential Mobile homes (park homes) are built to the BS3632 standard. This standard is issued by the British Standards Institute. The institute is a UK body who produce a range of standards for businesses and products to ensure they are fit for purpose. The majority of residential parks in the UK have a minimum age limit for their residents, and are generally marketed as retirement or semi-retirement parks. Holiday Homes, static caravans or holiday lodges aren't required to be built to BS3632 standards, but many are built to the standard.

In addition to mobile homes, static caravans are popular across the UK. Static caravans have wheels and a rudimentary chassis with no suspension or brakes and are therefore transported on the back of large flatbed lorries, the axle and wheels being used for movement to the final location when the static caravan is moved by tractor or 4×4. A static caravan normally stays on a single plot for many years and has many of the modern conveniences normally found in a home.

Mobile homes are designed and constructed to be transportable by road in one or two sections. Mobile homes are no larger than 20 m × 6.8 m (65 ft 7 in × 22 ft 4 in) with an internal maximum height of 3.05 m (10 ft 0 in). Legally, mobile homes can still be defined as ""caravans"".

Static holiday caravans generally have sleeping accommodation for 6 to 10 people in 2, 3 or 4 bedrooms and on convertible seating in the lounge referred to as a 'pull out bed'. They tend towards a fairly ""open-plan"" layout, and while some units are double glazed and centrally heated for year-round use, cheaper models without double glazing or central heating are available for mainly summer use. Static caravan holiday homes are intended for leisure use and are available in 10 and 12 ft (3.0 and 3.7 m) widths, a small number in 13 and 14 ft (4.0 and 4.3 m) widths, and a few 16 ft (4.9 m) wide, consisting of two 8 ft (2.4 m) wide units joined. Generally, holiday homes are clad in painted steel panels, but can be clad in PVC, timber or composite materials. Static caravans are sited on caravan parks where the park operator of the site leases a plot to the caravan owner. There are many holiday parks in the UK in which one's own static caravan can be owned. There are a few of these parks in areas that are prone to flooding and anyone considering buying a sited static caravan needs to take particular care in checking that their site is not liable to flooding.

Static caravans can be rented on an ad-hoc basis or purchased. Purchase prices range from £25,000 to £100,000. Once purchased, static caravans have various ongoing costs including insurance, site fees, local authority rates, utility charges, winterisation and depreciation. Depending on the type of caravan and the park these costs can range from £1,000 to £40,000 per year.[15] Some park owners used to have unfair conditions in their lease contracts but the Office of Fair Trading has produced a guidance document available for download called Unfair Terms in Holiday Caravan Agreements which aims to stop unfair practices.

Many Israeli settlements and outposts are originally composed of caravans (Hebrew: קראוואן caravan; pl. קראוואנים, caravanim). They are constructed of light metal, are not insulated but can be outfitted with heating and air-conditioning units, water lines, recessed lighting, and floor tiling to function in a full-service capacity. Starting in 2005, prefabricated homes, named caravillas (Hebrew: קרווילה), a portmanteau of the words caravan, and villa, begin to replace mobile homes in many Israeli settlements.

Because of similarities in the manufacturing process, some companies build both types in their factories. Modular homes are transported on flatbed trucks rather than being towed, and lack axles and an automotive-type frame. However, some modular homes are towed behind a semi-truck or toter on a frame similar to that of a trailer. The home is usually in two pieces and is hauled by two separate trucks. Each frame has five or more axles, depending on the size of the home. Once the home has reached its location, the axles and the tongue of the frame are then removed, and the home is set on a concrete foundation by a large crane.

Both styles are commonly referred to as factory-built housing, but that term's technical use is restricted to a class of homes regulated by the Federal National Mfd. Housing Construction and Safety Standards Act of 1974.

Most zoning restrictions on the homes have been found to be inapplicable or only applicable to modular homes. That occurs often after considerable litigation on the topic by affected jurisdictions and by plaintiffs failing to ascertain the difference. Most modern modulars, once fully assembled, are indistinguishable from site-built homes. Their roofs are usually transported as separate units. Newer modulars also come with roofs that can be raised during the setting process with cranes. There are also modulars with 2 to 4 storeys.
"
Commercial Construction Services,"

Construction is a general term meaning the art and science of forming objects, systems, or organizations.[1] It comes from the Latin word constructio (from com- ""together"" and struere ""to pile up"") and Old French construction.[2] To 'construct' is a verb: the act of building, and the noun is construction: how something is built or the nature of its structure.

In its most widely used context, construction covers the processes involved in delivering buildings, infrastructure, industrial facilities, and associated activities through to the end of their life. It typically starts with planning, financing, and design that continues until the asset is built and ready for use. Construction also covers repairs and maintenance work, any works to expand, extend and improve the asset, and its eventual demolition, dismantling or decommissioning.

The construction industry contributes significantly to many countries' gross domestic products (GDP). Global expenditure on construction activities was about $4 trillion in 2012. In 2022, expenditure on the construction industry exceeded $11 trillion a year, equivalent to about 13 percent of global GDP. This spending was forecasted to rise to around $14.8 trillion in 2030.[3]

The construction industry promotes economic development and brings many non-monetary benefits to many countries, but it is one of the most hazardous industries. For example, about 20% (1,061) of US industry fatalities in 2019 happened in construction.[4]

The first huts and shelters were constructed by hand or with simple tools. As cities grew during the Bronze Age, a class of professional craftsmen, like bricklayers and carpenters, appeared. Occasionally, slaves were used for construction work. In the Middle Ages, the artisan craftsmen were organized into guilds. In the 19th century, steam-powered machinery appeared, and later, diesel- and electric-powered vehicles such as cranes, excavators and bulldozers.

Fast-track construction has been increasingly popular in the 21st century. Some estimates suggest that 40% of construction projects are now fast-track construction.[5]

Broadly, there are three sectors of construction: buildings, infrastructure and industrial:[6]

The industry can also be classified into sectors or markets.[7] For example, Engineering News-Record (ENR), a US-based construction trade magazine, has compiled and reported data about the size of design and construction contractors. In 2014, it split the data into nine market segments: transportation, petroleum, buildings, power, industrial, water, manufacturing, sewage/waste, telecom, hazardous waste, and a tenth category for other projects.[8] ENR used data on transportation, sewage, hazardous waste and water to rank firms as heavy contractors.[9]

The Standard Industrial Classification and the newer North American Industry Classification System classify companies that perform or engage in construction into three subsectors: building construction, heavy and civil engineering construction, and specialty trade contractors. There are also categories for professional services firms (e.g., engineering, architecture, surveying, project management).[10][11]

Building construction is the process of adding structures to areas of land, also known as real property sites. Typically, a project is instigated by or with the owner of the property (who may be an individual or an organisation); occasionally, land may be compulsorily purchased from the owner for public use.[12]

Residential construction may be undertaken by individual land-owners (self-built), by specialist housebuilders, by property developers, by general contractors, or by providers of public or social housing (e.g.: local authorities, housing associations). Where local zoning or planning policies allow, mixed-use developments may comprise both residential and non-residential construction (e.g.: retail, leisure, offices, public buildings, etc.).

Residential construction practices, technologies, and resources must conform to local building authority's regulations and codes of practice. Materials readily available in the area generally dictate the construction materials used (e.g.: brick versus stone versus timber). Costs of construction on a per square meter (or per square foot) basis for houses can vary dramatically based on site conditions, access routes, local regulations, economies of scale (custom-designed homes are often more expensive to build) and the availability of skilled tradespeople.[13]

Depending upon the type of building, non-residential building construction can be procured by a wide range of private and public organisations, including local authorities, educational and religious bodies, transport undertakings, retailers, hoteliers, property developers, financial institutions and other private companies. Most construction in these sectors is undertaken by general contractors.

Civil engineering covers the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, tunnels, airports, water and sewerage systems, pipelines, and railways.[14][15] Some general contractors have expertise in civil engineering; civil engineering contractors are firms dedicated to work in this sector, and may specialise in particular types of infrastructure.

Industrial construction includes offshore construction (mainly of energy installations: oil and gas platforms, wind power), mining and quarrying, refineries, breweries, distilleries and other processing plants, power stations, steel mills, warehouses and factories.

Some construction projects are small renovations or repair jobs, like repainting or fixing leaks, where the owner may act as designer, paymaster and laborer for the entire project. However, more complex or ambitious projects usually require additional multi-disciplinary expertise and manpower, so the owner may commission one or more specialist businesses to undertake detailed planning, design, construction and handover of the work. Often the owner will appoint one business to oversee the project (this may be a designer, a contractor, a construction manager, or other advisors); such specialists are normally appointed for their expertise in project delivery and construction management and will help the owner define the project brief, agree on a budget and schedule, liaise with relevant public authorities, and procure materials and the services of other specialists (the supply chain, comprising subcontractors and materials suppliers). Contracts are agreed for the delivery of services by all businesses, alongside other detailed plans aimed at ensuring legal, timely, on-budget and safe delivery of the specified works.

Design, finance, and legal aspects overlap and interrelate. The design must be not only structurally sound and appropriate for the use and location, but must also be financially possible to build, and legal to use. The financial structure must be adequate to build the design provided and must pay amounts that are legally owed. Legal structures integrate design with other activities and enforce financial and other construction processes.

These processes also affect procurement strategies. Clients may, for example, appoint a business to design the project, after which a competitive process is undertaken to appoint a lead contractor to construct the asset (design–bid–build); they may appoint a business to lead both design and construction (design-build); or they may directly appoint a designer, contractor and specialist subcontractors (construction management).[16] Some forms of procurement emphasize collaborative relationships (partnering, alliancing) between the client, the contractor, and other stakeholders within a construction project, seeking to ameliorate often highly competitive and adversarial industry practices. DfMA (design for manufacture and assembly) approaches also emphasize early collaboration with manufacturers and suppliers regarding products and components.

Construction or refurbishment work in a ""live"" environment (where residents or businesses remain living in or operating on the site) requires particular care, planning and communication.[17]

When applicable, a proposed construction project must comply with local land-use planning policies including zoning and building code requirements. A project will normally be assessed (by the 'authority having jurisdiction', AHJ, typically the municipality where the project will be located) for its potential impacts on neighbouring properties, and upon existing infrastructure (transportation, social infrastructure, and utilities including water supply, sewerage, electricity, telecommunications, etc.). Data may be gathered through site analysis, site surveys and geotechnical investigations. Construction normally cannot start until planning permission has been granted, and may require preparatory work to ensure relevant infrastructure has been upgraded before building work can commence. Preparatory works will also include surveys of existing utility lines to avoid damage-causing outages and other hazardous situations.

Some legal requirements come from malum in se considerations, or the desire to prevent indisputably bad phenomena, e.g. explosions or bridge collapses. Other legal requirements come from malum prohibitum considerations, or factors that are a matter of custom or expectation, such as isolating businesses from a business district or residences from a residential district. An attorney may seek changes or exemptions in the law that governs the land where the building will be built, either by arguing that a rule is inapplicable (the bridge design will not cause a collapse), or that the custom is no longer needed (acceptance of live-work spaces has grown in the community).[18]

During the construction of a building, a municipal building inspector usually inspects the ongoing work periodically to ensure that construction adheres to the approved plans and the local building code. Once construction is complete, any later changes made to a building or other asset that affect safety, including its use, expansion, structural integrity, and fire protection, usually require municipality approval.

Depending on the type of project, mortgage bankers, accountants, and cost engineers may participate in creating an overall plan for the financial management of a construction project. The presence of the mortgage banker is highly likely, even in relatively small projects since the owner's equity in the property is the most obvious source of funding for a building project. Accountants act to study the expected monetary flow over the life of the project and to monitor the payouts throughout the process. Professionals including cost engineers, estimators and quantity surveyors apply expertise to relate the work and materials involved to a proper valuation.

Financial planning ensures adequate safeguards and contingency plans are in place before the project is started, and ensures that the plan is properly executed over the life of the project. Construction projects can suffer from preventable financial problems.[19] Underbids happen when builders ask for too little money to complete the project. Cash flow problems exist when the present amount of funding cannot cover the current costs for labour and materials; such problems may arise even when the overall budget is adequate, presenting a temporary issue. Cost overruns with government projects have occurred when the contractor identified change orders or project changes that increased costs, which are not subject to competition from other firms as they have already been eliminated from consideration after the initial bid.[20] Fraud is also an issue of growing significance within construction.[21]

Large projects can involve highly complex financial plans and often start with a conceptual cost estimate performed by a building estimator. As portions of a project are completed, they may be sold, supplanting one lender or owner for another, while the logistical requirements of having the right trades and materials available for each stage of the building construction project carry forward. Public–private partnerships (PPPs) or private finance initiatives (PFIs) may also be used to help deliver major projects. According to McKinsey in 2019, the ""vast majority of large construction projects go over budget and take 20% longer than expected"".[22]

A construction project is a complex net of construction contracts and other legal obligations, each of which all parties must carefully consider. A contract is the exchange of a set of obligations between two or more parties, and provides structures to manage issues. For example, construction delays can be costly, so construction contracts set out clear expectations and clear paths to manage delays. Poorly drafted contracts can lead to confusion and costly disputes.

At the start of a project, legal advisors seek to identify ambiguities and other potential sources of trouble in the contract structures, and to present options for preventing problems. During projects, they work to avoid and resolve conflicts that arise. In each case, the lawyer facilitates an exchange of obligations that matches the reality of the project.

Design-bid-build is the most common and well-established method of construction procurement. In this arrangement, the architect, engineer or builder acts for the client as the project coordinator. They design the works, prepare specifications and design deliverables (models, drawings, etc.), administer the contract, tender the works, and manage the works from inception to completion. In parallel, there are direct contractual links between the client and the main contractor, who, in turn, has direct contractual relationships with subcontractors. The arrangement continues until the project is ready for handover.

Design-build became more common from the late 20th century, and involves the client contracting a single entity to provide design and construction. In some cases, the design-build package can also include finding the site, arranging funding and applying for all necessary statutory consents. Typically, the client invites several Design & Build (D&B) contractors to submit proposals to meet the project brief and then selects a preferred supplier. Often this will be a consortium involving a design firm and a contractor (sometimes more than one of each). In the United States, departments of transportation usually use design-build contracts as a way of progressing projects where states lack the skills or resources, particularly for very large projects.[23]

In a construction management arrangement, the client enters into separate contracts with the designer (architect or engineer), a construction manager, and individual trade contractors. The client takes on the contractual role, while the construction or project manager provides the active role of managing the separate trade contracts, and ensuring that they complete all work smoothly and effectively together. This approach is often used to speed up procurement processes, to allow the client greater flexibility in design variation throughout the contract, to enable the appointment of individual work contractors, to separate contractual responsibility on each individual throughout the contract, and to provide greater client control.

In the industrialized world, construction usually involves the translation of designs into reality. Most commonly (i.e.: in a design-bid-build project), the design team is employed by (i.e. in contract with) the property owner. Depending upon the type of project, a design team may include architects, civil engineers, mechanical engineers, electrical engineers, structural engineers, fire protection engineers, planning consultants, architectural consultants, and archaeological consultants. A 'lead designer' will normally be identified to help coordinate different disciplinary inputs to the overall design. This may be aided by integration of previously separate disciplines (often undertaken by separate firms) into multi-disciplinary firms with experts from all related fields,[24] or by firms establishing relationships to support design-build processes.

The increasing complexity of construction projects creates the need for design professionals trained in all phases of a project's life-cycle and develop an appreciation of the asset as an advanced technological system requiring close integration of many sub-systems and their individual components, including sustainability. For buildings, building engineering is an emerging discipline that attempts to meet this new challenge.

Traditionally, design has involved the production of sketches, architectural and engineering drawings, and specifications. Until the late 20th century, drawings were largely hand-drafted; adoption of computer-aided design (CAD) technologies then improved design productivity, while the 21st-century introduction of building information modeling (BIM) processes has involved the use of computer-generated models that can be used in their own right or to generate drawings and other visualisations as well as capturing non-geometric data about building components and systems.

On some projects, work on-site will not start until design work is largely complete; on others, some design work may be undertaken concurrently with the early stages of on-site activity (for example, work on a building's foundations may commence while designers are still working on the detailed designs of the building's internal spaces). Some projects may include elements that are designed for off-site construction (see also prefabrication and modular building) and are then delivered to the site ready for erection, installation or assembly.

Once contractors and other relevant professionals have been appointed and designs are sufficiently advanced, work may commence on the project site. Some projects require preliminary works, such as land preparation and levelling, demolition of existing structures (see below), or laying foundations, and there are circumstances where this work may be contracted for in advance of finalising the contract and costs for the whole project.

Typically, a construction site will include a secure perimeter to restrict unauthorised access, site access control points, office and welfare accommodation for personnel from the main contractor and other firms involved in the project team, and storage areas for materials, machinery and equipment. According to the McGraw-Hill Dictionary of Architecture and Construction's definition, construction may be said to have started when the first feature of the permanent structure has been put in place, such as pile driving, or the pouring of slabs or footings.[25]

Commissioning is the process of verifying that all subsystems of a new building (or other assets) work as intended to achieve the owner's project requirements and as designed by the project's architects and engineers.

A period after handover (or practical completion) during which the owner may identify any shortcomings in relation to the building specification ('defects'), with a view to the contractor correcting the defect.[26]

Maintenance involves functional checks, servicing, repairing or replacing of necessary devices, equipment, machinery, building infrastructure, and supporting utilities in industrial, business, governmental, and residential installations.[27][28]

Demolition is the discipline of safely and efficiently tearing down buildings and other artificial structures. Demolition contrasts with deconstruction, which involves taking a building apart while carefully preserving valuable elements for reuse purposes (recycling – see also circular economy).

The output of the global construction industry was worth an estimated $10.8 trillion in 2017, and in 2018 was forecast to rise to $12.9 trillion by 2022,[29] and to around $14.8 trillion in 2030.[3] As a sector, construction accounts for more than 10% of global GDP (in developed countries, construction comprises 6–9% of GDP),[30] and employs around 7% of the total employed workforce around the globe[31] (accounting for over 273 million full- and part-time jobs in 2014).[32] Since 2010,[33] China has been the world's largest single construction market.[34] The United States is the second largest construction market with a 2018 output of $1.581 trillion.[35]

Construction is a major source of employment in most countries; high reliance on small businesses, and under-representation of women are common traits. For example:

According to McKinsey research, productivity growth per worker in construction has lagged behind many other industries across different countries including in the United States and in European countries. In the United States, construction productivity per worker has declined by half since the 1960s.[60]

The twenty-five largest countries in the world by construction GVA (2018)[61]

Some workers may be engaged in manual labour[62] as unskilled or semi-skilled workers; they may be skilled tradespeople; or they may be supervisory or managerial personnel. Under safety legislation in the United Kingdom, for example, construction workers are defined as people ""who work for or under the control of a contractor on a construction site"";[63] in Canada, this can include people whose work includes ensuring conformance with building codes and regulations, and those who supervise other workers.[64]

Laborers comprise a large grouping in most national construction industries. In the United States, for example, in May 2023, the construction sector employed just over 7.9 million people, of whom 859,000 were laborers, while 3.7 million were construction trades workers (including 603,000 carpenters, 559,000 electricians, 385,000 plumbers, and 321,000 equipment operators).[65] Like most business sectors, there is also substantial white-collar employment in construction - out of 7.9 million US construction sector workers, 681,000 were recorded by the United States Department of Labor in May 2023 as in 'office and administrative support occupations', 620,000 in 'management occupations' and 480,000 in 'business and financial operations occupations'.[65]

Large-scale construction requires collaboration across multiple disciplines. A project manager normally manages the budget on the job, and a construction manager, design engineer, construction engineer or architect supervises it. Those involved with the design and execution must consider zoning requirements and legal issues, environmental impact of the project, scheduling, budgeting and bidding, construction site safety, availability and transportation of building materials, logistics, and inconvenience to the public, including those caused by construction delays.

Some models and policy-making organisations promote the engagement of local labour in construction projects as a means of tackling social exclusion and addressing skill shortages. In the UK, the Joseph Rowntree Foundation reported in 2000 on 25 projects which had aimed to offer training and employment opportunities for locally based school leavers and unemployed people.[66] The Foundation published ""a good practice resource book"" in this regard at the same time.[67] Use of local labour and local materials were specified for the construction of the Danish Storebaelt bridge, but there were legal issues which were challenged in court and addressed by the European Court of Justice in 1993. The court held that a contract condition requiring use of local labour and local materials was incompatible with EU treaty principles.[68] Later UK guidance noted that social and employment clauses, where used, must be compatible with relevant EU regulation.[69] Employment of local labour was identified as one of several social issues which could potentially be incorporated in a sustainable procurement approach, although the interdepartmental Sustainable Procurement Group recognised that ""there is far less scope to incorporate [such] social issues in public procurement than is the case with environmental issues"".[70]

There are many routes to the different careers within the construction industry. There are three main tiers of construction workers based on educational background and training, which vary by country:

Unskilled and semi-skilled workers provide general site labor, often have few or no construction qualifications, and may receive basic site training.

Skilled tradespeople have typically served apprenticeships (sometimes in labor unions) or received technical training; this group also includes on-site managers who possess extensive knowledge and experience in their craft or profession. Skilled manual occupations include carpenters, electricians, plumbers, ironworkers, heavy equipment operators and masons, as well as those involved in project management. In the UK these require further education qualifications, often in vocational subject areas, undertaken either directly after completing compulsory education or through ""on the job"" apprenticeships.[71]

Professional, technical and managerial personnel often have higher education qualifications, usually graduate degrees, and are trained to design and manage construction processes. These roles require more training as they demand greater technical knowledge, and involve more legal responsibility. Example roles (and qualification routes) include:

Construction is one of the most dangerous occupations in the world, incurring more occupational fatalities than any other sector in both the United States and in the European Union.[4][72] In the US in 2019, 1,061, or about 20%, of worker fatalities in private industry occurred in construction.[4] In 2017, more than a third of US construction fatalities (366 out of 971 total fatalities) were the result of falls;[73] in the UK, half of the average 36 fatalities per annum over a five-year period to 2021 were attributed to falls from height.[74] Proper safety equipment such as harnesses, hard hats and guardrails and procedures such as securing ladders and inspecting scaffolding can curtail the risk of occupational injuries in the construction industry.[75] Other major causes of fatalities in the construction industry include electrocution, transportation accidents, and trench cave-ins.[76]

Other safety risks for workers in construction include hearing loss due to high noise exposure, musculoskeletal injury, chemical exposure, and high levels of stress.[77] Besides that, the high turnover of workers in construction industry imposes a huge challenge of accomplishing the restructuring of work practices in individual workplaces or with individual workers.[citation needed] Construction has been identified by the National Institute for Occupational Safety and Health (NIOSH) as a priority industry sector in the National Occupational Research Agenda (NORA) to identify and provide intervention strategies regarding occupational health and safety issues.[78][79] A study conducted in 2022 found “significant effect of air pollution exposure on construction-related injuries and fatalities”, especially with the exposure of nitrogen dioxide.[80]

Sustainability is an aspect of ""green building"", defined by the United States Environmental Protection Agency (EPA) as ""the practice of creating structures and using processes that are environmentally responsible and resource-efficient throughout a building's life-cycle from siting to design, construction, operation, maintenance, renovation and deconstruction.""[81]

The construction industry may require transformation at pace and at scale if it is to successfully contribute to achieving the target set out in The Paris Agreement of limiting global temperature rise to 1.5C above industrial levels.[82][83] The World Green Building Council has stated the buildings and infrastructure around the world can reach 40% less embodied carbon emissions but that this can only be achieved through urgent transformation.[84][85]

Conclusions from industry leaders have suggested that the net zero transformation is likely to be challenging for the construction industry, but it does present an opportunity. Action is demanded from governments, standards bodies, the construction sector, and the engineering profession to meet the decarbonising targets.[86]

In 2021, the National Engineering Policy Centre published its report Decarbonising Construction: Building a new net zero industry,[86] which outlined key areas to decarbonise the construction sector and the wider built environment. This report set out around 20 different recommendations to transform and decarbonise the construction sector, including recommendations for engineers, the construction industry and decision makers, plus outlined six-overarching ‘system levers’ where action taken now will result in rapid decarbonisation of the construction sector.[86] These levels are:

Progress is being made internationally to decarbonise the sector including improvements to sustainable procurement practice such as the CO2 performance ladder in the Netherlands and the Danish Partnership for Green Public Procurement.[87][88] There are also now demonstrations of applying the principles of circular economy practices in practice such as Circl, ABN AMRO's sustainable pavilion and the Brighton Waste House.[89][90][91]

 Architecture portal
 Engineering portal
"
Commercial Renovation Services,"Renovation (also called remodeling) is the process of improving broken, damaged, or outdated structures. Renovations are typically done on either commercial or residential buildings. Additionally, renovation can refer to making something new, or bringing something back to life and can apply in social contexts. For example, a community can be renovated if it is strengthened and revived. It can also be restoring something to a former better state (as by cleaning, repairing, or rebuilding). Renovation is very common in some places. For example, there are more than twenty thousand home improvement projects every year in Hong Kong, affecting more than a million residents[1] (population of HK is around 7.5 million in 2023).[2]

The building renovation process can usually, depending on the extents of the renovation, be broken down into several phases. The phases are as follow.[3]

Projects involving renovation require not just flexibility, but a plan that had been agreed upon by multiple parties. The planning process will involve feedback from financial investors of the project, and from the designer. Part of planning will also entail the collection of data for the completion of the project and then the project plan will be revised and given consent before continuing with renovations.[6]

Technology has had a meaningful effect on the renovation process, increasing the significance and strength of the planning stage. The availability of free online design tools has improved visualization of the changes, at a fraction of the cost of hiring a professional designer. The decision regarding changes is also influenced by the purpose of basement renovation [1]. Depending on the significance of these changes a professional may be required, especially if any changes other than cosmetic work (paint or finishes) is required. Many local codes require a professional to complete work in the built environment such as structural changes, new walls, new plumbing, or many others. Doing these changes without hiring a professional can result in health effects, safety concerns, damages, fines, and increased cost due to having to hire a professional after self-work.[7] Most builders focus on building new homes, so renovating is typically a part-time activity for such tradespeople. The processes and services required for renovations are quite specific and, once plans are signed off, building a new home is relatively predictable. However, during renovation projects, flexibility is often required from renovation companies to respond to unexpected issues that arise. Renovations usually require all of the sub-trades that are needed for the construction of a new building.

In case of a so-called ""fix-and-flip"" (repair and resell) objective, an ROI (return on investment) can result from changes to fix a structural issue, to fix design flow yield,[8] or to use light and color to make rooms appear more spacious. Because interior renovation could change of the internal structure of the house, ceiling construction, circuit configuration and partition walls, etc., such work related to the structure of the house, of course, also includes renovation of wallpaper posting, furniture settings, lighting, etc often times an interior designer is required as well.[9]

Many people renovate homes to create a new appearance. Builders may renovate homes to enhance the home's value as a stable source of income.[11] Homeowners often renovate their homes to increase the re-sale value and to turn a profit when selling. Homeowners may also want to add renovations that make their home more energy efficient, green or sustainable.[12] Also, over time, a homeowner’s personal preferences and needs may change, and the home will be renovated for improved aesthetics, comfort, or functionality.

Other types of renovations also can be initiated for similar reasons. The user or owner of a building can change which can effect the needs or wants for the space prompting a renovation. This is becoming more popular as buildings owners are renting or leasing floors or sections of the buildings to companies which have different spacial requirements than the previous users causing needed renovation. Renovations can also occur as companies increase size which could lead to needed additional retail, office, or other types of spaces. Similarly to homes other building owners could also want renovations to increased value, make the building more energy efficient, green or sustainable, or to update the building.[13] Sometimes shopping centres or shops are renovated to raise rent later.

Wood is versatile and flexible, making it the easiest construction material for renovations, and wood buildings can be redesigned to suit changing needs. Few homeowners or professional remodelers possess the skill and equipment that is required to alter steel-frame or concrete structures.

When looking at embodied carbon in building materials wood is often labeled as the most sustainable. This is because it sequesters carbon which if certified sustainably sourced will significantly reduce embodied carbon of buildings. This makes it a low emitting choice for a building material for an overall building and for renovations.[14]

Forest certification verifies that wood products have been sourced from well-managed forests. Most certification programs provide online search options so that consumers can find certified products—the Certification Canada program includes a search option for all of the certification programs that are in use in Canada.[15]

In North America, most structures are demolished because of external forces such as zoning changes and rising land values. Additionally, buildings that cannot be modified to serve the functional needs of the occupants are subject to demolition. Very few buildings on the continent are demolished due to structural degradation.[16]

The Athena Institute surveyed 227 commercial and residential buildings that were demolished in St. Paul, Minnesota, between 2000 and mid-2003. Thirty percent of the buildings were less than 50 years old, and 6% were less than 25 years old. The four top reasons for demolition were ""area redevelopment"" (35%), ""building’s physical condition"" (31%), ""not suitable for anticipated use"" (22%), and ""fire damage"" (7%). Lack of maintenance was cited as the specific problem for 54 of the 70 buildings where physical condition was given as the reason for demolition.[16]

Plastics are extensively used in the construction and renovation industry.[17] Airborne microplastic dust is produced during renovation, building, bridge and  road reconstruction projects[18] and the use of power tools.[19] It is also generated by deterioration of building materials[20]

Materials containing polyvinyl chloride (PVC), polycarbonate, polypropylene, and acrylic, can degrade overtime releasing microplastics.[17] During the construction process single use plastic containers and wrappers are discarded adding to plastic waste.[21] These plastics are difficult to recycle and end up in landfills where they break down over a long period of time causing potential leaching into the soil and the release of airborne microplastics.[22][23] Efforts have been made to reduce plastic waste by adding it to concrete as agglomerates. However, one solution for resolving the problem from the large amount of plastic wastes generated could bring another serious problem of leaching of microplastics. The unknown part of this area is huge and needs prompt investigation.[24]

Around twenty percent of all plastics and seventy percent of all polyvinyl chloride (PVC) produced in the world each year are used by the construction industry.[25][26] It is predicted that much more will be produced and used in the future.[25] ""In Europe, approximately 20% of all plastics produced are used in the construction sector including different classes of plastics, waste and nanomaterials.""[26]

While the type of material used will determine the composition of the dust generated, the size and amount of particulates produced are mainly determined by the type of tool used. Implementation of effective dust control measures may also play a role.

Use of angle grinder is not preferred as large amounts of harmful sparks and fumes (and particulates) are generated when compared with using reciprocating saw or band saw.[27] Angle grinders produce sparks when cutting ferrous metals. They also produce shards cutting other materials. The blades themselves may also break. This is a great hazard to the face and eyes especially, as well as other parts of the body.[28]

Use of power tools can cause adverse effects on people living nearby. Power tools can produce large amounts of particulates including ultrafine particles.[29]

Particulates are the most harmful form (other than ultra-fines) of air pollution[30] There is no safe level of particulates.[31]

Many tasks create dust. High dust levels are caused by one of more the following:[32]

Examples of high dust level tasks include:[32]

Some power tools are equipped with dust collection system (e.g. HEPA vacuum cleaner) or integrated water delivery system which extract the dust after emission.[33][34]

In the US, “About 75% of homes built before 1978 contain some lead-based paint. The older the home the more likely it is to contain lead-based paint. You should assume that any home built before 1978 contains some lead.”[35]

In April 2010 the U.S. Environmental Protection Agency (EPA) required that all renovators working in homes built before 1978 and disturbing more than 6 square feet (0.56 m2) of lead paint inside the home or 20 square feet (1.9 m2) outside the home be certified. EPA's Lead Renovation, Repair and Painting Rule (RRP Rule) lowers the risk of lead contamination from home renovation activities.[36] It requires that firms performing renovation, repair, and painting projects that disturb lead-based paint in homes, child care facilities and pre-schools (any child occupied facility) built before 1978 be certified by EPA and use certified renovators who are trained by EPA-approved training providers to follow lead-safe work practices.[37]

Careful stabilization of any deteriorated (peeling, chipping, cracking, etc.) paint in a lead-safe manner is also encouraged. Through authority vested in the United States Department of Housing and Urban Development (HUD), lead-based-paint removal by dry scraping, dry sanding, torching and burning, the use of heat guns over 1100°F, and machine-sanding / grinding without a HEPA-filtered vacuum or a HEPA filtered dust collection system, is prohibited, as these methods have been proven to produce significant amount of lead dust during renovation, remodeling and painting.[38]

At the end of any remodeling or repainting job, a dust test performed by an independent third-party professional is also required by HUD for ""clearance"". Lead evaluations are done using a method called X-Ray fluorescence (XRF), which gives a result in 4–8 seconds with a 95% accuracy at the 2-sigma level.

As of 2018[update], there are an estimated 37 million homes and apartments with lead paint in the United States.[39]

Currently, worldwide 38% of emissions and 35% of energy use come from the building sector, including building construction and operation. This means renovations contribute to emissions and energy use of the building sector. These percentages are the largest portion of the total emissions and energy use globally.[40] This makes buildings have the highest potential for decreasing these percentages as well as the largest need to decrease them. Renovations are also one way to do this. 

Renovations decrease emissions as instead of demolishing a building just to build a new one the building is reused. Reuse of buildings is not always desirable as it is often pursued to have a building designed for the many individual and unique needs building owners have but it is not always a necessity. Renovations can take a building and make it completely different from the old building just reusing the structure, which is often the largest contributor of embodied carbon to a building. However, in order to be able to do this buildings need to be design durably and re-use. Designing for durability and reuse is designing for new buildings to be ""long lasting, use-adaptable, and culturally valuable""[14] to allow for the building to be kept for longer to minimize emissions from a complete rebuild. 

Having these ideas in mind while designing new buildings significantly increases the likelihood for renovations to happen.[14] Buildings are more likely to be torn down because they can not accommodate the new desired use then because the structure is failing.[16] Renovations allow old buildings to fit new needs in a way that outputs less emissions than a complete tear down and construction of a new building which is often a feasible option.

Renovation has several effects on economies, including:[41]
"
Swimming Pool Installation Services,"A swimming pool service technician is a person who maintains swimming pools, including keeping the water clean and safe by fixing pool equipment such as pumps, motors and water filters.

Pool and spa service technicians provide services in the swimming pool and spa industry. There are various national trade associations in the United States that offer memberships in these services, including the Pool & Hot Tub Alliance,[1] the Independent Pool And Spa Service Association, Inc.[2] and the United Pool Association.[3] Certification can be obtained through various organizations, including the National Swimming Pool Foundation.

There are approximately 15 million residential pools and spas in the United States, and about 400,000 commercial and public swimming pools.[4][5] The two service industry trade organizations boast of having about 15,000 members. These workers generally clean either residential or commercial swimming pools. Not all pool service professionals are members of these organizations, and many residential pool owners clean their own pools.[6]

According to the National Swimming Pool Foundation,[7] which has certified hundreds of thousands of pool service operators with its Certified Pool Operators course,[8] only 26 states require that operators of public or commercial pools be properly trained and certified. The Centers for Disease Control and Prevention has published guidance to ""help ensure healthy and safe experiences in public pools"" in reports called the Model Aquatic Health Code.[9] [10]

An automated pool cleaner can also be used to clean some pools.

The swimming pool service technician industry has various jobs that extend beyond cleaning and sanitizing. Swimming pool service technicians are required to have an understanding of basic water chemistry readings, such as chlorine, pH, alkalinity, stabilizer, and salt levels.[11] Additionally, they must have knowledge in maintaining and repairing pool and spa equipment, including filters, pumps, chlorinators, heaters, pool lights, and automation systems and are also responsible for maintaining safety by ensuring that diving boards, water slides, and other pool accessories are in safe working condition.

Another aspect of repairing or installing pool equipment is setting up automation systems. This includes running high and low voltage wires from a main breaker box to a separate automation panel where that electricity is then sent to the various pool equipment. Communication wire is also sent to the equipment to switch them on and off via the automation motherboard. 

The average swimming pool technician hourly pay rate in the United States is $17.30.[12]
[13] This figure may vary depending on many factors, including the employer, location and worker's expertise.

A swimming pool service technician is the title character of the Seinfeld episode ""The Pool Guy"".[14]

There has also been an episode of Saturday Night Live called ""Pool Boy"", depicting a swimming pool service technician, played by comedian Pete Davidson. 

In Legally Blonde, a pool boy named Enrique Salvatore (played by Greg Serano) is a key witness in the trial at the center of the movie.[15]
"
Swimming Pool Maintenance Services,"A swimming pool service technician is a person who maintains swimming pools, including keeping the water clean and safe by fixing pool equipment such as pumps, motors and water filters.

Pool and spa service technicians provide services in the swimming pool and spa industry. There are various national trade associations in the United States that offer memberships in these services, including the Pool & Hot Tub Alliance,[1] the Independent Pool And Spa Service Association, Inc.[2] and the United Pool Association.[3] Certification can be obtained through various organizations, including the National Swimming Pool Foundation.

There are approximately 15 million residential pools and spas in the United States, and about 400,000 commercial and public swimming pools.[4][5] The two service industry trade organizations boast of having about 15,000 members. These workers generally clean either residential or commercial swimming pools. Not all pool service professionals are members of these organizations, and many residential pool owners clean their own pools.[6]

According to the National Swimming Pool Foundation,[7] which has certified hundreds of thousands of pool service operators with its Certified Pool Operators course,[8] only 26 states require that operators of public or commercial pools be properly trained and certified. The Centers for Disease Control and Prevention has published guidance to ""help ensure healthy and safe experiences in public pools"" in reports called the Model Aquatic Health Code.[9] [10]

An automated pool cleaner can also be used to clean some pools.

The swimming pool service technician industry has various jobs that extend beyond cleaning and sanitizing. Swimming pool service technicians are required to have an understanding of basic water chemistry readings, such as chlorine, pH, alkalinity, stabilizer, and salt levels.[11] Additionally, they must have knowledge in maintaining and repairing pool and spa equipment, including filters, pumps, chlorinators, heaters, pool lights, and automation systems and are also responsible for maintaining safety by ensuring that diving boards, water slides, and other pool accessories are in safe working condition.

Another aspect of repairing or installing pool equipment is setting up automation systems. This includes running high and low voltage wires from a main breaker box to a separate automation panel where that electricity is then sent to the various pool equipment. Communication wire is also sent to the equipment to switch them on and off via the automation motherboard. 

The average swimming pool technician hourly pay rate in the United States is $17.30.[12]
[13] This figure may vary depending on many factors, including the employer, location and worker's expertise.

A swimming pool service technician is the title character of the Seinfeld episode ""The Pool Guy"".[14]

There has also been an episode of Saturday Night Live called ""Pool Boy"", depicting a swimming pool service technician, played by comedian Pete Davidson. 

In Legally Blonde, a pool boy named Enrique Salvatore (played by Greg Serano) is a key witness in the trial at the center of the movie.[15]
"
Vacant Building Management,"Property management is the operation, control, maintenance, and oversight of real estate and physical property. This can include residential, commercial, and land real estate. Management indicates the need for real estate to be cared for and monitored, with accountability for and attention to its useful life and condition. This is much akin to the role of management in any business.

Property management is the administration of personal property, equipment, tooling, and physical capital assets acquired and used to build, repair, and maintain end-item deliverables. Property management involves the processes, systems, and workforce required to manage the life cycle of all acquired property as defined above, including acquisition, control, accountability, responsibility, maintenance, utilization, and disposition.

An owner of a single-family home, condominium, or multi-family building may engage the services of a professional property management company. The company will then advertise the rental property, handle tenant inquiries, screen applicants, select suitable candidates, draw up a lease agreement, conduct a move-in inspection, move the tenant(s) into the property and collect rental income. The company will then coordinate any maintenance issues, supply the owner(s) with financial statements and any relevant information regarding the property, etc.

This profession has many facets, including managing the accounts and finances of real estate properties and participating in or initiating litigation with tenants, contractors, and insurance agencies. Litigation is sometimes considered a separate function set aside for trained attorneys. Although a person will be responsible for this in their job description, there may be an attorney working under a property manager. Special attention is given to landlord/tenant law; most commonly, evictions, non-payment, harassment, reduction of pre-arranged services, and public nuisance are legal subjects that gain the most attention from property managers. Therefore, it is a necessity that a property manager is current with applicable municipal, county, state, and Federal Fair Housing laws and practices.

Every state of Australia except South Australia requires a license to manage property. This is to ensure that a property manager is as well prepared for the job as possible. (There may be exceptions, like managing an extremely small property for a relative.) In South Australia, a property management business must be run by a registered land agent.[1]

In Canada, the laws governing property management and landlord/tenant relations are, generally speaking, a Provincial responsibility.[citation needed] Each Province and Territory makes its own laws on these matters. In most cases, any person or company can offer property management services, and there are licensing requirements.[citation needed] Other than specific laws in each Province and Territory governing these matters, they are governed by English Common Law, except in the province of Quebec, where the Civil Code is used in place of English Common Law.[citation needed] In some cities, the Provincial Legislation is supplemented by City by-laws.

The licensing of property managers is regulated by the provincial government and licensing by the BCFSA is a regulatory agency established by the provincial government. Its mandate is to protect the public interest by enforcing the licensing and licensee conduct requirements of the Real Estate Services Act. The BCFSA is responsible for licensing individuals and brokerages engaged in real estate sales, rental and strata property management. The BCFSA  also enforces entry qualifications, investigates complaints against licensees and imposes disciplinary sanctions under the Act.

The BCFSA is responsible for ensuring that the interests of consumers who use the services of real estate licensees are adequately protected against wrongful actions by the licensees. A wrongful action may be deliberate or may be the consequence of inadequate exercise of reasonable judgment by a licensee in carrying out their duties and responsibilities.

The BCFSA is responsible for determining what is appropriate education in real estate matters for individuals seeking to be licensed as real estate practitioners and arranging for licensing courses and examinations as part of the qualification requirement for licensing. Under the authority of the BCFSA, licensing courses are delivered by the UBC Sauder School of Business, Real Estate Division.

In Ontario, no licensing is required to operate; however, ACMO—the Association of Condo Managers of Ontario—is a self-governing body for certification and designation of its members who run buildings with more than 600 units. (RECO), the Real Estate Council of Ontario, regulates licensed realtors in Ontario. The provincial government is revising its condominium act. After public consultation, it hopes to put forth legislation during the 2015–2016 session requiring Condo Management firms and staff or condo employees and boards to be accredited.

Both require property managers to hold a real estate license.

In Germany, property management is divided into the areas of home owner's association management (WEG-Verwaltung), rental management (Mietverwaltung), and special property management (Sondereigentumsverwaltung) with different clients and tasks. Since 2018, a license in accordance with the Trade Regulation Act (§ 34 C GewO) is mandatory for property managers. This requires sufficient insurance as well as sound financial circumstances and reliability. There are no requirements regarding professional trainings or degrees. However, there is a training obligation of twenty hours within a period of three years. Receiving a license as a property manager in Germany is accompanied by membership of the relevant chamber of industry and commerce.[2]

In Hong Kong, property management companies (PMCs)[3] and property management practitioners (PMPs)[4] are regulated under the Property Management Services Ordinance (PMSO) (Chapter 626 of the Laws of Hong Kong),[5] which was enacted in 2016. Only some sections under the PMSO have commenced operation and they are the ones concerning the establishment of the Property Management Services Authority (PMSA) as the regulator for the industry. Apart from establishing the PMSA, the PMSO provides a legal framework for the regulatory regime, and the details of the regime, including the licensing criteria for PMCs and PMPs, are being formulated by the PMSA (public consultation underway[6]) and will be set out in subsidiary legislation. Other sections of the PMSO will commence operation after the subsidiary legislation is passed by the Legislative Council of Hong Kong and commences operation.

Certain classes of persons are exempt from the licensing requirement.[7] Those not exempt are required to obtain a license, and failure to do so is a criminal offence subject to a maximum penalty of a fine of HK$500,000 and imprisonment for two years.[8]  Those who are licensed are subject to disciplinary actions (including verbal warning, written reprimand, penalty up to HK$300,000, imposition of a condition on licence, suspension and revocation of licences[9]) by the PMSA if they commit a ""disciplinary offence"" as defined under section 4 of the PMSO.[10] The PMSA may issue codes of conduct containing practical guidance to licensees, including the matters that the PMSA considers to be relevant to determining whether a licensee has committed a disciplinary offence.[11]

Under the PMSO, property management services (PMSs) are to be prescribed under seven specified categories[12][13] as follows:

Only those PMCs providing PMSs falling within more than one category of PMSs are required to be licensed,[14] and individuals who assume a managerial or supervisory role in these PMCs are also required to be licensed.[15] In other words, PMCs providing PMSs falling within only one category of PMSs are not required to be licensed, and individuals working in the front line without assuming a managerial or supervisory role are not required to be licensed either. All types of properties (i.e. whether residential, commercial or otherwise) are covered by the PMSO, but ""property"" is given a technical meaning and refers to those which have a deed of mutual covenant (DMC) (a document containing terms that are binding on all flat owners of a multi-unit or multi-story building[16]) since only PMSs provided to buildings with multi-ownership are intended to be regulated.[17] In other words, PMCs and PMPs providing PMSs to properties without a DMC are not to be regulated under the PMSO.

In India, there is no statutory regulation of property management companies, real estate agents or developers. In 2013, a Real Estate Regulation and Development Bill was passed by the Union Cabinet but has yet to take effect. The bill seeks to set up 3 regulatory bodies in the country. The Real Estate Regulation and Development Bill was passed by the Union Cabinet in early 2016 and this is expected to bring about a sea change in the management of real estate in India.[citation needed]

In the Republic of Ireland, there is no legal obligation to form a property management company. However, management companies are generally formed to manage multi-unit developments and must then follow the general rules of company law in terms of ownership and administration.

Since July 2012, it has become mandatory for all property service providers, including property management companies, to be registered and fully licensed by the Property Services Regulatory Authority of Ireland.

The National Consumer Agency (NCA) has campaigned in this area, and in September 2008 it launched a website explaining consumer rights. The NCA does not have a legislative or regulatory function in the area, unless a consumer complaint is in relation to a breach of consumer law.

In Kenya, the Estate Agents Registration Board (EARB)[18] is the regulatory body for estate agency practice, and it derives its mandate from the Estate Agents Act, 1984, Cap 533,[19] which was operationalized[clarification needed] in 1987. Under that Act, the Board is responsible for registering estate agents and ensuring that the competence and conduct of practicing estate agents are good enough to ensure the protection of the public. The Board also keeps a list of registered members on its website that is accessible to members of the public, in accordance with Section 9 of the Estate Agents Act.[20] The Board recently[when?] drafted a proposal with a set of amendments to the Estate Agents Act.[21]

Associations that real estate agents and property developers can join include:

Commercial Property Management leasing agents in New Zealand are required to have a real estate agents licence and operate an audited trust bank account. Commercial leases are covered by the Property Law Act 1952.

Residential property management in New Zealand is an unlicensed and unregulated industry. Property managers in New Zealand do not require any registration or minimum knowledge or skill. The New Zealand Government reviewed whether all forms of property management need any legislation.[24] Following completion of the review, the Associate Minister of Justice, Hon Nathan Guy, announced on 2 July 2009 that no new occupational regulation would be imposed on property managers[25] in part due to there already being existing laws which could be used to protect consumers.

New Zealand licensed Real Estate Agents may offer Residential Property Management service with qualified Real Estate Agents as Property managers or property manager's working under the Licensed Real estate agency. Member Agents are supposed to adhere to the Real Estate Institute of New Zealand property management code of practice,[26] which, according to the REAA, outlines industry best practices for dealing with the public. Critics state the Real Estate Agents Authority complaint committee as having less scope or jurisdiction for adverse judgement against negligent Property Management licences as they would otherwise to those in ""real estate agency work"",[27] Unsatisfactory property management conduct cases can receive findings of ""no further action"" as opposed to ""unsatisfactory conduct""[28] due to ""conduct unrelated to estate agency work"".[29] Best practice guidelines[26] imply Licensed Real Estate agencies conducting property management business should collect rent through an audited trust account, which brings some certainty to the security of the Landlord and Tenants rental Monies, though REAA cases, implies that this is not always so.[29]

The Residential Tenancies Act 1986 sets out the rights and responsibilities of residential landlords and tenants, including the requirement to have a written tenancy agreement and the need to lodge tenancy bonds (if one is required) with the Ministry of Business, Innovation and Employment. The Tenancy Tribunal[30] and its adjudicators/mediators hear and make judgement on disputes (between landlord and tenants) in relation to any breaches of The Residential Tenancies Act 1986 and The Unit Titles Act 2010.

On July 1, 2019, the Healthy Homes Standards became law. The healthy homes standards introduce specific and minimum standards for heating, insulation, ventilation, moisture ingress and drainage, and draft stopping in rental properties. All private rentals must comply within 90 days of any new or renewed tenancy after 1 July 2021, with all private rentals complying by 1 July 2024[31]

The Unit Titles Act 2010 sets out the law for the ownership and management of unit title developments, where multiple owners each hold a unit title. The Act covers the set-up of such developments, body corporate governance, the rights and obligations of the body corporate and unit owners, disclosure between buyers and sellers, dispute resolution, etc. The Unit Titles Regulations 2011 provide operational guidelines. The body corporate is responsible for financial and administrative functions relating to the common property and the development. All unit owners are members of the body corporate. A body corporate can delegate some of its powers and duties to a body corporate committee and a professional body corporate manager may be contracted to provide services.[32]

No license is required to manage properties in Panama, as long as the company is focused on managing properties. Nevertheless, a real estate company that plans to buy and sell properties requires a license.

No specific regulatory or licensing body exists at this time (November 2012). However, under financial business law, any business offering Property Management as a chargeable, fee-earning act of commerce may only do so if such services are listed in their Company Acts of Constitutions, i.e., legally pre-declared list of business activities. Under Romanian law, no business can derive income from any such service that is not declared in this way and should be demonstrable upon request by the client of legal entities.

In the United Kingdom, there is no statutory regulation concerning property management companies. Companies that manage rented residential property are often members of the Association of Residential Letting Agents. Companies or individual landlords who accept tenancy deposits for ""assured shorthold tenancies"" (the usual form of residential tenancy) are required by statute to be members of a Tenancy Deposit Scheme.

Most states, such as New York[33] and Colorado,[34] require property management companies to be licensed real estate brokers if they are collecting rent, listing properties for rent, or helping negotiate leases and doing inspections as a third-party. A property manager may be a licensed real estate salesperson but generally they must be working under a licensed real estate broker. Most states have a public license check system online for anyone holding a real estate salesperson or real estate broker's license.[35] A few states, such as Idaho, Maine, and Vermont, do not require property managers to have real estate licenses. Other states, such as Montana, Oregon, and South Carolina, allow property managers to work under a property management license rather than a broker's license. Some states, like Pennsylvania, allow property managers to work without a real estate license if they do not negotiate leases, hold tenants' money, or enter into leases on the property owner's behalf.

Owners who manage their own property are not required to have a real estate license in many states; however, they must at least have a business license to rent out their own home. Owners who do not live near the rental property may be required, by local government, to hire the services of a property management company.[citation needed] Some states with high tourism numbers, such as Hawaii,[36] have strict property management rules.

In California, third-party apartment property managers must be licensed with the California Bureau of Real Estate as a real estate broker. A broker's license is required for any person or company that, for compensation, leases or rents or offers to lease or rent, or places for rent, or solicits listing of places for rent, or solicits for prospective tenants, or negotiates the sale, purchase, or exchange of leases on real property, or on a business opportunity, or collects rents from real property, or improvements thereon, or from business opportunities.[37] California Code of Regulations, Title 25, Section 42, requires property owners of apartment buildings with 16 or more units to have on-site resident managers living on their properties. There is no such requirement for apartment buildings with less than 16 units.[38]

The designation Real Estate Broker is often confused by those unfamiliar with terms of the industry such as Realtor, real estate agent, or real estate salesperson, and definitions vary from US state to state.

[Are these all in the US, or what?]

Building Owners and Managers Association (BOMA International) offers industry-standard designations that certify the training to property managers:[39]

Institute of Real Estate Management (IREM)

Manufactured Housing Institute (MHI)

National Apartment Association (NAA) has the following designations:

National Association of Residential Property Managers (NARPM) offers designations to certify ethical and professional standards of conduct for property managers:[40]

State-specific designations include the following:

The Community Associations Institute also has designations in the United States for residential property managers who manage planned communities such as Condominiums, homeowner associations, and Cooperatives. National designations include:

The National Association of Home Builders has a specialized designation for the affordable housing industry through the Low Income Housing Tax Credit (LIHTC) program:

In the UK:

In Kenya:

Property management software continues to grow in popularity and importance. As it decreases in price, smaller companies and amateur property managers can function using some of the same best practices and efficiency as larger companies. Online asset management software (OPMS, or online property management software) has been a significant cause of the price declines. In addition to the core property management software options, a quickly growing number of closely related software products are being introduced to the industry.

A property management system, also known as a PMS, is a comprehensive software application used to cover objectives like coordinating the operational functions of the front office, bookings, communication with guests, planning, reporting, etc. This kind of software is used to run large-scale hotels and vacation properties.

This is the most common model and is used by property management companies in the residential space that manage multi-home units and single-family homes. The property owner in this case signs a property management agreement with the company, giving the latter the right to let it out to new tenants and collect rent. The owners don't usually even know who the tenants are. The property management company usually keeps 10-15% of the rent amount and shares the rest with the property owner.

This is the most common revenue model used by companies when monitoring empty homes or empty land sites. The work here involves monitoring the property and ensuring that it is safe and secure, and reporting back to the owner. As there is no income from these properties, a fixed monthly fee is usually charged to the owner.

This model is also used in the residential space, but mostly for small units in high-demand locations. Here, the company signs a rental agreement with the owner and pays them a fixed rent. As per the agreement, the company is given the right to sublet the property for a higher rent. The company's income is the difference between the two rents. As is evident, in this case, the company minimizes the rent paid to the owner, which is usually lower than market rates.

This model applies to the service apartment space and other commercial establishments, such as retail or business centers that generate revenue. In this case, the property manager signs an agreement with the property owner, with the right to convert the property into a revenue-generating business such as a business center, service apartment, etc. Instead of paying rent to the owner, the management company shares a percentage of revenue.[citation needed] There are also hybrid structures here, where a combination of a fixed rent and a share of revenue is shared with the property owner.[citation needed]
"
Vacant Land Services,"In real estate, a land lot or plot of land is a tract or parcel of land owned or meant to be owned by some owner(s). A plot is essentially considered a parcel of real property in some countries or immovable property (meaning practically the same thing) in other countries. Possible owners of a plot can be one or more persons or another legal entity, such as a company, corporation, organization, government, or trust. A common form of ownership of a plot is called fee simple in some countries.

A small area of land that is empty except for a paved surface or similar improvement, typically all used for the same purpose or in the same state is also often called a plot.[1]  Examples are a paved car park or a cultivated garden plot. This article covers plots (more commonly called lots in some countries) as defined parcels of land meant to be owned as units by an owner(s).

Like most other types of property, lots or plots owned by private parties are subject to a periodic property tax payable by the owners to local governments such as a county or municipality.  These real estate taxes are based on the assessed value of the real property; additional taxes usually apply to transfer of ownership and property sales.  Other fees by government are possible for improvements such as curbs and pavements or an impact fee for building a house on a vacant plot. Property owners in the United States and various other countries are also subject to zoning and other restrictions. These restrictions include building height limits, restrictions on architectural style of buildings and other structures, setback laws, etc.

In New Zealand land lots are generally described as sections.

A lot has defined boundaries (or borders) which are documented somewhere, but the boundaries need not be shown on the land itself.  Most lots are small enough to be mapped as if they are flat, in spite of the curvature of the Earth.  A characteristic of the size of a lot is its area.  The area is typically determined as if the land is flat and level, although the terrain of the lot may not be flat, i. e, the lot may be hilly.  The contour surface area of the land is changeable and may be too complicated for determining a lot's area.

Lots can come in various sizes and shapes.  To be considered a single lot, the land described as the ""lot"" must be contiguous.  Two separate parcels are considered two lots, not one.  Often a lot is sized for a single house or other building.  Many lots are rectangular in shape, although other shapes are possible as long as the boundaries are well-defined.  Methods of determining or documenting the boundaries of lots include metes and bounds, quadrant method, and use of a plat diagram.  Use of the metes and bounds method may be compared to drawing a polygon.  Metes are points which are like the vertices (corners) of a polygon.  Bounds are line segments between two adjacent metes.  Bounds are usually straight lines, but can be curved as long as they are clearly defined.

When the boundaries of a lot are not indicated on the lot, a survey of the lot can be made to determine where the boundaries are according to the lot descriptions or plat diagrams.  Formal surveys are done by qualified surveyors, who can make a diagram or map of the lot showing boundaries, dimensions, and the locations of any structures such as buildings, etc. Such surveys are also used to determine if there are any encroachments to the lot. Surveyors can sometimes place posts at the metes of a lot.

The part of the boundary of the lot next to a street or road is the frontage.  Developers try to provide at least one side of frontage for every lot, so owners can have transport access to their lots.  As the name implies, street frontage determines which side of the lot is the front, with the opposite side being the back. If the lot area is known, from the deed, then the frontage line can be calculated as depth by measuring the width (as area divided by width = depth). Sometimes minor, usually unnamed driveways called alleys, usually publicly owned, also provide access to the back of a lot.  When alleys are present, garages are often located in back of a lot with access from the alley.  Also when there are alleys, garbage collection may take place from the alley.  Lots at the corners of a block have two sides of frontage and are called corner lots.  Corner lots may have the advantage that a garage can be built with street access from the side, but have the disadvantage that there is more parkway lawn to mow and more pavement to shovel snow from. In areas with large blocks, homes are sometimes built in the center of the block. In this situation, the lot will usually include a long driveway to provide transport access. Because the shape is reminiscent of a flag (the home) on a flag pole (the driveway), these lots are called flag lots.

Local governments often pass zoning laws which control what buildings can be built on a lot and what they can be used for.  For example, certain areas are zoned for residential buildings such as houses.[2] Other areas can be commercially, agriculturally, or industrially zoned.  Sometimes zoning laws establish other restrictions, such as a minimum lot area and/or frontage length for building a house or other building, maximum building size, or minimum setbacks from a lot boundary for building a structure.  This is in addition to building codes which must be met.  Also, minimum lot sizes and separations must be met when wells and septic systems are used.  In urban areas, sewers and water lines often provide service to households.  There may also be restrictions based on covenants established by private parties such as the real estate developer.  There may be easements for utilities to run water, sewage, electric power, or telephone lines through a lot.

Something which is meant to improve the value or usefulness of a lot can be called an appurtenance to the lot.  Structures such as buildings, driveways, pavements, patios or other surfaces, wells, septic systems, signs, and similar improvements which are considered permanently attached to the land in the lot are considered to be real property, usually part of the lot but often parts of a building, such as condominiums, are owned separately.  Such structures owned by the lot owner(s), as well as easements which help the lot owners or users, can be considered appurtenances to the lot.  A lot without such structures can be called a vacant lot, urban prairie, spare ground, an empty lot, or an unimproved or undeveloped lot.

Many developers divide a large tract of land into lots as a subdivision.  Certain areas of the land are dedicated (given to local government for permanent upkeep) as streets and sometimes alleys for transport and access to lots.  Areas between the streets are divided up into lots to be sold to future owners.  The layout of the lots is mapped on a plat diagram, which is recorded with the government, typically the county recorder's office.  The blocks between streets and the individual lots in each block are given an identifier, usually a number or letter.

Land originally granted by the government was commonly done by documents called land patents.  Lots of land can be sold/bought by the owners or conveyed in other ways.  Such conveyances are made by documents called deeds which should be recorded by the government, typically the county recorder's office.  Deeds specify the lot by including a description such as one determined by the ""metes and bounds"" or quadrant methods, or referring to a lot number and block number in a recorded plat diagram.  Deeds often mention that appurtenances to the lot are included in order to convey any structures and other improvements also.

In front of many lots in urban areas, there are pavements, usually publicly owned.  Beyond the pavement, there may sometimes be a strip of land called a road verge, and then the roadway, being the driveable part of the road.

Queen Street in Toronto was referred to as Lot Street before 1837 as it was used by British surveyors to mark park lots of important land owners in York, Upper Canada.
"
Meat Processing Services,"The meat-packing industry (also spelled meatpacking industry or meat packing industry) handles the slaughtering, processing, packaging, and distribution of meat from animals such as cattle, pigs, sheep and other livestock. Poultry is generally not included. This greater part of the entire meat industry is primarily focused on producing meat for human consumption, but it also yields a variety of by-products including hides, dried blood, protein meals such as meat & bone meal, and, through the process of rendering, fats (such as tallow).

In the United States and some other countries, the facility where the meat packing is done is called a slaughterhouse, packinghouse or a meat-packing plant; in New Zealand, where most of the products are exported, it is called a freezing works.[1] An abattoir is a place where animals are slaughtered for food.

The meat-packing industry grew with the construction of railroads and methods of refrigeration for meat preservation. Railroads made possible the transport of stock to central points for processing, and the transport of products.

Before the American Civil War, the meat industry was localized, with farmers providing beef and hogs for nearby butchers to serve the local market. Large Army contracts during the Civil War attracted entrepreneurs with a vision for building much larger markets.[2] The 1865–1873 era provided five factors that expanded the industry to a national scale:

In Milwaukee, Philip Armour, an ambitious entrepreneur from New York who made his fortune in Army contracts during the war, partnered with Jacob Plankinton to build a highly efficient stockyard that serviced the upper Midwest. Chicago built the famous Union Stockyards in 1865 on 345 swampy acres to the south of downtown. Armour opened the Chicago plant, as did Nelson Morris, another wartime contractor. Cincinnati and Buffalo, both with good water and rail service, also opened stockyards. Perhaps the most energetic entrepreneur was Gustavus Franklin Swift, the Yankee who operated out of Boston and moved to Chicago in 1875, specializing in long distance refrigerated meat shipments to eastern cities.[2]

A practical refrigerated (ice-cooled) rail car was introduced in 1881. This made it possible to ship cattle and hog carcasses, which weighed only 40% as much as live animals; the entire national market, served by the railroads, was opened up, as well as transatlantic markets using refrigerated ships. Swift developed an integrated network of cattle procurement, slaughtering, meat-packing, and shipping meat to market. Up to that time, cattle were driven great distances to railroad shipping points, causing the cattle to lose considerable weight. Swift developed a large business, which grew in size with the entry of several competitors.[3] The Bureau of Corporations, predecessor of the Federal Trade Commission investigated the country's meatpackers for anti-competitive practices in the first decade of the 1900s.[4]

The Pure Food and Drug Act of 1906 was the first of a series of legislation that led to the establishment of the Food and Drug Administration (FDA). Another such act passed the same year was the Federal Meat Inspection Act. The new laws helped the large packers, and hurt small operations that lacked economy of scale or quality controls.[5]

Historian William Cronon concludes:

The industry after 1945 closed its stockyards in big cities like Chicago and moved operations to small towns close to cattle ranches, especially in Iowa, Nebraska and Colorado.   Historically, besides Cincinnati, Chicago and Omaha, the other major meat-packing cities had been South St. Paul, Minnesota; East St. Louis, Illinois; Dubuque, Iowa; Kansas City, Missouri; Austin, Minnesota; Sioux Falls, South Dakota; and Sioux City, Iowa.[citation needed]

Mid-century restructuring by the industry of the stockyards, slaughterhouses and meat-packing plants led to relocating facilities closer to cattle feedlots and swine production facilities, to more rural areas, as transportation shifted from rail to truck. It has been difficult for labor to organize in such locations. In addition, the number of jobs fell sharply due to technology and other changes. Wages fell during the latter part of the 20th century, and eventually, both Chicago (in 1971) and Omaha (in 1999) closed their stockyards. The workforce increasingly relied on recent migrants from Mexico.[7]

Argentina had the natural resources and human talent to build a world-class meat-packing industry. However, its success in reaching European markets was limited by the poor quality control in the production of its meat and the general inferiority of frozen meat to the chilled meat exported by the United States and Australia. By 1900, the Argentine government encouraged investment in the industry to improve quality. The British dominated the world shipping industry and began fitting their ships for cold air containers, and built new refrigerated steamers. When the Argentine industry finally secured a large slice of the British market, Pateros and trade restrictions limited its penetration of the Continent.[8]

Meat in China moved from a minor specialty commodity to a major factor in the food supply in the late 20th century thanks to the rapid emergence of a middle class with upscale tastes and plenty of money. It was a transition from a country able to provide a small ration of meat for urban citizens only to the world's largest meat producer; it was a movement from a handful of processing facilities in major cities to thousands of modern meat-packing and processing plants throughout the country, alongside the rapid growth of a middle-class with spending money.[9]

In the early 20th century, meatpacking companies employed new immigrants as strikebreakers during labor actions initiated by existing workers, who were often earlier immigrants themselves or the immediate descendants of immigrants. The publication of the Upton Sinclair novel The Jungle in the U.S. in 1906 shocked the public with the poor working conditions and unsanitary practices in the meat-packing plants in the United States, specifically Chicago.

Meat-packing plants, like many industries in the early 20th century, overworked their employees, failed to maintain adequate safety measures, and actively fought unionization. Meat-packing workers were exposed to dangerous chemicals and sharp machinery, and routinely suffered horrible injuries. Public pressure of the U.S. Congress led to the passage of the Meat Inspection Act and Pure Food and Drug Act (both passed in 1906 on the same day) to ensure better regulations of the meat-packing industry. 

In the 1920s and early 1930s, workers achieved unionization under the CIO's United Packinghouse Workers of America (UPWA). An interracial committee led the organizing in Chicago, where the majority of workers in the industry were black, and other major cities, such as Omaha, Nebraska, where they were an important minority in the industry. UPWA workers made important gains in wages, hours and benefits. In 1957, the stockyards and meat-packing plants employed half the workers of Omaha. The union supported a progressive agenda, including the Civil Rights Movement of the 1960s. While the work was still difficult, for a few decades workers achieved blue-collar, middle-class lives from it.

Though the meat-packing industry has made many improvements since the early 1900s, extensive changes in the industry since the late 20th century have caused new labor issues to arise. Today, the rate of injury in the meat-packing industry is three times that of the private industry overall, and meat-packing was noted by Human Rights Watch as being ""the most dangerous factory job in America"". The meatpacking industry continues to employ many immigrant laborers, including some who are undocumented workers. In the early 20th century the workers were immigrants from eastern and southern Europe and black migrants from the South. Today, many meatpacking workers are Hispanics hailing from Mexico, Central and South America. A notable amount of workers are from Peru, leading to the formation of a large Peruvian community in the industry. The isolated areas in which many plants are located put these workers at greater risk due to their limited ability to organize and seek redress for work-related injuries.[10][11][12]

American slaughterhouse workers are three times more likely to suffer serious injury than the average American worker.[13] NPR reports that pig and cattle slaughterhouse workers are nearly seven times more likely to suffer repetitive strain injuries than average.[14] The Guardian reports that on average there are two amputations a week involving slaughterhouse workers in the United States.[15] On average, one employee of Tyson Foods, the largest meat producer in America, is injured and amputates a finger or limb per month.[16] The Bureau of Investigative Journalism reported that over a period of six years in the UK, 78 slaughter workers lost fingers, parts of fingers or limbs, more than 800 workers had serious injuries, and at least 4,500 had to take more than three days off after accidents.[17] In a 2018 study in the Italian Journal of Food Safety, slaughterhouse workers are instructed to wear ear protectors to protect their hearing from the constant screams of animals being killed.[18] A 2004 study in the Journal of Occupational and Environmental Medicine found that ""excess risks were observed for mortality from all causes, all cancers, and lung cancer"" in workers employed in the New Zealand meat processing industry.[19]

The worst thing, worse than the physical danger, is the emotional toll. If you work in the stick pit [where hogs are killed] for any period of time—that lets [sic] you kill things but doesn't let you care. You may look a hog in the eye that's walking around in the blood pit with you and think, 'God, that really isn't a bad-looking animal.' You may want to pet it. Pigs down on the kill floor have come up to nuzzle me like a puppy. Two minutes later I had to kill them—beat them to death with a pipe. I can't care.
The act of slaughtering animals or of raising or transporting animals for slaughter may engender psychological stress or trauma in the people involved.[21][22][23][24] A 2016 study in Organization indicates, ""Regression analyses of data from 10,605 Danish workers across 44 occupations suggest that slaughterhouse workers consistently experience lower physical and psychological well-being along with increased incidences of negative coping behavior"".[25] A 2009 study by criminologist Amy Fitzgerald indicates, ""slaughterhouse employment increases total arrest rates, arrests for violent crimes, arrests for rape, and arrests for other sex offenses in comparison with other industries"".[26] As authors from the PTSD Journal explain, ""These employees are hired to kill animals, such as pigs and cows that are largely gentle creatures. Carrying out this action requires workers to disconnect from what they are doing and from the creature standing before them. This emotional dissonance can lead to consequences such as domestic violence, social withdrawal, anxiety, drug and alcohol abuse, and PTSD"".[27]

Slaughterhouses in the United States commonly illegally employ and exploit underage workers and illegal immigrants.[28][29] In 2010, Human Rights Watch described slaughterhouse line work in the United States as a human rights crime.[30] In a report by Oxfam America, slaughterhouse workers were observed not being allowed breaks, were often required to wear diapers, and were paid below minimum wage.[31]

Another problem in this context is that the pharmaceutical industry obtains basic materials for its products from the meat-packing industry; for example, tissue extracts from slaughterhouse waste. In the Covid-19 pandemic in 2020, this led to the paradoxical situation that mass slaughterhouses were infection drivers due to the bad labor conditions and at the same time suppliers of important therapeutics such as heparin, which subsequently became a scarce commodity.[32] Medical historian Benjamin Prinz has therefore pointed to the fragility of today's healthcare systems, which themselves participate in environmentally destructive and disease-causing production chains.[33]

Contemporary concerns about the meat industry within the American context have often been colored by the  COVID-19 Pandemic and the resulting supply chain issues. Beyond the consumer perspective, workers were expected to drastically increase the rate at which they process animals. For instance, workers were expected to process 175 birds per minute up from 140 birds per minute.[34] In part this was due to shortages of workers. Workers within the industry were often in the news for large outbreaks within factories.[35] By its nature meat processing requires close proximity to other workers and exposure to a slew of bacteria and viruses. Additionally, workers often have to yell over loud machinery which increases the amount of contaminated droplets in the air.[36] More than 50,000 meat packing workers contracted the disease and over 200 died.[36] Disease is not evenly distributed throughout factories and all workers in a given factory are not at equal risk for exposure and negative health outcomes despite working the same job. In particular, the overlap of immigration status and workplace exposures can result in a variety of negative health outcomes.[37]

By 1900, the dominant meat-packers were:[38]

In the 1990s, the dominant meat-packers were:[39]

Current significant meat-packers in the United States include:[40]

Beef Packers:

Pork Packers:

Broiler Chickens:

Outside the United States:
"
Seafood Processing Services,"The term fish processing refers to the processes associated with fish and fish products between the time fish are caught or harvested, and the time the final product is delivered to the customer. Although the term refers specifically to fish, in practice it is extended to cover any aquatic organisms harvested for commercial purposes, whether caught in wild fisheries or harvested from aquaculture or fish farming.

Larger fish processing companies often operate their own fishing fleets or farming operations. The products of the fish industry are usually sold to grocery chains or to intermediaries. Fish are highly perishable. A central concern of fish processing is to prevent fish from deteriorating, and this remains an underlying concern during other processing operations.

Fish processing can be subdivided into fish handling, which is the preliminary processing of raw fish, and the manufacture of fish products. Another natural subdivision is into primary processing involved in the filleting and freezing of fresh fish for onward distribution to fresh fish retail and catering outlets, and the secondary processing that produces chilled, frozen and canned products for the retail and catering trades.[1]

There is evidence humans have been processing fish since the early Holocene.[2] These days, fish processing is undertaken by artisan fishermen, on board fishing or fish processing vessels, and at fish processing plants.

Fish is a highly perishable food which needs proper handling and preservation if it is to have a long shelf life and retain a desirable quality and nutritional value.[3] The central concern of fish processing is to prevent fish from deteriorating. The most obvious method for preserving the quality of fish is to keep them alive until they are ready for cooking and eating. For thousands of years, China achieved this through the aquaculture of carp. Other methods used to preserve fish and fish products include[4]

Usually more than one of these methods is used. When chilled or frozen fish or fish products are transported by road, rail, sea or air, the cold chain must be maintained. This requires insulated containers or transport vehicles and adequate refrigeration. Modern shipping containers can combine refrigeration with a controlled atmosphere.[4]

Fish processing is also concerned with proper waste management and with adding value to fish products. There is an increasing demand for ready to eat fish products, or products that do not need much preparation.[4]

When fish are captured or harvested for commercial purposes, they need some preprocessing so they can be delivered to the next part of the marketing chain in a fresh and undamaged condition. This means, for example, that fish caught by a fishing vessel need handling so they can be stored safely until the boat lands the fish on shore. Typical handling processes are[3]

The number and order in which these operations are undertaken varies with the fish species and the type of fishing gear used to catch it, as well as how large the fishing vessel is and how long it is at sea, and the nature of the market it is supplying.[3] Catch processing operations can be manual or automated. The equipment and procedures in modern industrial fisheries are designed to reduce the rough handling of fish, heavy manual lifting and unsuitable working positions which might result in injuries.[3]

An alternative, and obvious way of keeping fish fresh is to keep them alive until they are delivered to the buyer or ready to be eaten. This is a common practice worldwide. Typically, the fish are placed in a container with clean water, and dead, damaged or sick fish are removed. The water temperature is then lowered and the fish are starved to reduce their metabolic rate. This decreases fouling of water with metabolic products (ammonia, nitrite and carbon dioxide) that become toxic and make it difficult for the fish to extract oxygen.[3]

Fish can be kept alive in floating cages, wells and fish ponds. In aquaculture, holding basins are used where the water is continuously filtered and its temperature and oxygen level are controlled. In China, floating cages are constructed in rivers out of palm woven baskets, while in South America simple fish yards are built in the backwaters of rivers. Live fish can be transported by methods which range from simple artisanal methods where fish are placed in plastic bags with an oxygenated atmosphere, to sophisticated systems which use trucks that filter and recycle the water, and add oxygen and regulate temperature.[3]

Preservation techniques are needed to prevent fish spoilage and lengthen shelf life. They are designed to inhibit the activity of spoilage bacteria and the metabolic changes that result in the loss of fish quality. Spoilage bacteria are the specific bacteria that produce the unpleasant odours and flavours associated with spoiled fish. Fish normally host many bacteria that are not spoilage bacteria, and most of the bacteria present on spoiled fish played no role in the spoilage.[5] To flourish, bacteria need the right temperature, sufficient water and oxygen, and surroundings that are not too acidic. Preservation techniques work by interrupting one or more of these needs. Preservation techniques can be classified as follows.[6]

If the temperature is decreased, the metabolic activity in the fish from microbial or autolytic processes can be reduced or stopped. This is achieved by refrigeration where the temperature is dropped to about 0 °C, or freezing where the temperature is dropped below -18 °C. On fishing vessels, the fish are refrigerated mechanically by circulating cold air or by packing the fish in boxes with ice. Forage fish, which are often caught in large numbers, are usually chilled with refrigerated or chilled seawater. Once chilled or frozen, the fish need further cooling to maintain the low temperature. There are key issues with fish cold store design and management, such as how large and energy efficient they are, and the way they are insulated and palletized.[6]

An effective method of preserving the freshness of fish is to chill with ice by distributing ice uniformly around the fish. It is a safe cooling method that keeps the fish moist and in an easily stored form suitable for transport. It has become widely used since the development of mechanical refrigeration, which makes ice easy and cheap to produce. Ice is produced in various shapes; crushed ice and Flake Ice, plates, tubes and blocks are commonly used to cool fish.[3] Particularly effective is slurry ice, made from micro crystals of ice formed and suspended within a solution of water and a freezing point depressant, such as common salt.[7]

A more recent development is pumpable ice technology. Pumpable ice flows like water, and because it is homogeneous, it cools fish faster than fresh water solid ice methods and eliminates freeze burns. It complies with HACCP and ISO food safety and public health standards, and uses less energy than conventional fresh water solid ice technologies.[8][9]

The water activity, aw, in a fish is defined as the ratio of the water vapour pressure in the flesh of the fish to the vapour pressure of pure water at the same temperature and pressure. It ranges between 0 and 1, and is a parameter that measures how available the water is in the flesh of the fish. Available water is necessary for the microbial and enzymatic reactions involved in spoilage. There are a number of techniques that have been or are used to tie up the available water or remove it by reducing the aw. Traditionally, techniques such as drying, salting and smoking have been used, and have been used for thousands of years. These techniques can be very simple, for example, by using solar drying. In more recent times, freeze-drying, water binding humectants, and fully automated equipment with temperature and humidity control have been added. Often a combination of these techniques is used.[6]

Heat or ionizing irradiation can be used to kill the bacteria that cause decomposition. Heat is applied by cooking, blanching or microwave heating in a manner that pasteurizes or sterilizes fish products. Cooking or pasteurizing does not completely inactivate microorganisms and may need to be followed with refrigeration to preserve fish products and increase their shelf life. Sterilised products are stable at ambient temperatures up to 40 °C, but to ensure they remain sterilized they need packaging in metal cans or retortable pouches before the heat treatment.[6]

Microbial growth and proliferation can be inhibited by a technique called biopreservation.[10] Biopreservation is achieved by adding antimicrobials or by increasing the acidity of the fish muscle. Most bacteria stop multiplying when the pH is less than 4.5. Acidity is increased by fermentation, marination or by directly adding acids (acetic, citric, lactic) to fish products. Lactic acid bacteria produce the antimicrobial nisin which further enhances preservation. Other preservatives include nitrites, sulphites, sorbates, benzoates and essential oils.[6]

Spoilage bacteria and lipid oxidation usually need oxygen, so reducing the oxygen around fish can increase shelf life. This is done by controlling or modifying the atmosphere around the fish, or by vacuum packaging. Controlled or modified atmospheres have specific combinations of oxygen, carbon dioxide and nitrogen, and the method is often combined with refrigeration for more effective fish preservation.[6]

Two or more of these techniques are often combined. This can improve preservation and reduce unwanted side effects such as the denaturation of nutrients by severe heat treatments. Common combinations are salting/drying, salting/marinating, salting/smoking, drying/smoking, pasteurization/refrigeration and controlled atmosphere/refrigeration.[6] Other process combinations are currently being developed along the multiple hurdle theory.[11]

""The search for higher productivity and the increase of labor cost has driven the development of computer vision technology,[12] electronic scales and automatic skinning and filleting machines.""[13]

Waste produced during fish processing operations can be solid or liquid.

Treatments can be primary and secondary.

Fish is transported widely in ships, and by land and air, and much fish is traded internationally. It is traded live, fresh, frozen, cured and canned. Live, fresh and frozen fish need special care.[15]

The International Organization for Standardization, ISO, is the worldwide federation of national standards bodies. ISO defines quality as ""the totality of features and characteristics of a product or service that bear on its ability to satisfy stated or implied needs.""(ISO 8402). The quality of fish and fish products depends on safe and hygienic practices. Outbreaks of fish-borne illnesses are reduced if appropriate practices are followed when handling, manufacturing, refrigerating and transporting fish and fish products. Ensuring standards of quality and safety are high also minimizes the post-harvest losses.""[16]

""The fishing industry must ensure that their fish handling, processing and transportation facilities meet requisite standards. Adequate training of both industry and control authority staff must be provided by support institutions, and channels for feedback from consumers established. Ensuring high standards for quality and safety is good economics, minimizing losses that result from spoilage, damage to trade and from illness among consumers.""[16]

Fish processing highly involves very strict controls and measurements in order to ensure that all processing stages have been carried out hygienically. Thus, all fish processing companies are highly recommended to join a certain type of food safety system. One of the certifications that are commonly known is the Hazard Analysis Critical Control Points (HACCP).

Fish quality has a direct impact on market price. Accurate assessment and prediction of fish quality are of main importance to set prices, increase competitiveness, resolve conflicts of interest and prevent food wastage due to conservative product shelf-life estimations. In last years, research in food science and technology has focused on developing new methodologies to predict fish freshness.[17][18]

HACCP is a system which identifies hazards and implements measures for their control. It was first developed in 1960 by NASA to ensure food safety for the crewed space program. The main objectives of NASA were to prevent food safety problems and control food borne diseases. HACCP has been widely used by food industry since the late 1970 and now it is internationally recognized as the best system for ensuring food safety.[19]

""The Hazard Analysis and Critical Control Points (HACCP) system of assuring food safety and quality has now gained worldwide recognition as the most cost-effective and reliable system available. It is based on the identification of risks, minimizing those risks through the design and layout of the physical environment in which high standards of hygiene can be assured, sets measurable standards and establishes monitoring systems. HACCP also establishes procedures for verifying that the system is working effectively. HACCP is a sufficiently flexible system to be successfully applied at all critical stages -- from harvesting of fish to reaching the consumer. For such a system to work successfully, all stakeholders must cooperate which entails increasing the national capacity for introducing and maintaining HACCP measures. The system's control authority needs to design and implement the system, ensuring that monitoring and corrective measures are put in place.""[16]

HACCP is endorsed by the:

There are seven basic principles:

Finfish, or parts of finfish, are typically presented physically for marketing in one of the following forms[21]

In general value addition means “any additional activity that in one way or the other change the nature of a product thus adding to its value at the time of sale.” Value addition is an expanding sector in the food processing industry, especially in export markets. Value is added to fish and fishery products depending on the requirement of different markets. Globally a transition period is taking place where cooked products are replacing traditional raw products in consumer preference.

""In addition to preservation, fish can be industrially processed into a wide array of products to increase their economic value and allow the fishing industry and exporting countries to reap the full benefits of their aquatic resources. In addition, value processes generate further employment and hard currency earnings. This is more important nowadays because of societal changes that have led to the development of outdoor catering, convenience products and food services requiring fish products ready to eat or requiring little preparation before serving.""[13]

""However, despite the availability of technology, careful consideration should be given to the economic feasibility aspects, including distribution, marketing, quality assurance and trade barriers, before embarking on a value addition fish process.""[13]

There is evidence humans have been processing fish since the early Holocene. For example, fishbones (c. 8140–7550 BP, uncalibrated) at Atlit-Yam, a submerged Neolithic site off Israel, have been analysed. What emerged was a picture of ""a pile of fish gutted and processed in a size-dependent manner, and then stored for future consumption or trade. This scenario suggests that technology for fish storage was already available, and that the Atlit-Yam inhabitants could enjoy the economic stability resulting from food storage and trade with mainland sites.""[2]
"
Dairy Production Services,"

Dairy farming is a class of agriculture for the long-term production of milk, which is processed (either on the farm or at a dairy plant, either of which may be called a dairy) for the eventual sale of a dairy product. Dairy farming has a history that goes back to the early Neolithic era, around the seventh millennium BC, in many regions of Europe and Africa. Before the 20th century, milking was done by hand on small farms. Beginning in the early 20th century, milking was done in large scale dairy farms with innovations including rotary parlors, the milking pipeline, and automatic milking systems that were commercially developed in the early 1990s.

Milk preservation methods have improved starting with the arrival of refrigeration technology in the late 19th century, which included direct expansion refrigeration and the plate heat exchanger. These cooling methods allowed dairy farms to preserve milk by reducing spoiling due to bacterial growth and humidity.

Worldwide, leading dairy industries in many countries including India, the United States, China, and New Zealand serve as important producers, exporters, and importers of milk. Since the late 20th century, there has generally been an increase in total milk production worldwide, with around 827,884,000 tonnes of milk being produced in 2017 according to the FAO.

There has been substantial concern over the amount of waste output created by dairy industries, seen through manure disposal and air pollution caused by methane gas. The industry's role in agricultural greenhouse gas emissions has also been noted to implicate environmental consequences. Various measures have been put in place in order to control the amount of phosphorus excreted by dairy livestock. The usage of rBST has also been controversial. Dairy farming in general has been criticized by animal welfare activists due to the health issues imposed upon dairy cows through intensive animal farming.

Although any mammal can produce milk, commercial dairy farms are typically one-species enterprises. In developed countries, dairy farms typically consist of high producing dairy cows. Other species used in commercial dairy farming include goats, sheep, water buffaloes, and camels. In Italy, donkey dairies are growing in popularity to produce an alternative milk source for human infants.[1]

While cattle were domesticated as early as 12,000 years ago as a food source and as beasts of burden, the earliest evidence of using domesticated cows for dairy production is from the seventh millennium BC – the early Neolithic era – in northwestern Anatolia.[2] Dairy farming developed elsewhere in the world in subsequent centuries: the sixth millennium BC in eastern Europe, the fifth millennium BC in Africa, and the fourth millennium BC in Britain and Northern Europe.[2]

In the last century or so larger farms specialising in dairy alone have emerged. Large scale dairy farming is only viable where either a large amount of milk is required for production of more durable dairy products such as cheese, butter, etc. or there is a substantial market of people with money to buy milk, but no cows of their own. In the 1800s, economist Johann Heinrich von Thünen argued that there was about a 100-mile radius surrounding a city where such fresh milk supply was economically viable.

Centralized dairy farming as we understand it primarily developed around villages and cities, where residents were unable to have cows of their own due to a lack of grazing land. Near the town, farmers could make some extra money on the side by having additional animals and selling the milk in town. The dairy farmers would fill barrels with milk in the morning and bring it to market on a wagon. Until the late 19th century, the milking of the cow was done by hand. In the United States, several large dairy operations existed in some northeastern states and in the west, that involved as many as several hundred cows, but an individual milker could not be expected to milk more than a dozen cows a day. Smaller operations predominated.

For most herds, milking took place indoors twice a day,[3] in a barn with the cattle tied by the neck with ropes or held in place by stanchions. Feeding could occur simultaneously with milking in the barn, although most dairy cattle were pastured during the day between milkings. Such examples of this method of dairy farming are difficult to locate, but some are preserved as a historic site for a glimpse into the days gone by. One such instance that is open for this is at Point Reyes National Seashore.[4]

Dairy farming has been part of agriculture for thousands of years. Historically it has been one part of small, diverse farms. In the last century or so larger farms concentrating on dairy production emerged. Large scale dairy farming is only viable where either a large amount of milk is required for production of more durable dairy products such as cheese, butter, etc. or there is a substantial market of people with cash to buy milk, but no cows of their own. Dairy farms were the best way to meet demand.

The first milking machines were an extension of the traditional milking pail. The early milker device fit on top of a regular milk pail and sat on the floor under the cow. Following each cow being milked, the bucket would be dumped into a holding tank. These were introduced in the early 20th century.

This developed into the Surge hanging milker. Prior to milking a cow, a large wide leather strap called a surcingle was put around the cow, across the cow's lower back. The milker device and collection tank hung underneath the cow from the strap. This innovation allowed the cow to move around naturally during the milking process rather than having to stand perfectly still over a bucket on the floor.

The next innovation in automatic milking was the milk pipeline, introduced in the late 20th century. This uses a permanent milk-return pipe and a second vacuum pipe that encircles the barn or milking parlor above the rows of cows, with quick-seal entry ports above each cow. By eliminating the need for the 
milk container, the milking device shrank in size and weight to the point where it could hang under the cow, held up only by the sucking force of the milker nipples on the cow's udder. The milk is pulled up into the milk-return pipe by the vacuum system, and then flows by gravity to the milkhouse vacuum-breaker that puts the milk in the storage tank. The pipeline system greatly reduced the physical labor of milking since the farmer no longer needed to carry around huge heavy buckets of milk from each cow.

The pipeline allowed barn length to keep increasing and expanding, but after a point farmers started to milk the cows in large groups, filling the barn with one-half to one-third of the herd, milking the animals, and then emptying and refilling the barn. As herd sizes continued to increase, this evolved into the more efficient milking parlor.

Innovation in milking focused on mechanizing the milking parlor (known in Australia and New Zealand as the 'cowshed') to maximize the number of cows per operator which streamlined the milking process to permit cows to be milked as if on an assembly line, and to reduce physical stresses on the farmer by putting the cows on a platform slightly above the person milking the cows to eliminate having to constantly bend over. Many older and smaller farms still have tie-stall or stanchion barns, but worldwide a majority of commercial farms have parlors.

In herringbone and parallel parlors, the milker generally milks one row at a time. The milker moves a row of cows from the holding yard into the milking parlor, and milks each cow in that row. Once all of the milking machines have been removed from the milked row, the milker releases the cows to their feed. A new group of cows is then loaded into the now vacant side and the process repeats until all cows are milked. Depending on the size of the milking parlor, which normally is the bottleneck, these rows of cows can range from four to sixty at a time. The benefits of a herringbone parlour are easy maintenance, durability, stability, and improved safety for animals and humans when compared to tie stall. [5] The first herringbone shed is thought to have been built in 1952 by a Gordonton farmer.[6]

In rotary parlors, the cows are loaded one at a time onto the parlor as it rotates in a circle. One milker stands near the entry to the parlor and pre-dips the teats on the udder to help prevent bacteria from entering. The next milker puts the machine on the cow to begin milking. By the time the platform has completed almost a full rotation, the cow is done milking and the unit will come off automatically. The last milker will post-dip her teats to protect them before entering back into the pen. Once this process is done, the cow will back out of the parlor and return to the barn. Rotary cowsheds, as they are called in New Zealand, started in the 1980s[7][8] but are expensive compared to Herringbone cowshed – the older New Zealand norm.[9]

It can be harmful to an animal for it to be over-milked past the point where the udder has stopped releasing milk. Consequently, the milking process involves not just applying the milker, but also monitoring the process to determine when the animal has been milked out and the milker should be removed. While parlor operations allowed a farmer to milk many more animals much more quickly, it also increased the number of animals to be monitored simultaneously by the farmer. The automatic take-off system was developed to remove the milker from the cow when the milk flow reaches a preset level, relieving the farmer of the duties of carefully watching over 20 or more animals being milked at the same time.[citation needed]

In the 1980s and 1990s, robotic milking systems were developed and introduced (principally in the EU). Thousands of these systems are now in routine operation. In these systems the cow has a high degree of autonomy to choose her time of milking freely during the day (some alternatives may apply, depending on cow-traffic solution used at a farm level). These systems are generally limited to intensively managed systems although research continues to match them to the requirements of grazing cattle and to develop sensors to detect animal health and fertility automatically. Every time the cow enters the milking unit she is fed concentrates and her collar is scanned to record production data.

Cool temperature has been the main method by which milk freshness has been extended. When windmills and well pumps were invented, one of their first uses on the farm, besides providing water for animals themselves, was for cooling milk, to extend its storage life, until it would be transported to the town market.

The naturally cold underground water would be continuously pumped into a cooling tub or vat.  Tall, ten-gallon metal containers filled with freshly obtained milk, which is naturally warm, were placed in this cooling bath. This method of milk cooling was popular before the arrival of electricity and refrigeration.

Refrigeration was initially used to cool cans of milk, which were filled by hand milking. These cans were placed into a cooled water bath to remove heat and keep them cool until they were able to be transported to collection facilities. As more automated milking methods were developed, hand milking was replaced and, as a result, the milk can was replaced by a bulk milk cooler. The first type of bulk milk coolers were ice banks – double-walled vessels with evaporator coils and water located between the walls at the bottom and sides of the tank. A small refrigeration compressor was used to remove heat from the evaporator coils. Ice eventually builds up around the coils, until it reaches a thickness of about three inches surrounding each pipe, and the cooling system shuts off. When the milking operation starts, only the milk agitator and the water circulation pump, which flows water across the ice and the steel walls of the tank, are needed to reduce the incoming milk to a temperature below 5°C (41°F).

This cooling method worked well for smaller dairies, however was fairly inefficient and was unable to meet the increasingly higher cooling demand of larger milking parlors. In the mid-1950s direct expansion refrigeration was first applied directly to the bulk milk cooler. This type of cooling utilizes an evaporator built directly into the inner wall of the storage tank to remove heat from the milk. Direct expansion is able to cool milk at a much faster rate than early ice bank type coolers and is still the primary method for bulk tank cooling today on small to medium-sized operations.

Another device which has contributed significantly to milk quality is the plate heat exchanger (PHE). This device utilizes a number of specially designed stainless steel plates with small spaces between them. Milk is passed between every other set of plates with water being passed between the balance of the plates to remove heat from the milk. This method of cooling can remove large amounts of heat from the milk in a very short time, thus drastically slowing bacteria growth and thereby improving milk quality. Ground water is the most common source of cooling medium for this device. Dairy cows consume approximately 3 gallons of water for every gallon of milk production and prefer to drink slightly warm water as opposed to cold ground water. For this reason, PHE's can result in drastically improved milk quality, reduced operating costs for the dairymen by reducing the refrigeration load on his bulk milk cooler, and increased milk production by supplying the cows with a source of fresh warm water.

Plate heat exchangers have also evolved as a result of the increase of dairy farm herd sizes in the United States. As a dairyman increases the size of his herd, he must also increase the capacity of his milking parlor in order to harvest the additional milk. This increase in parlor sizes has resulted in tremendous increases in milk throughput and cooling demand. Today's larger farms produce milk at a rate which direct expansion refrigeration systems on bulk milk coolers cannot cool in a timely manner. PHE's are typically utilized in this instance to rapidly cool the milk to the desired temperature (or close to it) before it reaches the bulk milk tank. Typically, ground water is still utilized to provide some initial cooling to bring the milk to between 55 and 70 °F (13 and 21 °C). A second (and sometimes third) section of the PHE is added to remove the remaining heat with a mixture of chilled pure water and propylene glycol. These chiller systems can be made to incorporate large evaporator surface areas and high chilled water flow rates to cool high flow rates of milk.

Milking machines are held in place automatically by a vacuum system that draws the ambient air pressure down from 15 to 21 pounds per square inch (100 to 140 kPa) of vacuum. The vacuum is also used to lift milk vertically through small diameter hoses, into the receiving can. A milk lift pump draws the milk from the receiving can through large diameter stainless steel piping, through the plate cooler, then into a refrigerated bulk tank.

Milk is extracted from the cow's udder by flexible rubber sheaths known as liners or inflations that are surrounded by a rigid air chamber. A pulsating flow of ambient air and vacuum is applied to the inflation's air chamber during the milking process. When ambient air is allowed to enter the chamber, the vacuum inside the inflation causes the inflation to collapse around the cow's teat, squeezing the milk out of teat in a similar fashion as a baby calf's mouth massaging the teat. When the vacuum is reapplied in the chamber the flexible rubber inflation relaxes and opens up, preparing for the next squeezing cycle.

It takes the average cow three to five minutes to give her milk. Some cows are faster or slower. Slow-milking cows may take up to fifteen minutes to let down all their milk. Though milking speed is not related to the quality of milk produced by the cow, it does impact the management of the milking process. Because most milkers milk cattle in groups, the milker can only process a group of cows at the speed of the slowest-milking cow. For this reason, many farmers will group slow-milking cows so as not to stress the faster milking cows.

The extracted milk passes through a strainer and plate heat exchangers before entering the tank, where it can be stored safely for a few days at approximately  40 °F (4 °C). At pre-arranged times, a milk truck arrives and pumps the milk from the tank for transport to a dairy factory where it will be pasteurized and processed into many products. The frequency of pick up depends and the production and storage capacity of the dairy; large dairies will have milk pick-ups once per day.

The dairy industry is a constantly evolving business. Management practices change with new technology and regulations that move the industry toward increased economic and environmental sustainability. Management strategies can also loosely be divided into intensive and extensive systems. Extensive systems operate based on a low input and low output philosophy, where intensive systems adopt a high input high output philosophy. These philosophies as well as available technologies, local regulations, and environmental conditions manifest in different management of nutrition, housing, health, reproduction and waste.

Most modern dairy farms divide the animals into different management units depending on their age, nutritional needs, reproductive status, and milk production status.[10] The group of cows that are currently lactating, the milking herd, is often managed most intensively to make sure their diet and environmental conditions are conducive to producing as much high quality milk as possible. On some farms the milking herd is further divided into milking strings, which are groups of animals with different nutritional needs.[10] The segment of the adult herd that are in the resting period before giving birth to their next calf are called dry cows because they are not being milked.[10] All female animals that have yet to give birth to their first calf are called heifers. Some of them will grow up to take the place of older animals in the milking herd and thus are sometimes generally referred to as the replacement herd.[10] The others, as well as most male calves are considered surplus dairy calves and are slaughtered for meat, such as veal dairy beef, or killed on farm.[11]

Dairy cattle housing systems vary greatly throughout the world depending on the climate, dairy size, and feeding strategies. Housing must provide access to feed, water and protection from relevant environmental conditions. One issue for housing cattle is temperature extremes. Heat stress can decrease fertility and milk production in cattle.[12] Providing shade is a very common method for reducing heat stress. Barns may also incorporate fans or tunnel ventilation into the architecture of the barn structure.[13] Overly cold conditions, while rarely deadly for cattle, cause increases in maintenance energy requirements and thus increased feed intake and decreased milk production.[14] During the winter months, where temperatures are low enough, dairy cattle are often kept inside barns which are warmed by their collective body heat.

Feed provision is also an important feature of dairy housing. Pasture based dairies are a more extensive option where cows are turned out to graze on pasture when the weather permits. Often the diet must be supplemented with when poor pasture conditions persist. Free stall barns and open lots are intensive housing options where feed is brought to the cattle at all times of year. Free stall barns are designed to allow the cows freedom to choose when they feed, rest, drink, or stand. They can be either fully enclosed or open air barns again depending on the climate. The resting areas, called free stalls, are divided beds lined with anything from mattresses to sand. In the lanes between rows of stalls, the floor is often make of grooved concrete. Most barns open onto uncovered corrals, which the cattle are free to enjoy as the weather allows. Open lots are dirt lots with constructed shade structures and a concrete pad where feed is delivered.

Life on a dairy farm revolves around the milking parlor. Each lactating cow will visit the parlor at least twice a day to be milked. A remarkable amount of engineering has gone into designing milking parlors and milking machines. Efficiency is crucial; every second saved while milking a single cow adds up to hours over the whole herd.

Milking is now performed almost exclusively by machine, though human technicians are still essential on most facilities. The most common milking machine is called a cluster milker. This milker consists of four metal cups‍— one per teat‍— each lined with rubber or silicone. The cluster is attached to both a milk collection system and a pulsating vacuum system. When the vacuum is on, it pulls air from between the outer metal cup and the liner, drawing milk out of the teat. When the vacuum turns off, it gives the teat an opportunity to refill with milk. In most milking systems, a milking technician must attach the cluster to each cow, but the machine senses when the cow has been fully milked and drops off independently.[10]

Every time a cow enters the parlor several things need to happen to ensure milk quality and cow health. First, the cow's udder must be cleaned and disinfected to prevent both milk contamination and udder infections. Then the milking technician must check each teat for signs of infection by observing the first stream of milk. During this processes, called stripping the teat, the milking technician is looking for any discoloration or chunkiness that would indicate mastitis, an infection in the cow's mammary gland. Milk from a cow with mastitis cannot enter the human milk supply, thus farmers must be careful that infected milk does not mix with the milk from healthy cows and that the cow gets the necessary treatment. If the cow passes the mastitis inspection, the milking technician will attach the milking cluster. The cluster will run until the cow is fully milked and then drop off. The milk travels immediately through a cooling system and then into a large cooled storage tank, where it will stay until picked up by a refrigerated milk truck. Before the cow is released from the milking stalls her teats are disinfected one last time to prevent infection.[10]

Feed for their cattle is by far one of the largest expenses for dairy producer whether it be provided by the land they graze or crops grown or purchased.[15] Pasture based dairy producers invest much time and effort into maintaining their pastures and thus feed for their cattle. Pasture management techniques such as rotational grazing are common for dairy production. Many large dairies that deliver food to their cattle have a dedicated nutritionist who is responsible for formulating diets with animal health, milk production, and cost efficiency in mind. For maximum productivity diets must be formulated differently depending on the growth rate, milk production, and reproductive status of each animal.

Cattle are classified as ruminants (suborder ruminantia belonging to the order artiodactyl) as they are able to acquire nutrients from even low quality plant-based food, thanks mainly to their symbiotic relationship with the microbes that ferment it in a chamber of their stomachs called the rumen. The rumen is a literal micro-ecosystem within each dairy cow. For optimal digestion, the environment of the rumen must be ideal for the microbes. In this way, the job of a ruminant nutritionist is to feed the microbes not the cow.

The nutritional requirements of cattle are usually divided into maintenance requirements, which depend on the cow's weight; and milk production requirements, which in turn depend on the volume of milk the cow is producing. The nutritional contents of each available feed are used to formulate a diet that meets all nutritional needs in the most cost effective way. Notably, cattle must be fed a diet high in fiber to maintain a proper environment for the rumen microbes. Farmers typically grow their own forage for their cattle. Crops grown may include corn, alfalfa, timothy, wheat, oats, sorghum and clover. These plants are often processed after harvest to preserve or improve nutrient value and prevent spoiling. Corn, alfalfa, wheat, oats, and sorghum crops are often anaerobically fermented to create silage. Many crops such as alfalfa, timothy, oats, and clover are allowed to dry in the field after cutting before being baled into hay.

To increase the energy density of their diet, cattle are commonly fed cereal grains. In many areas of the world, dairy rations also commonly include byproducts from other agricultural sectors. For example, in California cattle are commonly fed almond hulls and cotton seed.[16] Feeding of byproducts can reduce the environmental impact of other agricultural sectors by keeping these materials out of landfills.[16]

To meet all of their nutritional requirements cows must eat their entire ration. Unfortunately, much like humans, cattle have their favorite foods. To keep cattle from selectively eating the most desirable parts of the diet, most produces feed a total mixed ration (TMR). In this system all the components of the feed are well mixed in a mixing truck before being delivered to the cattle. Different TMRs are often prepared for groups of cows with different nutritional requirements.[17]

Female calves born on a dairy farm will typically be raised as replacement stock to take the place of older cows that are no longer sufficiently productive. The life of a dairy cow is a cycle of pregnancy and lactation starting at puberty. The timing of these events is very important to the production capacity of the dairy. A cow will not produce milk until she has given birth to a calf. Consequently, timing of the first breeding as well as all the subsequent breeding is important for maintaining milk production levels.[10]

Most dairy producers aim for a replacement heifer to give birth to her first calf, and thus join the milking herd, on her second birthday. As the cow's gestation period is a little over 9 months this means the cow must be inseminated by the age of 15 months. Because the breeding process is inefficient, most producers aim to first breed their heifers between 12 and 14 months. Before a heifer can be bred she must reach sexual maturity and attain the proper body condition to successfully bear a calf. Puberty in cattle depends largely on weight among other factors.[18] Holstein heifers reach puberty at an average body weight between 550 and 650 lbs.[18] Smaller breeds of cattle, such as Jerseys, usually reach puberty earlier at a lighter weight.[19] Under typical nutritional conditions, Holstein heifers will reach puberty at the age 9–10 months. Proper body condition for breeding is also largely judged by weight. At about 800lbs Holstein heifers will normally be able to carry a healthy calf and give birth with relative ease.[18] In this way, the heifers will be able to give birth and join the milking herd before their second birthday.[19]

Puberty coincides with the beginning of estrous cycles. Estrous cycles are the recurring hormonal and physiological changes that occur within the bodies of most mammalian females that lead to ovulation and the development of a suitable environment for embryonic and fetal growth. The cow is considered polyestrous, which means that she will continue to undergo regular estrous cycles until death unless the cycle is interrupted by a pregnancy.[19]

In cows, a complete estrous cycle lasts 21 days. Most commonly, dairy producers discuss the estrous cycle as beginning when the cow is receptive to breeding. This short phase lasting only about a day is also known as estrus or colloquially, heat. The cow will often exhibit several behavioral changes during this phase including increased activity and vocalizations. Most importantly, during estrus she will stand still when mounted by another cow or bull.[19]

In the United States, artificial insemination (AI) is a very important reproductive tool used on dairy facilities. AI, is the process by which sperm is deliberately delivered by dairy managers or veterinarians into the cow's uterus. Bulls “donate” semen at a stud farm but there is never any physical contact between the cow and the bull when using this method.[20]

This method of insemination quickly gained popularity among dairy producers for several reasons. Dairy bulls are notoriously dangerous to keep on the average dairy facility. AI also makes it possible to speed the genetic improvement of the dairy herd because every dairy farmer has access to sperm from genetically superior sires. Additionally, AI has been shown to reduce spread of venereal diseases within herd that would ultimately lead to fertility problems. Many producers also find it to be more economical than keeping a bull. On the other hand, AI does require more intensive reproductive management of the herd as well as more time and expertise. Detection of estrus, becomes reliant on observation in the absence of bulls. It takes considerable expertise to properly inseminate a cow and high quality sperm is valuable. Ultimately, because dairy production was already a management intensive industry the disadvantages are dwarfed by the advantages of the AI for many dairy producers.[20]

The majority of cows carry a single calf. Pregnancy lasts an average of 280 to 285 days or a little less than 9 and one half months.[19]

After the birth of a calf the cow begins to lactate. Lactation will normally continue for as long as the cow is milked but production will steadily decline. Dairy farmers are extremely familiar with the pattern of milk production and carefully time the cow's next breeding to maximize milk production. The pattern of lactation and pregnancy is known as the lactation cycle.

For a period of 20 days post parturition the cow is called a fresh cow. Milk production quickly increases during this phase but milk composition is also significantly different from the rest of the cycle. This first milk, called colostrum, is rich in fats, protein, and also maternal immune cells.[21] This colostrum is not usually commercially sold, but is extremely important for early calf nutrition. Perhaps most importantly, it conveys passive immunity to the calf before its immune system is fully developed.[10]

The next 30 to 60 days of the lactation cycle is characterized by peak milk production levels. The amount of milk produced per day during this period varies considerably by breed and by individual cow depending on her body condition, genetics, health, and nutrition.[10] During this period the body condition of the cow will suffer because the cow will draw on her body stores to maintain such high milk production. Food intake of the cow also will increase. After peak lactation, the cow's milk production levels will slowly decline for the rest of the lactation cycle. The producer will often breed the cow soon after she leaves peak production. For a while, the cow's food intake will remain high before also beginning a decline to pre lactation levels. After peak milk production her body condition will also steadily recover.[10]

Producers will typically continue to milk the cow until she is two months away from parturition then they will dry her off. Giving the cow a break during the final stages of pregnancy allows her mammary gland to regress and re-develop, her body condition to recover, and the calf to develop normally. Decreased body condition in the cow means she will not be as productive in subsequent milk cycles. Decreased health in the new born calf will negatively impact the quality of the replacement herd.[10] There is also evidence that increased rates of mammary cell proliferation occur during the dry period that is essential to maintaining high production levels in subsequent lactation cycles.[22]

As measured in phosphorus, the waste output of 5,000 cows roughly equals a municipality of 70,000 people.[23] In the U.S., dairy operations with more than 1,000 cows meet the EPA definition of a CAFO (Concentrated Animal Feeding Operation), and are subject to EPA regulations.[24] For example, in the San Joaquin Valley of California a number of dairies have been established on a very large scale. Each dairy consists of several modern milking parlor set-ups operated as a single enterprise. Each milking parlor is surrounded by a set of 3 or 4 loafing barns housing 1,500 or 2,000 cattle. Some of the larger dairies have planned 10 or more series of loafing barns and milking parlors in this arrangement, so that the total operation may include as many as 15,000 or 20,000 cows. The milking process for these dairies is similar to a smaller dairy with a single milking parlor but repeated several times. The size and concentration of cattle creates major environmental issues associated with manure handling and disposal, which requires substantial areas of cropland (a ratio of 5 or 6 cows to the acre, or several thousand acres for dairies of this size) for manure spreading and dispersion, or several-acre methane digesters. Air pollution from methane gas associated with manure management also is a major concern. As a result, proposals to develop dairies of this size can be controversial and provoke substantial opposition from environmentalists including the Sierra Club and local activists.[25][26]

The potential impact of large dairies was demonstrated when a massive manure spill occurred on a 5,000-cow dairy in Upstate New York, contaminating a 20-mile (32 km) stretch of the Black River, and killing 375,000 fish. On 10 August 2005, a manure storage lagoon collapsed releasing 3,000,000 US gallons (11,000,000 L; 2,500,000 imp gal) of manure into the Black River. Subsequently, the New York Department of Environmental Conservation mandated a settlement package of $2.2 million against the dairy.[23]

When properly managed, dairy and other livestock waste, due to its nutrient content (N, P, K), makes an excellent fertilizer promoting crop growth, increasing soil organic matter, and improving overall soil fertility and tilth characteristics.  Most dairy farms in the United States are required to develop nutrient management plans for their farms, to help balance the flow of nutrients and reduce the risks of environmental pollution.  These plans encourage producers to monitor all nutrients coming onto the farm as feed, forage, animals, fertilizer, etc. and all nutrients exiting the farm as product, crop, animals, manure, etc.[27] For example, a precision approach to animal feeding results in less overfeeding of nutrients and a subsequent decrease in environmental excretion of nutrients, such as phosphorus. In recent years, nutritionists have realized that requirements for phosphorus are much lower than previously thought.[28]  These changes have allowed dairy producers to reduce the amount of phosphorus being fed to their cows with a reduction in environmental pollution.[29]

It is possible to maintain higher milk production by supplementing cows with growth hormones known as recombinant BST or rBST, but this is controversial due to its effects on animal and possibly human health. The European Union, Japan, Australia, New Zealand and Canada have banned its use due to these concerns.[citation needed]

In the US however, no such prohibition exists, but rBST is not used on dairy farms. Most dairy processors, if not all, will not accept milk with rBST.[30] The U.S. Food and Drug Administration states that no ""significant difference"" has been found between milk from treated and non-treated cows[31] but based on consumer concerns several milk purchasers and resellers have elected not to purchase milk produced with rBST.
[32]
[33]
[34]

The practice of dairy production in a factory farm environment has been criticized by animal welfare activists.[35][36] Some of the ethical complaints regarding dairy production cited include how often the dairy cattle must remain pregnant, the separation of calves from their mothers, how dairy cattle are housed and environmental concerns regarding dairy production.

The production of milk requires that the cow be in lactation, which is a result of the cow having given birth to a calf. The cycle of insemination, pregnancy, parturition, and lactation, followed by a ""dry"" period of about two months of forty-five to fifty days, before calving which allows udder tissue to regenerate. A dry period that falls outside this time frame can result in decreased milk production in subsequent lactation.[37]

An important part of the dairy industry is the removal of the calves off the mother's milk after the three days of needed colostrum,[38] allowing for the collection of the milk produced.  On some dairies, in order for this to take place, the calves are fed milk replacer, a substitute for the whole milk produced by the cow.[38]  Milk replacer is generally a powder, which comes in large bags, and is added to precise amounts of water, and then fed to the calf via bucket, bottle or automated feeder.

Milk replacers are classified by three categories: protein source, protein/fat (energy) levels, and medication or additives (e.g. vitamins and minerals).[39]  Proteins for the milk replacer come from different sources; the more favorable and more expensive[40] all milk protein (e.g. whey protein- a by-product of the cheese industry) and alternative proteins including soy, animal plasma and wheat gluten.[39]  The ideal levels for fat and protein in milk replacer are 10-28% and 18-30%, respectively.[39]   
The higher the energy levels (fat and protein), the less starter feed (feed which is given to young animals) the animal will consume.  Weaning can take place when a calf is consuming at least two pounds of starter feed a day and has been on starter for at least three weeks.[40] 
Milk replacer has climbed in cost US$15–20 a bag in recent years, so early weaning is economically crucial to effective calf management.[41]

Common ailments affecting dairy cows include infectious disease (e.g. mastitis, endometritis and digital dermatitis), metabolic disease (e.g. milk fever and ketosis) and injuries caused by their environment (e.g. hoof and hock lesions).[42]

Lameness is commonly considered one of the most significant animal welfare issues for dairy cattle,[42][43][44][45] and is best defined as any abnormality that causes an animal to change its gait.[46] It can be caused by a number of sources, including infections of the hoof tissue (e.g. fungal infections that cause dermatitis) and physical damage causing bruising or lesions (e.g. ulcers or hemorrhage of the hoof).[45] 
Housing and management features common in modern dairy farms (such as concrete barn floors, limited access to pasture and suboptimal bed-stall design) have been identified as contributing risk factors to infections and injuries.[47]

Milk is estimated to have been responsible for 18% of agricultural greenhouse gas emissions in 2014.[48]

There is a great deal of variation in the pattern of dairy production worldwide. Many countries which are large producers consume most of this internally, while others (in particular New Zealand), export a large percentage of their production. Internal consumption is often in the form of liquid milk, while the bulk of international trade is in processed dairy products such as milk powder.[citation needed]

The milking of cows was traditionally a labor-intensive operation and still is in less developed countries. Small farms need several people to milk and care for only a few dozen cows, though for many farms these employees have traditionally been the children of the farm family, giving rise to the term ""family farm"".[citation needed]

Advances in technology have mostly led to the radical redefinition of ""family farms"" in industrialized countries such as Australia, New Zealand, and the United States. With farms of hundreds of cows producing large volumes of milk, the larger and more efficient dairy farms are more able to weather severe changes in milk price and operate profitably, while ""traditional"" family farms generally do not have the equity or income other larger scale farms do. The common public perception of large corporate farms supplanting smaller ones is generally a misconception, as many small family farms expand to take advantage of economies of scale, and incorporate the business to limit the legal liabilities of the owners and simplify such things as tax management.[citation needed]The transition from family farms to farms with employed staff who carry out the day-to-day management of the herd's animals has changed the farmer's duties and role on the farm. New questions have arisen concerning how the development of bigger farms places greater demands on strategies focused on financial control, leadership, and personnel issues.[49]

Before large scale mechanization arrived in the 1950s, keeping a dozen milk cows for the sale of milk was profitable. Now most dairies must have more than one hundred cows being milked at a time in order to be profitable, with other cows and heifers waiting to be ""freshened"" to join the milking herd. In New Zealand, the average herd size increased from 113 cows in the 1975–76 season to 435 cows in 2018–19 season.[50]

Worldwide, the largest cow milk producer is the United States,[51] the largest cow milk exporter is New Zealand,[52][53] and the largest importer is China.[54] The European Union with its present 27 member countries produced 158,800,000 metric tons (156,300,000 long tons; 175,000,000 short tons) in 2013[55](96.8% cow milk), the most by any politico-economic union.

The Canadian dairy industry is one of four sectors that is under the supply management system, a national agricultural policy framework that coordinates supply and demand through production and import control and pricing mechanisms designed to prevent shortages and surpluses, to ensure farmers a fair rate of return and Canadian consumer access to a high-quality, stable, and secure supply of these sensitive products.[56] The milk supply management system is a ""federated provincial policy"" with four governing agencies, organizations and committees—Canadian Dairy Commission, Canadian Milk Supply Management Committee (CMSMC), regional milk pools, and provincial milk marketing boards.[57]: 8  The dairy supply management system is administered by the federal government through the Canadian Dairy Commission (CDC), which was established in 1966 and is composed mostly of dairy farmers, administers the dairy supply management system for Canada's 12,000 dairy farms.[58] The federal government is involved in supply management through the CDC in the administration of imports and exports.[59] The Canadian Milk Supply Management Committee (CMSMC) was introduced in 1970 as the body responsible for monitoring the production rates of milk and setting the national Market Sharing Quota (MSQ) for industrial raw milk.[60]: 31 [61] The supply management system was authorized in 1972 through the Farm Products Agencies Act.[56] Supply management ensures consistent pricing of milk for farmers with no fluctuation in the market.[62] The prices are based on the demand for milk throughout the country and how much is being produced.  In order to start a new farm or increase production more share into the SMS needs to be bought into known as ""Quota"".  in this case farmers must remain up to or below the amount of ""quota"" they have bought share of.  Each province in Canada has their own cap on quota based on the demand in the market.[61][63] There is a cap on the countries quota known as total quota per month. In 2016 the total butter fat produced per month was 28,395,848 kg.[64]

In the United States, the top five dairy states are, in order by total milk production; California,[66] Wisconsin, New York, Idaho,  and Texas.[67] Dairy farming is also an important industry in Florida, Minnesota, Ohio and Vermont.[68] There are 40,000 dairy farms in the United States.[69]

Pennsylvania has 8,500 farms with 555,000 dairy cows. Milk produced in Pennsylvania yields an annual revenue of about US$1.5 billion.[70]

Milk prices collapsed in 2009. Senator Bernie Sanders accused Dean Foods of controlling 40% of the country's milk market. He has requested the United States Department of Justice to pursue an anti-trust investigation.[71] Dean Foods says it buys 15% of the country's raw milk.[72] In 2011, a federal judge approved a settlement of $30 million to 9,000  farmers in the Northeast.[73]

Herd size in the US varies between 1,200 on the West Coast and Southwest, where large farms are commonplace, to roughly 50 in the Midwest and Northeast, where land-base is a significant limiting factor to herd size. The average herd size in the U.S. is about one hundred cows per farm but the midpoint size is 900 cows with 49% of all cows residing on farms of 1000 or more cows.[74]
"
Frozen Food Processing,"Freezing food preserves it from the time it is prepared to the time it is eaten. Since early times,[when?] farmers, fishermen, and trappers have preserved grains and produce in unheated buildings during the winter season.[1] Freezing food slows decomposition by turning residual moisture into ice, inhibiting the growth of most bacterial species. In the food commodity industry, there are two processes: mechanical and cryogenic (or flash freezing). The freezing kinetics is important to preserve the food quality and texture. Quicker freezing generates smaller ice crystals and maintains cellular structure. Cryogenic freezing is the quickest freezing technology available due to the ultra low liquid nitrogen temperature −196 °C (−320 °F).[2]

Preserving food in domestic kitchens during modern times is achieved using household freezers. Accepted advice to householders was to freeze food on the day of purchase. An initiative by a supermarket group in 2012 (backed by the UK's Waste & Resources Action Programme) promotes the freezing of food ""as soon as possible up to the product's 'use by' date"". The Food Standards Agency was reported as supporting the change, provided the food had been stored correctly up to that time.[3]

Frozen products do not require any added preservatives because microorganisms do not grow when the temperature of the food is below −9.5 °C (15 °F), which is sufficient on its own in preventing food spoilage.  Long-term preservation of food may call for food storage at even lower temperatures. Carboxymethylcellulose (CMC), a tasteless and odorless stabilizer, is typically added to frozen food because it does not adulterate the quality of the product.[4]

Natural food freezing (using winter frosts) had been in use by people in cold climates for centuries.

In 1861 Thomas Sutcliffe Mort established at Darling Harbour in Sydney, Australia, the world's first freezing works, which later became the New South Wales Fresh Food and Ice Company. Mort financed experiments by Eugene Dominic Nicolle, a French born engineer who had arrived in Sydney in 1853 and registered his first ice-making patent in 1861. The first trial shipment of frozen meat to London was in 1868. Although their machinery was never used in the frozen meat trade, Mort and Nicolle developed commercially viable systems for domestic trade. The financial return on that investment was minimal for Mort. Regular shipments of frozen meat from Australia and New Zealand to Europe began in 1881, with a consignment of frozen New Zealand sheep exported to London on board the Dunedin.

By 1885 a small number of chickens and geese were being shipped from Russia to London in insulated cases using this technique. By March 1899, the ""British Refrigeration and Allied Interests"" reported that a food importing business, ""Baerselman Bros"", was shipping some 200,000 frozen geese and chickens per week from three Russian depots to New Star Wharf, Lower Shadwell, London over three or four winter months. This trade in frozen food was enabled by the introduction of Linde cold air freezing plants in three Russian depots and the London warehouse. The Shadwell warehouse stored the frozen goods until they were shipped to markets in London, Birmingham, Liverpool and Manchester. The techniques were later expanded to the meat-packing industry.

From 1929, Clarence Birdseye introduced ""flash freezing"" to the American public. Birdseye first became interested in food freezing during fur-trapping expeditions to Labrador in 1912 and 1916, where he saw the natives use natural freezing to preserve foods.[5]  A 1920s hunting trip to Canada, where he witnessed the traditional methods of the indigenous Inuit people, directly inspired Birdseye's food preserving method.[6]

The Icelandic Fisheries Commission was created in 1934 to initiate innovation in the industry, and encouraged fishermen to start quick-freezing their catch.
Íshúsfélag Ísfirðinga, one of the first frozen fish companies, was formed in Ísafjörður, Iceland, by a merger in 1937.[7]
More advanced attempts include food frozen for Eleanor Roosevelt on her trip to Russia. Other experiments involving orange juice, ice cream and vegetables were conducted by the military near the end of World War II.

The freezing technique itself, just like the frozen food market, is developing to become faster, more efficient and more cost-effective. As demonstrated by Birdseye's work, faster freezing means smaller ice crystals and a better-preserved product.[8]

Birdseye's original cryogenic freezing approach using immersion in liquid nitrogen is still used.[9] Due to its cost, however, use is limited to fish fillets, seafood, fruits, and berries. It is also possible to freeze food by immersion in the warmer (at −70 °C (−94 °F)), but cheaper, liquid carbon dioxide, which can be produced by mechanical freezing (see below).[8]

Most frozen food is instead frozen using a mechanical process using the vapor-compression refrigeration technology similar to ordinary freezers. Such a process is cheaper at scale, but is usually slower. (There is also more upfront investment in the form of construction.) Nevertheless, a wide variety of processes have been devised to achieve faster heat transfer from the food to the refrigerant:[8]

Individual Quick Freezing is a descriptive term that includes all forms of freezing that is ""individual"" (not in a whole block) and ""quick"" (taking a maximum of several minutes). It may correspond to cryogenic freezing, fluidized bed freezing, or any other technique that meets the definition.

Frozen food packaging must maintain its integrity throughout filling, sealing, freezing, storage, transportation, thawing, and often cooking.[10] As many frozen foods are cooked in a microwave oven, manufacturers have developed packaging that can go directly from freezer to the microwave.

In 1974, the first differential heating container (DHC) was sold to the public. A DHC is a sleeve of metal designed to allow frozen foods to receive the correct amount of heat. Various sized apertures were positioned around the sleeve. The consumer would put the frozen dinner into the sleeve according to what needed the most heat. This ensured proper cooking.[11]

Today there are multiple options for packaging frozen foods. Boxes, cartons, bags, pouches, Boil-in-Bags, lidded trays and pans, crystallized PET trays, and composite and plastic cans.[12]

Scientists continue to research new aspects of frozen food packaging. Active packaging offers many new technologies that can actively sense and then neutralize the presence of bacteria or other harmful species. Active packaging can extend shelf-life, maintain product safety, and help preserve the food over a longer period of time. Several functions of active packaging are being researched:

The process of flash freezing itself generally effectively retain the nutrient content of foodstuff with minor losses of vitamins, making them a cost-effective and nutritious substitute from fresh equivalents. However, pre-seasoned frozen food, such as packaged meals, may have a significant amounts of salt and fats added. It is therefore recommended to read the nutrition label and the ingredients list.[14]

Freezing is an effective form of food preservation because the pathogens that cause food spoilage are either killed or do not grow very rapidly at reduced temperatures. The process is less effective in food preservation than are thermal techniques, such as boiling, because pathogens are more likely to be able to survive cold temperatures rather than hot temperatures.[21] One of the problems surrounding the use of freezing as a method of food preservation is the danger that pathogens deactivated (but not killed) by the process will once again become active when the frozen food thaws.

Foods may be preserved for several months by freezing. Long-term frozen storage requires a constant temperature of −18 °C (0 °F) or less.[22]

To be used, many cooked foods that have been previously frozen require defrosting prior to consumption. Preferably, some frozen meats should be defrosted prior to cooking to achieve the best outcome: cooked through evenly and of good texture.

The defrost system in freezers helps the equipment to perform properly, without thick layers of ice developing, thus preventing the evaporator coil from absorbing heat and cooling the cabinet.

Ideally, most frozen foods should be defrosted in a refrigerator to avoid significant growth of pathogens. However, this can require considerable time.

Food is often defrosted in one of several ways:

People sometimes defrost frozen foods at room temperature because of time constraints or ignorance. Such foods should be promptly consumed after cooking or discarded and never be refrozen or refrigerated since pathogens are not killed by the refreezing process.[citation needed]

The speed of freezing has a direct impact on the size and the number of ice crystals formed within a food product's cells and extracellular space. Slow freezing leads to fewer but larger ice crystals while fast freezing leads to smaller but more numerous ice crystals. This difference in ice crystal size can affect the degree of residual enzymatic activity during frozen storage via the process of freeze concentration, which occurs when enzymes and solutes present in a fluid medium are concentrated between ice crystal formations.[25] Increased levels of freeze concentration, mediated by the formation of large ice crystals, can promote enzymatic browning.[26]

Large ice crystals can also puncture the walls of the cells of the food product which will cause a degradation of the texture of the product as well as the loss of its natural juices during thawing.[27] That is why there will be a qualitative difference observed between food products frozen by ventilated mechanical freezing, non-ventilated mechanical freezing or cryogenic freezing with liquid nitrogen.[28]

According to a 2007 study, an American consumes frozen food on average 71 times a year, most of which are pre-cooked frozen meals.[29]
"
Ice Production Services,"An icemaker, ice generator, or ice machine may refer to either a consumer device for making ice, found inside a home freezer; a stand-alone appliance for making ice, or an industrial machine for making ice on a large scale. The term ""ice machine"" usually refers to the stand-alone appliance.

The ice generator is the part of the ice machine that actually produces the ice. This would include the evaporator and any associated drives/controls/subframe that are directly involved with making and ejecting the ice into storage. When most people refer to an ice generator, they mean this ice-making subsystem alone, minus refrigeration.

An ice machine, however, particularly if described as 'packaged', would typically be a complete machine including refrigeration, controls, and dispenser, requiring only connection to power and water supplies.

The term icemaker is more ambiguous, with some manufacturers describing their packaged ice machine as an icemaker, while others describe their generators in this way.

In 1748, the first known artificial refrigeration was demonstrated by William Cullen at the University of Glasgow.[1] Mr. Cullen never used his discovery for any practical purposes. This may be the reason why the history of the icemakers begins with Oliver Evans, an American inventor who designed the first refrigeration machine in 1805. In 1834, Jacob Perkins built the first practical refrigerating machine using ether in a vapor compression cycle. The American inventor, mechanical engineer and physicist received 21 American and 19 English patents (for innovations in steam engines, the printing industry and gun manufacturing among others) and is considered today the father of the refrigerator.[2]

In 1844, an American physician, John Gorrie, built a refrigerator based on Oliver Evans' design to make ice to cool the air for his yellow fever patients.[3] His plans date back to 1842, making him one of the founding fathers of the refrigerator. Unfortunately for John Gorrie, his plans of manufacturing and selling his invention were met with fierce opposition by Frederic Tudor, the Boston “Ice King”. By then, Tudor was shipping ice from the United States to Cuba and was planning to expand his business to India. Fearing that Gorrie’s invention would ruin his business, he began a smear campaign against the inventor. In 1851, John Gorrie was awarded U.S. Patent 8080 for an ice machine.[4] After struggling with Tudor's campaign and the death of his partner, John Gorrie also died, bankrupt and humiliated. His original icemaker plans and the prototype machine are held today at the National Museum of American History, Smithsonian Institution in Washington, D.C.[5]

In 1853, Alexander Twining was awarded U.S. Patent 10221 for an icemaker. Twining’s experiments led to the development of the first commercial refrigeration system, built in 1856. He also established the first artificial method of producing ice.  Just like Perkins before him, James Harrison started experimenting with ether vapor compression. In 1854, James Harrison successfully built a refrigeration machine capable of producing 3,000 kilograms of ice per day and in 1855 he received an icemaker patent in Australia, similar to that of Alexander Twining. Harrison continued his experiments with refrigeration. Today he is credited for his major contributions to the development of modern cooling system designs and functionality strategies. These systems were later used to ship refrigerated meat across the globe. 

In 1867, Andrew Muhl built an ice-making machine in San Antonio, Texas, to help service the expanding beef industry before moving it to Waco in 1871.[6] In 1873, the patent for this machine was contracted by the Columbus Iron Works,[7] which produced the world's first commercial icemakers. William Riley Brown served as its president and George Jasper Golden served as its superintendent.

In 1876, German engineer Carl von Linde patented the process of liquefying gas that would later become an important part of basic refrigeration technology (U.S. Patent 1027862). In 1879 and 1891, two African American inventors patented improved refrigerator designs in the United States (Thomas Elkins – U.S. patent #221222  and respectively John Standard – U.S. patent #455891).

In 1902, the Teague family of Montgomery purchased control of the firm.  Their last advertisement in Ice and Refrigeration appeared in March 1904.[8] In 1925, controlling interest in the Columbus Iron Works passed from the Teague family to W.C. Bradely of W.C. Bradley, Co.[8]

Jurgen Hans is credited with the invention of the first ice machine to produce edible ice in 1929. In 1932 he founded a company called Kulinda and started manufacturing edible ice, but by 1949 the business switched its central product from ice to central air conditioning.[9]

The ice machines from the late 1800s to the 1930s used toxic gases such as ammonia (NH3), methyl chloride (CH3Cl), and sulfur dioxide (SO2) as refrigerants. During the 1920s, several fatal accidents were registered. They were caused by the refrigerators leaking methyl chloride. In the quest of replacing dangerous refrigerants – especially methyl chloride – collaborative research ensued in American corporations. The result of this research was the discovery of Freon. In 1930, General Motors and DuPont formed Kinetic Chemicals to produce Freon, which would later become the standard for almost all consumer and industrial refrigerators. The original ""Freon"" produced at this time was chlorofluorocarbon, a moderately toxic gas causing ozone depletion.[10]

All refrigeration equipment is made of four key components; the evaporator, the condenser, the compressor and the throttle valve. Ice machines all work the same way. The function of the compressor is to compress low-pressure refrigerant vapor to high-pressure vapor, and deliver it to the condenser.  Here, the high-pressure vapor is condensed into high-pressure liquid, and drained out through the throttle valve to become low-pressure liquid. At this point, the liquid is conducted to the evaporator, where heat exchanging occurs, and ice is created. This is one complete refrigeration cycle.

Automatic icemakers for the home were first offered by the Servel company around 1953.[11][12] They are usually found inside the freezer compartment of a refrigerator.  They produce crescent-shaped ice cubes from a metal mold.  An electromechanical or electronic timer first opens a solenoid valve for a few seconds, allowing the mold to fill with water from the domestic cold water supply.  The timer then closes the valve and lets the ice freeze for about 30 minutes.  Then, the timer turns on a low-power electric heating element inside the mold for several seconds, to melt the ice cubes slightly so they will not stick to the mold.  Finally, the timer runs a rotating arm that scoops the ice cubes out of the mold and into a bin, and the cycle repeats.  If the bin fills with ice, the ice pushes up a wire arm, which shuts off the icemaker until the ice level in the bin goes down again.  The user can also lift up the wire arm at any time to stop the production of ice.

Later automatic icemakers in Samsung refrigerators use a flexible plastic mold. When the ice cubes are frozen, which is sensed by a Thermistor, the timer causes a motor to invert the mold and twist it so that the cubes detach and fall into a bin.

Early icemakers dropped the ice into a bin in the freezer compartment; the user had to open the freezer door to obtain ice.  In 1965, Frigidaire introduced icemakers that dispensed from the front of the freezer door.[13]  In these models, pressing a glass against a cradle on the outside of the door runs a motor, which turns an auger in the bin and delivers ice cubes to the glass.  Most dispensers can optionally route the ice through a crushing mechanism to deliver crushed ice.  Some dispensers can also dispense chilled water.

There are alternatives to freezer compartment icemakers developed by manufacturers such as Whirlpool, LG, Samsung. This new type of icemaker located in the fresh food compartment is becoming a more popular feature among customers shopping for a new refrigerator with an icemaker. In order to function properly, the icemaker compartment should keep temperature inside around 0 °C (32 °F) and needs to be properly sealed from the outside, since it is located in the fresh food compartment where temperatures are usually higher than 2 °C (36 °F). Unfortunately, there are some disadvantages for this type of icemakers and due to design flaws of icemaker compartment in the Samsung refrigerator, warm air getting inside through the seals and create water condensation. This condensation turning into ice chunks and jamming icemaker mechanism.[14] Thousands of people in the United States were experiencing this issue and in 2017 was created a lawsuit against Samsung refusing to properly fix this issue.[15]

Portable icemakers are units that can fit on a countertop.[16] They are the fastest and smallest icemakers on the market. The ice produced by a portable icemaker is bullet-shaped and has a cloudy, opaque appearance. The first batch of ice can be made within 10 minutes of turning the appliance on and adding water.  The water is pumped into a small tube with metal pegs immersed in the water. Because the unit is portable, water must be filled manually. The water is pumped from the bottom of the reservoir to the freeze tray. The pegs use a heating and cooling system inside to freeze the water around them and then heat up so the ice slips off the peg and into the storage bin.[17] Ice begins to form in a matter of minutes, however, the size of ice cubes depends on the freezing cycle - a longer cycle results in thicker cubes. Portable icemakers will not keep the ice from melting, but the appliance will recycle the water to make more ice. Once the storage tray is full, the system will turn off automatically.

Built-in icemakers are engineered to fit under a kitchen or bar counter, but they can be used as freestanding units. Some produce crescent-shaped ice like the ice from a freezer icemaker; the ice is cloudy and opaque instead of clear, because the water is frozen faster than in others which are clear cube icemakers. In the process, tiny air bubbles get trapped, causing the cloudy appearance of the ice. However, most under-counter ice makers are clear ice makers in which the ice is missing the air bubbles, and therefore the ice is clear and melts much slower.

Commercial ice makers improve the quality of ice by using moving water. The water is run down a high nickel content stainless steel evaporator. The surface must be below freezing. Salt water requires lower temperatures to freeze and will last longer. Generally used to package seafood products. Air and undissolved solids will be washed away to such an extent that in horizontal evaporator machines the water has 98% of the solids removed, resulting in very hard, virtually pure, clear ice.  In vertical evaporators the ice is softer, more so if there are actual individual cube cells. Commercial ice machines can make different sizes of ice like flakes, crushed, cubes, octagons, and tubes.

When the sheet of ice on the cold surface reaches the desired thickness, the sheet is slid down onto a grid of wires, where the sheet's weight causes it to be broken into the desired shapes, after which it falls into a storage bin.

Flake ice is made of the mixture of brine and water (max 500 g [18 oz] salt per ton of water), in some cases can be directly made from brine water. Thickness between 1 and 15 mm (1⁄16 and 9⁄16 in), irregular shape with diameters from 12 to 45 mm (1⁄2 to 1+3⁄4 in).

The evaporator of the flake ice machine is a vertically placed drum-shaped stainless steel container, equipped with a rotating blade that spins and scratches the ice off the inner wall of the drum. When operating, the principal shaft and blade spin anti-clockwise pushed by the reducer. Water is sprayed down from the sprinkler; ice is formed from the water brine on the inner wall. The water tray at the bottom catches the cold water while deflecting Ice and re-circulates it back into the sump. The sump will typically use a float valve to fill as needed during production. Flake machines have a tendency to form an ice ring inside the bottom of the drum. Electric heaters are in wells at the very bottom to prevent this accumulation of ice where the crusher does not reach. Some machines use scrapers to assist this. This system utilizes a low-temperature condensing unit; like all ice machines. Most manufactures also utilize an evaporator pressure regulating valve (EPRV).

Sea water flake ice machine can make ice directly from the seawater. This ice can be used in the fast cooling of fish and other sea products. The fishing industry is the largest user of flake ice machines. Flake ice can lower the temperature of cleaning water and sea products, therefore it resists the growth of bacteria and keeps the seafood fresh.

Because of its large contact and less damage with refrigerated materials, it is also applied in vegetable, fruit, and meat storing and transporting.

In baking, during the mixing of flour and milk, flake ice can be added to prevent the flour from self-raising.

In most cases of biosynthesis and chemosynthesis, flake ice is used to control the reaction rate and maintain the liveness. Flake ice is sanitary, clean with a rapid temperature reduction effect.

Flake ice is used as the direct source of water in the concrete cooling process, more than 80% in weight. Concrete will not crack if has been mixed and poured at a constant and low temperature.

Flake ice is also used for artificial snow, so it is widely applied in ski resorts and entertainment parks.

Cube ice machines are classified as small ice machines, in contrast to tube ice machines, flake ice machines, or other ice machines. Common capacities range from 30 kg (66 lb) to 1,755 kg (3,869 lb). Since the emergence of cube ice machines in the 1970s, they have evolved into a diverse family of ice machines.

Cube ice machines are commonly seen as vertical modular devices. The upper part is an evaporator, and the lower part is an ice bin. The refrigerant circulates inside pipes of a self-contained evaporator[further explanation needed], where it conducts the heat exchange with water, and freezes the water into ice cubes. Once frozen, an ejection mechanism releases the cubes into a collection bin. Frigidaire ice makers introduced in various types such as under counter, countertop, and commercial models, cube ice makers cater to diverse settings including food and beverage industries, healthcare, and residential use.  When the water is thoroughly frozen into ice, it is automatically released, and falls into the ice bin.

Ice machines can have either a self-contained refrigeration system where the compressor is built into the unit, or a remote refrigeration system where the refrigeration components are located elsewhere, often the roof of the business.

Most compressors are either positive displacement compressors or radial compressors. Positive displacement compressors are currently the most efficient type of compressor, and have the largest refrigerating effect per single unit (400–2500 RT)[further explanation needed]. They have a large range of possible power supplies, and can be 380 V, 1000 V, or even higher. The principle behind positive displacement compressors utilizes a turbine to compress refrigerant into high-pressure vapor. Positive displacement compressors are of four main types: screw compressor, rolling piston compressor, reciprocating compressor, and rotary compressor.

Screw compressors[18] can yield the largest refrigerating effect among positive displacement compressors, with their refrigerating capacity normally ranging from 50 RT to 400 RT[further explanation needed]. Screw compressors also can be divided into single-screw type and dual-screw type. The Dual-screw type is more often seen in use because it is very efficient.

Rolling piston compressors and reciprocating compressors have similar refrigerating effects, and the maximum refrigerating effect can reach 600 kW.[further explanation needed]

Reciprocating compressors are the most common type of compressor because the technology is mature and reliable. Their refrigerating effect ranges from 2.2 kW to 200 kW.[further explanation needed] They compress gas by utilizing a piston pushed by a crank shaft.

Rotary compressors, mainly used in air conditioning equipment, have a very low refrigerating effect, normally not exceeding 5 kW. They work by compressing gas using a piston pushed by a rotor, which spins in an isolated compartment.[19]

All condensers can be classified as one of three types: air cooling, water cooling, or evaporative cooling.

A tube ice generator is an ice generator in which the water is frozen in tubes that are extended vertically within a surrounding casing—the freezing chamber. At the bottom of the freezing chamber, there is a distributor plate having apertures surrounding the tubes and attached to the separate chamber into which a warm gas is passed to heat the tubes and cause the ice rods to slide down.[20]

Tube ice can be used in cooling processes, such as temperature controlling, fresh fish freezing, and beverage bottle freezing. It can be consumed alone or with food or beverages.

As of 2019 there were approximately 2 billion household refrigerators and over 40 million square meters of cold-storage facilities operating worldwide.[21]  In the US in 2018 almost 12 million refrigerators were sold.[22]  This data supports the assertion that refrigeration has global applications with positive impact upon the economy, technology, social dynamics, health, and the environment.

Refrigeration is necessary for the implementation of many current or future energy sources (hydrogen liquefying for alternative fuels in the automotive industry and thermonuclear fusion production for the alternative energy industries). 

In the food industry, refrigeration contributes to reducing post-harvest losses while supplying foods to consumers, enabling perishable foods to be preserved at all stages from production to consumption.

In the medical sector, refrigeration is used for transport of vaccines, organs, and stem cells, while cryotechnology is used in surgery and other medical research courses of action.

Refrigeration is used in biodiversity maintenance based on the cryopreservation of genetic resources (cells; tissues; and organs of plants, animals and micro-organisms).

Refrigeration enables the liquefaction of CO2 for underground storage, allowing the potential separation of CO2 from fossil fuels in power stations via cryogenic technology.

At an environmental level, the impact of refrigeration is caused by atmospheric emissions of refrigerant gases used in refrigerating installations and the energy consumption of these refrigerating installations which contribute to CO2 emissions – and consequently to global warming – thus reducing global energy resources. The atmospheric emissions of refrigerant gases are based on the leaks occurring in insufficiently leak-tight refrigerating installations or during maintenance-related refrigerant-handling processes.

Depending on the refrigerants used, these installations and their subsequent leaks can lead to ozone depletion (chlorinated refrigerants like CFCs and HCFCs) and/or climate change, by exerting an additional greenhouse effect (fluorinated refrigerants: CFCs, HCFCs and HFCs).

In their continuous research of methods to replace ozone-depleting refrigerants and greenhouse refrigerants (CFCs, HCFCs and HFCs, respectively) the scientific community together with the refrigerant industry came up with alternative all-natural refrigerants which are eco-friendly. According to a report issued by the UN Environment Programme, “the increase in HFC emissions is projected to offset much of the climate benefit achieved by the earlier reduction in the emissions of Ozone depleting substances”.[23] Among non-HFC refrigerants found to successfully replace the traditional ones are ammonia, hydrocarbons and carbon dioxide.

The history of refrigeration began with the use of ammonia. After more than 120 years, this substance is still the preeminent refrigerant used by household, commercial and industrial refrigeration systems. The major problem with ammonia is its toxicity at relatively low concentrations. On the other hand, ammonia has zero impact on the ozone layer and very low global warming effects. While deaths caused by ammonia exposure are extremely rare, the scientific community has come up with safer and technologically solid mechanisms of preventing ammonia leakage in modern refrigerating equipment. This problem out of the way, ammonia is considered an eco-friendly refrigerant with numerous applications.

Carbon dioxide has been used as a refrigerant for many years. Just like ammonia, it has fallen in almost complete disuse due to its low critical point and its high operating pressure. Carbon dioxide has zero impact on the ozone layer and the global warming effects of the quantities required for use as a refrigerant are also negligible. Modern technology is solving such issues and CO2 is widely used today as an alternative to traditional refrigeration[24] in several fields: industrial refrigeration (CO2 is usually combined with ammonia, either in cascade systems or as a volatile brine), the food industry (food and retail refrigeration), heating (heat pumps) and the transportation industry (transport refrigeration).

Hydrocarbons are natural products with high thermodynamic properties, zero ozone-layer impact and negligible global warming effects. One issue with hydrocarbons is that they are highly flammable, restricting their use to specific applications in the refrigeration industry.

In 2011, the EPA has approved three alternative refrigerants to replace hydrofluorocarbons (HFCs) in commercial and household freezers via the Significant New Alternatives Policy (SNAP) program.[25] The three alternative refrigerants legalized by the EPA were hydrocarbons propane, isobutane and a substance called HCR188C[26] – a hydrocarbon blend (ethane, propane, isobutane and n-butane). HCR188C is used today in commercial refrigeration applications (supermarket refrigerators, stand-alone refrigerators and refrigerating display cases), in refrigerated transportation, automotive air-conditioning systems and retrofit safety valve (for automotive applications) and residential window air-conditioners.

In October 2016, negotiators from 197 countries have reached an agreement to reduce emissions of chemical refrigerants that contribute to global warming, re-emphasizing the historical importance of the Montreal Protocol and aiming to increase its impact upon the use greenhouse gases besides the efforts made to reduce ozone depletion caused by the chlorofluorocarbons. The agreement, closed at a United Nations meeting in Kigali, Rwanda set the terms for a rapid phasedown of hydrofluorocarbons (HFCs)[27] which would be stopped from manufacturing altogether and have their uses reduced over time.

The UN agenda and the Rwanda deal aims to find a new generation of refrigerants to be safe from both an ozone layer and greenhouse effect point of view. The legally binding agreement could reduce projected emissions by as much as 88% and lower global warming with almost 0.5 degrees Celsius (nearly 1 degree Fahrenheit) by 2100.[28]
"
Canning Services,"

Canning is a method of food preservation in which food is processed and sealed in an airtight container (jars like Mason jars, and steel and tin cans). Canning provides a shelf life that typically ranges from one to five years,[a] although under specific circumstances, it can be much longer.[2] A freeze-dried canned product, such as canned dried lentils, could last as long as 30 years in an edible state.

In 1974, samples of canned food from the wreck of the Bertrand, a steamboat that sank in the Missouri River in 1865, were tested by the National Food Processors Association. Although appearance, smell, and vitamin content had deteriorated, there was no trace of microbial growth and the 109-year-old food was determined to be still safe to eat.[3]

Shortly before the Napoleonic Wars, the French government offered a hefty cash award of 12,000 francs to any inventor who could devise a cheap and effective method of preserving large amounts of food to create well-preserved military rations for the Grande Armée. The larger armies of the period required increased and regular supplies of quality food. Limited food availability was among the factors limiting military campaigns to the summer and autumn months. In 1809, Nicolas Appert, a French confectioner and brewer, observed that food cooked inside a jar did not spoil unless the seals leaked, and developed a method of sealing food in glass jars.[4] Appert was awarded the prize in 1810 by Count Montelivert, a French minister of the interior.[5] The reason for lack of spoilage was unknown at the time, since it would be another 50 years before Louis Pasteur demonstrated the role of microbes in food spoilage and developed pasteurization.

The Grande Armée began experimenting with issuing canned foods to its soldiers, but the slow process of canning and the even slower development and transport stages prevented the army from shipping large amounts across the French Empire, and the wars ended before the process was perfected.

Following the end of the Napoleonic Wars, the canning process was gradually employed in other European countries and the United States.

Based on Appert's methods of food preservation, the tin can process was allegedly developed by Frenchman Philippe de Girard, who came to London and used British merchant Peter Durand as an agent to patent his own idea in 1810.[6] Durand did not pursue food canning himself, selling his patent in 1811 to Bryan Donkin and John Hall, who were in business as Donkin Hall and Gamble, of Bermondsey.[7] Bryan Donkin developed the process of packaging food in sealed airtight cans, made of tinned wrought iron. Initially, the canning process was slow and labour-intensive, as each large can had to be hand-made, and took up to six hours to cook, making canned food too expensive for ordinary people.

The main market for the food at this stage was the British Army and Royal Navy. By 1817, Donkin recorded that he had sold £3000 (equal to £277,470 today) worth of canned meat in six months. In 1824, Sir William Edward Parry took canned beef and pea soup with him on his voyage to the Arctic in HMS Fury, during his search for a northwestern passage to India. In 1829, Admiral Sir James Ross also took canned food to the Arctic, as did Sir John Franklin in 1845.[8] Some of his stores were found by the search expedition led by Captain (later Admiral Sir) Leopold McClintock in 1857.

During the mid-19th century, canned food became a status symbol among middle-class households in Europe, being something of a frivolous novelty. Early methods of manufacture employed poisonous lead solder for sealing the cans. Studies in the 1980s attributed the lead from the cans as a factor in the disastrous outcome of the 1845 Franklin expedition to chart and navigate the Northwest Passage.[9] However, studies in 2013 and 2016 suggested that lead poisoning was likely not a factor, and that the crew's ill health may, in fact, have been due to malnutrition—specifically zinc deficiency—possibly due to a lack of meat in their diet.[10][11]

Increasing mechanization of the canning process, coupled with a huge increase in urban populations across Europe, resulted in a rising demand for canned food. A number of inventions and improvements followed, and by the 1860s smaller machine-made steel cans were possible, and the time to cook food in sealed cans had been reduced from around six hours to thirty minutes.

Canned food also began to spread beyond Europe—Robert Ayars established the first American canning factory in New York City in 1812, using improved tin-plated wrought-iron cans for preserving oysters, meats, fruits, and vegetables. Demand for canned food greatly increased during wars. Large-scale wars in the nineteenth century, such as the Crimean War, American Civil War, and Franco-Prussian War, introduced increasing numbers of working-class men to canned food, and allowed canning companies to expand their businesses to meet military demands for non-perishable food, enabling companies to manufacture in bulk and sell to wider civilian markets after wars ended. Urban populations in Victorian Britain demanded ever-increasing quantities of cheap, varied, quality food that they could keep at home without having to go shopping daily. In response, companies such as Underwood, Nestlé, Heinz, and others provided quality canned food for sale to working class city-dwellers. The late 19th century saw the range of canned food available to urban populations greatly increase, as canners competed with each other using novel foodstuffs, highly decorated printed labels, and lower prices.

Demand for canned food skyrocketed during World War I, as military commanders sought vast quantities of cheap, high-calorie food to feed their millions of soldiers, which could be transported safely, survive trench conditions, and not spoil in transport. Throughout the war, British soldiers generally subsisted on low-quality canned food, such as the British bully beef, pork and beans, canned sausages, and Maconochie's stew, but by 1916, widespread dissatisfaction and increasing complaints about the poor quality canned food among soldiers resulted in militaries seeking better-quality food to improve morale, and complete meals-in-a-can began to appear. In 1917, the French Army began issuing canned French cuisine such as coq au vin, beef bourguignon, french onion soup, and Vichyssoise, while the Italian Army experimented with canned ravioli, spaghetti bolognese, minestrone, and pasta e fagioli. After the war, companies that had supplied military canned food began to improve the quality of their goods for civilian sale.

The original fragile and heavy glass containers presented challenges for transportation, and glass jars were largely replaced in commercial canneries with cylindrical tin can or wrought-iron canisters (later shortened to ""cans"") following the work of Peter Durand (1810). Cans are cheaper and quicker to make, and much less fragile than glass jars.

Can openers were not invented for another thirty years. At first, soldiers would cut the cans open with bayonets or smash them open with rocks.[citation needed] Today, tin-coated steel is the material most commonly used. Aseptically processed retort pouches are also used for canning.

Glass jars have remained popular for some high-value products and in home canning.

To prevent the food from being spoiled before and during containment, a number of methods are used: pasteurisation, boiling (and other applications of high temperature over a period of time), refrigeration, freezing, drying, vacuum treatment, antimicrobial agents that are natural to the recipe of the foods being preserved, a sufficient dose of ionizing radiation, submersion in a strong saline solution, acid, base, osmotically extreme (for example very sugary) or other microbially-challenging environments.

Other than sterilization, no method is perfectly dependable as a preservative. Sterilization is done after the can is sealed, so that both the container and the food are secured.

The spores of the microorganism Clostridium botulinum (which causes botulism) can be eliminated only at temperatures above the boiling point of water. As a result, from a public safety point of view, foods with low acidity (a pH more than 4.6) need sterilization under high temperature (116–130 °C). To achieve temperatures above the boiling point requires the use of a pressure canner. Foods that must be pressure canned include most vegetables, meat, seafood, poultry, and dairy products. The only foods that may be safely canned in an ordinary boiling water bath are highly acidic ones with a pH below 4.6, such as fruits, pickled vegetables, or other foods to which acidic additives have been added. Although an ordinary boiling temperature does not kill botulism spores, the acidity is enough to stop them from growing.[12]

Invented in 1888 by Max Ams,[13] modern double seams provide an airtight seal to a can. This airtight nature is crucial to keeping micro-organisms out of the can and keeping the can's contents sealed inside. Thus, double seamed cans are also known as Sanitary Cans. Developed in 1900 in Europe, this sort of can was made of the traditional cylindrical body made with tin plate. The two ends (lids) were attached using what is now called a double seam. A can thus sealed is impervious to contamination by creating two tight continuous folds between the can's cylindrical body and the lids. This eliminated the need for solder and allowed improvements in manufacturing speed, reducing cost.

Double seaming uses rollers to shape the can, lid and the final double seam. To make a sanitary can and lid suitable for double seaming, manufacture begins with a sheet of coated tin plate. To create the can body, rectangles are cut and curled around a die, and welded together creating a cylinder with a side seam.

Rollers are then used to flare out one or both ends of the cylinder to create a quarter circle flange around the circumference. Precision is required to ensure that the welded sides are perfectly aligned, as any misalignment will cause inconsistent flange shape, compromising its integrity.

A circle is then cut from the sheet using a die cutter. The circle is shaped in a stamping press to create a downward countersink to fit snugly into the can body. The result can be compared to an upside down and very flat top hat. The outer edge is then curled down and around about 140 degrees using rollers to create the end curl.

The result is a steel tube with a flanged edge, and a countersunk steel disc with a curled edge. A rubber compound is put inside the curl.

The body and end are brought together in a seamer and held in place by the base plate and chuck, respectively. The base plate provides a sure footing for the can body during the seaming operation and the chuck fits snugly into the end (lid). The result is the countersink of the end sits inside the top of the can body just below the flange. The end curl protrudes slightly beyond the flange.

Once brought together in the seamer, the seaming head presses a first operation roller against the end curl. The end curl is pressed against the flange curling it in toward the body and under the flange. The flange is also bent downward, and the end and body are now loosely joined. The first operation roller is then retracted. At this point five thicknesses of steel exist in the seam. From the outside in they are:

The seaming head then engages the second operation roller against the partly formed seam. The second operation presses all five steel components together tightly to form the final seal. The five layers in the final seam are then called; a) End, b) Body Hook, c) Cover Hook, d) Body, e) Countersink. All sanitary cans require a filling medium within the seam because otherwise the metal-to-metal contact will not maintain a hermetic seal. In most cases, a rubberized compound is placed inside the end curl radius, forming the critical seal between the end and the body.

Probably the most important innovation since the introduction of double seams is the welded side seam. Prior to the welded side seam, the can body was folded and/or soldered together, leaving a relatively thick side seam. The thick side seam required that the side seam end juncture at the end curl to have more metal to curl around before closing in behind the Body Hook or flange, with a greater opportunity for error.

Many different parts during the seaming process are critical in ensuring that a can is airtight and vacuum sealed. The dangers of a can that is not hermetically sealed are contamination by foreign objects (bacteria or fungicide sprays), or that the can could leak or spoil.

One important part is the seamer setup. This process is usually performed by an experienced technician. among the parts that need setup are seamer rolls and chucks which have to be set in their exact position (using a feeler gauge or a clearance gauge). The lifter pressure and position, roll and chuck designs, tooling wear, and bearing wear all contribute to a good double seam.

Incorrect setups can be non-intuitive. For example, due to the springback effect, a seam can appear loose, when in reality it was closed too tight and has opened up like a spring. For this reason, experienced operators and good seamer setup are critical to ensure that double seams are properly closed.

Quality control usually involves taking full cans from the line – one per seamer head, at least once or twice per shift, and performing a teardown operation (wrinkle/tightness), mechanical tests (external thickness, seamer length/height and countersink) as well as cutting the seam open with a twin blade saw and measuring with a double seam inspection system. The combination of these measurements will determine the seam's quality.

Use of a statistical process control (SPC) software in conjunction with a manual double-seam monitor, computerized double seam scanner, or even a fully automatic double seam inspection system makes the laborious process of double seam inspection faster and much more accurate. Statistically tracking the performance of each head or seaming station of the can seamer allows for better prediction of can seamer issues, and may be used to plan maintenance when convenient, rather than to simply react after bad or unsafe cans have been produced.[14]

Canning is a way of processing food to extend its shelf life. The idea is to make food available and edible long after the processing time. A 1997 study found that canned fruits and vegetables are as rich with dietary fiber and vitamins as the same corresponding fresh or frozen foods, and in some cases the canned products are richer than their fresh or frozen counterparts.[15] The heating process during canning appears to make dietary fiber more soluble, and therefore more readily fermented in the colon into gases and physiologically active byproducts. Canned tomatoes have a higher available lycopene content. Consequently, canned meat and vegetables are often among the list of food items that are stocked during emergencies.[16]

In the beginning of the 19th century the process of canning foods was mainly done by small canneries. These canneries were full of overlooked sanitation problems, such as poor hygiene and unsanitary work environments. Since the refrigerator did not exist and industrial canning standards were not set in place it was very common for contaminated cans to slip onto the grocery store shelves.[17]

According to The Fruits of Empire: Art, Food and the Politics of Race in the Age of American Expansion by Shana Klein, ""Workers also suffered injuries, specifically bruised knuckles and open sores, from trimming and packaging pineapples. Gloves were one preventative measure to protect a canner's hands from the acidity of the pineapple, but gloves did not always help.""[18]

In canning toxicology, migration is the movement of substances from the can itself into the contents.[19] Potential toxic substances that can migrate are lead, causing lead poisoning, or bisphenol A (BPA), a potential endocrine disruptor that is an ingredient in the epoxy commonly used to coat the inner surface of cans. Some cans are manufactured with a BPA-free enamel lining produced from plant oils and resins.[20] In February 2018, the Can Manufacturers Institute, a  trade association in the United States, surveyed the industry and reported that at least 90% of food cans no longer contained BPA.[21]

Salt (sodium chloride), dissolved in water, is used in the canning process, which helps prevent spoilage and can improve sensory characteristics.[22] As a result, canned food can be a major source of dietary salt.[23] Too much salt increases the risk of health problems, including high blood pressure. Therefore, health authorities have recommended limitations of dietary sodium.[24][25][26][27][28] Many canned products are available in low-salt and no-salt alternatives.

Rinsing thoroughly after opening may reduce the amount of salt in canned vegetables, since much of the salt content is thought to be in the liquid, rather than the food itself.[29]

Foodborne botulism results from contaminated foodstuffs in which C. botulinum spores have been allowed to germinate and produce botulism toxin,[30] and this typically occurs in canned non-acidic food substances that have not received a strong enough thermal heat treatment. C. botulinum prefers low oxygen environments and is a poor competitor to other bacteria, but its spores are resistant to thermal treatments. When a canned food is sterilized insufficiently, most other bacteria besides the C. botulinum spores are killed, and the spores can germinate and produce botulism toxin.[30] Botulism is a rare but serious paralytic illness, leading to paralysis that typically starts with the muscles of the face and then spreads towards the limbs.[31] The botulinum toxin is extremely dangerous because it cannot be detected by sight or smell, and ingestion of even a small amount of the toxin can be deadly.[32] In severe forms, it leads to paralysis of the breathing muscles and causes respiratory failure. In view of this life-threatening complication, all suspected cases of botulism are treated as medical emergencies, and public health officials are usually involved to prevent further cases from the same source.[31]

Canned goods and canning supplies sell particularly well in times of recession due to the tendency of financially stressed individuals to engage in cocooning, a term used by retail analysts to describe the phenomenon in which people choose to stay at home instead of adding expenditures to their budget by dining out and socializing outside the home.[citation needed]. Also, some people may become preppers and proceed to stockpile canned food.[33] A doomer would also be interested in stockpiling canned food upon learning about a recession.[citation needed]

In February 2009 during a recession, the United States saw an 11.5% rise in sales of canning-related items.[34]

Some communities in the US have county canning centers which are available for teaching canning, or shared community kitchens which can be rented for canning one's own foods.[35]
"
Animal Feed Manufacturing,"Feed manufacturing refers to the process of producing animal feed from raw agricultural products. Fodder produced by manufacturing is formulated to meet specific animal nutrition requirements for different species of animals at different life stages. According to the American Feed Industry Association (AFIA),[1] there are four basic steps:

The Washington State Department of Agriculture defines feed as a mix of whole or processed grains, concentrates, and commercial feeds for all species of animals to include customer formula and labeled feeds, and pet feed. These feed are now commercially produced for the livestock, poultry, swine, and fish industries. The commercial production of feed is governed by state and national laws. For example, in Texas, whole or processed grains, concentrates, and commercial feeds with the purpose of feeding wildlife and pets should be duly described in words or animation for distribution by sellers. Most State and Federal codes have clearly stated that commercial feeds should not be adulterated.[2][3]

Animal feeds have been broadly classified as follows:

The quality of the prepared feed ultimately depends on the quality of the material such as the grain or grass used; the raw material should be of very good quality. Commercial feed manufacturing is an industrial process, and therefore should follow HACCP procedures. The Food and Drug Administration (FDA) defines HACCP as ""a management system in which food safety is addressed through the analysis and control of biological, chemical, and physical hazards from raw material production, procurement and handling, to manufacturing, distribution and consumption of the finished product"".[4]

The FDA regulates human food and animal feed for poultry, livestock, swine, and fish. Additionally, the FDA regulates pet food, which they estimate feeds over 177 million dogs, cats, and horses in America. Similar to human foods, animal feeds must be unadulterated and wholesome, prepared under good sanitary conditions, and truthfully be labeled to provide the required information to the consumer.[5]

Feed makes up approximately 60% to 80% of the total cost of producing hogs. Manufactured feeds are not merely for satiety but also must provide animals the nutrients required for healthy growth. Formulating a swine ration considers the required nutrients at various growth stages in creating an appropriate feed. Three basic methods are used to formulate swine diets: Pearson square, algebraic equations and linear programs (computers). In recent times, microcomputer programs are available that will balance a diet for many nutrients and assist with economic decisions.[6]

The basic nutrients required are crude protein, metabolizable energy, minerals, vitamins and water. The formulation procedure has both fixed and variable portions. Swine rations are generally based on a ground cereal grain as a carbohydrate source, soybean meal as a protein source, minerals like calcium and phosphorus are added, and vitamins. The feed can be fortified with byproducts of milk, meat by-products, cereal grains; and ""specialty products."" Antibiotics may also be added to fortify the feed and help the animal's health and growth.[6][7][8]

Distillers dried grains with solubles (DDGS), which are rich in energy and protein, have been used in place of corn and soybean meal in some livestock and poultry feeds and corn DDGS have become the most popular, economical, and widely available alternative feed ingredient for use in U.S. swine diets in all phases of production. The U.S. Grain Council reported that corn DDGS is used primarily as an energy source in swine diets because it contains approximately the same amount of digestible energy (DE) and metabolizable energy (ME) as corn, although the ME content may be slightly reduced when feeding reduced-oil DDGS.[9][10]

A 2007 study highlighted the recent trends in the use of DDGS, as many producers are including 20% DDGS in diets of swine in all categories. Although 20% is the recommended level of inclusion, some producers are successfully using greater inclusion rates. Inclusion rate of up to 35% DDGS has been used in diets fed to nursery pigs and finishing pigs.[11]

Farmed fish eat specially formulated pellet feeds containing the required nutrients for both fish health and the health of humans who eat fish. A fish feed should be nutritionally well-balanced and provide a good energy source for better growth. Commercially farmed fish are broadly classified into herbivorous fish, which eat mostly plant proteins like soy or corn, vegetable oils, minerals, and vitamins; and carnivorous fish, which are given fish oils and proteins. Carnivorous fish feed contains 30-50% fish meal and oil, but recent research suggests finding alternatives to fish meal in aquaculture diets.[12]

Among the various feeds investigated, soybean meal appears to be a better alternative to fishmeal. Soybean meal prepared for the fish industry is heavily dependent on the particle sizes contained in the feed pellets. Today technology to process these types of feed is based on fish feed extruder machines. Fish feed extruder is essential for vegetable protein processing. Particle size influences feed digestibility. The particle sizes of fish pellet feed are influenced by both grain properties and the milling process. Properties of the grain include hardness and moisture content. The milling process affects particle size based on the mill equipment type used, and some properties of the mill equipment (for example corrugations, gap, speed, and energy consumption).[13]

As reports have indicated, feeding makes up the major cost in raising poultry animals as birds in general require feeding more than any other animals, particularly due to their faster growth rate and high rate of productivity. Feeding efficiency is reflected on the birds' performance and their products. According to National Research Council (1994), poultry requires at least 38% components in their feed. The ration of each feed components, although differing for each different stage of birds, must include carbohydrates, fats, proteins, minerals and vitamins. Carbohydrates, which are usually supplied by grains including corn, wheat, barley, etc. serves as a major energy source in poultry feed. Fats, usually from tallow, lard or vegetable oil are essentially required to provide important fatty acid in poultry feed for membrane integrity and hormone synthesis.

Proteins are important to supply the essential amino acids for the development of body tissues like muscles, nerves, cartilage, etc. Meals from soybean, canola, and corn gluten are the major source of plant protein in poultry diets. Supplementation of minerals are often required because grains, which are the main component of commercial feed contain very little amounts of these. Calcium, phosphorus, chlorine, magnesium, potassium and sodium are required in larger amounts by poultry. Vitamins, such as vitamin A, B, C, D, E, and K, on the other hand, are the components that are required in lower amounts by poultry animals.[14]

Fanatico (2003) reported that the easiest and most popular way to feed birds is to use pelleted feeds. Aside from the convenience to the farmer, pelleted feeds enable the bird to eat more at a time. In addition, some researchers also found improvement of feed conversion, decreased feed wastage, improved palatability and destruction of pathogens when birds were fed with pellet feed as compared to birds fed with mash feed.[15]

Commercial manufacturing of pelleted feed usually involves a series of major processes including grinding, mixing and pelleting. The produced pellets are then tested as to pellet durability index (PDI) to determine quality. To enhance good health and growth, antibiotics are often added to the pelleted feed.

Researchers have concluded that smaller particle-sized feed will improve digestion due to the increased surface area for acid and enzyme digestion in the gastrointestinal tract. However, some researchers have recently brought to our attention the necessity of coarse particles for poultry feed to complement the natural design and function of the gastrointestinal tract (GIT). Hetland et al (2002) and Svihus et al. (2004) discussed that the GIT retention time decreased due to lack of gizzard function that eventually gave a negative impact on live performance. Zanotto & Bellaver (1996) compared the performance of 21-day-old broilers fed with different feed particle size; 0.716 mm and 1.196 mm. They found that the subject fed with larger particle size feed showed better performance. Parsons et al. (2006), evaluating different corn particle sizes in the broiler feed found that the largest particle size (2.242 mm) gave better feed intake than the other particle sizes tested (0.781, 0.950, 1.042 and 1.109 mm). Nir et al. (1994), however, argued that the development of broilers was influenced by changing particle sizes. However, variation in particle size between 0.5–1 mm usually did not have any effect on the broilers. Very fine particles (<0.5 mm) may impair the broilers performance due to the presence of dust that cause respiratory problems, increased water intake, feed presence in the drinkers and increased litter moisture.[16] Chewning et al. (2012), in their recent study, concluded that although fine particle sizes (0.27 mm) enhanced broilers live performance, the pelleted feed did not.[17]

All of this data shows that both fine and coarse particle sizes do have different functions in poultry feed. Appropriate proportions of these two ingredients must be used with respect to the live performance of the broilers. Xu et al. (2013) compared the performance of non-pelleted feed to pellets with fine particles and found that the addition of coarse particles improved feed conversion and body weight. Similar results were also obtained by other researchers like Auttawong et al. (2013) and Lin et al. (2013).

Livestock includes beef cattle, dairy cattle, horses, goats, sheep and llamas. There is no specific requirement of feed intake for each livestock because their feed continuously varies based on the animals' age, sex, breed, environment, etc. However, basic nutrient requirements of a livestock's feed must consist of protein, carbohydrates, vitamins and minerals.[18]

Dairy cattle need more energy in their feed than other types of cattle. Studies have shown that energy supplied by feed is provided by various carbohydrate sources, including non-fiber carbohydrates (NFC) such as fermentable feeds or neutral detergent fiber (NDF) such as forage. Feeds with high NDF are good for rumen health, however, they provide less energy and vice versa. Fats are added in the livestock feed to increase energy concentration, especially when the NFC content is already too high since excessive NFC lessens the NDF fraction, affecting the rumen digestion. In ruminants, most proteins consumed are broken down by microorganisms and the microorganisms later get digested by the small intestine.[19]

The National Research Council suggested that the crude protein required in livestock feed should be less than 7%. Lactating ruminants, especially dairy cattle require the highest amount of protein, especially for milk synthesis. Minerals including calcium, phosphorus and selenium are required by livestock for maintaining growth, reproduction and bone health.[20]

Like other animals, livestock also require appropriate proportions of fine and coarse particles in their feed. Theoretically, finer particles will be easier to digest in the rumen, however the presence of coarse particles might increase the amount of starch entering the small intestine, thus increasing energetic efficiency.[21]

Livestock could be fed by grazing on grasslands, integrated or non-integrated with crop production. Livestock produced in stalls or feedlots are landless and are typically fed by processed feed containing veterinary drugs, growth hormones, feed additives, or nutraceuticals to improve production. Similarly, livestock consume grains as the main feed or as a supplement to the forage based feed. Processing grains for feed is aimed at getting the easiest digestible grains to maximize starch availability, thus increasing the energy supply.[22]

Hutjens (1999) reported that milk performance was significantly better when the cattle were fed with ground corn. A study compared the digestibility of various corn particle sizes and distribution and concluded that to have 80% digestibility, a particle size of 0.5 mm should be used (for 16 hr incubation).[23]

A research team from the University of Maryland and the USDA studied the development, fermentation in rumen and starch digestion sites in dairy cow feeding on corn grain from different harvests and differently processing, and concluded that digestion, metabolism and heat energy were higher for high moisture corn compared to dry corn. Grinding increased DMI and resulted in increased yields of milk, protein, lactose and non-fat solids.

Depending on the type of feed, the manufacturing process usually start with the grinding process. Figure 1 illustrates the workflow for general feed manufacturing process. Grinding of selected raw material is to produce particle sizes to be optimally and easily accepted by the animals. Depending on the formulation, feed could contain up to 10 different components including carbohydrate, protein, vitamins, minerals and additives. The feed ration can be pelleted by proportionally homogenizing the specific compositions. Pelleting is achieved by various methods, but the most common means is by extrusion. Formula and feed making machine are very important during the entire process of the feed production to ensure quality feed.[24]

Corn, sorghum, wheat and barley are the most used cereals in the preparation of feed for the livestock, poultry, swine, and fish industry. Roller and hammer mills are the two types of processing equipment generally used to grind grains into smaller particle sizes.[25][26]

Milling cereal grains by mechanical action involves several forces like compression, shearing, crushing, cutting, friction and collision. The particle size of the ground cereal is very important in the animal feed production; smaller particle sizes increase the number of particles and the surface area per unit volume which increase access to digestive enzymes. Other benefits are increased ease of handling and easier mixing of ingredients.[25] The average particle size is given as geometric mean diameter (GMD), expressed in mm or microns (μm) and the range of variation is described by geometric standard deviation (GSD), with a larger GSD representing lower uniformity.[27]

According to Lucas (2004), GMD and GSD are accurate descriptors of particle size distribution when the particle size distribution is expressed as log data, and are distributed log normally. Studies have shown that grinding different grains with the same mill under similar conditions results in products with different particle sizes. The hardness of a grain sample is related to the percentage of fine particles obtained after grinding, with a higher percentage of fine particles from lower hardness grains.[28][29]

Rose et al. (2001) discussed that hard endosperm produces irregularly-shaped larger particles, while soft endosperm produces smaller size particles. The correlation between particle size and energy consumed is although not positive but, to obtain very fine particle sizes require higher energy which reduces the rate of production. Moreover, a very fine grind of grain has no impact on the efficiency of pelleting, nor on the power consumed during pelleting. Amerah et al. (2007) discussed the availability of more data suggesting grain particle sizes are very important in mashed diets than in pelleted diets.[30][31]
"
Pet Food Manufacturing,"Pet food is animal feed intended for consumption by pets. Typically sold in pet stores and supermarkets, it is usually specific to the type of animal, such as dog food or cat food. Most meat used for animals is a byproduct of the human food industry, and is not regarded as ""human grade"".[1] Examples of foods for pets would be canned foods and dry mix.[2] Pet food production has environmental, land-use and climate change impacts.

In 2019, the world pet food market was valued at US$87.09 billion and is projected to grow to US$113.2 billion by the year 2024. The pet food market is dominated by five major companies, as of 2020: Mars, Inc., Nestle Purina Petcare, J. M. Smucker, Hill's Pet Nutrition, Inc. (owned by Colgate-Palmolive), and Blue Buffalo Co. Ltd (owned by General Mills).[2]

In the United States, pet-food sales in 2016 reached an all-time high of $28.23 billion.[3] Mars is the leading company in the pet food industry, making about $17 billion annually in pet-care products.[4] Online sales of pet food are increasing and contributing to this growth. Online sales in the US increased 15 percent in 2015.[5]  Worldwide, the compound annual growth rate of pet food purchased online was more than 25% between 2013 and 2018.[6] As of 2015[update], the U.S. leads the world in pet-food spending.[7]

As of 2018, there are around 470 million pet dogs and around 370 million pet cats.[8][better source needed] Given the carnivorous diets fed to many pets (especially cats and dogs), involving the consumption of an estimated fifth of the world's meat and fish, the impact of pet-food production on climate change, land-use and other environmental impacts becomes an issue.[9][10] Pet food production is responsible for 20-30% of the environmental impacts from animal production.[11] It has been estimated that global greenhouse gas emissions from dog and cat dry food represents around 1.1%−2.9% of global emissions, an amount close to the total emissions of countries such as Mozambique or the Philippines.[12]

Like humans, dogs are omnivores.[13][14][15] There is research on alternative protein sources for pet food including insects and algae.[16][17] Although cats are obligate carnivores, they digest plant proteins well.[18]

A life-cycle analysis of contemporary pet foods suggests wet foods for cats and dogs tend to have a larger impact than dry foods.[9] It also suggests there are substantial opportunities for improvement in ""all phases of the pet food life cycle, including formulation, ingredient selection, manufacturing processes"" and so on.[9]

Fish foods normally contain macronutrients, trace elements and vitamins necessary to keep captive fish in good health. Approximately 80% of fishkeeping hobbyists feed their fish exclusively prepared foods that most commonly are produced in flake, pellet or tablet form.[19] Pelleted forms, some of which sink rapidly, are often used for larger fish or bottom-feeding species such as loaches or catfish.[20] Some fish foods also contain additives, such as beta carotene or sex hormones, to artificially enhance the color of ornamental fish.[21]

Bird foods are used both in birdfeeders and to feed pet birds. It typically consist of a variety of seeds. However, not all birds eat seeds. Nectar (essentially sugar water) attracts hummingbirds.[22]

Cats are obligate carnivores, which means that they have a nutritional requirement for nutrients that are only naturally found in cats. Cat food is formulated to address the specific nutritional requirements of cats, in particular containing the amino acid taurine, as cats cannot thrive on taurine-deficient food.[23] Optimal levels of taurine for cat food have been established by the Waltham Centre for Pet Nutrition.[24] Modern standards such as those established by the AAFCO specify not only taurine amounts, but also amounts of every essential nutrient.[25]

Most commercial cat food contains both animal and plant material supplemented with vitamins, minerals and other nutrients. Cats absorb plant proteins quite well: their protein absorption is increased when half of their protein intake is switched to a plant-based source.[18]

Recommendations differ on what diet is best for dogs. Some people argue dogs have thrived on leftovers and scraps from their human owners for thousands of years, and commercial dog foods (which have only been available for the past century) contain poor-quality meats, additives, and other ingredients dogs should not ingest, or that commercial dog food is not nutritionally sufficient for their dogs. Many commercial brands are formulated using insights gained from scientific nutritional studies.[26]

Many dog food manufacturers refer to AAFCO standards to ensure the safety of their dog food. AAFCO is a collaborative organization involving feed control officers and regulatory authorities from the United States and Canada. It plays an important role in ensuring the safety and quality of pet food and animal feed, providing reliable standards for the entire industry. However, AAFCO itself does not have the authority to create laws. Instead, its standards serve as the basis for individual states, regions, and federal governments in the U.S. and Canada to establish and enforce laws and regulations regarding pet food.

Additionally, based on the recommended nutritional guidelines for dog food and cat food that AAFCO published in 1993, nutritional standards have been set for different life stages (growth stage for puppies and maintenance stage for adult dogs). Many dog food manufacturers now refer to these standards.[27] The latest AAFCO standard was published in 2013.[25]

Raw feeding is the practice of feeding domestic dogs, cats and other animals a diet consisting primarily of uncooked meat, edible bones, and organs. The ingredients used to formulate raw diets can vary. Some pet owners choose to make home-made raw diets to feed their animals, but commercial raw food diets are also available. Veterinary associations such as the American Veterinary Medical Association, British Veterinary Association and Canadian Veterinary Medical Association have warned of the animal and public health risk that could arise from feeding raw meat to pets and have stated that there is no scientific evidence to support the claimed benefits of raw feeding.[28][29]

The practice of feeding raw diets has raised some concerns due to the risk of food borne illnesses, zoonosis and nutritional imbalances.[30] People who feed their dogs raw food do so for a multitude of reasons, including but not limited to: culture, beliefs surrounding health, nutrition and what is perceived to be more natural for their pets.[31] Feeding raw food can be perceived by owners as allowing the pet to stay in touch with their wild, carnivorous ancestry.[31] The raw food movement has occurred in parallel to the change in human food trends for more natural and organic products.[32]

In some cases, raw food have added preservatives to extend their shelf life. One preservative used, sulfur dioxide (E220), destroys thiamine in meat and sometimes other food eaten with meat. This has led to at least three clusters of thiamine deficiency in Australian cats and dogs. Sulfites (E221–E229) also rapidly release sulfur dioxide. In addition, sulfur dioxide does not prevent the spoilage of meat, but only delays the appearance of signs of spoilage (malodor and color changing from pink to brown). As of 2005, there is no law in New South Wales mandating that all preservatives be listed on ""pet mince"" and ""food rolls"".[33][34] All reports of this issue on PubMed are from Australia.[35]

Prepared foods and some raw ingredients may be toxic for animals, and care should be taken when feeding animals leftover food.  It is known that the following foods are potentially unsafe for cats, dogs and pigs:

Generally, cooked and marinated foods should be avoided, as well as sauces and gravies, which may contain ingredients that, although well tolerated by humans, may be toxic to animals. Xylitol, an alternative sweetener found in chewing gum and baked goods designed for diabetics, is highly toxic to cats, dogs, and ferrets.[38][39]

In the United States and its associated territories, all pet food is regulated by the Food and Drug Administration (FDA), the United States Department of Agriculture (USDA), and the Federal Trade Commission (FTC).  It is further regulated at the state level.[40] State Department of Agriculture officials, major feed manufacturers, and ingredient suppliers form the Association of American Feed Control Officials (AAFCO), a non-government agency that establishes guidelines and standards on feed laws and regulations. Although government officials do comprise a large portion of AAFCO, it has no regulatory authority and acts simply as an advisory body, working closely with the FDA to develop standards that food consumed by animals must meet. AAFCO leaves the responsibility of regulating these standards to the individual states. Most states have adopted the guidelines set forth by AAFCO.[41]

AAFCO requires that all pet food products sold in the United States have labels that contain eight components:

Dog and cat foods labeled as ""complete and balanced"" must meet standards established by the AAFCO either by meeting a nutrient profile or by passing a feeding trial. Cat and dog food nutrient profiles were established by the AAFCO's Feline Nutrition Expert Subcommittee (1991–1992) and the Canine Nutrition Expert Subcommittee (1990–1991), respectively. The nutrient profiles were updated in 2016.[43]

The ""Family Rule"" allows a manufacturer to have a product that is ""nutritionally similar"" to another product in the same ""family"" to adopt the latter's ""complete and balanced"" statement without itself undergoing any feeding tests. The ""similar"" food must be of the same processing type; contain the same moisture content; bear a statement of nutritional adequacy for the same or less demanding life stage as the lead product; contain a dry matter, metabolizable energy (ME) content within 7.5% of the lead product's dry matter; meet the same levels of crude protein, calcium, phosphorus, zinc, lysine, thiamine (and for cat foods, potassium and taurine) as the lead food; and meet or exceed the nutrient levels and ratios of the lead family product or the AAFCO nutrient profiles, whichever is lower. The label statement on the similar food can be the same as the lead product if the ME is substantiated by the 10-day ME feeding study.[42]

Critics of the AAFCO standards argue that such requirements are too lax. Generational studies conducted by researchers at University of California, Davis have shown some foods that pass AAFCO's feeding trials are still not suitable for long-term use and estimated that of 100 foods that pass the nutritional analysis, 10 to 20 would not pass the feeding trials. Although maximum levels of intake of some nutrients have been established because of concerns with overnutrition, many still lack a maximum allowed level and some contains large disparity between maximum and minimum values. The NRC accepts that despite ongoing research, large gaps still exist in the knowledge of quantitative nutritional information for specific nutrients.[44] Some professionals acknowledge the possibilities of phytochemicals and other vital nutrients that have yet to be recognized as essential by nutritional science. With such broad guidelines and loose feeding trial standards, critics argue that the term ""complete and balanced"" is inaccurate and even deceptive.  An AAFCO panel expert has stated that ""although the AAFCO profiles are better than nothing, they provide false securities.""[45]

Certain manufacturers label their products with terms such as premium, ultra premium, and holistic. Such terms currently have no official definitions. The AAFCO is currently considering defining some of the terms. However, the terms ""natural"" and ""organic"" do have definitions;[46] e.g., organic products must meet the same USDA regulations as for organic human food.[citation needed]

In Canada, products that pass the Canadian Veterinary Medical Association (CVMA) Pet Food Certification Program, which involves a feeding trial, carry a CVMA label on their packaging. Participation in the program is voluntary.  The program was discontinued at the end of 2007.[47] There is no government regulation of pet food manufactured in Canada. However, imported pet food does receive stringent oversight.[48]

In the European Union, pet food is regulated by the same harmonised standards across the EU, via the Feeding Stuffs Act.[49]

All ingredients used for pet food have to be fit for human consumption according to EU requirements. But regulations require that pet food that contains by-products be labelled as ""Not for human consumption"" even though such by-products have to be derived from animals declared fit for human consumption. Raw pet food has to be labelled ""Pet food only"".[50]

Products meant for daily feeding are labelled ""complete feedingstuff"" or ""complete petfood"" or other EU languages equivalent. Products meant for intermittent feeding are labeled ""complementary feedingstuff or ""complementary pet food"" while products with an ash content of over 40% are labeled ""mineral feedingstuff"". Ingredients are listed in descending order by weight.[51]

With the released Commission Regulation (EU) No 107/2013, the European Union has set new maximum levels for melamine in canned pet food. According to results of an in-depth research of the 2007 pet food crisis, melamine used in coatings for pet food cans can migrate into the food. Therefore, the regular melamine migration limit (SML) of 2.5 mg/kg for food and feed has been expanded to pet food. This limit is valid for canned wet pet food on an 'as sold' basis.[52]

The European Union does not use a unified nutrient requirement.[53] A manufacturer committee called FEDIAF (European Pet Food Industry Federation) makes recommendations for cats and dogs that members follow.[54]

Beginning in March 2007, there were massive recalls of many brands of cat and dog foods. The recalls came in response to reports of kidney failure in pets consuming mostly wet pet foods made with wheat gluten from a single Chinese company, beginning in February 2007.  After more than three weeks of complaints from consumers, the recall began voluntarily with the Canadian company Menu Foods on March 16, 2007, when a company test showed sickness and death in some of the test animals.  Soon after, there were numerous media reports of animal deaths as a result of kidney failure, and several other companies who received the contaminated wheat gluten also voluntarily recalled dozens of pet food brands. Menu Foods recalled almost over 50 brands of dog food,[55] and over 40 brands of cat food.[56] Nestlé Purina PetCare withdrew all sizes and varieties of Alpo ""Prime Cuts in Gravy"".[57][58] Some companies were not affected and utilized the situation to generate sales for alternative pet foods.[59]

In early January 2021, Midwestern Pet Food products recalled its Sportmix products which were linked to the death of over 70 dogs and sickness in about 80 others. Dog and cat food, sold by retailers across the United States over the internet, were being investigated by the US Food and Drug Administration for the possibility that fatal levels of aflatoxins were present in the food. Midwestern, which is based in Evansville, Indiana, broadened its recall to include all its pet food products manufactured in its Oklahoma facility that contain corn and have expiration dates on or before July 9, 2022.[60]
"
Grain Processing Services,"A grain is a small, hard, dry fruit (caryopsis) – with or without an attached hull layer – harvested for human or animal consumption.[1] A grain crop is a grain-producing plant. The two main types of commercial grain crops are cereals and legumes.

After being harvested, dry grains are more durable than other staple foods, such as starchy fruits (plantains, breadfruit, etc.) and tubers (sweet potatoes, cassava, and more). This durability has made grains well suited to industrial agriculture, since they can be mechanically harvested, transported by rail or ship, stored for long periods in silos, and milled for flour or pressed for oil. Thus, the grain market is a major global commodity market that includes crops such as maize, rice, soybeans, wheat and other grains.

In the grass family, a grain (narrowly defined) is a caryopsis,[2] a fruit with its wall fused on to the single seed inside, belonging to a cereal such as wheat, maize, or rice. More broadly, in agronomy and commerce, seeds or fruits from other plant families are called grains if they resemble cereal caryopses. For example, amaranth is sold as ""grain amaranth"", and amaranth products may be described as ""whole grains"". The pre-Hispanic civilizations of the Andes had grain-based food systems, but at higher elevations none of the grains belonged the cereal family. All three grains native to the Andes (kaniwa, kiwicha, and quinoa) are broad-leaved plants rather than grasses.[3]

Many different species of cereal are cultivated for their grains.[4]

Starchy grains from broadleaf (dicot) plant families are cultivated as nutritious alternatives to cereals. The three major pseudocereal grains are:[5]

Pulses or grain legumes,[6] members of the pea family, have a higher protein content than most other plant foods, at around 20%, while soybeans have as much as 35%. As is the case with all other whole plant foods, pulses also contain carbohydrates and fat. Common pulses include:

Oilseed grains[7] are grown primarily for the extraction of their edible oil. Vegetable oils provide dietary energy and some essential fatty acids.[8] They are also used as fuel and lubricants.[9]

Because grains are small, hard and dry, they can be stored, measured, and transported more readily than can other kinds of food crops such as fresh fruits, roots and tubers.[10] The development of grain agriculture allowed excess food to be produced and stored easily which could have led to the creation of the first temporary settlements and the division of society into classes.[11]

This assumption that grain agriculture led to early settlements and social stratification has been challenged by James Scott in his book Against the Grain.[12] He argues that the transition from hunter-gatherer societies to settled agrarian communities was not a voluntary choice driven by the benefits of increased food production due to the long storage potential of grains, but rather that the shift towards settlements was a coerced transformation imposed by dominant members of a society seeking to expand control over labor and resources.

The grain trade refers to the local and international trade in cereals such as wheat, barley, maize, rice, and other food grains. Grain is an important trade item because it is easily stored and transported with limited spoilage, unlike other agricultural products. Healthy grain supply and trade is important to many societies, providing a caloric base for most food systems as well as important role in animal feed for animal agriculture.

The grain trade is as old as agricultural settlement, identified in many of the early cultures that adopted sedentary farming. Major societal changes have been directly connected to the grain trade, such as the fall of the Roman Empire. From the early modern period onward, grain trade has been an important part of colonial expansion and international power dynamics. The geopolitical dominance of countries like Australia, the United States, Canada and the Soviet Union during the 20th century was connected with their status as grain surplus countries.

Those who handle grain at grain facilities may encounter numerous occupational hazards and exposures. Risks include grain entrapment, where workers are submerged in the grain and unable to extricate themselves;[17] explosions caused by fine particles of grain dust,[18] and falls.
"
Coffee Processing Services,"Coffee production is the industrial process of converting the raw fruit of the coffee plant into the finished coffee. The coffee cherry has the fruit or pulp removed leaving the seed or bean which is then dried. While all green coffee is processed, the method that is used varies and can have a significant effect on the flavor of roasted and brewed coffee.   Coffee production is a major source of income for 12.5 million households, most in developing countries.[1]

A coffee plant usually starts to produce flowers three to four years after it is planted,[2] and it is from these flowers that the fruits of the plant (commonly known as coffee cherries) appear, with the first useful harvest possible around five years after planting. The cherries ripen around eight months after the emergence of the flower, by changing color from green to red, and it is at this time that they should be harvested. In most coffee-growing countries, there is one major harvest a year; though in countries like Colombia, where there are two flowerings a year, there is a main and secondary crop, the main one April to June and a smaller one in November to December.[3]

In most countries, the coffee crop is picked by hand, a labor-intensive and difficult process, though in places like Brazil, where the landscape is relatively flat and the coffee fields are immense, the process has been mechanized.[3] Whether picked by hand or by machine, all coffee is harvested in one of two ways: 	

All coffee fruit is removed from the tree, regardless of maturation state. This can either be done by machine or by hand. In the first method, pickers generally place a canvas on the ground. They then grab the branch next to the trunk with their hands and pull outward, knocking all of the fruit onto the ground. After doing this with all branches and trees for the length of the canvas, the pickers then collect the coffee in bags. This process can be facilitated through the use of mechanical strippers.

Only the ripe cherries are harvested and they are picked individually by hand. Pickers rotate among the trees every eight to ten days, choosing only the cherries which are at the peak of ripeness. It usually takes two to four years after planting for a coffee plant to produce coffee beans that are ripe enough to harvest. The plant eventually grows small white blossoms that drop and are replaced by green berries. These green berries will become a deep red color as they ripen. It takes about 9 months for the green cherries to reach their deepest red color. Because this kind of harvest is labor-intensive, and thus more costly, it is used primarily to harvest the finer arabica beans.[3]

The laborers who pick coffee by hand receive payment by the basketful. As of 2003[update], payment per basket is between US$1.00 to $10 with the overwhelming majority of the laborers receiving payment at the lower end. An experienced coffee picker can collect up to six or seven baskets a day. Depending on the grower, coffee pickers are sometimes specifically instructed to not pick green coffee berries since the seeds in the berries are not fully formed or mature. This discernment typically only occurs with growers who harvest for higher end/specialty coffee where the pickers are paid better for their labor.

Lots including unripe coffee fruit are often used to produce cheaper mass consumer coffee beans, which are characterized by a displeasingly bitter/astringent flavor and a sharp odor. Red berries, with their higher aromatic oil and lower organic acid content, are more fragrant, smooth, and mellow. As such, coffee picking is one of the most important stages in coffee production.[4]

In the ""wet process"", the fruit covering the coffee beans is removed before they are dried. Coffee processed by the wet method is called wet processed or washed coffee.[5] The wet method requires the use of specific equipment and substantial quantities of water.

The coffee cherries are sorted by immersion in water. Bad or unripe fruit will float and the good ripe fruit will sink. The skin of the cherry and some of the pulp is removed by pressing the fruit by machine in water through a screen. The bean will still have a significant amount of the pulp clinging to it that needs to be removed. This is done either by the classic ferment-and-wash method or a newer procedure variously called machine-assisted wet processing, aquapulping or mechanical demucilaging.

In the ferment-and-wash method of wet processing, the remainder of the pulp is removed by breaking down the cellulose by fermenting the beans with microbes and then washing them with large amounts of water. Fermentation can be done with extra water or, in ""dry fermentation"", in the fruit's own juices only.

The fermentation process has to be carefully monitored to ensure that the coffee does not acquire undesirable, sour flavors. For most coffees, mucilage removal through fermentation takes between 8 and 36 hours, depending on the temperature, thickness of the mucilage layer, and concentration of the enzymes. The end of the fermentation is assessed by feel, as the parchment surrounding the beans loses its slimy texture and acquires a rougher ""pebbly"" feel. When the fermentation is complete, the coffee is thoroughly washed with clean water in tanks or in special washing machines.[6]

The fermentation process produces wastewater that contains a high organic load, which should be prevented from entering fresh water supplies.[7] In machine-assisted wet processing, fermentation is not used to separate the bean from the remainder of the pulp; rather, this is done through mechanical scrubbing. This process reduce both water use and the generation of wastewater. In addition, removing mucilage by machine is easier and more predictable than removing it by fermenting and washing.[8][9] However, by eliminating the fermentation step and prematurely separating fruit and bean, mechanical demucilaging can remove an important tool that mill operators have of influencing coffee flavor. Furthermore, the ecological criticism of the ferment-and-wash method increasingly has become moot, since a combination of low-water equipment plus settling tanks allows mill operators to carry out fermentation with limited pollution.[5][dubious – discuss] The downside in using a machine assisted process or ""semi-wash"" is a high chance of the beans being chipped or damaged.[10] The damaged beans are more prominent on lower altitude-grown beans and certain varietals with porous features.[citation needed]

Any wet processing of coffee produces coffee wastewater, which can be a pollutant.[11] Ecologically sensitive farms reprocess the wastewater along with the shell and mucilage as compost to be used in soil fertilization programs. The amount of water used in processing can vary, but most often is used in a 1 to 1 ratio.

After the pulp has been removed, what is left is the bean surrounded by two additional layers: the silver skin and the parchment. The beans must be dried to a water content of about 10% before they are stable. Coffee beans can be dried in the sun or by machine but in most cases it is dried in the sun to 12–13% moisture and brought down to 10% by machine. Drying entirely by machine is normally only done where space is at a premium or the humidity is too high for the beans to dry before mildewing.

When dried in the sun, coffee is most often spread out in rows on large patios where it needs to be raked every six hours to promote even drying and prevent the growth of mildew. Some coffee is dried on large raised tables where the coffee is turned by hand. Drying coffee this way has the advantage of allowing air to circulate better around the beans promoting more even drying but increases cost and labor significantly.

After the drying process (in the sun or through machines), the parchment skin or pergamino is thoroughly dry and crumbly, and easily removed in the hulling process. Coffee occasionally is sold and shipped in parchment or en pergamino, but most often a machine called a huller is used to crunch off the parchment skin before the beans are shipped.[5][dubious – discuss]

Dry process, also known as unwashed or natural coffee, is the oldest method of processing coffee. The entire cherry after harvest is first cleaned and then placed in the sun to dry on tables or in thin layers on patios:[6]

The harvested cherries are usually sorted and cleaned, to separate the unripe, overripe and damaged cherries and to remove dirt, soil, twigs and leaves. This can be done by winnowing, which is commonly done by hand, using a large sieve. Any unwanted cherries or other material not winnowed away can be picked out from the top of the sieve. The ripe cherries can also be separated by flotation in washing channels close to the drying areas.

The coffee cherries are spread out in the sun, either on large concrete or brick patios or on matting raised to waist height on trestles. As the cherries dry, they are raked or turned by hand to ensure even drying and prevent mildew.[12] It may take up to four weeks before the cherries are dried to the optimum moisture content, depending on the weather conditions. On larger plantations, machine-drying is sometimes used to speed up the process. Various types of mechanical driers exist and can be fueled by gas, wood, or sometimes discarded parchment.[13] The technique used to dry coffees mechanically can be viewed similarly to the roasting process; a drying regime can be employed in a way to preserve the quality of the beans.

The drying operation is the most important stage of the process, since it affects the final quality of the green coffee. A coffee that has been overdried will become brittle and produce too many broken beans during hulling (broken beans are considered defective beans). Coffee that has not been dried sufficiently will be too moist and prone to rapid deterioration caused by the attack of fungi and bacteria.

The dried cherries are stored in bulk in special silos until they are sent to the mill where hulling, sorting, grading and bagging take place. All the outer layers of the dried cherry are removed in one step by the hulling machine.

The dry method is used for about 90% of the Arabica coffee produced in Brazil, most of the coffees produced in Ethiopia, Haiti and Paraguay, as well as for some Arabicas produced in India and Ecuador. Almost all Robustas are processed by this method. It is not practical in very rainy regions, where the humidity of the atmosphere is too high or where it rains frequently during harvesting.[6]

Semi-dry is a hybrid process used in Indonesia and Brazil. The process is also called ""wet-hulled"", ""semi-washed"", ""pulped natural"" or, in Indonesia, ""Giling Basah"". Literally translated from Indonesian, Giling Basah means ""wet grinding"", and refers to an earlier ""hulling"" step than compared to the common washed/wet process.[14] This process is said to reduce acidity and increase body.[15]

Most small-scale farmers in Sumatra, Sulawesi, Flores and Papua use the giling basah process. In this process, farmers remove the outer skin from the cherries mechanically, using locally built pulping machines. The coffee beans, still coated with mucilage, are then stored for up to a day. Following this waiting period, the mucilage is washed off and the parchment coffee is partially dried in the sun and sold at 25% to 40% moisture content. Directly hereafter the parchment layer is hulled off and the beans are dried further to 10–12% moisture content. Due to the ""wet hulling"" the beans end with a blue(ish) hue color.[15]

The tricky part during the semi-washed process method are bacteria which are always around. Fermentation can start immediately as honey dried coffee beans have a remaining ""sugar"" layer which is vulnerable to any sort of mold and offers feeding ground for bacteria. Drying carefully and under supervision is crucial to the success of this processing method. The beans need to constantly move during the drying process to prevent mold and fungal infections. The processor needs to rake the green coffee beans 2–3 times per hour to ensure a safe drying process. Once the beans have reached a sufficient moisture level, again, the beans are dry milled to remove the ""parchment"" layers and are sent off to roasters and wholesalers globally.

Honey processing bridges the gap between washed and natural coffees as it generally possesses some of the body and sweetness of a natural while retaining some of the acidity of a washed. Honey coffees often have a syrupy body with enhanced sweetness, round acidity and earthy undertones.

The final steps in coffee processing involve removing the last layers of dry skin and remaining fruit residue from the now-dry coffee, and cleaning and sorting it. These steps are often called dry milling to distinguish them from the steps that take place before drying, which collectively are called wet milling.[3][5][dubious – discuss]

The first step in dry milling is the removal of what is left of the fruit from the bean, whether it is the crumbly parchment skin of wet-processed coffee, the parchment skin and dried mucilage of semi-dry-processed coffee, or the entire dry, leathery fruit covering of the dry-processed coffee. Hulling is done with the help of machines, which can range from simple millstones to sophisticated machines that gently whack at the coffee.[3]

This is an optional process in which any silver skin that remains on the beans after hulling is removed in a polishing machine.[3] This is done to improve the appearance of green coffee beans and eliminate a byproduct of roasting called chaff. It is described by some to be detrimental to the taste because it raises the temperature of the bean through friction, which changes the chemical makeup of the bean.[verification needed]

Most fine coffee goes through a battery of machines that sort the coffee by the density of bean and by bean size, all the while removing sticks, rocks, nails, and miscellaneous debris that may have become mixed with the coffee during drying. First machines blow the beans into the air; those that fall into bins closest to the air source are heaviest and biggest; the lightest (and likely defective) beans plus chaff are blown in the farthest bin. Other machines shake the beans through a series of sieves, sorting them by size. Finally, a machine called a gravity separator shakes the sized beans on a tilted table, so that the heaviest, densest and best vibrate to one side of the pulsating table, and the lightest to the other.[5][dubious – discuss][16]

The final step in the cleaning and sorting procedure is called color sorting, or separating defective beans from sound beans on the basis of color rather than density or size. Color sorting is the trickiest and perhaps most important of all the steps in sorting and cleaning. With most high-quality coffees color sorting is done in the simplest possible way: by hand. Teams of workers pick discolored and other defective beans from the sound beans. The very best coffees may be hand-cleaned twice (double picked) or even three times (triple picked). Coffee that has been cleaned by hand is usually called European preparation; most specialty coffees have been cleaned and sorted in this way.[5][dubious – discuss]

Color sorting can also be done by machines. Streams of beans fall rapidly, one at a time, past sensors that are set according to parameters that identify defective beans by value (dark to light) or by color. A tiny, decisive puff of compressed air pops each defective bean out of the stream of sound beans the instant the machine detects an anomaly. However, these machines are currently not used widely in the coffee industry for two reasons. First, the capital investment to install these delicate machines and the technical support to maintain them is daunting. Second, sorting coffee by hand supplies much-needed work for the small rural communities that often cluster around coffee mills. Nevertheless, computerized color sorters are essential to coffee industries in regions with relatively high standards of living and high wage demands.[5][dubious – discuss]

Grading is the process of categorizing coffee beans by various criteria such as size of the bean, where and at what altitude it was grown, how it was prepared and picked, and how good it tastes (cup quality). Coffees also may be graded by the number of imperfections (broken, under-ripe, or otherwise defective beans; pebbles; sticks; etc.) per sample. For the finest coffees, origin of the beans (farm or estate, region, cooperative) is especially important. Growers of premium estate or cooperative coffees may impose a level of quality control that goes well beyond conventionally defined grading criteria, as this allows their coffee to command the higher price that goes with recognition of consistent quality.[5]

All coffee when it was introduced in Europe came from the port of Mocha in what is now Yemen. Importing the beans to Europe required a lengthy sea voyage around the Horn of Africa, which ultimately changed the coffee's flavor due to age and exposure to saline air. Coffee later spread to India and Indonesia but still required a long sea voyage. Once the Suez Canal was opened, shipment time to Europe was greatly reduced and coffee with flavor less affected by salt and age began arriving. This fresher coffee was, to some degree, rejected as Europeans had not developed a taste for unaged coffee.[17] To meet the demand for aged coffee, some product was aged in large, open-sided warehouses at port for six or more months in an attempt to expose the coffee to the same conditions that shipments used to require.[17]

Although it is still widely debated and subject to personal taste, certain types of green coffee are believed to improve with age – especially strains valued for their low acidity, such as beans from Indonesia or India. Several coffee producers sell purposely aged beans, some aging for as long as eight years. However, coffee experts consensus is that a green coffee peaks in flavor and freshness within one year of harvest and that over-aged coffee beans lose much of their essential oil content.[citation needed]

Decaffeination is the process of extracting caffeine from green coffee beans prior to roasting. The most common decaffeination process used in the United States is supercritical carbon dioxide (CO2) extraction. In this process, moistened green coffee beans are contacted with large quantities of supercritical CO2 (CO2 maintained at a pressure of about 4,000 pounds force per square inch (28 MPa) and temperatures between 90 and 100 °C (194 and 212 °F)), which removes about 97% of the caffeine from the beans. The caffeine is then recovered from the CO2, typically using an activated carbon adsorption system.

Another commonly used method is solvent extraction, typically using oil (extracted from roasted coffee) or ethyl acetate as a solvent. In this process, solvent is added to moistened green coffee beans to extract most of the caffeine from the beans. After the beans are removed from the solvent, they are steam-stripped to remove any residual solvent. The caffeine is then recovered from the solvent, and the solvent is re-used. The Swiss Water Process is also used for decaffeination. Decaffeinated coffee beans have a residual caffeine content of about 0.1% on a dry basis. Not all facilities have decaffeination operations, and decaffeinated green coffee beans are purchased by many facilities that produce decaffeinated coffee.

Green coffee is usually transported in jute bags or woven poly bags. While green coffee may be usable for several years, it is vulnerable to quality degradation based on how it is stored. Jute bags are extremely porous, exposing the coffee to whatever elements it is surrounded by. Coffee that is poorly stored may develop a burlap-like taste known as ""bagginess"", and its positive qualities may fade.

In recent years, the specialty coffee market has begun to utilize enhanced storage methods. A gas barrier liner to jute bags is sometimes used to preserve the quality of green coffee. Less frequently, green coffee is stored in vacuum packaging; while vacuum packs further reduce the ability of green coffee to interact with oxygen at atmospheric moisture, it is a significantly more expensive storage option.

Although not considered part of the processing pipeline proper, nearly all coffee sold to consumers throughout the world is sold as roasted coffee in general one of four degrees of roasting: light, medium, medium-dark, and dark.[18] Consumers can also elect to buy unroasted coffee to be roasted at home. Green coffee can also be used for the preparation of infusions or ingested as ground powder, but this is of limited relevance to the global coffee market.[19]

Bibliography
"
Seed Processing Services,"Seed companies produce and sell seeds for flowers, fruits and vegetables to commercial growers and amateur gardeners. The production of seed is a multibillion-dollar global business, which uses growing facilities and growing locations worldwide. While most of the seed is produced by large specialist growers, large amounts are also produced by small growers who produce only one to a few crop types. The larger companies supply seed both to commercial resellers and wholesalers. The resellers and wholesalers sell to vegetable and fruit growers, and to companies who package seed into packets and sell them on to the amateur gardener.

Most seed companies or resellers that sell to retail produce a catalog, for seed to be sown the following spring, that is generally published during early winter. These catalogs are eagerly awaited by the amateur gardener, as during winter months there is little that can be done in the garden so this time can be spent planning the following year’s gardening. The largest collection of nursery and seed trade catalogs in the U.S. is held at the National Agricultural Library where the earliest catalogs date from the late 18th century, with most published from the 1890s to the present.[1]

Seed companies produce a huge range of seeds from highly developed F1 hybrids to open pollinated wild species. They have extensive research facilities to produce plants with genetic materials that result in improved uniformity and appeal. These qualities might include disease resistance, higher yields, dwarf habit and vibrant or new colors. These improvements are often closely guarded to protect them from being utilized by other producers, thus plant cultivars are often sold under the company's own name and protected by international laws from being grown for seed production by others. Along with the growth in the allotment movement, and the increasing popularity of gardening, there have emerged many small independent seed companies. Many of these are active in seed conservation and encouraging diversity. They often offer organic and open pollinated varieties of seeds as opposed to hybrids. Many of these varieties are heirloom varieties. The use of old varieties maintains diversity in the horticultural gene pool. It may be more appropriate for amateur gardeners to use older (heirloom) varieties as the modern seed types are often the same as those grown by commercial producers, and so characteristics which are useful to them (e.g. vegetables ripening at the same time) may be unsuited to home growing.

Shakers were among the earliest commercial producers of garden seeds; the first seeds sold in paper packets were produced by the Watervliet Shakers in Colonie, New York.[2][3]

Until 1924, US farmers received seed from the federal government's extensive free seed program that distributed millions of packages of seed annually. At its high point in 1897, over 2 million packages of seed were distributed to farmers. Even at the time the program was eliminated in 1924, it was the third largest line item in the United States Department of Agriculture's budget.

In 1930, seed companies were not primarily concerned with varietal production, but were still trying to successfully commodify seeds. There was no need to protect seed breeding at that time because there were few markets for seeds. Seed companies' first priority was simply to establish a market, and they continued to view congressional seed distribution as a principal constraint.[4]

From 1994 to 2010, seed prices increased drastically due to a consolidation of the commercial seed industry into six major companies. During this time, companies introduced six genetically engineered crops for just two traits: herbicide tolerance and insect resistance. In 1996, Monsanto introduced its first RoundUp Ready seeds engineered to tolerate the companies proprietary herbicide.[5]


By 2019, four seed companies, Bayer, Corteva, ChemChina and BASF had consolidated to dominate the commercial seed market, controlling 60% of the global proprietary seed sales. Economists have claimed that the industry has lost its competitive edge and anticipate less choice and higher prices for farmers. There is further concern that due to the companies' interest in intellectual property, there will in future be less innovation and more restrictions on seed availability, which could make the seeds inaccessible to public researchers, farmers, and independent breeders, thereby threatening the world's food security.[6][7] Activists have called for stronger antitrust measures in the face of these mergers and acquisitions, and recommended a United Nations treaty on competition to make changes internationally.[8] Pope Francis refers to these issues in his 2015 encyclical letter Laudato si', on ""care for our common home"":
In various countries, we see an expansion of oligopolies for the production of cereals and other products needed for their cultivation. This dependency would be aggravated were the production of infertile seeds to be considered; the effect would be to force farmers to purchase them from larger producers.[9]: Para. 134 
 Francis calls for a dialogue on seed production issues involving seed producers and all parties affected.[9]: Para. 135 

Generally, seed packet labels include information covering:

This information may be represented graphically.
"
Bakery Production Services,"

A bakery is an establishment that produces and sells flour-based baked goods made in an oven such as bread, cookies, cakes, doughnuts, bagels, pastries, and pies.[1] Some retail bakeries are also categorized as cafés, serving coffee and tea to customers who wish to consume the baked goods on the premises.  In some countries, a distinction is made between bakeries, which primarily sell breads, and pâtisseries, which primarily sell sweet baked goods.

Baked goods have been around for thousands of years. The art of baking was very popular during the Roman Empire. It was highly famous art as Roman citizens loved baked goods and demanded them frequently for important occasions such as feasts and weddings. Because of the fame of the art of baking, around 300 BC, baking was introduced as an occupation and respectable profession for Romans. Bakers began to prepare bread at home in an oven, using grist mills to grind grain into flour for their breads. The demand for baked goods persisted, and the first bakers' guild was established in 168 BC in Rome. The desire for baked goods promoted baking throughout Europe and expanded into eastern parts of Asia. Bakers started baking bread and other goods at home and selling them on the streets.[citation needed]

This trend became common, and soon, baked products were sold in streets of Rome, Germany, London, and more. A system of delivering baked goods to households arose as the demand increased significantly. This prompted bakers to establish places where people could purchase baked goods. The first open-air market for baked goods was established in Paris, and since then bakeries have become a common place to purchase delicious goods and to socialize.[citation needed] On July 7, 1928, a bakery in Chillicothe, Missouri introduced sliced bread using the automatic bread-slicing machine, invented by Otto Frederick Rohwedder. While the bread initially failed to sell, due to its ""sloppy"" aesthetic, and the fact it went stale faster, it later became popular.[2] In World War II bread slicing machines were effectively banned, as the metal in them was required for wartime use. When they were requisitioned, creating 100 tons of metal alloy, the decision proved very unpopular with housewives.[3]

World War II directly affected the bread industry in the UK. Baking schools closed during this time, so when the war ended there was a lack of skilled bakers. This resulted in new methods being developed to satisfy the world's desire for bread, including chemical additives, premixes and specialised machinery. Old methods of baking were almost completely eradicated when these new methods were introduced and the industry became industrialised. The old methods were seen as unnecessary and financially unsound. During this period there were not many traditional bakeries left.

Some bakeries provide services for special occasions (such as weddings, anniversaries, birthday parties, business networking events, etc.) or customized baked products for people who have allergies or sensitivities to certain foods (such as nuts, peanuts, dairy or gluten, etc.). Bakeries can provide a wide range of cake designs such as sheet cakes, layer cakes, wedding cakes, tiered cakes, etc. Other bakeries may specialize in traditional or hand-made types of baked products made with locally milled flour, without flour bleaching agents or flour treatment agents, baking what is sometimes referred to as artisan bread.[1]

In many countries, many grocery stores and supermarkets sell ""sliced bread"" (prepackaged/presliced bread), cakes, and other pastries. They may also offer in-store baking, with products either fully baked on site or part-baked prior to delivery to store,[4] and some offer cake decoration.[5] Nonetheless, many people still prefer to get their baked goods from a small artisanal bakery, either out of tradition, the availability of a greater variety of baked products, or due to the higher quality products characteristic of the trade of baking.[1]

BEST BREAD AND BAKERY MANUFACTURER in Meerut== External links ==
"
Confectionery Manufacturing,"

Confectionery is the art[1][2] of making confections, or sweet foods.[1][2] Confections are items that are rich in sugar and carbohydrates although exact definitions are difficult.[3] In general, however, confections are divided into two broad and somewhat overlapping categories: bakers' confections and sugar confections.[4]

Bakers' confectionery, also called flour confections, includes principally sweet pastries, cakes, and similar baked goods. Baker's confectionery excludes everyday breads, and thus is a subset of products produced by a baker.

Sugar confectionery includes candies (also called sweets, short for sweetmeats,[5] in many English-speaking countries), candied nuts, chocolates, chewing gum, bubble gum, pastillage, and other confections that are made primarily of sugar. In some cases, chocolate confections (confections made of chocolate) are treated as a separate category, as are sugar-free versions of sugar confections.[6] The words candy (Canada and US), sweets (UK, Ireland, and others), and lollies (Australia and New Zealand) are common words for some of the most popular varieties of sugar confectionery.

The occupation of confectioner encompasses the categories of cooking performed by both the French patissier (pastry chef) and the confiseur (sugar worker).[5]  The confectionery industry also includes specialized training schools and extensive historical records.[7] Traditional confectionery goes back to ancient times and continued to be eaten through the Middle Ages and into the modern era.

The oldest recorded use of the word confectionery discovered so far by the Oxford English Dictionary is by Richard Jonas in 1540, who spelled or misspelled it as ""confection nere"" in a passage ""Ambre, muske, frankencense, gallia muscata [fr] and confection nere"", thus in the sense of ""things made or sold by a confectioner"". Also according to the OED, the sense of ""the art and business of a confectioner"" is first recorded in 1743, and the earliest use in the sense of a ""confectioner's shop"" dates to 1803.[2]

Before sugar was readily available in the ancient western world, confectionery was based on honey.[9] Honey was used in Ancient China, Ancient India, Ancient Egypt, Ancient Greece and Ancient Rome to coat fruits and flowers to preserve them or to create sweetmeats.[10] Between the 6th and 4th centuries BCE, the Persians, followed by the Greeks, made contact with the Indian subcontinent and its ""reeds that produce honey without bees"". They adopted and then spread sugar and sugarcane agriculture.[11] Sugarcane is indigenous to tropical Indian subcontinent and Southeast Asia.[12][13][14]

In the early history of sugar usage in Europe, it was initially the apothecary who had the most important role in the production of sugar-based preparations. Medieval European physicians learned the medicinal uses of the material from the Arabs and Byzantine Greeks. One Middle Eastern remedy for rheums and fevers were little, twisted sticks of pulled sugar called in Arabic al fänäd or al pänäd. These became known in England as alphenics, or more commonly as penidia, penids, pennet or pan sugar. They were the precursors of barley sugar and modern cough drops. In 1390, the Earl of Derby paid ""two shillings for two pounds of penydes.[citation needed]""

As the non-medicinal applications of sugar developed, the comfitmaker, or confectioner gradually came into being as a separate trade. In the late medieval period the words confyt, comfect or cumfitt were generic terms for all kinds of sweetmeats made from fruits, roots, or flowers preserved with sugar. By the 16th century, a cumfit was more specifically a seed, nut or small piece of spice enclosed in a round or ovoid mass of sugar. The production of comfits was a core skill of the early confectioner, who was known more commonly in 16th and 17th century England as a comfitmaker. Reflecting their original medicinal purpose, however, comfits were also produced by apothecaries and directions on how to make them appear in dispensatories as well as cookery texts. An early medieval Latin name for an apothecary was confectionarius, and it was in this sort of sugar work that the activities of the two trades overlapped and that the word ""confectionery"" originated.[7]

In the cuisine of the Late Ottoman Empire diverse cosmopolitan cultural influences were reflected in published recipes such as European-style molded jellies flavored with cordials. In Europe, Ottoman confections (especially ""lumps of delight"" (Turkish delight) became very fashionable among European and British high society.[15] An important study of Ottoman confectionery called Conditorei des Orients was published by the royal confectioner Friedrich Unger in 1838.[16]

The first confectionery in Manchester, England was opened by Elizabeth Raffald who had worked six years in domestic service as a housekeeper.[17]

Confections are defined by the presence of sweeteners. These are usually sugars, but it is possible to buy sugar-free candies, such as sugar-free peppermints. The most common sweetener for home cooking is table sugar, which is chemically a disaccharide containing both glucose and fructose. Hydrolysis of sucrose gives a mixture called invert sugar, which is sweeter and is also a common commercial ingredient. Finally, confections, especially commercial ones, are sweetened by a variety of syrups obtained by hydrolysis of starch. These sweeteners include all types of corn syrup.[18]

Bakers' confectionery includes sweet baked goods, especially those that are served for the dessert course. Bakers' confections are sweet foods that feature flour as a main ingredient and are baked. Major categories include cakes, sweet pastries, doughnuts, scones, and cookies.[19] In the Middle East and Asia, flour-based confections predominate.

The definition of which foods are ""confectionery"" vs ""bread"" can vary based on cultures and laws. In Ireland, the definition of ""bread"" as a ""staple food"" for tax purposes requires that the sugar or fat content be no more than 2% of the weight of the flour, so some products sold as bread in the US would be treated as confectionery there.[20]

Cakes have a somewhat bread-like texture, and many earlier cakes, such as the centuries-old stollen (fruit cake), or the even older king cake, were rich yeast breads. The variety of styles and presentations extends from simple to elaborate. Major categories include butter cakes, tortes, and foam cakes. Confusingly, some confections that have the word cake in their names, such as cheesecake, are not technically cakes, while others, such as Boston cream pie are cakes despite seeming to be named something else.

Pastry is a large and diverse category of baked goods, united by the flour-based doughs used as the base for the product. These doughs are not always sweet, and the sweetness may come from the sugar, fruit, chocolate, cream, or other fillings that are added to the finished confection. Pastries can be elaborately decorated, or they can be plain dough.

Doughnuts may be fried or baked.

Scones and related sweet quick breads, such as bannock, are similar to baking powder biscuits and, in sweeter, less traditional interpretations, can seem like a cupcake.

Cookies are small, sweet baked treats. They originated as small cakes, and some traditional cookies have a soft, cake-like texture. Others are crisp or hard.

Sugar confections include sweet, sugar-based foods, which are usually eaten as snack food. This includes sugar candies, chocolates, candied fruits and nuts, chewing gum, and sometimes ice cream. In some cases, chocolate confections are treated as a separate category, as are sugar-free versions of sugar confections.[22]

Different dialects of English use regional terms for sugar confections:

In the US, a chocolate-coated candy bar (e.g. Snickers) would be called a candy bar, in Britain more likely a chocolate bar than unspecifically a sweet.

The United Nations' International Standard Industrial Classification of All Economic Activities (ISIC) scheme (revision 4) classifies both chocolate and sugar confectionery as ISIC 1073, which includes the manufacture of chocolate and chocolate confectionery; sugar confectionery proper (caramels, cachous, nougats, fondant, white chocolate), chewing gum, preserving fruit, nuts, fruit peels, and making confectionery lozenges and pastilles.[25] In the European Union, the Statistical Classification of Economic Activities in the European Community (NACE) scheme (revision 2) matches the UN classification, under code number 10.82.

In the United States, the North American Industry Classification System (NAICS 2012) splits sugar confectionery across three categories: National industry code 311340 for all non-chocolate confectionery manufacturing, 311351 for chocolate and confectionery manufacturing from cacao beans, and national industry 311352 for confectionery manufacturing from purchased chocolate.[26]

Ice cream and sorbet are classified with dairy products under ISIC 1050, NACE 10.52, and NAICS 311520.[27]

Sugar confectionery items include candies, lollipops, candy bars, chocolate, cotton candy, and other sweet items of snack food. Some of the categories and types of sugar confectionery include the following:[18]

Shelf life is largely determined by the amount of water present in the candy and the storage conditions.[29] High-sugar candies, such as boiled candies, can have a shelf life of many years if kept covered in a dry environment. Spoilage of low-moisture candies tends to involve a loss of shape, color, texture, and flavor, rather than the growth of dangerous microbes. Impermeable packaging can reduce spoilage due to storage conditions.

Candies spoil more quickly if they have different amounts of water in different parts of the candy (for example, a candy that combines marshmallow and nougat), or if they are stored in high-moisture environments.[29] This process is due to the effects of water activity, which results in the transfer of unwanted water from a high-moisture environment into a low-moisture candy, rendering it rubbery, or the loss of desirable water from a high-moisture candy into a dry environment, rendering the candy dry and brittle.

Another factor, affecting only non-crystalline amorphous candies, is the glass transition process.[29] This can cause amorphous candies to lose their intended texture.

Both bakers' and sugar confections are used to offer hospitality to guests.

Confections are used to mark celebrations or events, such as Christmas, Easter, a wedding cake, birthday cake, or Halloween. 

The chocolate company Cadbury (under the guidance of Richard Cadbury) was the first to commercialize the connection between romance and confectionery, producing a heart-shaped box of chocolates for Valentine's Day in 1868.[30]

Tourists commonly eat confections as part of their travels. The indulgence in rich, sugary foods is seen as a special treat, and choosing local specialties is popular. For example, visitors to Vienna eat Sachertorte and visitors to seaside resorts in the UK eat Blackpool rock candy. Transportable confections like fudges and tablet may be purchased as souvenirs.[31]

Generally, confections are low in micronutrients and protein but high in calories. They may be fat-free foods, although some confections, especially fried doughs and chocolate, are high-fat foods. Many confections are considered empty calories and ultra-processed foods. Specially formulated chocolate has been manufactured in the past for military use as a high-density food energy source.

Many sugar confections, especially caramel-coated popcorn and the different kinds of sugar candy, are defined in US law as foods of minimal nutritional value.[32]

Contaminants and coloring agents in confectionery can be particularly harmful to children. Therefore, confectionery contaminants, such as high levels of lead, have been restricted to 1 ppm in the US. There is no specific maximum in the EU.[33]

Candy colorants, particularly yellow colorants such as E102 Tartrazine, E104 Quinoline Yellow WS and E110 Sunset Yellow FCF, have many restrictions around the world. Tartrazine, for example, can cause allergic and asthmatic reactions and was once banned in Austria, Germany, and Norway. Some countries such as the UK have asked the food industry to phase out the use of these colorants, especially for products marketed to children.[34]
"
Distilling Services,"

Distillation, also classical distillation, is the process of separating the component substances of a liquid mixture of two or more chemically discrete substances; the separation process is realized by way of the selective boiling of the mixture and the condensation of the vapors in a still.

Distillation can operate over a wide range of pressures from 0.14 bar (e.g., ethylbenzene/styrene) to nearly 21 bar (e.g.,propylene/propane) and is capable of separating feeds with high volumetric flowrates and various components that cover a range of relative volatilities from only 1.17 (o-xylene/m-xylene) to 81.2 (water/ethylene glycol).[2] Distillation provides a convenient and time-tested solution to separate a diversity of chemicals in a continuous manner with high purity. However, distillation has an enormous environmental footprint, resulting in the consumption of approximately 25% of all industrial energy use.[3] The key issue is that distillation operates based on phase changes, and this separation mechanism requires vast energy inputs.

Dry distillation (thermolysis and pyrolysis) is the heating of solid materials to produce gases that condense either into fluid products or into solid products. The term dry distillation includes the separation processes of destructive distillation and of chemical cracking, breaking down large hydrocarbon molecules into smaller hydrocarbon molecules. Moreover, a partial distillation results in partial separations of the mixture's components, which process yields nearly-pure components; partial distillation also realizes partial separations of the mixture to increase the concentrations of selected components. In either method, the separation process of distillation exploits the differences in the relative volatility of the component substances of the heated mixture.

In the industrial applications of classical distillation, the term distillation is used as a unit of operation that identifies and denotes a process of physical separation, not a chemical reaction; thus an industrial installation that produces distilled beverages, is a distillery of alcohol. These are some applications of the chemical separation process that is distillation:

Early evidence of distillation was found on Akkadian tablets dated c. 1200 BCE describing perfumery operations. The tablets provided textual evidence that an early, primitive form of distillation was known to the Babylonians of ancient Mesopotamia.[7]

According to British chemist T. Fairley, neither the Greeks nor the Romans had any term for the modern concept of distillation. Words like ""distill"" would have referred to something else, in most cases a part of some process unrelated to what now is known as distillation. In the words of Fairley and German chemical engineer Norbert Kockmann respectively:

The Latin ""distillo,"" from de-stillo, from stilla, a drop, referred to the dropping of a liquid by human or artificial means, and was applied to any process where a liquid was separated in drops. To distil in the modern sense could only be expressed in a roundabout manner.[8]
Distillation had a broader meaning in ancient and medieval times because nearly all purification and separation operations were subsumed under the term distillation, such as filtration, crystallization, extraction, sublimation, or mechanical pressing of oil.[9]
According to Dutch chemical historian Robert J. Forbes, the word distillare (to drip off) when used by the Romans, e.g. Seneca and Pliny the Elder, was ""never used in our sense"".[10]

Aristotle knew that water condensing from evaporating seawater is fresh:[11]

I have proved by experiment that salt water evaporated forms fresh, and the vapour does not, when it condenses, condense into sea water again.
Letting seawater evaporate and condense into freshwater cannot be called ""distillation"" for distillation involves boiling, but the experiment may have been an important step towards distillation.[12]

Early evidence of distillation has been found related to alchemists working in Alexandria in Roman Egypt in the 1st century CE.[16]: 57, 89 

Distilled water has been in use since at least c. 200 CE, when Alexander of Aphrodisias described the process.[17][18] Work on distilling other liquids continued in early Byzantine Egypt under Zosimus of Panopolis in the 3rd century.

Distillation was practiced in the ancient Indian subcontinent, which is evident from baked clay retorts and receivers found at Taxila, Shaikhan Dheri, and Charsadda in Pakistan and Rang Mahal in India dating to the early centuries of the Common Era.[19][20][21] Frank Raymond Allchin says these terracotta distill tubes were ""made to imitate bamboo"".[22] These ""Gandhara stills"" were only capable of producing very weak liquor, as there was no efficient means of collecting the vapors at low heat.[23]

Distillation in China may have begun at the earliest during the Eastern Han dynasty (1st–2nd century CE).[24]

Medieval Muslim chemists such as Jābir ibn Ḥayyān (Latin: Geber, ninth century) and Abū Bakr al-Rāzī (Latin: Rhazes, c. 865–925) experimented extensively with the distillation of various substances.  The fractional distillation of organic substances plays an important role in the works attributed to Jābir, such as in the Kitāb al-Sabʿīn ('The Book of Seventy'), translated into Latin by Gerard of Cremona (c. 1114–1187) under the title Liber de septuaginta.[25] The Jabirian experiments with fractional distillation of animal and vegetable substances, and to a lesser degree also of mineral substances, is the main topic of the De anima in arte alkimiae, an originally Arabic work falsely attributed to Avicenna that was translated into Latin and would go on to form the most important alchemical source for Roger Bacon (c. 1220–1292).[26]

The distillation of wine is attested in Arabic works attributed to al-Kindī (c. 801–873 CE) and to al-Fārābī (c. 872–950), and in the 28th book of al-Zahrāwī's (Latin: Abulcasis, 936–1013) Kitāb al-Taṣrīf (later translated into Latin as Liber servatoris).[27] In the twelfth century, recipes for the production of aqua ardens (""burning water"", i.e., ethanol) by distilling wine with salt started to appear in a number of Latin works, and by the end of the thirteenth century it had become a widely known substance among Western European chemists.[28] The works of Taddeo Alderotti (1223–1296) describe a method for concentrating alcohol involving repeated distillation through a water-cooled still, by which an alcohol purity of 90% could be obtained.[29]

The distillation of beverages began in the Southern Song (10th–13th century) and Jin (12th–13th century) dynasties, according to archaeological evidence.[24] A still was found in an archaeological site in Qinglong, Hebei province, China, dating back to the 12th century. Distilled beverages were common during the Yuan dynasty (13th–14th century).[24]

In 1500, German alchemist Hieronymus Brunschwig published Liber de arte distillandi de simplicibus (The Book of the Art of Distillation out of Simple Ingredients),[30] the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. Right after that, in 1518, the oldest surviving distillery in Europe, The Green Tree Distillery, was founded.[31]

In 1651, John French published The Art of Distillation,[32] the first major English compendium on the practice, but it has been claimed[33] that much of it derives from Brunschwig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.

As alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle to act as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour.[34] These alembics often featured a cooling system around the beak, using cold water, for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols, such as cognac, Scotch whisky, Irish whiskey, tequila, rum, cachaça, and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for use in the domestic production[35] of flower water or essential oils.

Early forms of distillation involved batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists reportedly carried out as many as 500 to 600 distillations in order to obtain a pure compound.[36]

In the early 19th century, the basics of modern techniques, including pre-heating and reflux, were developed.[36] In 1822, Anthony Perrier developed one of the first continuous stills, and then, in 1826, Robert Stein improved that design to make his patent still. In 1830, Aeneas Coffey got a patent for improving the design even further.[37] Coffey's continuous still may be regarded as the archetype of modern petrochemical units. The French engineer Armand Savalle developed his steam regulator around 1846.[16]: 323  In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation,[38] and the same and subsequent years saw developments in this theme for oils and spirits.

With the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods, such as the McCabe–Thiele method by Ernest Thiele and the Fenske equation. The first industrial plant in the United States to use distillation as a means of ocean desalination opened in Freeport, Texas in 1961 with the hope of bringing water security to the region.[39]
The availability of powerful computers has allowed direct computer simulations of distillation columns.

The application of distillation can roughly be divided into four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate in the processing of beverages and herbs.

The main difference between laboratory scale distillation and industrial distillation are that laboratory scale distillation is often performed on a batch basis, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds, and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions, which are collected sequentially from most volatile to less volatile, with the bottoms – remaining least or non-volatile fraction – removed at the end. The still can then be recharged and the process repeated.

In continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a more detailed control of the separation process.

The boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.

It is a misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure, allowing the vapors of each component to collect separately and purely. However, this does not occur, even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law and assume that vapor–liquid equilibria are attained.

Raoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up, a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.

Dalton's law states that the total pressure is the sum of the partial pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. A mixture with a given composition has one boiling point at a given pressure when the components are mutually soluble. A mixture of constant composition does not have multiple boiling points.

An implication of one boiling point is that lighter components never cleanly ""boil first"". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and, thus, are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily vaporize also, albeit at a lower concentration in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch vaporizes, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures. Therefore, starting from a given mixture, it appears to have a boiling range instead of a boiling point, although this is because its composition changes: each intermediate mixture has its own, singular boiling point.

The idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is when the vapor phase and liquid phase contain the same composition. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.

It is not possible to completely purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is vaporized and the other component, e.g., a salt, has zero partial pressure for practical purposes, the process is simpler.

Heating an ideal mixture of two volatile substances, A and B, with A having the higher volatility, or lower boiling point, in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid that contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid. The ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This, in turn, means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than in the starting liquid).

The result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio of A : B in the distillate.

If the difference in vapour pressure between the two components A and B is large – generally expressed as the difference in boiling points – the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.

Continuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.

Continuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.

Both batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.

There are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include:

Laboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a still, consists at a minimum of a reboiler or pot in which the source material is heated, a condenser in which the heated vapor is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also distillation types).

A completely sealed distillation apparatus could experience extreme and rapidly varying internal pressure, which could cause it to burst open at the joints.  Therefore, some path is usually left open (for instance, at the receiving flask) to allow the internal pressure to equalize with atmospheric pressure. Alternatively, a vacuum pump may be used to keep the apparatus at a lower than atmospheric pressure.  If the substances involved are air- or moisture-sensitive, the connection to the atmosphere can be made through one or more drying tubes packed with materials that scavenge the undesired air components, or through bubblers that provide a movable liquid barrier.  Finally, the entry of undesired air components can be prevented by pumping a low but steady flow of suitable inert gas, like nitrogen, into the apparatus.

In simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.

As a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C)[40] or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.

A cutaway schematic of a simple distillation operation is shown at right. The starting liquid 15 in the boiling flask 2 is heated by a combined hotplate and magnetic stirrer 13 via a silicone oil bath (orange, 14). The vapor flows through a short Vigreux column 3, then through a Liebig condenser 5, is cooled by water (blue) that circulates through ports 6 and 7. The condensed liquid drips into the receiving flask 8, sitting in a cooling bath (blue, 16). The adapter 10 has a connection 9 that may be fitted to a vacuum pump. The components are connected by ground glass joints.

For many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.[41]

As the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors is determined once again by Raoult's law. Each vaporization-condensation cycle (called a theoretical plate) will yield a purer solution of the more volatile component.[42] In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; theoretical plate is thus a concept rather than an accurate description.

More theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of PTFE or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.[43]

Like vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive.[1]: 151–153  The temperature of the steam is easier to control than the surface of a heating element and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.

Steam distillation of various aromatic herbs and flowers can result in two products: an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.

Some compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.

This technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.

Molecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e., the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.

Short path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure.[1]: 150  A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr apparatus is a kind of short path distillation method which often contains multiple chambers to collect distillate fractions.

Some compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a ""cow"" or ""pig"" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.

The Perkin triangle has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.

Zone distillation is a distillation process in a long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with the number of iterations.
Zone distillation is the distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization – with the replacement of the distribution co-efficient k of crystallization - for the separation factor α of distillation.[44][45][46]

Non-condensable gas can be expelled from the apparatus by the vapor of relatively volatile co-solvent, which spontaneously evaporates during initial pumping, and this can be achieved with regular oil or diaphragm pump.[47][48]

The unit process of evaporation may also be called ""distillation"":

Other uses:

Interactions between the components of the solution create properties unique to the solution, as most processes entail non-ideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not result in separation. For example, 95.6% ethanol (by mass) in water forms an azeotrope at 78.1 °C.

If the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a more pure distillate. These techniques are known as azeotropic distillation. Some techniques achieve this by ""jumping"" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent (or desiccant, such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.

Immiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.

High boiling azeotropes, such as a 20 percent by weight mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.

The boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it is possible to bias the boiling point of one component away from the other by exploiting the differing vapor pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to remain identical further along the pressure axis to either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.

This method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.

Under negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapors being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.

Alternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.

A unidirectional distillation will rely on a pressure change in one direction, either positive or negative.

Pressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.

This improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.

One example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.

Large scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.

To control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.

Industrial distillation[41][49] is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about 0.65 to 16 metres (2 ft 2 in to 52 ft 6 in) and heights ranging from about 6 to 90 metres (20 to 295 ft) or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different fractions or products having different boiling points or boiling ranges. The ""lightest"" products (those with the lowest boiling point) exit from the top of the columns and the ""heaviest"" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.

Industrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.

Such industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.

Design and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method[41][50] or the Fenske equation[41] can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as ""plates"" or ""trays"") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.

In modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.

This packing material can either be random or dumped packing (25–76 millimetres (1–3 in) wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of ""theoretical stages"" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both these factors affect packing performance.

Another factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references.[52][53] Considerable work has been done on this topic by Fractionation Research, Inc. (commonly known as FRI).[54]

The goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m3 of water recovered figure and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m3:

There are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.

Carbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.
"
Brewery Operations,"A brewery or brewing company is a business that makes and sells beer. The place at which beer is commercially made is either called a brewery or a beerhouse, where distinct sets of brewing equipment are called plant.[1] The commercial brewing of beer has taken place since at least 2500 BC;[2] in ancient Mesopotamia, brewers derived social sanction and divine protection from the goddess Ninkasi.[3][4] Brewing was initially a cottage industry, with production taking place at home; by the ninth century, monasteries and farms would produce beer on a larger scale, selling the excess; and by the eleventh and twelfth centuries larger, dedicated breweries with eight to ten workers were being built.[5]

The diversity of size in breweries is matched by the diversity of processes, degrees of automation, and kinds of beer produced in breweries. A brewery is typically divided into distinct sections, with each section reserved for one part of the brewing process.

Beer may have been known in Neolithic Europe[6] and was mainly brewed on a domestic scale.[7]
In some form, it can be traced back almost 5000 years to Mesopotamian writings describing daily rations of beer and bread to workers. Before the rise of production breweries, the production of beer took place at home and was the domain of women, as baking and brewing were seen as ""women's work"".

Breweries, as production facilities reserved for making beer, did not emerge until monasteries and other Christian institutions started producing beer not only for their own consumption but also to use as payment. This industrialization of brewing shifted the responsibility of making beer to men.

The oldest, still functional, brewery in the world is believed to be the German state-owned Weihenstephan brewery in the city of Freising, Bavaria. It can trace its history back to 1040 AD.[8] The nearby Weltenburg Abbey brewery, can trace back its beer-brewing tradition to at least 1050 AD.[9]: 30  The Žatec brewery in the Czech Republic claims it can prove that it paid a beer tax in 1004 AD.[citation needed]

Early breweries were almost always built on multiple stories, with equipment on higher floors used earlier in the production process, so that gravity could assist with the transfer of product from one stage to the next. This layout often is preserved in breweries today, but mechanical pumps allow more flexibility in brewery design. Early breweries typically used large copper vats in the brewhouse, and fermentation and packaging took place in lined wooden containers. Such breweries were common until the Industrial Revolution, when better materials became available, and scientific advances led to a better understanding of the brewing process. Today, almost all brewery equipment is made of stainless steel.
During the Industrial Revolution, the production of beer moved from artisanal manufacture to industrial manufacture, and domestic manufacture ceased to be significant by the end of the 19th century.[10] In addition to changes in manufacturing capacity, the number of breweries grew throughout industrialization. For example, in America, number of breweries rose from 431 to 4,131 between 1850 and 1873.[11]

A handful of major breakthroughs have led to the modern brewery and its ability to produce the same beer consistently. The steam engine, vastly improved in 1775 by James Watt, brought automatic stirring mechanisms and pumps into the brewery. It gave brewers the ability to mix liquids more reliably while heating, particularly the mash, to prevent scorching, and a quick way to transfer liquid from one container to another. Almost all breweries now use electric-powered stirring mechanisms and pumps. The steam engine also allowed the brewer to make greater quantities of beer, as human power was no longer a limiting factor in moving and stirring.

Carl von Linde, along with others, is credited with developing the refrigeration machine in 1871. Refrigeration allowed beer to be produced year-round, and always at the same temperature. Yeast is very sensitive to temperature, and, if a beer were produced during summer, the yeast would impart unpleasant flavours onto the beer. Most brewers would produce enough beer during winter to last through the summer, and store it in underground cellars, or even caves, to protect it from summer's heat.

The discovery of microbes by Louis Pasteur was instrumental in the control of fermentation. The idea that yeast was a microorganism that worked on wort to produce beer led to the isolation of a single yeast cell by Emil Christian Hansen. Pure yeast cultures allow brewers to pick out yeasts for their fermentation characteristics, including flavor profiles and fermentation ability. Some breweries in Belgium, however, still rely on ""spontaneous"" fermentation for their beers (see lambic).
The development of hydrometers and thermometers changed brewing by allowing the brewer more control of the process, and greater knowledge of the results.

Breweries today are made predominantly of stainless steel, although vessels often have a decorative copper cladding for a nostalgic look. Stainless steel has many favourable characteristics that make it a well-suited material for brewing equipment. It imparts no flavour in beer, it reacts with very few chemicals, which means almost any cleaning solution can be used on it (concentrated chlorine [bleach] being a notable exception).

Heating in the brewhouse usually is achieved through pressurized steam, although direct-fire systems are not unusual in small breweries. Likewise, cooling in other areas of the brewery is typically done by cooling jackets on tanks, which allow the brewer to control precisely the temperature on each tank individually, although whole-room cooling is also common.

Today, modern brewing plants perform myriad analyses on their beers for quality control purposes. Shipments of ingredients are analyzed to correct for variations. Samples are pulled at almost every step and tested for [oxygen] content, unwanted microbial infections, and other beer-aging compounds. A representative sample of the finished product often is stored for months for comparison, when complaints are received.

Brewing is typically divided into 9 steps: milling, malting, mashing, lautering, boiling, fermenting, conditioning, filtering, and filling.

Mashing is the process of mixing milled, usually malted, grain with water, and heating it with rests at certain temperatures to allow enzymes in the malt to break down the starches in the grain into sugars, especially maltose. Lautering is the separation of the extracts won during mashing from the spent grain to create wort. It is achieved in either a lauter tun, a wide vessel with a false bottom, or a mash filter, a plate-and-frame filter designed for this kind of separation. Lautering has two stages: first wort run-off, during which the extract is separated in an undiluted state from the spent grains, and sparging, in which extract that remains with the grains is rinsed off with hot water.

Boiling the wort ensures its sterility, helping to prevent contamination with undesirable microbes. During the boil, hops are added, which contribute aroma and flavour compounds to the beer, especially their characteristic bitterness. Along with the heat of the boil, they cause proteins in the wort to coagulate and the pH of the wort to fall, and they inhibit the later growth of certain bacteria. Finally, the vapours produced during the boil volatilize off-flavours, including dimethyl sulfide precursors. The boil must be conducted so that it is even and intense. The boil lasts between 60 and 120 minutes, depending on its intensity, the hop addition schedule, and volume of wort the brewer expects to evaporate.

Fermentation begins as soon as yeast is added to the cooled wort. This is also the point at which the product is first called beer. It is during this stage that fermentable sugars won from the malt (maltose, maltotriose, glucose, fructose and sucrose) are metabolized into alcohol and carbon dioxide. Fermentation tanks come in many shapes and sizes, from enormous cylindroconical vessels that can look like storage silos, to 20-litre (5 US gal) glass carboys used by homebrewers. Most breweries today use cylindroconical vessels (CCVs), which have a conical bottom and a cylindrical top. The cone's aperture is typically around 70°, an angle that will allow the yeast to flow smoothly out through the cone's apex at the end of fermentation, but is not so steep as to take up too much vertical space. CCVs can handle both fermenting and conditioning in the same tank. At the end of fermentation, the yeast and other solids have fallen to the cone's apex can be simply flushed out through a port at the apex. Open fermentation vessels are also used, often for show in brewpubs, and in Europe in wheat beer fermentation. These vessels have no tops, making it easy to harvest top-fermenting yeasts. The open tops of the vessels increase the risk of contamination, but proper cleaning procedures help to control the risk.

Fermentation tanks are typically made of stainless steel. Simple cylindrical tanks with beveled ends are arranged vertically, and conditioning tanks are usually laid out horizontally. A very few breweries still use wooden vats for fermentation but wood is difficult to keep clean and infection-free and must be repitched often, perhaps yearly. After high kräusen, the point at which fermentation is most active and copious foam is produced, a valve known in German as the spundapparat may be put on the tanks to allow the carbon dioxide produced by the yeast to naturally carbonate the beer. This bung device can regulate the pressure to produce different types of beer; greater pressure produces a more carbonated beer.

When the sugars in the fermenting beer have been almost completely digested, the fermentation process slows and the yeast cells begin to die and settle at the bottom of the tank. At this stage, especially if the beer is cooled to around freezing, most of the remaining live yeast cells will quickly become dormant and settle, along with the heavier protein chains, due simply to gravity and molecular dehydration. Conditioning can occur in fermentation tanks with cooling jackets. If the whole fermentation cellar is cooled, conditioning must be done in separate tanks in a separate cellar. Some beers are conditioned only lightly, or not at all. An active yeast culture from an ongoing batch may be added to the next boil after a slight chilling in order to produce fresh and highly palatable beer in mass quantity.

Filtering the beer stabilizes flavour and gives it a polished, shiny look. It is an optional process. Many craft brewers simply remove the coagulated and settled solids and forgo active filtration. In localities where a tax assessment is collected by government pursuant to local laws, any additional filtration may be done using an active filtering system, the filtered product finally passing into a calibrated vessel for measurement just after any cold conditioning and prior to final packaging where the beer is put into the containers for shipment or sale. The container may be a bottle, can,  of keg, cask or bulk tank.

Filters come in many types. Many use pre-made filtration media such as sheets or candles. Kieselguhr, a fine powder of diatomaceous earth, can be introduced into the beer and circulated through screens to form a filtration bed. Filtration ratings are divided into rough, fine, and sterile. Rough filters remove yeasts and other solids, leaving some cloudiness, while finer filters can remove body and color. Sterile filters remove almost all microorganisms.

Brewing companies range widely in the volume and variety of beer produced, ranging from small breweries to massive multinational conglomerates, like Molson Coors or Anheuser-Busch InBev, that produce hundreds of millions of barrels annually. There are organizations that assist the development of brewing, such as the Siebel Institute of Technology in the United States and the Institute of Brewing and Distilling in the UK. In 2012 the four largest brewing companies (Anheuser-Busch InBev, SABMiller, Heineken International, and Carlsberg Group) controlled 50% of the market[12] The biggest brewery in the world is the Belgian company Anheuser-Busch InBev.

In the United States, there were 69,359 people employed in breweries in 2017. This is up from 27,805 in 2001.[13]

Some commonly used descriptions of breweries are:

Contract brewing –When one brewery hires another brewery to produce its beer. The contracting brewer generally handles all of the beer's marketing, sales, and distribution, while leaving the brewing and packaging to the producer-brewery (which confusingly may also be referred to as a contract brewer). Often the contract brewing is performed when a small brewery can not supply enough beer to meet demands and contracts with a larger brewery to help alleviate their supply issues. Some breweries do not own a brewing facility, these contract brewers have been criticized by traditional brewing companies for avoiding the costs associated with a physical brewery.[15]

Gypsy, or nomad, brewing usually falls under the category of contract brewing. Gypsy breweries generally do not have their own equipment or premises. They operate on a temporary or itinerant basis out of the facilities of another brewery, generally making ""one-off"" special occasion beers.[16]  The trend of gypsy brewing spread early in Scandinavia.[17]  Their beers and collaborations later spread to America and Australia.[18]  Gypsy brewers typically use facilities of larger makers with excess capacity.[18]

Prominent examples include Pretty Things, Stillwater Artisanal Ales, Gunbarrel Brewing Company, Mikkeller, and Evil Twin.[19][20] For example, one of Mikkeller's founders, Mikkel Borg Bjergsø, has traveled around the world between 2006 and 2010, brewing more than 200 different beers at other breweries.[21]

Breweries and football have had a symbiotic relationship since the very beginnings of the game. The English Football League was founded in 1888, and by the next decade several teams already had their own brewery sponsor. In return for their financial support, the breweries were given concessions to sell beer to spectators and advertise their products in stadiums. The most outwardly visible sign of sponsorship are the adverts printed on football team's kit. For example, Liverpool F.C. had the logo of the Denmark-based Carlsberg brewery group on the front of its shirts for nearly twenty years, from 1992 to 2010.

Nowadays major brewing corporations are involved in sponsorship on a number of different levels. The prevailing trend is for the leading
brand not to be linked to individual teams; rather, they achieve visibility as sponsor of tournaments and leagues, so all fans can engage
with them regardless of which team they support. Heineken sponsors the UEFA Champions League with its namesake lager; Carlsberg sponsors the English Premier League as well as the 2012 and 2016 UEFA European Championships. Meanwhile, the AB InBev Group supports the FA Cup and the FIFA World Cup.[22]



The head brewer (UK) or brewmaster (US) is in charge of the production of beer. The major breweries employ engineers with a chemistry/biotechnology background.

Brewmasters may have had a formal education in the subject from institutions such as the Siebel Institute of Technology, VLB Berlin, Heriot-Watt University, American Brewers Guild,[23] University of California at Davis, University of Wisconsin,[23] Olds College[24] or Niagara College.[25] They may hold membership in professional organisations such as the Brewers Association, Master Brewers Association, American Society of Brewing Chemists, the Institute of Brewing and Distilling,[26] and the Society of Independent Brewers. Depending on a brewery's size, a brewer may need anywhere from five to fifteen years of professional experience before becoming a brewmaster.[23]
"
Non-Alcoholic Beverage Manufacturing,"An alcohol-free or non-alcoholic drink, also known as a temperance drink, is a version of an alcoholic drink made without alcohol, or with the alcohol removed or reduced to almost zero. These may take the form of a non-alcoholic mixed drink or non-alcoholic beer, and are widely available where alcoholic drinks are sold.

Sparkling apple cider, soft drinks, and juice naturally contain trace amounts or no alcohol. Some fresh orange juices are above the UK 'alcohol free' limit of 0.05% ABV, as are some yogurts and rye bread.[1]

Ethanol distillation is used to separate alcoholic drinks into what are advertised as non-alcoholic drinks and spirits. Distilled wine produces low alcohol wine[2] and brandy (from brandywine, derived from Dutch brandewijn, ""burning wine"").[3] Distilled beer may be used to produce low-alcohol beer and whisky.

However, alcoholic drinks cannot be further purified to 0.00% alcohol by volume by distillation, although several brands market their non-alcoholic drinks with '0%' on the packaging. Most drinks labeled ""non-alcoholic"" contain 0.5% ABV, as it is more profitable than distilling it to 0.05% ABV often found in products sold by companies specializing in non-alcoholic drinks.[citation needed][4]

Alcohol is legal in most countries of the world where a drinking culture exists. In countries where alcohol is illegal, similar non-alcoholic drinks are permitted. The definition of ""alcohol-free"" may vary from country to country. The term ""non-alcoholic"" (e.g., alcohol-free beer) is often used to describe a drink that contains 0.0% ABV.

However, most drinks advertised as ""non-alcoholic"" or ""alcohol free"" sold by countries with zero tolerance with state-run liquor monopoly, actually contain alcohol. In the European Union, the labels of drinks containing only more than 1.2% ABV must state the actual alcoholic strength (i.e., show the word ""alcohol"" or the abbreviation ""alc."" followed by the symbol ""% vol."").[5]

Alcohol is a psychoactive drug and some critics say that the label ""non-alcoholic"" is misleading, due to the presence of small amounts of alcohol in most drinks labelled as such, and a threat to recovering alcoholics.[6]

Non-alcoholic cocktails often resemble alcoholic cocktails without any liquor. They can be a blend of fresh fruit juices, syrups, cream, herbs and spices, or can use non-alcoholic spirits. These drinks are designed specifically for those who are sober, and are particularly favored over cocktails by teetotalers, underage persons, drivers, pregnant women, and others who choose drinks that are alcohol-free.[7]

The Woman's Christian Temperance Union publishes several recipes for fruestas, which are nonalcoholic fruit drinks for large functions, such as proms and weddings.[8] As a locution, fruesta drinks are etymologically derived from ""fruit"" and ""fiesta"", being a portmanteau of the two words.[8]

In the European Union, the labels of drinks containing more than 1.2% ABV must state the actual alcoholic strength (i.e., show the word ""alcohol"" or the abbreviation ""alc."" followed by the symbol ""% vol."").[9]

The government of Denmark have decided to change the alcohol free legal definition from 0.1% alcohol by volume to 0.5%.
This is due to the different taste of 0.5% than of 0.1%.[10]

Non-alcoholic beverage means a beverage which contains a maximum of 1.2 percentage by volume ethyl alcohol.[11]

Non-alcoholic beer, termed as ""birra analcolica"", is regulated as equal to or less than 1.2% ABV.[12]

Systembolaget defines alcohol-free as a drink that contains no more than 0.5% alcohol by volume.[13]

Licensing laws only place restrictions on the sale or consumption of drinks with an alcohol content of over 0.5%.[14]

In Japanese Liquor Tax Law, alcoholic drinks (酒類, shurui) are defined as equal to or more than 1% ABV,[15] so that drinks that are less than 1% ABV are not treated as alcoholic drink.[16] However, Advertisement Judging Committee on Alcoholic Drink (酒類の広告審査委員会, Shurui no Kōkoku Shinsa Īnkai), organization for making self‐imposed regulation, defines non-alcoholic drinks (ノンアルコール飲料, non’arukōru inryō) as drinks that 0.00% ABV.[17]

An alcohol free drink is defined as under 0.7% alcohol by volume.[18]

Non-alcoholic drinks are defined as containing less than 0.5% abv in general, or less than 1.2% abv if based on a fermentation product, including drinks like kefir, kvass and medovukha. This also includes low-alcohol beers by definition.[19]

A malt drink that contains less than 0.5% alcohol by volume does not have to be labeled.

(e) Non-alcoholic.  The term “non-alcoholic” may be used on labels of malt beverages only if the statement “contains less than 0.5 percent (or .5%) alcohol by volume” appears immediately adjacent to it, in readily legible printing, and on a completely contrasting background. No tolerances are permitted for malt beverages labeled as “non-alcoholic” and containing less than 0.5 percent alcohol by volume. A malt beverage may not be labeled with an alcohol content of 0.0 percent alcohol by volume, unless it is also labeled as “alcohol free” in accordance with paragraph (f) of this section, and contains no alcohol.
(f) Alcohol free.  The term “alcohol free” may be used only on malt beverages containing no alcohol. No tolerances are permitted for “alcohol free” malt beverages. "
Oil and Fat Manufacturing,"Cooking oil (also known as edible oil) is a plant or animal liquid fat used in frying, baking, and other types of cooking. Oil allows higher cooking temperatures than water, making cooking faster and more flavorful, while likewise distributing heat, reducing burning and uneven cooking. It sometimes imparts its own flavor. Cooking oil is also used in food preparation and flavoring not involving heat, such as salad dressings and bread dips.

Cooking oil is typically a liquid at room temperature, although some oils that contain saturated fat, such as coconut oil, palm oil and palm kernel oil are solid.[1]

There are a wide variety of cooking oils from plant sources such as olive oil, palm oil, soybean oil, canola oil (rapeseed oil), corn oil, peanut oil, sesame oil, sunflower oil and other vegetable oils, as well as animal-based oils like butter and lard.

Oil can be flavored with aromatic foodstuffs such as herbs, chilies or garlic. Cooking spray is an aerosol of cooking oil.

While consumption of small amounts of saturated fats is common in diets,[2] meta-analyses found a significant correlation between high consumption of saturated fats and blood LDL concentration,[3] a risk factor for cardiovascular diseases.[4] Other meta-analyses based on cohort studies and on controlled, randomized trials found a positive,[5] or neutral,[6] effect from consuming polyunsaturated fats instead of saturated fats (a 10% lower risk for 5% replacement).[6]

Mayo Clinic has highlighted certain oils that are high in saturated fats, including coconut, palm oil and palm kernel oil. Those having lower amounts of saturated fats and higher levels of unsaturated (preferably omega-3) fats like olive oil, peanut oil, canola oil, soy and cottonseed oils are generally healthier.[7] The US National Heart, Lung and Blood Institute[8] urged saturated fats be replaced with polyunsaturated and monounsaturated fats, listing olive and canola oils as sources of healthier monounsaturated oils while soybean and sunflower oils as good sources of polyunsaturated fats. One study showed that consumption of non-hydrogenated unsaturated oils like soybean and sunflower is preferable to the consumption of palm oil for lowering the risk of heart disease.[9]

Cashew oil and other nut-based oils do not present a danger to persons with a nut allergy, because oils are primarily lipids, and allergic reactions are due to surface proteins on the nut.[10]

The seeds of most cultivated plants contain higher levels of omega-6 fatty acids than omega-3, with some notable exceptions. Growth at colder temperatures tends to result in higher levels of omega-3 fatty acids in seed oils.[11]

Unlike other dietary fats, trans fats are not essential, and they do not promote good health.[12] The consumption of trans fats increases one's risk of coronary heart disease[13] by raising levels of LDL cholesterol and lowering levels of HDL cholesterol.[14] Trans fats from partially hydrogenated oils are more harmful than naturally occurring oils.[15]

Several large studies[16][17][18][19] indicate a link between the consumption of high amounts of trans fat and coronary heart disease, and possibly some other diseases. The United States Food and Drug Administration (FDA), the National Heart, Lung and Blood Institute and the American Heart Association (AHA) all have recommended limiting the intake of trans fats. In the US, trans fats are no longer ""generally recognized as safe"", and cannot be added to foods, including cooking oils, without special permission.[20]

Heating, as well as heating vessels rapidly change characteristics of cooking oil.[21] Oils that are healthy at room temperature can become unhealthy when heated above certain temperatures, especially when heating repeatedly. The toxic risk is linked to oxidation of fatty acids and fatty acids with higher levels of unsaturation are oxidized more rapidly during heating in air.[22]
So, when choosing a cooking oil, it is important to match the oil's heat tolerance with the temperature which will be used.[23] and to change frying oil a few times per week.[22]
Deep-fat frying temperatures are commonly in the range of 170–190 °C (338–374 °F), less commonly, lower temperatures ≥ 130 °C (266 °F) are used.[24]

Palm oil contains more saturated fats than canola oil, corn oil, linseed oil, soybean oil, safflower oil, and sunflower oil. Therefore, palm oil can withstand deep frying at higher temperatures and is resistant to oxidation compared to high-polyunsaturated vegetable oils.[25] Since the 1900s, palm oil has been increasingly added into food by the global commercial food industry because it remains stable in deep frying, or in baking at very high temperatures,[26][27] and for its high levels of natural antioxidants, though the refined palm oil used in industrial food has lost most of its carotenoid content (and its orange-red color).[28]

The following oils are suitable for high-temperature frying due to their high smoke point:

Less aggressive frying temperatures are frequently used.[30] A quality frying oil has a bland flavor, at least 200 °C (392 °F) smoke and 315 °C (599 °F) flash points, with maximums of 0.1% free fatty acids and 3% linolenic acid.[31] Those oils with higher linolenic fractions are avoided due to polymerization or gumming marked by increases in viscosity with age.[30] Olive oil resists thermal degradation and has been used as a frying oil for thousands of years.[30]

All oils degrade in response to heat, light, and oxygen.[32] To delay the onset of rancidity, a blanket of an inert gas, usually nitrogen, is applied to the vapor space in the storage container immediately after production – a process called tank blanketing.[citation needed][33]

In a cool, dry place, oils have greater stability, but may thicken, although they will soon return to liquid form if they are left at room temperature. To minimize the degrading effects of heat and light, oils should be removed from cold storage just long enough for use.[citation needed]

Refined oils high in monounsaturated fats, such as macadamia oil,[32] keep up to a year, while those high in polyunsaturated fats, such as soybean oil, keep about six months. Rancidity tests have shown that the shelf life of walnut oil is about 3 months, a period considerably shorter than the best before date shown on labels.[32]

By contrast, oils high in saturated fats, such as avocado oil, have relatively long shelf lives and can be safely stored at room temperature, as the low polyunsaturated fat content facilitates stability.[32]

Cooking oils are composed of various fractions of fatty acids.[34] For the purpose of frying food, oils high in monounsaturated or saturated fats are generally popular, while oils high in polyunsaturated fats are less desirable.[24] High oleic acid oils include almond, macadamia, olive, pecan, pistachio, and high-oleic cultivars of safflower and sunflower.[35]

Cold-Pressed vs. Refined Cooking Oils


Cold-pressed oils are extracted mechanically without the use of heat or chemical solvents, preserving nutrients and natural flavors, whereas refined oils undergo additional processes like bleaching and deodorization, which can strip beneficial compounds.[36]
The smoke point is marked by ""a continuous wisp of smoke"".[54] It is the temperature at which an oil starts to burn, leading to a burnt flavor in the foods being prepared and degradation of nutrients and phytochemicals characteristic of the oil.[55]

Above the smoke point are flash and fire points.[54] The flash point is the temperature at which oil vapors will ignite but are not produced in sufficient quantities to stay lit. The flash point generally occurs at about 275–330 °C (527–626 °F).[56] The fire point is the temperature at which hot oil produces sufficient vapors they will catch on fire and burn.[56] As frying hours increase, all these temperature points decrease.[56] They depend more on an oil's acidity than fatty-acid profile.[57]

The smoke point of cooking oils varies generally in association with how oil is refined: a higher smoke point results from removal of impurities and free fatty acids.[55] Residual solvent remaining from the refining process may decrease the smoke point.[57] It has been reported to increase with the inclusion of antioxidants (BHA, BHT, and TBHQ). For these reasons, the published smoke points of oils may vary.[57]

Oils are extracted from nuts, seeds, olives, grains or legumes by extraction using industrial chemicals or by mechanical processes. Expeller pressing is a chemical-free process that collects oils from a source using a mechanical press with minimal heat. Cold-pressed oils are extracted under a controlled temperature setting usually below 105 °C (221 °F) intended to preserve naturally occurring phytochemicals, such as polyphenols, tocotrienols, plant sterols and vitamin E which collectively affect color, flavor, aroma and nutrient value.[55][79]

[80][81]

Cooking oil extraction and refinement are separate processes. Extraction first removes the oil, typically from a seed, nut or fruit. Refinement then alters the appearance, texture, taste, smell, or stability of the oil to meet buyer expectations.

There are three broad types of oil extraction:

In large-scale industrial oil extraction you will often see some combination of pressing, chemical extraction and/or centrifuging in order to extract the maximum amount of oil possible.[105]

Cooking oil can either be unrefined, or refined using one or more of the following refinement processes (in any combination):[106]

Filtering, a non-chemical process which screens out larger particles, could be considered a step in refinement, although it does not alter the state of the oil.

Most large-scale commercial cooking oil refinement will involve all of these steps in order to achieve a product that's uniform in taste, smell and appearance, and has a longer shelf life.[105] Cooking oil intended for the health food market will often be unrefined, which can result in a less stable product but minimizes exposure to high temperatures and chemical processing.

Proper disposal of used cooking oil is an important waste-management concern. Oil can congeal in pipes, causing sanitary sewer overflow.[109] Because of this, cooking oil should never be dumped in the kitchen sink or in the toilet bowl. The proper way to dispose of oil is to put it in a sealed non-recyclable container and discard it with regular garbage.[110] Placing the container of oil in the refrigerator to harden also makes disposal easier and less messy.

Cooking oil can be recycled. It can be used in animal feed, soap, make-up, clothes, rubber, detergents, directly as fuel, and to produce biodiesel.[111][112][113]

In the recycling industry, used cooking oil recovered from restaurants and food-processing industries (typically from deep fryers or griddles) is called yellow grease, recycled vegetable oil (RVO), used vegetable oil (UVO), or waste vegetable oil (WVO).[114]

Grease traps or interceptors collect fats and oils from kitchen sinks and floor drains. The result is called brown grease, and unlike yellow grease its severe contaminants make it much harder to recycle.

Gutter oil and trench oil are terms used in China to describe recycled oil processed to resemble virgin oil, but containing toxic contaminants and sold illegally for cooking; its origin is frequently brown grease from garbage.[115]

In Kenya, thieves sell stolen electric transformers to operators of roadside food stalls for reuse of the oil in deep frying, suitable for prolonged use longer than regular cooking oil, but a threat to consumer health due to the presence of PCBs and polycyclic aromatic hydrocarbons.[116]
"
Rendering Services,"Rendering is the process of generating a photorealistic or non-photorealistic image from input data such as  3D models. The word ""rendering"" (in one of its senses) originally meant the task performed by an artist when depicting a real or imaginary thing (the finished artwork is also called a ""rendering""). Today, to ""render"" commonly means to generate an image or video from a precise description (often created by an artist) using a computer program.[1][2][3][4]

A software application or component that performs rendering is called a rendering engine,[5] render engine, rendering system, graphics engine, or simply a renderer.

A distinction is made between real-time rendering, in which images are generated and displayed immediately (ideally fast enough to give the impression of motion or animation), and offline rendering (sometimes called pre-rendering) in which images, or film or video frames, are generated for later viewing. Offline rendering can use a slower and higher-quality renderer. Interactive applications such as games must primarily use real-time rendering, although they may incorporate pre-rendered content.

Rendering can produce images of scenes or objects defined using coordinates in 3D space, seen from a particular viewpoint. Such 3D rendering uses knowledge and ideas from optics, the study of visual perception, mathematics, and software engineering, and it has applications such as video games, simulators, visual effects for films and television, design visualization, and medical diagnosis. Realistic 3D rendering requires finding approximate solutions to the rendering equation, which describes how light propagates in an environment.

Real-time rendering uses high-performance rasterization algorithms that process a list of shapes and determine which pixels are covered by each shape. When more realism is required (e.g. for architectural visualization or visual effects) slower pixel-by-pixel algorithms such as ray tracing are used instead. (Ray tracing can also be used selectively during rasterized rendering to improve the realism of lighting and reflections.) A type of ray tracing called path tracing is currently the most common technique for photorealistic rendering. Path tracing is also popular for generating high-quality non-photorealistic images, such as frames for 3D animated films. Both rasterization and ray tracing can be sped up (""accelerated"") by specially designed microprocessors called GPUs.

Rasterization algorithms are also used to render images containing only 2D shapes such as polygons and text. Applications of this type of rendering include digital illustration, graphic design, 2D animation, desktop publishing and the display of user interfaces.

Historically, rendering was called image synthesis[6]: xxi  but today this term is likely to mean AI image generation.[7] The term ""neural rendering"" is sometimes used when a neural network is the primary means of generating an image but some degree of control over the output image is provided.[8] Neural networks can also assist rendering without replacing traditional algorithms, e.g. by removing noise from path traced images.

A large proportion of computer graphics research has worked towards producing images that resemble photographs. Fundamental techniques that make this possible were invented in the 1980s, but at the end of the decade, photorealism for complex scenes was still considered a distant goal.[9]: x  Today, photorealism is routinely achievable for offline rendering, but remains difficult for real-time rendering.[10]: 1–2 

In order to produce realistic images, rendering must simulate how light travels from light sources, is reflected, refracted, and scattered (often many times) by objects in the scene, passes through a camera lens, and finally reaches the film or sensor of the camera. The physics used in these simulations is primarily geometrical optics, in which particles of light follow (usually straight) lines called  rays, but in some situations (such as when rendering thin films, like the surface of soap bubbles) the wave nature of light must be taken into account.[11][12]

Effects that may need to be simulated include:

In realistic scenes, objects are illuminated both by light that arrives directly from a light source (after passing mostly unimpeded through air), and light that has bounced off other objects in the scene. The simulation of this complex lighting is called global illumination. In the past, indirect lighting was often faked (especially when rendering animated films) by placing additional hidden lights in the scene, but today path tracing is used to render it accurately.[14]: 3 [13]: 108 

For true photorealism, the camera used to take the photograph must be simulated. The thin lens approximation allows combining perspective projection with depth of field (and bokeh) emulation. Camera lens simulations can be made more realistic by modeling the way light is refracted by the components of the lens. Motion blur is often simulated if film or video frames are being rendered.[11][15] Simulated lens flare and bloom are sometimes added to make the image appear subjectively brighter (although the design of real cameras tries to reduce these effects).[16]: 12.4 

Realistic rendering uses mathematical descriptions of how different surface materials reflect light, called reflectance models or (when physically plausible) bidirectional reflectance distribution functions (BRDFs).[11] Rendering materials such as marble, plant leaves, and human skin requires simulating an effect called subsurface scattering, in which a portion of the light travels into the material, is scattered, and then travels back out again.[13]: 143  The way color, and properties such as roughness, vary over a surface can be represented efficiently using texture mapping.[16]: 6.1 

For some applications (including early stages of 3D modeling), simplified rendering styles such as wireframe rendering may be appropriate, particularly when the material and surface details have not been defined and only the shape of an object is known.[17]: 5.3  Games and other real-time applications may use simpler and less realistic rendering techniques as an artistic or design choice, or to allow higher frame rates on lower-end hardware.

Orthographic and isometric projections can be used for a stylized effect or to ensure that parallel lines are depicted as parallel in CAD rendering.[16]: 4.7 [17]: 3.7 

Non-photorealistic rendering (NPR) uses techniques like edge detection and posterization to produce 3D images that resemble technical illustrations, cartoons, or other styles of drawing or painting.[16]: ch 15 

Before a 3D scene or 2D image can be rendered, it must be described in a way that the rendering software can understand. Historically, inputs for both 2D and 3D rendering were usually text files, which are easier than binary files for humans to edit and debug. For 3D graphics, text formats have largely been supplanted by more efficient binary formats, and by APIs which allow interactive applications to communicate directly with a rendering component without generating a file on disk (although a scene description is usually still created in memory prior to rendering).[18]: 1.2, 3.2.6, 3.3.1, 3.3.7 

Traditional rendering algorithms use geometric descriptions of 3D scenes or 2D images. Applications and algorithms that render visualizations of data scanned from the real world, or scientific simulations, may require different types of input data.

The PostScript format (which is often credited with the rise of desktop publishing) provides a standardized, interoperable way to describe 2D graphics and page layout. The Scalable Vector Graphics (SVG) format is also text-based, and the PDF format uses the PostScript language internally. In contrast, although many 3D graphics file formats have been standardized (including text-based formats such as VRML and X3D), different rendering applications typically use formats tailored to their needs, and this has led to a proliferation of proprietary and open formats, with binary files being more common.[18]: 3.2.3, 3.2.5, 3.3.7 [19]: vii [20][21]: 16.5.2. [22]

A vector graphics image description may include:[19][20]

A geometric scene description may include:[18]: Ch. 4-7, 8.7 [23]

Many file formats exist for storing individual 3D objects or ""models"". These can be imported into a larger scene, or loaded on-demand by rendering software or games. A realistic scene may require hundreds of items like household objects, vehicles, and trees, and 3D artists often utilize large libraries of models. In game production, these models (along with other data such as textures, audio files, and animations) are referred to as ""assets"".[22][24]: Ch. 4 

Scientific and engineering visualization often requires rendering volumetric data generated by 3D scans or simulations. Perhaps the most common source of such data is medical CT and MRI scans, which need to be rendered for diagnosis. Volumetric data can be extremely large, and requires specialized data formats to store it efficiently, particularly if the volume is sparse (with empty regions that do not contain data).[16]: 14.3.1 [25][26]

Before rendering, level sets for volumetric data can be extracted and converted into a mesh of triangles, e.g. by using the marching cubes algorithm. Algorithms have also been developed that work directly with volumetric data, for example to render realistic depictions of the way light is scattered and absorbed by clouds and smoke, and this type of volumetric rendering is used extensively in visual effects for movies. When rendering lower-resolution volumetric data without interpolation, the individual cubes or ""voxels"" may be visible, an effect sometimes used deliberately for game graphics.[27]: 4.6 [16]: 13.10, Ch. 14, 16.1 

Photographs of real world objects can be incorporated into a rendered scene by using them as textures for 3D objects. Photos of a scene can also be stitched together to create panoramic images or environment maps, which allow the scene to be rendered very efficiently but only from a single viewpoint. Scanning of real objects and scenes using structured light or lidar produces point clouds consisting of the coordinates of millions of individual points in space, sometimes along with color information. These point clouds may either be rendered directly or converted into meshes before rendering. (Note: ""point cloud"" sometimes also refers to a minimalist rendering style that can be used for any 3D geometry, similar to wireframe rendering.)[16]: 13.3, 13.9 [18]: 1.3 

A more recent, experimental approach is description of scenes using radiance fields which define the color, intensity, and direction of incoming light at each point in space. (This is conceptually similar to, but not identical to, the light field recorded by a hologram.) For any useful resolution, the amount of data in a radiance field is so large that it is impractical to represent it directly as volumetric data, and an approximation function must be found. Neural networks are typically used to generate and evaluate these approximations, sometimes using video frames, or a collection of photographs of a scene taken at different angles, as ""training data"".[28][29]

Algorithms related to neural networks have recently been used to find approximations of a scene as 3D Gaussians. The resulting representation is similar to a point cloud, except that it uses fuzzy, partially-transparent blobs of varying dimensions and orientations instead of points. As with neural radiance fields, these approximations are often generated from photographs or video frames.[30]

The output of rendering may be displayed immediately on the screen (many times a second, in the case of real-time rendering such as games) or saved in a raster graphics file format such as JPEG or PNG. High-end rendering applications commonly use the OpenEXR file format, which can represent finer gradations of colors and high dynamic range lighting, allowing tone mapping or other adjustments to be applied afterwards without loss of quality.[31][32]: Ch. 14, Ap. B 

Quickly rendered animations can be saved directly as video files, but for high-quality rendering, individual frames (which may be rendered by different computers in a cluster or render farm and may take hours or even days to render) are output as separate files and combined later into a video clip.[33][24]: 1.5, 3.11, 8.11 

The output of a renderer sometimes includes more than just RGB color values. For example, the spectrum can be sampled using multiple wavelengths of light, or additional information such as depth (distance from camera) or the material of each point in the image can be included (this data can be used during compositing or when generating texture maps for real-time rendering, or used to assist in removing noise from a path-traced image). Transparency information can be included, allowing rendered foreground objects to be composited with photographs or video. It is also sometimes useful to store the contributions of different lights, or of specular and diffuse lighting, as separate channels, so lighting can be adjusted after rendering. The OpenEXR format allows storing many channels of data in a single file. Renderers such as Blender and Pixar RenderMan support a large variety of configurable values called Arbitrary Output Variables (AOVs).[31][32]: Ch. 14, Ap. B [34]

Choosing how to render a 3D scene usually involves trade-offs between speed, memory usage, and realism (although realism is not always desired). The algorithms developed over the years follow a loose progression, with more advanced methods becoming practical as computing power and memory capacity increased. Multiple techniques may be used for a single final image.

An important distinction is between image order algorithms, which iterate over pixels in the image, and object order algorithms, which iterate over objects in the scene. For simple scenes, object order is usually more efficient, as there are fewer objects than pixels.[35]: Ch. 4 

Each of the above approaches has many variations, and there is some overlap. Path tracing may be considered either a distinct technique or a particular type of ray tracing.[6]: 846, 1021  Note that the usage of terminology related to ray tracing and path tracing has changed significantly over time.[37]: 7 

Ray marching is a family of algorithms, used by ray casting, for finding intersections between a ray and a complex object, such as a volumetric dataset or a surface defined by a signed distance function. It is not, by itself, a rendering method, but it can be incorporated into ray tracing and path tracing, and is used by rasterization to implement screen-space reflection and other effects.[37]: 13 

A technique called photon mapping traces paths of photons from a light source to an object, accumulating data about irradiance which is then used during conventional ray tracing or path tracing.[6]: 1037-1039  Rendering a scene using only rays traced from the light source to the camera is impractical, even though it corresponds more closely to reality, because a huge number of photons would need to be simulated, only a tiny fraction of which actually hit the camera.[40]: 7-9 [36]: 587 

Some authors call conventional ray tracing ""backward"" ray tracing because it traces the paths of photons backwards from the camera to the light source, and call following paths from the light source (as in photon mapping) ""forward"" ray tracing.[40]: 7-9  However sometimes the meaning of these terms is reversed.[41] Tracing rays starting at the light source can also be called particle tracing or light tracing, which avoids this ambiguity.[14]: 92 [42]: 4.5.4 

Real-time rendering, including video game graphics, typically uses rasterization, but increasingly combines it with ray tracing and path tracing.[10]: 2  To enable realistic global illumination, real-time rendering often relies on pre-rendered (""baked"") lighting for stationary objects. For moving objects, it may use a technique called light probes, in which lighting is recorded by rendering omnidirectional views of the scene at chosen points in space (often points on a grid to allow easier interpolation). These are similar to environment maps, but typically use a very low resolution or an approximation such as spherical harmonics.[43] (Note: Blender uses the term 'light probes' for a more general class of pre-recorded lighting data, including reflection maps.[44])

The term rasterization (in a broad sense) encompasses many techniques used for 2D rendering and real-time 3D rendering. 3D animated films were rendered by rasterization before ray tracing and path tracing became practical.

A renderer combines rasterization with geometry processing (which is not specific to rasterization) and pixel processing which computes the RGB color values to be placed in the framebuffer for display.[16]: 2.1 [35]: 9 

The main tasks of rasterization (including pixel processing) are:[16]: 2, 3.8, 23.1.1 

3D rasterization is typically part of a graphics pipeline in which an application provides lists of triangles to be rendered, and the rendering system transforms and projects their coordinates, determines which triangles are potentially visible in the viewport, and performs the above rasterization and pixel processing tasks before displaying the final result on the screen.[16]: 2.1 [35]: 9 

Historically, 3D rasterization used algorithms like the Warnock algorithm and scanline rendering (also called ""scan-conversion""), which can handle arbitrary polygons and can rasterize many shapes simultaneously. Although such algorithms are still important for 2D rendering, 3D rendering now usually divides shapes into triangles and rasterizes them individually using simpler methods.[45][46][36]: 456, 561–569 

High-performance algorithms exist for rasterizing 2D lines, including anti-aliased lines, as well as ellipses and filled triangles. An important special case of 2D rasterization is text rendering, which requires careful anti-aliasing and rounding of coordinates to avoid distorting the letterforms and preserve spacing, density, and sharpness.[35]: 9.1.1 [47]

After 3D coordinates have been projected onto the image plane, rasterization is primarily a 2D problem, but the 3rd dimension necessitates hidden surface removal. Early computer graphics used geometric algorithms or ray casting to remove the hidden portions of shapes, or used the painter's algorithm, which sorts shapes by depth (distance from camera) and renders them from back to front. Depth sorting was later avoided by incorporating depth comparison into the scanline rendering algorithm. The z-buffer  algorithm performs the comparisons indirectly by including a depth or ""z"" value in the framebuffer. A pixel is only covered by a shape if that shape's z value is lower (indicating closer to the camera) than the z value currently in the buffer. The z-buffer requires additional memory (an expensive resource at the time it was invented) but simplifies the rasterization code and permits multiple passes. Memory is now faster and more plentiful, and a z-buffer is almost always used for real-time rendering.[48][49][36]: 553–570 [16]: 2.5.2 

A drawback of the basic z-buffer algorithm is that each pixel ends up either entirely covered by a single object or filled with the background color, causing jagged edges in the final image. Early anti-aliasing approaches addressed this by detecting when a pixel is partially covered by a shape, and calculating the covered area. The A-buffer (and other sub-pixel and multi-sampling techniques) solve the problem less precisely but with higher performance. For real-time 3D graphics, it has become common to use complicated heuristics (and even  neural-networks) to perform anti-aliasing.[49][50][35]: 9.3 [16]: 5.4.2 

In 3D rasterization, color is usually determined by a pixel shader or fragment shader, a small program that is run for each pixel. The shader does not (or cannot) directly access 3D data for the entire scene (this would be very slow, and would result in an algorithm similar to ray tracing) and a variety of techniques have been developed to render effects like shadows and reflections using only texture mapping and multiple passes.[35]: 17.8 

Older and more basic 3D rasterization implementations did not support shaders, and used simple shading techniques such as flat shading (lighting is computed once for each triangle, which is then rendered entirely in one color), Gouraud shading (lighting is computed using normal vectors defined at vertices and then colors are interpolated across each triangle), or Phong shading (normal vectors are interpolated across each triangle and lighting is computed for each pixel).[35]: 9.2 

Until relatively recently, Pixar used rasterization for rendering its animated films. Unlike the renderers commonly used for real-time graphics, the Reyes rendering system in Pixar's RenderMan software was optimized for rendering very small (pixel-sized) polygons, and incorporated stochastic sampling techniques more typically associated with ray tracing.[18]: 2, 6.3 [51]

One of the simplest ways to render a 3D scene is to test if a ray starting at the viewpoint (the ""eye"" or ""camera"") intersects any of the geometric shapes in the scene, repeating this test using a different ray direction for each pixel. This method, called ray casting, was important in early computer graphics, and is a fundamental building block for more advanced algorithms. Ray casting can be used to render shapes defined by constructive solid geometry (CSG) operations.[37]: 8-9 [52]: 246–249 

Early ray casting experiments include the work of Arthur Appel in the 1960s. Appel rendered shadows by casting an additional ray from each visible surface point towards a light source. He also tried rendering the density of illumination by casting random rays from the light source towards the object and plotting the intersection points (similar to the later technique called photon mapping).[53]

When rendering scenes containing many objects, testing the intersection of a ray with every object becomes very expensive. Special data structures are used to speed up this process by allowing large numbers of objects to be excluded quickly (such as objects behind the camera). These structures are analogous to database indexes for finding the relevant objects. The most common are the bounding volume hierarchy (BVH), which stores a pre-computed bounding box or sphere for each branch of a tree of objects, and the k-d tree which recursively divides space into two parts. Recent GPUs include hardware acceleration for BVH intersection tests. K-d trees are a special case of binary space partitioning, which was frequently used in early computer graphics (it can also generate a rasterization order for the painter's algorithm). Octrees, another historically popular technique, are still often used for volumetric data.[10]: 16–17 [54][52][21]: 36.2 

Geometric formulas are sufficient for finding the intersection of a ray with shapes like spheres, polygons, and polyhedra, but for most curved surfaces there is no analytic solution, or the intersection is difficult to compute accurately using limited precision floating point numbers. Root-finding algorithms such as Newton's method can sometimes be used. To avoid these complications, curved surfaces are often approximated as meshes of triangles. Volume rendering (e.g. rendering clouds and smoke), and some surfaces such as fractals, may require ray marching instead of basic ray casting.[55][37]: 13 [16]: 14, 17.3 

Ray casting can be used to render an image by tracing light rays backwards from a simulated camera. After finding a point on a surface where a ray originated, another ray is traced towards the light source to determine if anything is casting a shadow on that point. If not, a reflectance model (such as Lambertian reflectance for matte surfaces, or the Phong reflection model for glossy surfaces) is used to compute the probability that a photon arriving from the light would be reflected towards the camera, and this is multiplied by the brightness of the light to determine the pixel brightness. If there are multiple light sources, brightness contributions of the lights are added together. For color images, calculations are repeated for multiple wavelengths of light (e.g. red, green, and blue).[16]: 11.2.2 [37]: 8 

Classical ray tracing (also called Whitted-style or recursive ray tracing) extends this method so it can render mirrors and transparent objects. If a ray traced backwards from the camera originates at a point on a mirror, the reflection formula from geometric optics is used to calculate the direction the reflected ray came from, and another ray is cast backwards in that direction. If a ray originates at a transparent surface, rays are cast backwards for both reflected and refracted rays (using Snell's law to compute the refracted direction), and so ray tracing needs to support a branching ""tree"" of rays. In simple implementations, a recursive function is called to trace each ray.[16]: 11.2.2 [37]: 9 

Ray tracing usually performs anti-aliasing by taking the average of multiple samples for each pixel. It may also use multiple samples for effects like depth of field and motion blur. If evenly-spaced ray directions or times are used for each of these features, many rays are required, and some aliasing will remain. Cook-style, stochastic, or Monte Carlo ray tracing avoids this problem by using random sampling instead of evenly-spaced samples. This type of ray tracing is commonly called distributed ray tracing, or distribution ray tracing because it samples rays from probability distributions. Distribution ray tracing can also render realistic ""soft"" shadows from large lights by using a random sample of points on the light when testing for shadowing, and it can simulate chromatic aberration by sampling multiple wavelengths from the spectrum of light.[37]: 10 [40]: 25 

Real surface materials reflect small amounts of light in almost every direction because they have small (or microscopic) bumps and grooves. A distribution ray tracer can simulate this by sampling possible ray directions, which allows rendering blurry reflections from glossy and metallic surfaces. However if this procedure is repeated recursively to simulate realistic indirect lighting, and if more than one sample is taken at each surface point, the tree of rays quickly becomes huge. Another kind of ray tracing, called path tracing, handles indirect light more efficiently, avoiding branching, and ensures that the distribution of all possible paths from a light source to the camera is sampled in an unbiased way.[40]: 25–27 [39]

Ray tracing was often used for rendering reflections in animated films, until path tracing became standard for film rendering. Films such as Shrek 2 and Monsters University also used distribution ray tracing or path tracing to precompute indirect illumination for a scene or frame prior to rendering it using rasterization.[13]: 118–121 

Advances in GPU technology have made real-time ray tracing possible in games, although it is currently almost always used in combination with rasterization.[10]: 2  This enables visual effects that are difficult with only rasterization, including reflection from curved surfaces and interreflective objects,[56]: 305  and shadows that are accurate over a wide range of distances and surface orientations.[57]: 159-160  Ray tracing support is included in recent versions of the graphics APIs used by games, such as DirectX, Metal, and Vulkan.[58]

Ray tracing has been used to render simulated black holes, and the appearance of objects moving at close to the speed of light, by taking spacetime curvature and relativistic effects into account during light ray simulation.[59][60]

Radiosity (named after the radiometric quantity of the same name) is a method for rendering objects illuminated by light bouncing off rough or matte surfaces. This type of illumination is called indirect light, environment lighting, diffuse lighting, or diffuse interreflection, and the problem of rendering it realistically is called global illumination. Rasterization and basic forms of ray tracing (other than distribution ray tracing and path tracing) can only roughly approximate indirect light, e.g. by adding a uniform ""ambient"" lighting amount chosen by the artist. Radiosity techniques are also suited to rendering scenes with area lights such as rectangular fluorescent lighting panels, which are difficult for rasterization and traditional ray tracing. Radiosity is considered a physically-based method, meaning that it aims to simulate the flow of light in an environment using equations and experimental data from physics, however it often assumes that all surfaces are opaque and perfectly Lambertian, which reduces realism and limits its applicability.[16]: 10, 11.2.1 [6]: 888, 893 [61][62]: 6 

In the original radiosity method (first proposed in 1984) now called classical radiosity, surfaces and lights in the scene are split into pieces called patches, a process called meshing (this step makes it a finite element method). The rendering code must then determine what fraction of the light being emitted or diffusely reflected (scattered) by each patch is received by each other patch. These fractions are called form factors or view factors (first used in engineering to model radiative heat transfer). The form factors are multiplied by the albedo of the receiving surface and put in a matrix. The lighting in the scene can then be expressed as a matrix equation (or equivalently a system of linear equations) that can be solved by methods from linear algebra.[61][63]: 46 [6]: 888, 896 

Solving the radiosity equation gives the total amount of light emitted and reflected by each patch, which is divided by area to get a value called radiosity that can be used when rasterizing or ray tracing to determine the color of pixels corresponding to visible parts of the patch. For real-time rendering, this value (or more commonly the irradiance, which does not depend on local surface albedo) can be pre-computed and stored in a texture (called an irradiance map) or stored as vertex data for 3D models. This feature was used in architectural visualization software to allow real-time walk-throughs of a building interior after computing the lighting.[6]: 890 [16]: 11.5.1 [62]: 332 

The large size of the matrices used in classical radiosity (the square of the number of patches) causes problems for realistic scenes. Practical implementations may use Jacobi or Gauss-Seidel iterations, which is equivalent (at least in the Jacobi case) to simulating the propagation of light one bounce at a time until the amount of light remaining (not yet absorbed by surfaces) is insignificant. The number of iterations (bounces) required is dependent on the scene, not the number of patches, so the total work is proportional to the square of the number of patches (in contrast, solving the matrix equation using Gaussian elimination requires work proportional to the cube of the number of patches). Form factors may be recomputed when they are needed, to avoid storing a complete matrix in memory.[6]: 901, 907 

The quality of rendering is often determined by the size of the patches, e.g. very fine meshes are needed to depict the edges of shadows accurately. An important improvement is hierarchical radiosity, which uses a coarser mesh (larger patches) for simulating the transfer of light between surfaces that are far away from one another, and adaptively sub-divides the patches as needed. This allows radiosity to be used for much larger and more complex scenes.[6]: 975, 939 

Alternative and extended versions of the radiosity method support non-Lambertian surfaces, such as glossy surfaces and mirrors, and sometimes use volumes or ""clusters"" of objects as well as surface patches. Stochastic or Monte Carlo radiosity uses random sampling in various ways, e.g. taking samples of incident light instead of integrating over all patches, which can improve performance but adds noise (this noise can be reduced by using deterministic iterations as a final step, unlike path tracing noise). Simplified and partially precomputed versions of radiosity are widely used for real-time rendering, combined with techniques such as octree radiosity that store approximations of the light field.[6]: 979, 982 [63]: 49 [64][16]: 11.5 

As part of the approach known as physically based rendering, path tracing has become the dominant technique for rendering realistic scenes, including effects for movies.[65] For example, the popular open source 3D software Blender uses path tracing in its Cycles renderer.[66] Images produced using path tracing for global illumination are generally noisier than when using radiosity (the main competing algorithm for realistic lighting), but radiosity can be difficult to apply to complex scenes and is prone to artifacts that arise from using a tessellated representation of irradiance.[65][6]: 975-976, 1045 

Like distributed ray tracing, path tracing is a kind of stochastic or randomized ray tracing that uses Monte Carlo or Quasi-Monte Carlo integration. It was proposed and named in 1986 by Jim Kajiya in the same paper as the rendering equation. Kajiya observed that much of the complexity of distributed ray tracing could be avoided by only tracing a single path from the camera at a time (in Kajiya's implementation, this ""no branching"" rule was broken by tracing additional rays from each surface intersection point to randomly chosen points on each light source). Kajiya suggested reducing the noise present in the output images by using stratified sampling and importance sampling for making random decisions such as choosing which ray to follow at each step of a path. Even with these techniques, path tracing would not have been practical for film rendering, using computers available at the time, because the computational cost of generating enough samples to reduce variance to an acceptable level was too high. Monster House, the first feature film rendered entirely using path tracing, was not released until 20 years later.[39][65][67]

In its basic form, path tracing is inefficient (requiring too many samples) for rendering caustics and scenes where light enters indirectly through narrow spaces. Attempts were made to address these weaknesses in the 1990s. Bidirectional path tracing has similarities to photon mapping, tracing rays from the light source and the camera separately, and then finding ways to connect these paths (but unlike photon mapping it usually samples new light paths for each pixel rather than using the same cached data for all pixels). Metropolis light transport samples paths by modifying paths that were previously traced, spending more time exploring paths that are similar to other ""bright"" paths, which increases the chance of discovering even brighter paths. Multiple importance sampling provides a way to reduce variance when combining samples from more than one sampling method, particularly when some samples are much noisier than the others.[65][14]

This later work was summarized and expanded upon in Eric Veach's 1997 PhD thesis, which helped raise interest in path tracing in the computer graphics community. The Arnold renderer, first released in 1998, proved that path tracing was practical for rendering frames for films, and that there was a demand for unbiased and physically based rendering in the film industry; other commercial and open source path tracing renderers began appearing. Computational cost was addressed by rapid advances in CPU and cluster performance.[65]

Path tracing's relative simplicity and its nature as a Monte Carlo method (sampling hundreds or thousands of paths per pixel) have made it attractive to implement on a GPU, especially on recent GPUs that support ray tracing acceleration technology such as Nvidia's RTX and OptiX.[68] However bidirectional path tracing and Metropolis light transport are more difficult to implement efficiently on a GPU.[69][70]

Research into improving path tracing continues. Many variations of bidirectional path tracing and Metropolis light transport have been explored, and ways of combining path tracing with photon mapping.[71][72] Recent path guiding approaches construct approximations of the light field probability distribution in each volume of space, so paths can be sampled more effectively.[72] Techniques have been developed to denoise the output of path tracing, reducing the number of paths required to achieve acceptable quality, at the risk of losing some detail or introducing small-scale artifacts that are more objectionable than noise;[73][74] neural networks are now widely used for this purpose.[75][76][77]

Neural rendering is a rendering method using artificial neural networks.[78][79] Neural rendering includes image-based rendering methods that are used to reconstruct 3D models from 2-dimensional images.[78]One of these methods are photogrammetry, which is a method in which a collection of images from multiple angles of an object are turned into a 3D model. There have also been recent developments in generating and rendering 3D models from text and coarse paintings by notably Nvidia, Google and various other companies.

The implementation of a realistic renderer always has some basic element of physical simulation or emulation –  some computation which resembles or abstracts a real physical process.

The term ""physically based"" indicates the use of physical models and approximations that are more general and widely accepted outside rendering. A particular set of related techniques have gradually become established in the rendering community.

The basic concepts are moderately straightforward, but intractable to calculate; and a single elegant algorithm or approach has been elusive for more general purpose renderers. In order to meet demands of robustness, accuracy and practicality, an implementation will be a complex combination of different techniques.

Rendering research is concerned with both the adaptation of scientific models and their efficient application.

Mathematics used in rendering includes: linear algebra, calculus, numerical mathematics, signal processing, and Monte Carlo methods.

This is the key academic/theoretical concept in rendering. It serves as the most abstract formal expression of the non-perceptual aspect of rendering. All more complete algorithms can be seen as solutions to particular formulations of this equation.

Meaning: at a particular position and direction, the outgoing light (Lo) is the sum of the emitted light (Le) and the reflected light. The reflected light being the sum of the incoming light (Li) from all directions, multiplied by the surface reflection and incoming angle. By connecting outward light to inward light, via an interaction point, this equation stands for the whole 'light transport' –  all the movement of light –  in a scene.

The bidirectional reflectance distribution function (BRDF) expresses a simple model of light interaction with a surface as follows:

Light interaction is often approximated by the even simpler models: diffuse reflection and specular reflection, although both can ALSO be BRDFs.

Rendering is practically exclusively concerned with the particle aspect of light physics –  known as geometrical optics. Treating light, at its basic level, as particles bouncing around is a simplification, but appropriate: the wave aspects of light are negligible in most scenes, and are significantly more difficult to simulate. Notable wave aspect phenomena include diffraction (as seen in the colours of CDs and DVDs) and polarisation (as seen in LCDs). Both types of effect, if needed, are made by appearance-oriented adjustment of the reflection model.

Though it receives less attention, an understanding of human visual perception is valuable to rendering. This is mainly because image displays and human perception have restricted ranges. A renderer can simulate a wide range of light brightness and color, but current displays –  movie screen, computer monitor, etc. –  cannot handle so much, and something must be discarded or compressed. Human perception also has limits, and so does not need to be given large-range images to create realism. This can help solve the problem of fitting images into displays, and, furthermore, suggest what short-cuts could be used in the rendering simulation, since certain subtleties will not be noticeable. This related subject is tone mapping.

One problem that any rendering system must deal with, no matter which approach it takes, is the sampling problem.  Essentially, the rendering process tries to depict a continuous function from image space to colors by using a finite number of pixels.  As a consequence of the Nyquist–Shannon sampling theorem (or Kotelnikov theorem), any spatial waveform that can be displayed must consist of at least two pixels, which is proportional to image resolution.  In simpler terms, this expresses the idea that an image cannot display details, peaks or troughs in color or intensity, that are smaller than one pixel.

If a naive rendering algorithm is used without any filtering, high frequencies in the image function will cause ugly aliasing to be present in the final image.  Aliasing typically manifests itself as jaggies, or jagged edges on objects where the pixel grid is visible.  In order to remove aliasing, all rendering algorithms (if they are to produce good-looking images) must use some kind of low-pass filter on the image function to remove high frequencies, a process called antialiasing.

Rendering is usually limited by available computing power and memory bandwidth, and so specialized hardware has been developed to speed it up (""accelerate"" it), particularly for real-time rendering. Hardware features such as a framebuffer for raster graphics are required to display the output of rendering smoothly in real time.

In the era of vector monitors (also called calligraphic displays), a display processing unit (DPU) was a dedicated CPU or coprocessor that maintained a list of visual elements and redrew them continuously on the screen by controlling an electron beam. Advanced DPUs such as Evans & Sutherland's Line Drawing System-1 (and later models produced into the 1980s) incorporated 3D coordinate transformation features to accelerate rendering of wire-frame images.[36]: 93–94, 404–421 [80] Evans & Sutherland also made the Digistar planetarium projection system, which was a vector display that could render both stars and wire-frame graphics (the vector-based Digistar and Digistar II were used in many planetariums, and a few may still be in operation).[81][82][83] A Digistar prototype was used for rendering 3D star fields for the film Star Trek II: The Wrath of Khan – some of the first 3D computer graphics sequences ever seen in a feature film.[84]

Shaded 3D graphics rendering in the 1970s and early 1980s was usually implemented on general-purpose computers, such as the PDP-10 used by researchers at the University of Utah[85][49]. It was difficult to speed up using specialized hardware because it involves a pipeline of complex steps, requiring data addressing, decision-making, and computation capabilities typically only provided by CPUs (although dedicated circuits for speeding up particular operations were proposed [85]). Supercomputers or specially designed multi-CPU computers or clusters were sometimes used for ray tracing.[52] In 1981, James H. Clark and Marc Hannah designed the Geometry Engine, a VLSI chip for performing some of the steps of the 3D rasterization pipeline, and started the company Silicon Graphics (SGI) to commercialize this technology.[86][87]

Home computers and game consoles in the 1980s contained graphics coprocessors that were capable of scrolling and filling areas of the display, and drawing sprites and lines, though they were not useful for rendering realistic images.[88][89] Towards the end of the 1980s PC graphics cards and arcade games with 3D rendering acceleration began to appear, and in the 1990s such technology became commonplace. Today, even low-power mobile processors typically incorporate 3D graphics acceleration features.[86][90]

The 3D graphics accelerators of the 1990s evolved into modern GPUs. GPUs are general-purpose processors, like CPUs, but they are designed for tasks that can be broken into many small, similar, mostly independent sub-tasks (such as rendering individual pixels) and performed in parallel. This means that a GPU can speed up any rendering algorithm that can be split into subtasks in this way, in contrast to 1990s 3D accelerators which were only designed to speed up specific rasterization algorithms and simple shading and lighting effects (although tricks could be used to perform more general computations).[16]: ch3 [91]

Due to their origins, GPUs typically still provide specialized hardware acceleration for some steps of a traditional 3D rasterization pipeline, including hidden surface removal using a z-buffer, and texture mapping with mipmaps, but these features are no longer always used.[16]: ch3  Recent GPUs have features to accelerate finding the intersections of rays with a bounding volume hierarchy, to help speed up all variants of ray tracing and path tracing,[54] as well as neural network acceleration features sometimes useful for rendering.[92]

GPUs are usually integrated with high-bandwidth memory systems to support the read and write bandwidth requirements of high-resolution, real-time rendering, particularly when multiple passes are required to render a frame, however memory latency may be higher than on a CPU, which can be a problem if the critical path in an algorithm involves many memory accesses. GPU design accepts high latency as inevitable (in part because a large number of threads are sharing the memory bus) and attempts to ""hide"" it by efficiently switching between threads, so a different thread can be performing computations while the first thread is waiting for a read or write to complete.[16]: ch3 [93][94]

Rendering algorithms will run efficiently on a GPU only if they can be implemented using small groups of threads that perform mostly the same operations. As an example of code that meets this requirement: when rendering a small square of pixels in a simple ray-traced image, all threads will likely be intersecting rays with the same object and performing the same lighting computations. For performance and architectural reasons, GPUs run groups of around 16-64 threads called warps or wavefronts in lock-step (all threads in the group are executing the same instructions at the same time). If not all threads in the group need to run particular blocks of code (due to conditions) then some threads will be idle, or the results of their computations will be discarded, causing degraded performance.[16]: ch3 [94]

The following is a rough timeline of frequently mentioned rendering techniques, including areas of current research. Note that even in cases where an idea was named in a specific paper, there were almost always multiple researchers or teams working in the same area (including earlier related work). When a method is first proposed it is often very inefficient, and it takes additional research and practical efforts to turn it into a useful technique.[6]: 887 

The list focuses on academic research and does not include hardware. (For more history see #External links, as well as Computer graphics#History and Golden_age_of_arcade_video_games#Technology.)
"
Textile Manufacturing Services,"

Textile manufacturing or textile engineering is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.[1]

Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work. It became mechanised in the 18th and 19th centuries, and has continued to develop through science and technology since the twentieth century.[2] Specifically, ancient civilizations in India, Egypt, China, sub-Saharan Africa, Eurasia, South America, and North and East Africa all had some forms of textile production. The first book about textile manufacturing is considered to be 'A Treatise on the Art of Weaving' by John Murphy.[3]

Cotton is the world's most important natural fibre. In the year 2007, the global yield was 25 million tons from 35 million hectares cultivated in more than 50 countries.[4]

There are six stages to the manufacturing of cotton textiles:[5]

Cotton is grown in locations with long, hot, dry summers with plenty of sunshine and low humidity. Indian cotton, Gossypium arboreum, is finer but the staple is only suitable for hand processing. American cotton, Gossypium hirsutum, produces the longer staple needed for mechanised textile production.[6] The planting season is from September to mid-November, and the crop is harvested between March and June. The cotton bolls are harvested by stripper harvesters and spindle pickers that remove the entire boll from the plant. The cotton boll is the seed pod of the cotton plant; attached to each of the thousands of seeds are fibres about 2.5 cm long.[7] There is a higher rate of cotton being produced compared to the actual workers needed to produce the material. In 2013 a cotton farmer in Mississippi, Bower Flowers, produced around 13,000 bales of cotton in that year alone. This amount of cotton could be used to produce up to 9.4 million T-shirts.[8]

The seed cotton goes into a cotton gin. The cotton gin separates seeds and removes the ""trash"" (dirt, stems and leaves) from the fibre. In a saw gin, circular saws grab the fibre and pull it through a grating that is too narrow for the seeds to pass. A roller gin is used with longer-staple cotton. Here, a leather roller captures the cotton. A knife blade, set close to the roller, detaches the seeds by drawing them through teeth in circular saws and revolving brushes which clean them away.[9] The ginned cotton fibre, known as lint, is then compressed into bales which are about 1.5 m tall and weigh almost 220 kg. Only 33% of the crop is usable lint. Commercial cotton is graded and priced according to its quality; this broadly relates to the average length of the staple and the variety of the plant. Longer-staple cotton (2½ in to 1¼ in) is called Egyptian, medium staple (1¼ in to ¾ in) is called American upland, and short staple (less than ¾ in) is called Indian.[10] The cotton seed is pressed into cooking oil. The husks and meal are processed into animal feed, and the stems into paper.

Ginning, bale-making and transportation are done in the country of origin.

Cotton is shipped to mills in large 500-pound bales. When the cotton comes out of a bale, it is all packed together and still contains vegetable matter. The bale is broken open using a machine with large spikes, called an opener. To fluff up the cotton and remove the vegetable matter, the cotton is sent through a picker or a similar machine. In a picker, the cotton is beaten with a beater bar to loosen it up. It is then fed through various rollers, which serve to remove the vegetable matter. The cotton, aided by fans, then collects on a screen and gets fed through more rollers where it emerges as a continuous soft fleecy sheet, known as a lap.[10][11]

Scutching refers to the process of cleaning cotton of its seeds and other impurities. The first scutching machine was invented in 1797, but did not come into further mainstream use until after 1808 or 1809, when it was introduced and used in Manchester, England. By 1816, it had become generally adopted. The scutching machine worked by passing the cotton through a pair of rollers, and then striking it with iron or steel bars called beater bars or beaters. The beaters, which turn very quickly, strike the cotton hard and knock the seeds out. This process is done over a series of parallel bars so as to allow the seeds to fall through. At the same time, air is blown across the bars, which carries the cotton into a cotton chamber.

In the carding process, the fibres are separated and then assembled into a loose strand (sliver or tow). The cotton comes off of the picking machine in laps, and is then taken to carding machines. The carders line up the fibres neatly to make them easier to spin. The carding machine consists mainly of one big roller with smaller ones surrounding it. All of the rollers are covered in small teeth, and as the cotton is moved forwards, the teeth get finer (i.e. closer together). The cotton leaves the carding machine in the form of a sliver: a large rope of fibres.[12] In a wider sense, carding can refer to these four processes: 

Combing is optional, but is used to remove the shorter fibres, creating a stronger yarn.[13]

Several slivers are combined. Each sliver will have thin and thick spots, and by combining several slivers together, a more consistent size can be reached. Since combining several slivers produces a very thick rope of cotton fibres, the slivers are separated into rovings. Generally speaking, for machine processing, a roving is about the width of a pencil. These rovings (or slubbings) are then what are used in the spinning process.[14]

Most spinning today is done using break, or open-end spinning. This is a technique where the fibres are blown by air into a rotating drum, where they attach themselves to the tail of formed yarn that is continually being drawn out of the chamber. Other methods of break spinning use needles and electrostatic forces.[15] This method has replaced the older methods of ring and mule spinning. It is also easily adapted for artificial fibres.

The spinning machines takes the roving, thins it and twists it, creating yarn which it winds onto a bobbin.[16]

In mule spinning the roving is pulled off a bobbin and fed through rollers, which are feeding at several different speeds. This thins the roving at a consistent rate. If the roving was not a consistent size, then this step could cause a break in the yarn, or jam the machine. The yarn is twisted through the spinning of the bobbin as the carriage moves out, and is rolled onto a cylinder called a spindle, which then produces a cone-shaped bundle of fibres known as a ""cop"", as the carriage returns. Mule spinning produces a finer thread than ring spinning.[17]

The mule was an intermittent process, as the frame advanced and returned a distance of five feet. It was the descendant of the 1779 Crompton device. It produces a softer, less twisted thread that was favoured for fine fabrics and wefts.

The ring was a descendant of the Arkwright Water frame of 1769. It was a continuous process, the yarn was coarser, had a greater twist and was stronger, thus suitable for use as warp thread. Ring spinning is slow due to the distance the thread must pass around the ring.

Sewing thread was made of several threads twisted together, or doubled.

This is the process where each of the bobbins is rewound to give a tighter bobbin.

Plying is done by pulling yarn from two or more bobbins and twisting it together, in a direction opposite to the one it was spun in. Depending on the weight desired, the cotton may or may not be plied, and the number of strands twisted together varies.[18]

Gassing is the process of passing yarn very rapidly through a series of Bunsen gas flames in a gassing frame, to burn off the projecting fibres and to make the thread round and smooth and bright. Only the better qualities of yarn are gassed, like the kinds used for voiles, poplins, venetians, gabardines, Egyptian cottons, etc. The thread loses around 5-8% of its weight if it's gassed. The gassed yarn is darker in shade afterwards, but should not be scorched.[19]

The weaving process uses a loom. The lengthwise threads are known as the warp, and the crosswise threads are known as the weft. The warp, which must be strong, needs to be presented to loom on a warp beam. The weft passes across the loom in a shuttle that carries the yarn on a pirn. These pirns are automatically changed by the loom. Thus, the yarn needs to be wrapped onto a beam, and onto pirns before weaving can commence.[23]

After being spun and plied, the cotton thread is taken to a warping room where the winding machine takes the required length of yarn and winds it onto warpers' bobbins.

Racks of bobbins are set up to hold the thread while it is wound onto the warp beam of a loom. Because the thread is fine, often three of these would be combined to get the desired number of ends.[24]

A sizing machine is needed for strengthening the warp by adding starch, to reduce breakage.

The process of drawing each end of the warp separately through the dents of the reed and the eyes of the healds, in the order indicated by the draft.

A pirn-winding frame was used to transfer the weft from cheeses of yarn onto the pirns that would fit into the shuttle.

At this point, the thread is woven. Depending on the era, one person could manage anywhere from 3 to 100 machines. In the mid-nineteenth century, four was the standard number. A skilled weaver in 1925 could run 6 Lancashire Looms. As time progressed, new mechanisms were added that stopped the loom any time something went wrong. The mechanisms checked for such things as broken warp or weft threads, the shuttle going straight across, and if the shuttle was empty. Forty of these Northrop Looms or automatic looms could be operated by one skilled worker.[25]

The three primary movements of a loom are shedding, picking, and beating-up.

The Lancashire Loom was the first semi-automatic loom. Jacquard looms and Dobby looms are looms that have sophisticated methods of shedding. They may be separate looms or mechanisms added to a plain loom. A Northrop Loom was fully automatic and was mass-produced between 1909 and the mid-1960s. Modern looms run faster and do not use a shuttle: there are air jet looms, water jet looms, and rapier looms.

Ends and Picks: Picks refer to the weft, ends refer to the warp. The coarseness of the cloth can be expressed as the number of picks and ends per quarter-inch square, or per inch square. Ends is always written first. For example: Heavy domestics are made from coarse yarns, such as 10's to 14's warp and weft, and about 48 ends and 52 picks.[27]

Associated job titles include piecer, scavenger, weaver, tackler, draw boy.

When a hand loom was located in the home, children helped with the weaving process from an early age. Piecing needs dexterity, and a child can be as productive as an adult. When weaving moved from the home to the mill, children were often allowed to help their older sisters, and laws had to be made to prevent child labour from becoming established. The working conditions of cotton production were often harsh, with long hours, low pay, and dangerous machinery. Children, above all, were also prone to physical abuse and often forced to work in unsanitary conditions. It should also be noted that children who worked in handlooms often faced extreme poverty and were unable to obtain an education. The working conditions of cotton production were often harsh, with long hours, low pay, and dangerous machinery. unable to obtain an education.

Knitting by machine is done in two different ways; warp and weft. Weft knitting (as seen in the pictures) is similar in method to hand knitting with stitches all connected to each other horizontally. Various weft machines can be configured to produce textiles from a single spool of yarn or multiple spools, depending on the size of the machine cylinder (in which the needles are bedded). In a warp knit, there are many pieces of yarn and there are vertical chains, zigzagged together by crossing the cotton yarn.

Warp knits do not stretch as much as a weft knits, and they are run-resistant. A weft knit is not run-resistant, but it has more stretch. This is especially true if spools of elastane are processed from separate spool containers and interwoven through the cylinder with cotton yarn, giving the finished product more flexibility and preventing it from having a 'baggy' appearance.  The average t-shirt is a weft knit.[28]

Finishing is a broad range of physical and chemical processes/treatments that complete one stage of textile manufacturing, sometimes in preparation for the next step. Finishing adds value to the product and makes it more attractive, useful and functional for the end-user.[29] Fresh off the loom, cotton fabric not only contains impurities, including warp size, but it also requires further treatment to develop its full potential and to add to its value.[30][31]

Depending on the size that has been used, the cloth may be steeped in a dilute acid and then rinsed, or enzymes may be used to break down the size.[32]

Scouring is a chemical washing process carried out on cotton fabric to remove natural waxes and non-fibrous impurities (like the remains of seed fragments) from the fibres and any soiling or dirt that might remain. Scouring is usually carried out in iron vessels called kiers. The fabric is boiled in an alkali solution, which forms a soap with free fatty acids. A kier is usually enclosed, so the solution of sodium hydroxide can be boiled under pressure, excluding oxygen, which would degrade the cellulose in the fibre. If the appropriate reagents are used, scouring will also remove size from the fabric, although desizing often precedes scouring and is considered to be a separate process. Preparation and scouring are prerequisites to most of the other finishing processes. At this stage, even the most naturally white cotton fibres are yellowish, and bleaching is required.[32]

Bleaching improves whiteness by removing natural colouration and whatever impurities remain in the cotton; the degree of bleaching is determined by the levels of whiteness and absorbency required of the fabric. Cotton, being a vegetable fibre, is bleached using an oxidizing agent, such as diluted sodium hypochlorite or diluted hydrogen peroxide. If the fabric is to be dyed a deep shade, then lower levels of bleaching are acceptable. However, for white bedding and for medical applications, the highest levels of whiteness and absorbency are essential.[33]

A further possibility is mercerising, during which the fabric is treated with a caustic soda solution, to cause swelling of the fibres. This results in improved lustre, strength and dye affinity. Cotton is mercerised under tension, and all alkali must be washed out before the tension is released, or shrinkage will take place.[34]

Many other chemical treatments may be applied to cotton fabrics to produce low flammability, crease-resistance and other qualities, but the four most important non-chemical finishing treatments are:

Singeing is designed to burn off the surface fibres from the fabric to produce smoothness. The fabric passes over brushes to raise the fibres, then passes over a plate heated by gas flames.

During raising, the fabric surface is treated with sharp teeth to lift the surface fibres, thereby imparting downiness, softness and warmth, as in flannelette.

Calendering is a process in which the fabric is passed between heated rollers to generate smooth, polished or embossed effects.

Sanforisation is a form of mechanical pre-shrinking, so that the fabric will shrink less upon laundering.

Dyeing is commonly carried out with an anionic direct dye by completely immersing the fabric (or yarn) in an aqueous dye bath according to a prescribed procedure. For improved fastness to washing, rubbing and light, further dyeing methods can be used. These require more complex chemistry during processing, and are thus more expensive to apply.

Printing is the application of colour in the form of a paste or ink to the surface of a fabric in a predetermined pattern. It can be described as a form of localised dyeing. Printing designs onto previously dyed fabric is also possible.

Production of cotton requires arable land.
In addition, cotton is farmed intensively and uses large amounts of fertilizer and 25% of the world's insecticides. Native Indian varieties of cotton were rainwater fed, but modern hybrids used for the mills need irrigation, which spreads pests. The 5% of cotton-bearing land in India uses 55% of all pesticides used in India.[6]

The consumption of energy in form of water and electricity is relatively high, especially in processes like washing, de-sizing, bleaching, rinsing, dyeing, printing, coating and finishing. Processing is time-consuming. The major portion of water in textile industry is used for wet processing of textile (70 per cent). Approximately 25 per cent of energy in the total textile production like fibre production, spinning, twisting, weaving, knitting, clothing manufacturing etc. is used in dyeing. About 34 per cent of energy is consumed in spinning, 23 per cent in weaving, 38 per cent in chemical wet processing and five per cent in miscellaneous processes. Power dominates consumption pattern in spinning and weaving, while thermal energy is the major factor for chemical wet processing.[4]

Cotton acts as a carbon sink as it contains cellulose and this contains 44.44% carbon.  However, due to carbon emissions from fertiliser application, use of mechanized tools to harvest the cotton and so forth cotton manufacture tends to emit more CO2 than is stored in the form of cellulose.[35]

The growth of cotton is divided into two segments i.e. organic and genetically modified.[4] Cotton crop provides livelihood to millions of people but its production is becoming expensive because of high water consumption, use of expensive pesticides, insecticides and fertiliser. Genetically modified products aim to increase disease resistance and reduce the water required. The organic sector in India was worth $583 million. Genetically modified cotton, in 2007, occupied 43% of cotton growing areas in India.[6]

Before mechanisation, cotton was harvested manually by farmers in India and by African slaves in America. In 2012 Uzbekistan was a major exporter of cotton and uses manual labour during the harvest. Human rights groups have expressed concerns over healthcare professionals and children being forced to pick cotton.[36]

There was a 1.5 million tonne cotton deficit in 2018 due to adverse weather conditions, limited water, and pest issues.[37]

Flax is a bast fibre, which means it comes in bundles under the bark of the Linum usitatissimum plant. The plant flowers and is harvested. It is subjected to retting, breaking, scutching, hackling or combing. It is then treated like cotton.[38]

Jute is a bast fibre, which comes from the inner bark of the plants of the Corchorus genus. It is retted like flax, sundried and baled. When spinning a small amount of oil must be added to the fibre. It can be bleached and dyed. It was used for sacks and bags but is now used for the backing for carpets.[39] Jute can be blended with other fibres to make composite fabrics and work continues in Bangladesh to refine the processes and extend the range of usage possible. In the 1970s, jute-cotton composite fabrics were known as jutton fabrics.[40]

Hemp is a bast fibre from the inner bark of Cannabis sativa. It is difficult to bleach, and is used for making cord and rope. It is subject to retting, separating and pounding[41]

These bast fibres can also be used: kenaf, urena, ramie, nettle.

Sisal is the main leaf fibre used; others are abacá and henequen.

Wool comes from domesticated sheep. It is used to create two kinds of yarn, woolens and worsteds. These are distinguished by the direction of the wool fibres in relation to the thread; woolens are perpendicularly arranged, allowing for fluffy yarns that trap air, while worsteds have parallel fibres, creating a strong and smooth yarn.

Modern sheep have uniform fleeces, while primitive and landrace sheep often have dual coats; a soft, short under layer and a hardier, coarser, and longer guard layer. These can be sorted to be processed separately, or spun together. The differing characteristics of each coat allows for very different yarn; the guard hairs can be used for durable outerwear, while the inner coat is what is traditionally used to produce the ultrafine wedding ring shawls across Europe.[42] Spinning them together, like in lopi, produces a unique yarn that combines the strength of the guard hairs with the loft and softness of the undercoat.

Wool that has never been used is known as virgin wool and can be mixed with wool that has been recovered from rags. ""Shoddy"" is the term for recovered wool that is not matted, while ""mungo"" comes from felted wool. Extract is recovered chemically from mixed cotton/wool fabrics.

The fleece is shorn in one piece from the sheep. Ideally, the wool is cut as close to the skin as possible to maximise fibre length. Going over the same spot twice produces small fibres that will produce pills in finished fabric, something that skilled shearers are usually able to avoid. This is then skirted to remove the soiled wool from around the legs and anus, graded, and baled. Grading is done on quality as well as length of the fibres. Long wool fibres can be up to 15 in, but anything over 2.5 inches is suitable for combing into worsteds. Fibres less than that form short wool and are described as clothing or carding wool, and are best suited for the jumbled arrangement of woolens.

At the mill the wool is scoured in a detergent to remove grease (the yolk) and impurities. This is done mechanically in the opening machine. Vegetable matter can be removed chemically using sulphuric acid (carbonising). Washing uses a solution of soap and sodium carbonate. The wool is oiled before carding or combing.

The processes in silk production are similar to those of cotton but take account that reeled silk is a continuous fibre. The terms used are different.

Both wool and silk require farmland. Whereas silkworms require mulberry leaves, sheep eat grass, clover, forbs and other pasture plants. Sheep, like all ruminants emit CO2 via their digestive system.[44] Also, their pastures may sometimes be fertilised which further increases emissions.[45]

Synthetic fibres are the result of extensive development by scientists to improve upon the naturally occurring animal and plant fibres. In general, synthetic fibres are created by forcing, or extruding, fibre forming materials through holes (called spinnerets) into the air, thus forming a thread. Before synthetic fibres were developed, cellulose fibres were made from natural cellulose, which comes from plants.

The first artificial fibre, known as art silk from 1799 onwards, became known as viscose around 1894, and finally rayon in 1924. A similar product known as cellulose acetate was discovered in 1865. Rayon and acetate are both artificial fibres, but not truly synthetic, being made from wood. Although these artificial fibres were discovered in the mid-nineteenth century, successful modern manufacture began much later in the 1930s. Nylon, the first synthetic fibre, made its debut in the United States as a replacement for silk, and was used for parachutes and other military uses. [citation needed]

The techniques used to process these fibres in yarn are essentially the same as with natural fibres, modifications have to be made as these fibres are of great length, and have no texture such as the scales in cotton and wool that aid meshing.[citation needed]

Unlike natural fibres, produced by plants, animals or insects, synthetic fibres are made from fossil fuels, and thus require no farmland.[46]

 Media related to Textile manufacturing at Wikimedia Commons
"
Carpet Manufacturing Services,"

A carpet is a textile floor covering typically consisting of an upper layer of pile attached to a backing. The pile was traditionally made from wool, but since the 20th century synthetic fibers such as polypropylene, nylon, or polyester have often been used, as these fibers are less expensive than wool. The pile usually consists of twisted tufts that are typically heat-treated to maintain their structure. The term carpet is often used in a similar context to the term rug, but rugs are mostly considered to be smaller than a room and not attached to the floor.[1]

Carpets are used for a variety of purposes. These include insulating a person's feet from a cold tile or concrete floor, making a room more comfortable as a place to sit on the floor (e.g., when playing with children or as a prayer rug), reducing sound from walking (particularly in apartment buildings), and adding decoration or color to a room. Carpets can be made in any color by using differently dyed fibers. Carpets can have many different types of patterns and motifs used to decorate the surface. Carpets are used in industrial and commercial establishments such as retail stores and hotels and in private homes. Today, a huge range of carpets and rugs are available at many price and quality levels, ranging from inexpensive, synthetic carpets that are mass-produced in factories and used in commercial buildings to costly hand-knotted wool rugs that are used in private residences.

Carpets can be produced on a loom quite similarly to woven fabric, made using needle felts, knotted by hand (in oriental rugs), made with their pile injected into a backing material (called tufting), flatwoven, made by hooking wool or cotton through the meshes of a sturdy fabric, or embroidered. Carpet is commonly made in widths of 12 and 15 feet (3.7 and 4.6 m) in the US and 4 and 5 m (13 and 16 ft) in Europe. Since the 19th and 20th century, where necessary for wall-to-wall carpet, different widths of carpet can be seamed together with a seaming iron and seam tape (formerly it was sewn together) and fixed to a floor over a cushioned underlay (pad) using nails, tack strips (known in the UK as gripper rods), adhesives, or occasionally decorative metal stair rods. Wall-to-wall carpet is distinguished from rugs or mats, which are loose-laid floor coverings, as wall-to-wall carpet is fixed to the floor and covers a much larger area.

The term carpet comes from Latin carpita and Old French carpite.[2] One derivation of the term states that the French term came from the Old Italian carpita, from the verb carpire meaning 'to pluck'.[1][3] The Online Etymology Dictionary states that the term carpet was first used in English in the late 13th century, with the meaning 'coarse cloth', and by the mid-14th century, ""tablecloth, [or] bedspread"".[4] The word comes from Old French carpite 'heavy decorated cloth, carpet', from Medieval Latin or Old Italian carpita 'thick woolen cloth', which may derive from Latin carpere 'to card, pluck'.[4] The Latin word ""carpet"" was introduced in the 13th century by the Florentines from the Middle Armenian word կարպետ (carpet). The meaning of the term carpet shifted in the 15th century to refer to floor coverings.[4]

The terms carpet and rug are often used interchangeably. A carpet is sometimes defined as stretching from wall to wall.[5] Another definition treats rugs as of lower quality or of smaller size, with carpets quite often having finished ends. A third common definition is that a carpet is permanently fixed in place while a rug is simply laid out on the floor. Historically, the term carpet was also applied to table and wall coverings, as carpets were not commonly used on the floor in European interiors until the 15th century.[citation needed]

The term rug was first used in English in the 1550s, with the meaning 'coarse fabric'. The term is of Scandinavian origin, comparable to Norwegian rugga 'coarse coverlet', from Old Norse rogg 'shaggy tuft', from Proto-Germanic *rawwa-.[6] The meaning of rug ""evolved to 'coverlet, wrap' (1590s), then 'mat for the floor' (1808)"".[6]

The carpet is produced on a loom quite similar to woven fabric. The pile can be plush or Berber. Plush carpet is a cut pile and Berber carpet is a loop pile. There are new styles of carpet combining the two styles called cut and loop carpeting. Normally many colored yarns are used and this process is capable of producing intricate patterns from predetermined designs (although some limitations apply to certain weaving methods with regard to accuracy of pattern within the carpet). These carpets are usually the most expensive due to the relatively slow speed of the manufacturing process. These are very famous in Turkey, Iran, India, Pakistan, and Arabia.[citation needed]

Needle felt carpets are more technologically advanced. These carpets are produced by intermingling and felting individual synthetic fibers using barbed and forked needles forming an extremely durable carpet. These carpets are normally found in commercial settings where there is frequent traffic, such as hotels and restaurants.[citation needed]

On a knotted pile carpet (formally, a ""supplementary weft cut-loop pile"" carpet), the structural weft threads alternate with a supplementary weft that rises at right angles to the surface of the weave. This supplementary weft is attached to the warp by one of three knot types (see below), such as shag carpet which was popular in the 1970s, to form the pile or nap of the carpet. Knotting by hand is most prevalent in oriental rugs and carpets. Kashmir carpets are also hand-knotted. Pile carpets, like flat carpets, can be woven on a loom. Both vertical and horizontal looms have been used in the production of European and oriental carpets. The warp threads are set up on the frame of the loom before weaving begins. A number of weavers may work together on the same carpet. A row of knots is completed and cut. The knots are secured with (usually one to four) rows of weft. The warp in woven carpet is usually cotton and the weft is jute.[citation needed]

There are several styles of knotting, but the two main types of knot are the symmetrical (also called Turkish or Ghiordes) and asymmetrical (also called Persian or Senna). Contemporary centres of knotted carpet production are: Lahore and Peshawar (Pakistan), Kashmir (India), Mirzapur and Bhadohi (India),[7]Tabriz (Iran), Afghanistan, Armenia, Azerbaijan, Turkey, Northern Africa, Nepal, Spain, Turkmenistan, and Tibet. The importance of carpets in the culture of Turkmenistan is such that the national flag features a vertical red stripe near the hoist side, containing five carpet guls (designs used in producing rugs). Kashmir and bhadohi is known for handknotted carpets of silk or wool.[citation needed]

These are carpets that have their pile injected with the help of tufting gun into a backing material, which is itself then bonded to a secondary backing made of a woven hessian weave or a man made alternative to provide stability. The pile is often sheared in order to achieve different textures. This is the most common method of manufacturing of domestic carpets for floor covering purposes in the world.[citation needed]

A flatweave carpet is created by interlocking warp (vertical) and weft (horizontal) threads. Types of oriental flatwoven carpet include kilim, soumak, plain weave, and tapestry weave. Types of European flatwoven carpets include Venetian, Dutch, damask, list, haircloth, and ingrain (aka double cloth, two-ply, triple cloth, or three-ply).[citation needed]

A hooked rug is a simple type of rug handmade by pulling strips of cloth such as wool or cotton through the meshes of a sturdy fabric such as burlap. This type of rug is now generally made as a handicraft. The process of creating a hooked rug is called rug hooking.[8]

Unlike woven carpets, embroidery carpets are not formed on a loom. Their pattern is established by the application of stitches to a cloth (often linen) base. The tent stitch and the cross stitch are two of the most common. Embroidered carpets were traditionally made by royal and aristocratic women in the home, but there has been some commercial manufacture since steel needles were introduced (earlier needles were made of bone) and linen weaving improved in the 16th century. Mary, Queen of Scots, is known to have been an avid embroiderer. 16th century designs usually involve scrolling vines and regional flowers (for example, the Bradford carpet). They often incorporate animal heraldry and the coat of arms of the maker. Production continued through the 19th century. Victorian embroidered carpet compositions include highly illusionistic, 3-dimensional flowers. Patterns for tiled carpets made of a number of squares, called Berlin wool work, were introduced in Germany in 1804, and became extremely popular in England in the 1830s. Embroidered carpets can also include other features such as a pattern of shapes, or they can even tell a story.[citation needed]

Carpet can be formulated from many single or blended natural and synthetic fibres. Fibres are chosen for durability, appearance, ease of manufacture, and cost. In terms of scale of production, the dominant yarn constructions are polyamides (nylons) and polypropylene with an estimated 90% of the commercial market.[9]

Since the 20th century, nylon is one of the most common materials for the construction of carpets. Both nylon 6 and nylon 6-6 are used. Nylon can be dyed topically or dyed in a molten state (solution dying). Nylon can be printed easily and has excellent wear characteristics. Due to nylon's excellent wear-resistance, it is widely used in industrial and commercial carpeting. In carpets, nylon tends to stain easily due to the presence of dye sites. These dye sites need to be filled in order to give nylon carpet any type of stain resistance. As nylon is petroleum-based it varies in price with the price of oil.[citation needed]

Polypropylene, a polyolefin stiffer than the cheaper polyethylene, is used to produce carpet yarns because it is still less expensive than the other materials used for carpets. It is difficult to dye and does not wear as well as wool or nylon. Polypropylene, sometimes referred to simply as ""olefin"", is commonly used to construct berber carpets. Large looped olefin berber carpets are usually only suited for light domestic use and tend to mat down quickly. Berber carpets with smaller loops tend to be more resilient and retain their new appearance longer than large looped berber styles. Commercial grade level-loop carpets have very small loops, and commercial grade cut-pile styles can be well constructed. When made with polypropylene, commercial grade styles wear very well, making them very suitable for areas with heavy foot traffic such as offices. Polypropylene carpets are known to have good stain resistance, but not against oil-based agents. If a stain does set, it can be difficult to clean. Commercial grade carpets can be glued directly to the floor or installed over a 1/4"" thick, 8-pound density padding. Outdoor grass carpets are usually made from polypropylene.[citation needed]

Wool has excellent durability, can be dyed easily and is fairly abundant. When blended with synthetic fibres such as nylon the durability of wool is increased. Blended wool yarns are extensively used in production of modern carpet, with the most common blend being 80% wool to 20% synthetic fibre, giving rise to the term ""80/20"". Wool is relatively expensive and consequently, it only comprises a small portion of the market.[citation needed]

The polyester known as ""PET"" (polyethylene terephthalate) is used in carpet manufacturing in both spun and filament constructions. After the price of raw materials for many types of carpet rose in the early 2000s, polyester became more competitive. Polyester has good physical properties and is inherently stain-resistant because it is hydrophobic, however oil-based stains can pose a problem for this type of material and it can be prone to soiling. Similar to nylon, colour can be added after production or it can be infused in a molten state (solution dyeing). Polyester has the disadvantage that it tends to crush or mat down easily. It is typically used in mid- to low-priced carpeting.[citation needed]

Another polyester, ""PTT"" (Polytrimethylene terephthalate), also called Sorona or 3GT (Dupont) or Corterra (Shell), is a variant of PET. Lurgi Zimmer PTT was first patented in 1941, but it was not produced until the 1990s, when Shell Chemicals developed the low-cost method of producing high-quality 1,3 propanediol (PDO), the starting raw material for PTT Corterra Polymers. DuPont subsequently commercialized a biological process for making 1,3-propanediol from corn syrup, imparting significant renewable content on the corresponding Sorona polyester carpet fibers.[10] These carpet fibers have resiliency comparable to nylon.[11]

Acrylic is a synthetic material first created by the Dupont Corporation in 1941 but has gone through various changes since it was first introduced. In the past, acrylic carpet used to fuzz or ""pill"" easily. This happened when the fibres degraded over time and short strands broke away with contact or friction. Over the years, new types of acrylics have been developed to alleviate some of these problems, although the issues have not been completely removed. Acrylic is fairly difficult to dye but is colourfast, washable, and has the feel and appearance of wool, making it a good rug fabric.[citation needed]

The knotted pile carpet probably originated in the Caspian Sea area (Northern Iran) [14] or the Armenian Highland.[15] Although there is evidence of goats and sheep being sheared for wool and hair which was spun and woven as far back at the 7th millennium, the earliest surviving pile carpet is the ""Pazyryk carpet"", which dates from the 5th-4th century BC. It was excavated by Sergei Ivanovich Rudenko in 1949 from a Pazyryk burial mound in the Altai Mountains in Siberia. This richly coloured carpet is 200 cm × 183 cm (6 ft 7 in × 6 ft 0 in) and framed by a border of griffins.[16]

Although claimed by many cultures, this square tufted carpet, almost perfectly intact, is considered by many experts to be of Caucasian, specifically Armenian, origin. The rug is woven using the Armenian double knot, and the red filaments' color was made from Armenian cochineal.[17][18] The eminent authority of ancient carpets, Ulrich Schurmann, says of it, ""From all the evidence available I am convinced that the Pazyryk rug was a funeral accessory and most likely a masterpiece of Armenian workmanship"".[19] Gantzhorn concurs with this thesis. At the ruins of Persepolis in Iran where various nations are depicted as bearing tribute, the horse design from the Pazyryk carpet is the same as the relief depicting part of the Armenian delegation.[15] The historian Herodotus writing in the 5th century BC also informs us that the inhabitants of the Caucasus wove beautiful rugs with brilliant colors which would never fade.[20]

There has recently been a surge in demand for Afghan carpets, although many Afghan carpet manufacturers market their products under the name of a different country.[21] The carpets are made in Afghanistan, as well as by Afghan refugees who reside in Pakistan and Iran. Famous Afghan rugs include the Shindand or Adraskan (named after local Afghan villages), woven in the Herat area in western Afghanistan.[citation needed]

Afghan carpets are commonly known as Afghan rugs. Afghan carpets are a unique and widely recognized handmade material design that originates from Afghanistan. They often exhibit intricate detailing, mainly using traditional tribal designs originating from the Turkmens, Kazakhs, Balochs, and Uzbeks. The handmade rugs come in many patterns and colors, yet the traditional and most common example of Afghan carpet is the octagon-shaped elephant-foot (Bukhara). The rugs with this print are most commonly red in color. Many dyes, such as vegetable dyes, are used to impart rich color.[citation needed]

Various rug fragments have been excavated in Armenia dating back to the 7th century BC or earlier. The oldest single surviving knotted carpet in existence is the Pazyryk carpet, excavated from a frozen tomb in Siberia, dated from the 5th to the 3rd century BC, now in the Hermitage Museum in St. Petersburg. This square tufted carpet, almost perfectly intact, is considered by many experts to be of Caucasian, specifically Armenian, origin. The eminent authority of ancient carpets, Ulrich Schurmann, says of it, ""From all the evidence available I am convinced that the Pazyryk rug was a funeral accessory and most likely a masterpiece of Armenian workmanship"".[22] Gantzhorn concurs with this thesis. At the ruins of Persepolis in Iran where various nations are depicted as bearing tribute, the horse design from the Pazyryk carpet is the same as the relief depicting part of the Armenian delegation.
Armenian carpets were renowned by foreigners who travelled to Artsakh; the Arab geographer and historian Al-Masudi noted that, among other works of art, he had never seen such carpets elsewhere in his life.[23]

Art historian Hravard Hakobyan notes that ""Artsakh carpets occupy a special place in the history of Armenian carpet-making.""[24] Common themes and patterns found on Armenian carpets were the depiction of dragons and eagles. They were diverse in style, rich in colour and ornamental motifs, and were even separated in categories depending on what sort of animals were depicted on them, such as artsvagorgs (eagle-carpets), vishapagorgs (dragon-carpets) and otsagorgs (serpent-carpets).[24] The rug mentioned in the Kaptavan inscriptions is composed of three arches, ""covered with vegatative ornaments"", and bears an artistic resemblance to the illuminated manuscripts produced in Artsakh.[24]

The art of carpet weaving was in addition intimately connected to the making of curtains as evidenced in a passage by Kirakos Gandzaketsi, a 13th-century Armenian historian from Artsakh, who praised Arzu-Khatun, the wife of regional prince Vakhtang Khachenatsi, and her daughters for their expertise and skill in weaving.[25]

According to ancient perceptions, the carpet is the universe where, according to mythological conceptions, there is

The Gultapin excavations discovered several carpet weaving tools which date back to the 4th-3rd millennium BC. According to Iranica Online, ""The main weaving zone was in the eastern TransCaucasus south of the mountains that bisect the region diagonally, the area now comprised in the Azerbaijan SSR; it is the homeland of a Turkic population known today as Azeri. Other ethnic groups also practiced weaving, some of them in other parts of the Caucasus, but they were of lesser importance.""[27] Azerbaijan was one of the most important centers of carpet weaving; as a result, several different schools have evolved. While traditionally schools are divided into four main branches, each region has its own version of the carpets. The schools are divided into four main branches: Kuba-Shirvan, Ganja-Kazakh carpet-weaving school, Baku carpet school, and Karabakh school of carpet weaving.[28] Carpet weaving is a family tradition in Azerbaijan that is transferred verbally and with practice, and is associated with the daily life and customs of its people. A variety of carpet and rug types are made in Azerbaijan such as silk, wool, gold and silver threads, pile and pileless carpets, as well as kilim, sumakh, zili, verni, mafrashi and khurjun. In 2010, the traditional art of Azerbaijani carpet weaving was added to the Representative List of the Intangible Cultural Heritage of Humanity of UNESCO.[29][28]

Balochi Rugs are a group of carpets that are woven by the Baloch tribes.[30] The size of Baloch rugs are eight feet in length, which made them lighter and easier to transport.[31] Their material typically include of wool or a mixture of wool and goat hair, newer carpets have a warp made of cotton and sturdy wool pile rugs.[citation needed]

As opposed to most antique rug manufactory practices, early Chinese carpets were woven almost exclusively for internal consumption.[32] China has a long history of exporting traditional goods; however, it was not until the first half of the 19th century that the Chinese began to export their rugs. Once in contact with western influences, there was a large change in production: Chinese manufactories began to produce art deco rugs with commercial look and price point. The centuries-old Chinese textile industry is rich in history. While most antique carpets are classified according to a specific region or manufactory, scholars attribute the age of any specific Chinese rug to the ruling emperor of the time. The earliest surviving examples of the craft were produced during the time of Chen Shubao, the last emperor of the Chen dynasty.[citation needed]

Carpet weaving may have been introduced into the area as far back as the 11th century with the coming of the first Muslim conquerors, the Ghaznavids and the Ghurids, from the West. It can with more certainty be traced to the beginning of the Mughal Empire in the early 16th century, when the last successor of Timur, Babur, extended his rule from Kabul to India to found the Mughal Empire. Under the patronage of the Mughals, Indian crafters adopted Persian techniques and designs. Carpets woven in the Punjab made use of motifs and decorative styles found in Mughal architecture.[citation needed]

Akbar, a Muhal emperor, is credited with introducing the art of carpet weaving to India during his reign. The Mughal emperors patronized Persian carpets for their royal courts and palaces. During this period, he brought Persian crafters from their homeland and established them in India. Initially, these Mughal carpets showed the classic Persian style of fine knotting, then gradually the style blended with Indian art. Thus the carpets produced became typical of Indian origin and the industry began to diversify and spread all over the subcontinent. During the Mughal period, carpets made on the Indian subcontinent became so famous that demand for them spread abroad. These carpets had distinctive designs and boasted a high density of knots. Carpets made for the Mughal emperors, including Jahangir and Shah Jahan, were of the finest quality. Under Shah Jahan's reign, Mughal carpet weaving took on a new aesthetic and entered its classical phase.[citation needed] Indian carpets are well known for their designs with attention to detail and presentation of realistic attributes. The carpet industry in India flourished more in its northern part, with major centres found in Kashmir, Jaipur, Agra and Bhadohi.[citation needed]

Indian carpets are known for their high density of knotting. Hand-knotted carpets are a speciality and widely in demand in the West. The carpet industry in India has been successful in establishing social business models that help underprivileged sections of the society. Notable examples of social entrepreneurship ventures are Jaipur rugs[33] and the Fabindia retail chain.[34]

Another category of Indian rugs which, though quite popular in most western countries, have not received much press, is hand-woven rugs of Khairabad (Citapore rugs). [citation needed] Khairabad, a small town in the Citapore (now spelled as ""Sitapur"") district of India had been ruled by Raja Mehmoodabad. Khairabad (Mehmoodabad Estate) was part of Oudh province which had been ruled by shi'i Muslims having Persian linkages. Citapore rugs made in Khairabad and neighbouring areas are hand-woven and distinct from tufted and knotted rugs. Flat weave is the basic weaving technique of Citapore rugs and generally cotton is the main weaving material here but jute, rayon, and chenille are also popular. IKEA and Agocha have been major buyers of rugs from this area.[citation needed]

Iranian carpet is derived from Persian art and culture. Carpet-weaving in Persia dates back to the Bronze Age. The earliest surviving corpus of Persian carpets comes from the Safavid dynasty (1501–1736) in the 16th century.[35] However, painted depictions prove a longer history of production. There is much variety among classical Persian carpets of the 16th and 17th centuries. Common motifs include scrolling vine networks, arabesques, palmettes, cloud bands, medallions, and overlapping geometric compartments rather than animals and humans.[citation needed] This is because Islam, the dominant religion in that part of the world, forbids their depiction.[citation needed] Still, some show figures engaged either in the hunt or feasting scenes. The majority of these carpets are wool, but several silk examples produced in Kashan survive.[36]

Iran is also the world's largest producer and exporter of handmade carpets, producing three-quarters of the world's total output and having a share of 30% of world's export markets.[37][38] The world's largest hand-woven carpet was produced by Iran Carpet Company (ICC) at the order of the Diwan of the Royal Court of Sultanate of Oman to cover the entire floor of the main praying hall of the Sultan Qaboos Grand Mosque (SQGM) in Muscat.[39]

Balochi Rugs (Balochi:قالی بلوچ، فرش بلوچ) are a group of carpets that are woven by the Baloch tribes.[30] The size of Baloch rugs are eight feet in length, which made them lighter and easier to transport.[40]

Baloch rugs tend to be a dark combination of reds, browns, and blues, with touches of white and their material often includes wool or a mixture of wool and goat hair; newer carpets have a warp made of cotton and sturdy wool pile rugs.[citation needed]

Baloch rugs are typically eight feet in length, which made them lighter and easier to transport.[citation needed]

Nature, animal figurines, religious beliefs in Baluch prayer rugs, and objects of interest and use by the people of the tribe and the villagers are visualized in these designs. They are mostly designed geometrically with lines and surfaces, creating abstract and non-abstract patterns.[citation needed]

Mehrabi is a prayer rug designed in the Balochi style, and it typically features a mihrab or arch at one end of the rug.[citation needed]

The art of weaving developed in South Asia at a time when few other civilizations employed it. Excavations at Harappa and Mohenjo-Daro, ancient cities of the Indus Valley civilization, have established that the inhabitants used spindles and spun a wide variety of weaving materials. Some historians consider that the Indus Valley civilization first developed the use of woven textiles. As of the late 1990s, hand-knotted carpets were among Pakistan's leading export products and their manufacture is the second largest cottage and small industry. Pakistani craftsmen have the capacity to produce any type of carpet using all the popular motifs of gulls, medallions, paisleys, traceries, and geometric designs in various combinations.[41] At the time of independence, manufacturing of carpets was set up in Sangla Hill, a small town of Sheikhupura District. Chaudary Mukhtar Ahmad Member, son of Maher Ganda, introduced and taught this art to locals and immigrants. He is considered founder of this industry in Pakistan. Sangla Hill is now a focal point of the carpet industry in Pakistan. Almost all the exporters and manufacturers who are running their business at Lahore, Faisalabad, and Karachi have their area offices in Sangla Hill.[citation needed]

In Pakistan, multiple material types are used including: wool, silk and cotton or jute etc. Carpet textures are typically soft and light in Pakistan.[citation needed]

Scandinavian rugs are among the most popular of all weaves in modern design. Preferred by influential modernist thinkers, designers, and advocates for a new aesthetic in the mid-twentieth century, Scandinavian rugs have become widespread in many different avenues of contemporary interior design. With a long history of adaptation and evolution, the tradition of Scandinavian rug-making is among the most storied of all European rug-making traditions.[citation needed]

Turkish carpets (also known as Anatolian), whether hand knotted or flat woven, are among the most well-known and established handcrafted artworks in the world.[42] Historically: religious, cultural, environmental, sociopolitical and socioeconomic conditions created widespread utilitarian need and have provided artistic inspiration among the many tribal peoples and ethnic groups in Central Asia and Turkey.[43] Turks, nomadic or pastoral, agrarian or town dwellers, living in tents or in sumptuous houses in large cities, have protected themselves from the extremes of the cold weather by covering the floors, and sometimes walls and doorways, with carpets and rugs. The carpets are always hand made of wool or sometimes cotton, with occasional additions of silk. These carpets are natural barriers against the cold. Turkish pile rugs and kilims are also frequently used as tent decorations, grain bags, camel and donkey bags, ground cushions, oven covers, sofa covers, bed and cushion covers, blankets, curtains, eating blankets, table top spreads, prayer rugs and for ceremonial occasions.[citation needed]

The oldest records of flat woven kilims come from Çatalhöyük Neolithic pottery, circa 7000 B.C. One of the oldest settlements ever to have been discovered, Çatalhöyük is located south east of Konya in the middle of the Anatolian region.[44] The excavations to date (only three percent of the town) not only found carbonized fabric but also fragments of kilims painted on the walls of some of the dwellings. The majority of them represent geometric and stylized forms that are similar or identical to other historical and contemporary designs.[45]

The knotted rug is believed to have reached Asia Minor and the Middle East with the expansion of various nomadic tribes peoples during the latter period of the great Turkic migration of the 8th and 9th centuries. Famously depicted in European paintings of The Renaissance, beautiful Anatolian rugs were often used from then until modern times, to indicate the high economic and social status of the owner.[citation needed]

Women learn their weaving skills at an early age, taking months or even years to complete the beautiful pile rugs and flat woven kilims that were created for their use in every aspect of daily life. As is true in most weaving cultures, traditionally and nearly exclusively, it is women and girls who are both artisan and weaver.[46][47][48]

Türkmen carpet (also called ""Bukhara Uzbekistan"") is a type of handmade floor-covering textile traditionally originating in Central Asia. It is useful to distinguish between the original Turkmen tribal rugs and the rugs produced in large numbers for export in the 2000s, mainly in Pakistan and Iran. The original Turkmen rugs were produced by the Turkmen tribes who are the main ethnic group in Turkmenistan and are also found in Afghanistan and Iran. They are used for various purposes, including tent rugs, door hangings and bags of various sizes.[49]

Weaving was traditionally done by men in Uyghur society. Scholars speculate that when the Mongols invaded northwest China in the 13th century, under the leadership of General Subutai, they may have taken as captives some of these skilled carpet weavers.[50]

Oriental carpets began to appear in Europe after the Crusades in the 11th century, due to contact by Crusaders with Eastern traders. Until the mid-18th century they were mostly used on walls and tables. Except in royal or ecclesiastical settings, they were considered too precious to cover the floor. Starting in the 13th century, oriental carpets begin to appear in paintings (notably from Italy, Flanders, England, France, and the Netherlands). Carpets of Indo-Persian design were introduced to Europe via the Dutch, British, and French East India Companies of the 17th and 18th century[51] and in the Polish–Lithuanian Commonwealth by Armenian merchants (Polish carpets or Polonaise carpets).[13]

Although isolated instances of carpet production pre-date the Muslim invasion of Spain, the Hispano-Moresque examples are the earliest significant body of European-made carpets. Documentary evidence shows production beginning in Spain as early as the 10th century AD. The earliest extant Spanish carpet, the so-called Synagogue carpet in the Museum of Islamic Art, Berlin, is a unique survival dated to the 14th century. The earliest group of Hispano-Moresque carpets, Admiral carpets (also known as armorial carpets), has an all-over geometric, repeat pattern punctuated by blazons of noble Christian Spanish families. The variety of this design was analyzed most thoroughly by May Beattie. Many of the 15th century Spanish carpets rely heavily on designs originally developed on the Anatolian Peninsula.[citation needed] Carpet production continued after the Reconquest of Spain and eventual expulsion of the Muslim population in the 15th century. Sixteenth century Renaissance Spanish carpet design is a derivative of silk textile design. Some of the most popular motifs are wreaths, acanthus leaves and pomegranates.[citation needed]

During the Moorish (Muslim) period, production took place in Alcaraz in the province of Albacete, as well as being recorded in other towns. Carpet production after the Christian reconquest continued in Alcaraz while Cuenca, first recorded as a weaving centre in the 12th century, became increasingly important, and was dominant in the 17th and early 18th century. Carpets of completely different French-based designs began to be woven in a royal workshop, the Royal Tapestry Factory (Real Fábrica de Tapices de Santa Bárbara) in Madrid in the 18th century. Cuenca was closed down by the royal degree of Carlos IV in the late 18th century to stop it competing with the new workshop. Madrid continued as a weaving centre through to the 20th century, producing brightly coloured carpets, most of whose designs are strongly influenced by French carpet design, and which are frequently signed (on occasions with the monogram MD; also sometimes with the name Stuyck) and dated in the outer stripe. After the Spanish Civil War General Franco revived the carpet weaving industry in workshops named after him, weaving designs that are influenced by earlier Spanish carpets, usually in a very limited range of colours.[52]

Pirot carpet[a] (Serbian: Пиротски ћилим, Pirotski ćilim) refers to a variety of flat tapestry-woven carpets or rugs traditionally produced in Pirot, a town in southeastern Serbia. Pirot kilims with some 122 ornaments and 96 different types have been protected by geographical indication in 2002. They are one of the most important traditional handicrafts in Serbia. In the late 19th century and up to the Second World War, Pirot kilims have been frequently used as insignia of Serbian and Yugoslav royalty. This tradition was revived in 2011 when Pirot kilims were reintroduced for state ceremonies in Serbia. Carpet weaving in Pirot dates back to the Middle Ages.[53][full citation needed] One of the first mentions of the Pirot kilim in written sources date to 1565, when it was said that the šajkaši boats on the Danube and Drava were covered with Pirot kilims. Pirot was once the most important rug-making centre in the Balkans. Pirot is located on the historical main highway which linked central Europe with Constantinople. Pirot was also known as Şarköy in Turkish. The Pirot carpet varieties are also found in Bulgaria and Turkey, and in many other international collections. One of the chief qualities are the colour effects achieved through the choice and arrangement of colours.[citation needed]

In the beginning of the 19th century, plant dyes were replaced by aniline colourings. ""The best product of the country is the Pirot carpet, worth about ten shillings a square metre. The designs are extremely pretty, and the rugs, without being so heavy as the Persian, or so ragged and scant in the web and weft as Caramanian, wear for ever. The manufacture of these is almost entirely confined to Pirot. From Pirots old Turkish signification as Şarköy stems the traditional trade name of the rugs as Şarköy-kilims. Stemming from the homonym to the today's Turkish settlement of Şarköy in Thracia, which had no established rug making tradition, Şarköys are often falsely ascribed to originate from Turkey. Also in the rug-selling industry, Şarköy are mostly labeled as being of oriental or Turkish origin as to easier sell them to non-familiar customers as they prefer rugs with putative oriental origin. In fact, Şarköys have been established from the 17th century in the region of the Western Balkan or Stara Planina mountains in the towns of Pirot, Berkowiza, Lom, Chiprovtsi and Samokov. Later they have been also produced in Knjaževac and Caribrod.[citation needed]

The Chiprovtsi carpet (Чипровци килим) is a type of handmade carpet with two absolutely identical sides, part of Bulgarian national heritage, traditions, arts and crafts. Its name is derived from the town of Chiprovtsi where their production started in the 17th century. The carpet weaving industry played a key role in the revival of Chiprovtsi in the 1720s after the devastation of the failed 1688 Chiprovtsi Uprising against Ottoman rule. The western traveller Ami Boué, who visited Chiprovtsi in 1836–1838, reported that ""mainly young girls, under shelters or in corridors, engage in carpet weaving. They earn only five francs a month and the payment was even lower before"". By 1868, the annual production of carpets in Chiprovtsi had surpassed 14,000 square metres. In 1896, almost 1,400 women from Chiprovtsi and the region were engaged in carpet weaving. In 1920, the locals founded the Manual Labour carpet-weaving cooperative society, the first of its kind in the country.[54] At present, the carpet (kilim) industry remains dominant in the town.[55] Carpets have been crafted according to traditional designs, but in recent years it is up to the customers to decide the pattern of the carpet they have ordered. The production of a single 3 by 4 m (9.8 by 13.1 ft) carpet takes about 50 days; primarily women engage in carpet weaving. Work is entirely manual and all used materials are natural; the primary material is wool, coloured using plant or mineral dyes. The local carpets have been prized at exhibitions in London, Paris, Liège and Brussels. In recent decades, however, the Chiprovtsi carpet industry has been in decline as it had lost its firm foreign markets. As a result, the town and the municipality have been experiencing a demographic crisis.[citation needed]

In 1608 Henry IV initiated the French production of ""Turkish style"" carpets under the direction of Pierre DuPont. This production was soon moved to the Savonnerie factory in Chaillot just west of Paris. The earliest, well-known group produced by the Savonnerie, then under the direction of Simon Lourdet, are the carpets that were produced in the early years of Louis XIV's reign. They are densely ornamented with flowers, sometimes in vases or baskets, against dark blue or brown grounds in deep borders. The designs are based on Netherlandish and Flemish textiles and paintings. The most famous Savonnerie carpets are the series made for the Grande Galerie and the Galerie d'Apollon in the Palais du Louvre between c. 1665 and c. 1685. These 105 masterpieces, made under the artistic direction of Charles Le Brun, were never installed, as Louis XIV moved the court to Versailles in 1688. Their design combines rich acanthus leaves, architectural framing, and mythological scenes (inspired by Cesare Ripa's Iconologie) with emblems of Louis XIV's royal power.[citation needed]

Pierre-Josse Joseph Perrot is the best-known of the mid-eighteenth-century carpet designers. His many surviving works and drawings display graceful rococo s-scrolls, central rosettes, shells, acanthus leaves, and floral swags. The Savonnerie manufactory was moved to the Gobelins in Paris in 1826.[56] The Beauvais manufactory, better known for their tapestry, also made knotted pile carpets from 1780 to 1792. Carpet production in small, privately owned workshops in the town of Aubusson began in 1743. Carpets produced in France employ the symmetrical knot.[52]

Knotted pile carpet weaving technology probably came to England in the early 16th century with Flemish Calvinists fleeing religious persecution. Because many of these weavers settled in south-eastern England, particularly in Norwich, the fourteen extant 16th and 17th century carpets are sometimes referred to as ""Norwich carpets"". These works are either adaptations of Anatolian or Indo-Persian designs, or employ Elizabethan-Jacobean scrolling vines and blossoms; all but one are dated or bear a coat of arms. Like the French, English weavers used the symmetrical knot. There are documented and surviving examples of carpets from three 18th-century manufactories: Exeter (1756–1761, owned by Claude Passavant, 3 extant carpets), Moorfields (1752–1806, owned by Thomas Moore, 5 extant), and Axminster (1755–1835, owned by Thomas Whitty, numerous extant).[citation needed]

Exeter and Moorfields were both staffed with renegade weavers from the French Savonnerie and, therefore, employ the weaving structure of that factory and Perrot-inspired designs. Neoclassical designer Robert Adam supplied designs for both Moorfields and Axminster carpets based on Roman floor mosaics and coffered ceilings. Some of the most well-known rugs of his design were made for Syon House, Osterley House, Harewood House, Saltram House, and Newby Hall.[citation needed]

Axminster carpet was a unique floor covering first made in a factory founded at Axminster, Devon, in 1755 by the cloth weaver Thomas Whitty. Resembling somewhat the Savonnerie carpets produced in France, Axminster carpets were symmetrically knotted by hand in wool on woollen warps, and had a weft of flax or hemp. Like the French carpets, they often featured Renaissance architectural or floral patterns; others mimicked oriental patterns. Similar carpets were produced at the same time in Exeter and in the Moorfields area of London and, shortly before, at Fulham in Middlesex. The Whitty factory closed in 1835 with the advent of machine-made carpeting. The name Axminster, however, survived as a generic term for machine-made carpets whose pile is produced by techniques similar to those used in making velvet or chenille,[57] and Axminster Carpets resumed production at a new site in the town in 1937.[58]

Axminster carpets can use the three main types of broadloom carpet construction: machine-woven, tufted and hand-knotted. Machine-woven carpet is an investment that will last 20 or 30 years, and woven Axminster and Wilton carpets are still popular in areas where longevity and design flexibility are a big part of the purchasing decision. Hotels and leisure venues almost always choose these types, and many homes use woven Axminsters as design statements.[citation needed]

Machine-woven carpets like Axminster and Wilton are made by massive looms that weave together 'bobbins' of carpet yarn and backing. The finished result, which can be intricately patterned, creates a floor that provides supreme underfoot luxury with high performance. Tufted carpets are also popular in the home. They are relatively speedy to make: a pre-woven backing has yarns tufted into it. Needles push the yarn through the backing, which is then held in place with underlying ""loopers"". Tufted carpets can be twist pile, velvet, or loop pile. Twist pile carpets are produced when one or more fibres are twisted in the tufting process, so that in the finished carpet they appear to be bound together. Velvet pile carpets tend to have a shorter pile and a tighter construction, giving the finished article a smooth, velvety appearance. Loop pile carpets are renowned for being hard wearing and lend carpets great texture. The traditional domain of rugs from faraway continents, hand knotted squares and rugs use the expertise of weavers to produce work of the finest quality. Traditional rugs often feature a deliberate mistake on behalf of the weaver to guarantee their authenticity.[citation needed]

Six patterns of Axminster carpet are known as the Lansdowne group. These have a tripartite design with reeded circles and baskets of flowers in the central panel, flanked by diamond lozenges in the side panels. Axminster Rococo designs often have a brown ground and include birds copied from popular, contemporary engravings. Even now a large percentage of the 55,000 population of the town still seek employment in this industry.[citation needed]

The town of Wilton, Wiltshire is also known for its carpet weaving, which dates back to the 18th century.[59]

The Brussels loom was introduced into England towards the middle of the 18th century and marked the beginning of a new era in carpet-weaving. It was the first loom on which a pile carpet could be woven mechanically, the pile consisting of rows of loops, formed over wires inserted weftwise during weaving and subsequently withdrawn. Brussels was the first type of carpet to be woven in a loom incorporating the Jacquard pattern-selecting mechanism, and in 1849 power was applied to the loom by Biglow in the United States.[citation needed]

Later, when bladed wires were developed, the pile loops were severed on withdrawal of the wires to produce a carpet known as Wilton, and after this development the loom became known as the Wilton loom. In modern usage the designation Wilton applies to both cut-pile and loop-pile carpets made in this loom. The latter are now variously described as Brussels-Wilton, round wire Wilton, loop-pile Wilton, and round wired Jacquard. The methods of manufacture, including the principles of design, preparatory processes, and weaving, are the same in most respects for both Brussels and Wilton qualities. The chief difference between them is that whereas Brussels loop-pile is secured satisfactorily by the insertion of two picks of weft to each wire (2-shot), the Wilton cut-pile is woven more often with three picks of weft to each wire (3-shot) to ensure that the tufts are firmly secured in the carpet backing.[citation needed]

Brussels carpets have a smooth slightly ribbed surface and their patterning is well defined, a characteristic feature of the carpet. Closeness of pile rather than height contributes to their neat appearance and hard-wearing properties, although they do not simulate the luxury of cut-pile carpets. Brussels Wilton carpets were initially produced on 27-inch (3/4) looms and were sewn together by hand. The looms could incorporate up to five frames, each with a different colour, thus enabling figured or pattern carpets to be manufactured. With judicial and skilful planting of colours in the frames the number of colours could be increased to about twenty, enabling complex designs to be produced. Due to the additional costs in labour these carpets were normally only produced for the bespoke market.[citation needed]

After the First World War, the carpets started to be produced for the general market using popular designs and colourways but they always remained at the luxury end of the general market. The growing middle class of the twentieth century aspired to acquire a Wilton carpet for their 'best' room. Despite the impact of industrialization, the areas where Brussels Wilton carpets were produced remained centred around the towns of Wilton, Kidderminster in the West Midlands, and in West Yorkshire where the firm of John Crossley and Sons in Halifax became synonymous with carpet manufacture. There were smaller areas of manufacture in Scotland and Durham. With the development of different manufacturing methods and looms capable of the mass production of carpets, the public began change their décor, including carpets, on a regular basis, which increased the demand for carpets. The last quarter of the 20th century saw the rapid decline of the labour-intensive Brussels Wilton carpets. Very few of the original ¾ Wilton looms still exist, and the few that do are either in museums or used by small manufacturers that continue to produce custom made luxury carpets for the elite and to replace carpets in historic buildings in the UK and abroad.[60]

The U.S. carpet industry began as early as the end of the 18th century but struggled to compete with imported carpets. Protective tariffs enacted by the United States Congress in 1816 and expanded in the 1820s helped protect the industry along with other textile industries. Surveys showed that the industry included 20 carpet mills that produced as much as 1 million square yards of carpet as of 1834 and 116 mills that produced 8 million square yards of carpets and rugs as of 1850, which increased to 215 mills that produced more than 20 million square yards, and employed 12,000 persons as of 1870. Handloom carpets used to dominate the industry, and it was only later that improvements in power loom technology allowed the industry to match the quality of handloom produced carpets.[61]

The city of Dalton, Georgia, where the majority of the carpet currently produced in the United States is manufactured, has come to be known as the ""Carpet Capital of the World"".[62] The U.S. carpet industry accounts for approximately 45% of the production of carpet in the world,[61] and, as of the turn of the 21st century, 85% to 90% of the carpet that makes up the U.S. carpet market was produced in and around Dalton.[61][63] The Dalton carpet industry traces its history to the work of Catherine Evans Whitener who revived the tufting technique for the production of textiles which she employed for the creation of tufted bedspreads.[62][64] Whitener inspired a cottage industry for tufting bedspreads centered on Dalton.[65]

In the 1930s, tufting became industrialized as bedspread manufacturing was moved to factories and the production process centralized and standardized. The industry moved from handmade production to adapting sewing machines. Eventually, the industry in Dalton began producing carpets.[64] During the New Deal era, the 1938 Fair Labor Standards Act spawned a surge in the carpet industry in Dalton and adaptation of machine tufting to lower labor costs.[62]

Popularity of tufted carpets continued to increase in the 1950s with sales of cotton-made, tufted carpet surpassing the traditional wool-woven industry. By 1951, sales of the tufted carpet had reached 6 million yards per year and increased to 400 million yards per year by 1968. Carpet became the standard floor covering in most homes. Along with the rest of the US economy, the carpet industry began to slow in the 1970s. To adapt to the slowing economy, some firms in the industry began vertical integration by acquiring the production of raw materials all the way to finishing of carpets including the dyeing facilities.[61]

In the early 1980s, many companies in the industry shutdown with the number of carpet mills in operation reduced from 285 mills in 1980 to 100 mills in 1990.[61] In the late 1980s, the carpet industry in Dalton again experienced an economic boom with the industry's demand for labour reaching an all-time high.[66] By 1990, a few firms had consolidated the industry by acquiring their smaller competitors so that the top four firms accounted for 80% of the total production.[61] The Carpet and Rug Institute, the trade association which represents the carpet industry in the US is based in Dalton.[67] While Dalton remains a major carpet manufacturing centre in the U.S., the city has begun to expand its dominant industry to other products including becoming a significant producer of solar panels since passage of the 2022 Inflation Reduction Act.[68]

Carpet is commonly made in widths of 12 and 15 feet (3.7 and 4.6 m) in the US, 4 m and 5 m in Europe. Where necessary different widths can be seamed together with a seaming iron and seam tape (formerly it was sewn together) and it is fixed to a floor over a cushioned underlay (pad) using nails, tack strips (known in the UK as gripper rods), adhesives, or occasionally decorative metal stair rods, thus distinguishing it from rugs or mats, which are loose-laid floor coverings. For environmental reasons, the use of wool, natural bindings, natural padding, and formaldehyde-free glues is becoming more common. These options are almost always at a premium cost.[citation needed]

In the UK, some carpets are still manufactured for yachts, hotels, pubs and clubs in a narrow width of 27 inches (0.69 m) and then sewn to size. Carpeting which covers an entire room area is loosely referred to as 'wall-to-wall', but carpet can be installed over any portion thereof with use of appropriate transition moldings where the carpet meets other types of floor coverings. Carpeting is more than just a single item; it is, in fact, a system comprising the carpet itself, the carpet backing (often made of latex), the cushioning underlay, and a method of installation.
Carpet tiles are also available, typically 50 centimetres (20 in) square. These are usually only used in commercial settings and are affixed using a special pressure-sensitive glue, which holds them into place while allowing easy removal (in an office environment, for example) or allowing rearrangement in order to spread wear.[69]

""Carpet binding"" is a term used for any material being applied to the edge of a carpet to make a rug. Carpet binding is usually cotton or nylon, but also comes in many other materials such as leather. Non-synthetic binding is frequently used with bamboo, grass and wool rugs, but is often used with carpet made from other materials.[citation needed]

The GoodWeave labelling scheme used throughout Europe and North America assures that child labour has not been used: importers pay for the labels, and the revenue collected is used to monitor centres of production and educate previously exploited children.[70]

For the year 2018 in the U.S., the recycling of carpet fiber, backing, and padding was 310,000 tons, which was 9.2 percent of carpet generation. A slightly larger proportion (17.8 percent) was combusted for energy recovery, while the majority of rugs and carpets were landfilled (73 percent).[71]

As of 2023 according to the EPA over four billion pounds of carpet enter the solid waste stream in the United States every year, accounting for more than one percent by weight and about two percent by volume of all municipal solid waste (MSW). Furthermore, according to EPA, ""the bulky nature of carpet creates collection and handling problems for solid waste operations and the variety of materials present in carpet makes it difficult to recycle.""[72]

There are many stories about magic carpets, legendary flying carpets that can be used to transport people who are on it instantaneously or quickly to their destination. Disney's Aladdin depicts a magic carpet found by Aladdin and Abu in the Cave of Wonders while trying to find Genie's lamp. Aladdin and Jasmine ride on the carpet to travel around the world. The term ""magic carpet"" is first attested in 1816.[4] From the 16th to the 19th century, the term ""carpet"" was used ""...as an adjective often with a tinge of contempt, when used of men (as in carpet-knight, 1570s)"", which meant a man who was associated with ""...luxury, ladies' boudoirs, and drawing rooms"".[4] Rolling out the red carpet is an expression that means to welcome a guest lavishly and handsomely. In some cases, an actual red carpet is used for VIPs and celebrities to walk on, such as at the Cannes Film Festival and when foreign dignitaries are welcomed to a country.[citation needed]

In 1820s British servant slang, to ""carpet"" someone means to call them for a reprimand.[4] To be called on the carpet means to be summoned for a serious reason, typically a scolding reprimand; this usage dates from 1900,[73] referring to the carpeted office of a person in authority, such as a schoolmaster or employer. A stronger variant of this expression, to be ""hauled on the carpet"", implies an even sterner reprimand. Carpet bombing is a type of bombing from airplanes which developed in the 20th century in which an entire city is bombed (rather than precise strikes on military targets). The slang expression ""laugh at the carpet"" means to vomit on the floor (especially a carpeted floor).[74] The expression ""on the carpet"" refers to a matter which is under discussion or consideration.[74] The term ""carpet muncher"" is a derogatory slang term for a lesbian; this expression is first attested in 1992.[75]

The term carpet bag, which literally refers to a suitcase made from a piece of carpet, is used in several figurative contexts. The term gained a popular usage after the American Civil War to refer to carpetbaggers, Northerners who moved to the South after the war, especially during the Reconstruction era (1865–1877). Carpetbaggers allegedly politically manipulated and controlled former Confederate states for financial and power gains. In modern usage in the U.S., the term is sometimes used derisively to refer to a politician who runs for public office in an area where they do not have deep community ties, or have lived only for a short time. In the United Kingdom, the term was adopted to refer informally to those who join a mutual organization, such as a building society, in order to force it to demutualize, that is, to convert into a joint stock company, solely for personal financial gain.[citation needed]

Cutting the rug is a slang term for dancing which originated in 1942.[6] The use of the term ""rug"" as an informal term for a ""toupee"" (man's wig) is theater slang from 1940.[6] The term ""sweep [something] under the rug"" or ""sweep [something] under the carpet"" figuratively refers to situations where a person or organization is hiding something embarrassing or negative; this use was first recorded in 1953.[4] The figurative expression ""pull the rug out from under (someone)"", meaning to suddenly deprive of important support, or to upset their plans or make their plans fall through, is first attested to in 1936, in American English.[76]

A related figurative expression used centuries earlier was ""cut the grass under (one's) feet"", which is attested to in the 1580s.[6] A ""rugrat"" or ""rug-rat"" is a slang term for a baby or child, first attested in 1968.[6] The expression ""snug as a bug in a rug"" means wrapped up tight, warm, and comfortable.[77] To ""lie like a rug"" means to tell lies shamelessly.[78]

В ковре нити темно-синего и голубого цвета окрашены индиго по карминоносным червецам, нити красного цвета – аналогичными червецами типа араратской кошенили."
Apparel Manufacturing,"

Textile manufacturing or textile engineering is a major industry. It is largely based on the conversion of fibre into yarn, then yarn into fabric. These are then dyed or printed, fabricated into cloth which is then converted into useful goods such as clothing, household items, upholstery and various industrial products.[1]

Different types of fibres are used to produce yarn. Cotton remains the most widely used and common natural fiber making up 90% of all-natural fibers used in the textile industry. People often use cotton clothing and accessories because of comfort, not limited to different weathers. There are many variable processes available at the spinning and fabric-forming stages coupled with the complexities of the finishing and colouration processes to the production of a wide range of products.

Textile manufacturing in the modern era is an evolved form of the art and craft industries. Until the 18th and 19th centuries, the textile industry was a household work. It became mechanised in the 18th and 19th centuries, and has continued to develop through science and technology since the twentieth century.[2] Specifically, ancient civilizations in India, Egypt, China, sub-Saharan Africa, Eurasia, South America, and North and East Africa all had some forms of textile production. The first book about textile manufacturing is considered to be 'A Treatise on the Art of Weaving' by John Murphy.[3]

Cotton is the world's most important natural fibre. In the year 2007, the global yield was 25 million tons from 35 million hectares cultivated in more than 50 countries.[4]

There are six stages to the manufacturing of cotton textiles:[5]

Cotton is grown in locations with long, hot, dry summers with plenty of sunshine and low humidity. Indian cotton, Gossypium arboreum, is finer but the staple is only suitable for hand processing. American cotton, Gossypium hirsutum, produces the longer staple needed for mechanised textile production.[6] The planting season is from September to mid-November, and the crop is harvested between March and June. The cotton bolls are harvested by stripper harvesters and spindle pickers that remove the entire boll from the plant. The cotton boll is the seed pod of the cotton plant; attached to each of the thousands of seeds are fibres about 2.5 cm long.[7] There is a higher rate of cotton being produced compared to the actual workers needed to produce the material. In 2013 a cotton farmer in Mississippi, Bower Flowers, produced around 13,000 bales of cotton in that year alone. This amount of cotton could be used to produce up to 9.4 million T-shirts.[8]

The seed cotton goes into a cotton gin. The cotton gin separates seeds and removes the ""trash"" (dirt, stems and leaves) from the fibre. In a saw gin, circular saws grab the fibre and pull it through a grating that is too narrow for the seeds to pass. A roller gin is used with longer-staple cotton. Here, a leather roller captures the cotton. A knife blade, set close to the roller, detaches the seeds by drawing them through teeth in circular saws and revolving brushes which clean them away.[9] The ginned cotton fibre, known as lint, is then compressed into bales which are about 1.5 m tall and weigh almost 220 kg. Only 33% of the crop is usable lint. Commercial cotton is graded and priced according to its quality; this broadly relates to the average length of the staple and the variety of the plant. Longer-staple cotton (2½ in to 1¼ in) is called Egyptian, medium staple (1¼ in to ¾ in) is called American upland, and short staple (less than ¾ in) is called Indian.[10] The cotton seed is pressed into cooking oil. The husks and meal are processed into animal feed, and the stems into paper.

Ginning, bale-making and transportation are done in the country of origin.

Cotton is shipped to mills in large 500-pound bales. When the cotton comes out of a bale, it is all packed together and still contains vegetable matter. The bale is broken open using a machine with large spikes, called an opener. To fluff up the cotton and remove the vegetable matter, the cotton is sent through a picker or a similar machine. In a picker, the cotton is beaten with a beater bar to loosen it up. It is then fed through various rollers, which serve to remove the vegetable matter. The cotton, aided by fans, then collects on a screen and gets fed through more rollers where it emerges as a continuous soft fleecy sheet, known as a lap.[10][11]

Scutching refers to the process of cleaning cotton of its seeds and other impurities. The first scutching machine was invented in 1797, but did not come into further mainstream use until after 1808 or 1809, when it was introduced and used in Manchester, England. By 1816, it had become generally adopted. The scutching machine worked by passing the cotton through a pair of rollers, and then striking it with iron or steel bars called beater bars or beaters. The beaters, which turn very quickly, strike the cotton hard and knock the seeds out. This process is done over a series of parallel bars so as to allow the seeds to fall through. At the same time, air is blown across the bars, which carries the cotton into a cotton chamber.

In the carding process, the fibres are separated and then assembled into a loose strand (sliver or tow). The cotton comes off of the picking machine in laps, and is then taken to carding machines. The carders line up the fibres neatly to make them easier to spin. The carding machine consists mainly of one big roller with smaller ones surrounding it. All of the rollers are covered in small teeth, and as the cotton is moved forwards, the teeth get finer (i.e. closer together). The cotton leaves the carding machine in the form of a sliver: a large rope of fibres.[12] In a wider sense, carding can refer to these four processes: 

Combing is optional, but is used to remove the shorter fibres, creating a stronger yarn.[13]

Several slivers are combined. Each sliver will have thin and thick spots, and by combining several slivers together, a more consistent size can be reached. Since combining several slivers produces a very thick rope of cotton fibres, the slivers are separated into rovings. Generally speaking, for machine processing, a roving is about the width of a pencil. These rovings (or slubbings) are then what are used in the spinning process.[14]

Most spinning today is done using break, or open-end spinning. This is a technique where the fibres are blown by air into a rotating drum, where they attach themselves to the tail of formed yarn that is continually being drawn out of the chamber. Other methods of break spinning use needles and electrostatic forces.[15] This method has replaced the older methods of ring and mule spinning. It is also easily adapted for artificial fibres.

The spinning machines takes the roving, thins it and twists it, creating yarn which it winds onto a bobbin.[16]

In mule spinning the roving is pulled off a bobbin and fed through rollers, which are feeding at several different speeds. This thins the roving at a consistent rate. If the roving was not a consistent size, then this step could cause a break in the yarn, or jam the machine. The yarn is twisted through the spinning of the bobbin as the carriage moves out, and is rolled onto a cylinder called a spindle, which then produces a cone-shaped bundle of fibres known as a ""cop"", as the carriage returns. Mule spinning produces a finer thread than ring spinning.[17]

The mule was an intermittent process, as the frame advanced and returned a distance of five feet. It was the descendant of the 1779 Crompton device. It produces a softer, less twisted thread that was favoured for fine fabrics and wefts.

The ring was a descendant of the Arkwright Water frame of 1769. It was a continuous process, the yarn was coarser, had a greater twist and was stronger, thus suitable for use as warp thread. Ring spinning is slow due to the distance the thread must pass around the ring.

Sewing thread was made of several threads twisted together, or doubled.

This is the process where each of the bobbins is rewound to give a tighter bobbin.

Plying is done by pulling yarn from two or more bobbins and twisting it together, in a direction opposite to the one it was spun in. Depending on the weight desired, the cotton may or may not be plied, and the number of strands twisted together varies.[18]

Gassing is the process of passing yarn very rapidly through a series of Bunsen gas flames in a gassing frame, to burn off the projecting fibres and to make the thread round and smooth and bright. Only the better qualities of yarn are gassed, like the kinds used for voiles, poplins, venetians, gabardines, Egyptian cottons, etc. The thread loses around 5-8% of its weight if it's gassed. The gassed yarn is darker in shade afterwards, but should not be scorched.[19]

The weaving process uses a loom. The lengthwise threads are known as the warp, and the crosswise threads are known as the weft. The warp, which must be strong, needs to be presented to loom on a warp beam. The weft passes across the loom in a shuttle that carries the yarn on a pirn. These pirns are automatically changed by the loom. Thus, the yarn needs to be wrapped onto a beam, and onto pirns before weaving can commence.[23]

After being spun and plied, the cotton thread is taken to a warping room where the winding machine takes the required length of yarn and winds it onto warpers' bobbins.

Racks of bobbins are set up to hold the thread while it is wound onto the warp beam of a loom. Because the thread is fine, often three of these would be combined to get the desired number of ends.[24]

A sizing machine is needed for strengthening the warp by adding starch, to reduce breakage.

The process of drawing each end of the warp separately through the dents of the reed and the eyes of the healds, in the order indicated by the draft.

A pirn-winding frame was used to transfer the weft from cheeses of yarn onto the pirns that would fit into the shuttle.

At this point, the thread is woven. Depending on the era, one person could manage anywhere from 3 to 100 machines. In the mid-nineteenth century, four was the standard number. A skilled weaver in 1925 could run 6 Lancashire Looms. As time progressed, new mechanisms were added that stopped the loom any time something went wrong. The mechanisms checked for such things as broken warp or weft threads, the shuttle going straight across, and if the shuttle was empty. Forty of these Northrop Looms or automatic looms could be operated by one skilled worker.[25]

The three primary movements of a loom are shedding, picking, and beating-up.

The Lancashire Loom was the first semi-automatic loom. Jacquard looms and Dobby looms are looms that have sophisticated methods of shedding. They may be separate looms or mechanisms added to a plain loom. A Northrop Loom was fully automatic and was mass-produced between 1909 and the mid-1960s. Modern looms run faster and do not use a shuttle: there are air jet looms, water jet looms, and rapier looms.

Ends and Picks: Picks refer to the weft, ends refer to the warp. The coarseness of the cloth can be expressed as the number of picks and ends per quarter-inch square, or per inch square. Ends is always written first. For example: Heavy domestics are made from coarse yarns, such as 10's to 14's warp and weft, and about 48 ends and 52 picks.[27]

Associated job titles include piecer, scavenger, weaver, tackler, draw boy.

When a hand loom was located in the home, children helped with the weaving process from an early age. Piecing needs dexterity, and a child can be as productive as an adult. When weaving moved from the home to the mill, children were often allowed to help their older sisters, and laws had to be made to prevent child labour from becoming established. The working conditions of cotton production were often harsh, with long hours, low pay, and dangerous machinery. Children, above all, were also prone to physical abuse and often forced to work in unsanitary conditions. It should also be noted that children who worked in handlooms often faced extreme poverty and were unable to obtain an education. The working conditions of cotton production were often harsh, with long hours, low pay, and dangerous machinery. unable to obtain an education.

Knitting by machine is done in two different ways; warp and weft. Weft knitting (as seen in the pictures) is similar in method to hand knitting with stitches all connected to each other horizontally. Various weft machines can be configured to produce textiles from a single spool of yarn or multiple spools, depending on the size of the machine cylinder (in which the needles are bedded). In a warp knit, there are many pieces of yarn and there are vertical chains, zigzagged together by crossing the cotton yarn.

Warp knits do not stretch as much as a weft knits, and they are run-resistant. A weft knit is not run-resistant, but it has more stretch. This is especially true if spools of elastane are processed from separate spool containers and interwoven through the cylinder with cotton yarn, giving the finished product more flexibility and preventing it from having a 'baggy' appearance.  The average t-shirt is a weft knit.[28]

Finishing is a broad range of physical and chemical processes/treatments that complete one stage of textile manufacturing, sometimes in preparation for the next step. Finishing adds value to the product and makes it more attractive, useful and functional for the end-user.[29] Fresh off the loom, cotton fabric not only contains impurities, including warp size, but it also requires further treatment to develop its full potential and to add to its value.[30][31]

Depending on the size that has been used, the cloth may be steeped in a dilute acid and then rinsed, or enzymes may be used to break down the size.[32]

Scouring is a chemical washing process carried out on cotton fabric to remove natural waxes and non-fibrous impurities (like the remains of seed fragments) from the fibres and any soiling or dirt that might remain. Scouring is usually carried out in iron vessels called kiers. The fabric is boiled in an alkali solution, which forms a soap with free fatty acids. A kier is usually enclosed, so the solution of sodium hydroxide can be boiled under pressure, excluding oxygen, which would degrade the cellulose in the fibre. If the appropriate reagents are used, scouring will also remove size from the fabric, although desizing often precedes scouring and is considered to be a separate process. Preparation and scouring are prerequisites to most of the other finishing processes. At this stage, even the most naturally white cotton fibres are yellowish, and bleaching is required.[32]

Bleaching improves whiteness by removing natural colouration and whatever impurities remain in the cotton; the degree of bleaching is determined by the levels of whiteness and absorbency required of the fabric. Cotton, being a vegetable fibre, is bleached using an oxidizing agent, such as diluted sodium hypochlorite or diluted hydrogen peroxide. If the fabric is to be dyed a deep shade, then lower levels of bleaching are acceptable. However, for white bedding and for medical applications, the highest levels of whiteness and absorbency are essential.[33]

A further possibility is mercerising, during which the fabric is treated with a caustic soda solution, to cause swelling of the fibres. This results in improved lustre, strength and dye affinity. Cotton is mercerised under tension, and all alkali must be washed out before the tension is released, or shrinkage will take place.[34]

Many other chemical treatments may be applied to cotton fabrics to produce low flammability, crease-resistance and other qualities, but the four most important non-chemical finishing treatments are:

Singeing is designed to burn off the surface fibres from the fabric to produce smoothness. The fabric passes over brushes to raise the fibres, then passes over a plate heated by gas flames.

During raising, the fabric surface is treated with sharp teeth to lift the surface fibres, thereby imparting downiness, softness and warmth, as in flannelette.

Calendering is a process in which the fabric is passed between heated rollers to generate smooth, polished or embossed effects.

Sanforisation is a form of mechanical pre-shrinking, so that the fabric will shrink less upon laundering.

Dyeing is commonly carried out with an anionic direct dye by completely immersing the fabric (or yarn) in an aqueous dye bath according to a prescribed procedure. For improved fastness to washing, rubbing and light, further dyeing methods can be used. These require more complex chemistry during processing, and are thus more expensive to apply.

Printing is the application of colour in the form of a paste or ink to the surface of a fabric in a predetermined pattern. It can be described as a form of localised dyeing. Printing designs onto previously dyed fabric is also possible.

Production of cotton requires arable land.
In addition, cotton is farmed intensively and uses large amounts of fertilizer and 25% of the world's insecticides. Native Indian varieties of cotton were rainwater fed, but modern hybrids used for the mills need irrigation, which spreads pests. The 5% of cotton-bearing land in India uses 55% of all pesticides used in India.[6]

The consumption of energy in form of water and electricity is relatively high, especially in processes like washing, de-sizing, bleaching, rinsing, dyeing, printing, coating and finishing. Processing is time-consuming. The major portion of water in textile industry is used for wet processing of textile (70 per cent). Approximately 25 per cent of energy in the total textile production like fibre production, spinning, twisting, weaving, knitting, clothing manufacturing etc. is used in dyeing. About 34 per cent of energy is consumed in spinning, 23 per cent in weaving, 38 per cent in chemical wet processing and five per cent in miscellaneous processes. Power dominates consumption pattern in spinning and weaving, while thermal energy is the major factor for chemical wet processing.[4]

Cotton acts as a carbon sink as it contains cellulose and this contains 44.44% carbon.  However, due to carbon emissions from fertiliser application, use of mechanized tools to harvest the cotton and so forth cotton manufacture tends to emit more CO2 than is stored in the form of cellulose.[35]

The growth of cotton is divided into two segments i.e. organic and genetically modified.[4] Cotton crop provides livelihood to millions of people but its production is becoming expensive because of high water consumption, use of expensive pesticides, insecticides and fertiliser. Genetically modified products aim to increase disease resistance and reduce the water required. The organic sector in India was worth $583 million. Genetically modified cotton, in 2007, occupied 43% of cotton growing areas in India.[6]

Before mechanisation, cotton was harvested manually by farmers in India and by African slaves in America. In 2012 Uzbekistan was a major exporter of cotton and uses manual labour during the harvest. Human rights groups have expressed concerns over healthcare professionals and children being forced to pick cotton.[36]

There was a 1.5 million tonne cotton deficit in 2018 due to adverse weather conditions, limited water, and pest issues.[37]

Flax is a bast fibre, which means it comes in bundles under the bark of the Linum usitatissimum plant. The plant flowers and is harvested. It is subjected to retting, breaking, scutching, hackling or combing. It is then treated like cotton.[38]

Jute is a bast fibre, which comes from the inner bark of the plants of the Corchorus genus. It is retted like flax, sundried and baled. When spinning a small amount of oil must be added to the fibre. It can be bleached and dyed. It was used for sacks and bags but is now used for the backing for carpets.[39] Jute can be blended with other fibres to make composite fabrics and work continues in Bangladesh to refine the processes and extend the range of usage possible. In the 1970s, jute-cotton composite fabrics were known as jutton fabrics.[40]

Hemp is a bast fibre from the inner bark of Cannabis sativa. It is difficult to bleach, and is used for making cord and rope. It is subject to retting, separating and pounding[41]

These bast fibres can also be used: kenaf, urena, ramie, nettle.

Sisal is the main leaf fibre used; others are abacá and henequen.

Wool comes from domesticated sheep. It is used to create two kinds of yarn, woolens and worsteds. These are distinguished by the direction of the wool fibres in relation to the thread; woolens are perpendicularly arranged, allowing for fluffy yarns that trap air, while worsteds have parallel fibres, creating a strong and smooth yarn.

Modern sheep have uniform fleeces, while primitive and landrace sheep often have dual coats; a soft, short under layer and a hardier, coarser, and longer guard layer. These can be sorted to be processed separately, or spun together. The differing characteristics of each coat allows for very different yarn; the guard hairs can be used for durable outerwear, while the inner coat is what is traditionally used to produce the ultrafine wedding ring shawls across Europe.[42] Spinning them together, like in lopi, produces a unique yarn that combines the strength of the guard hairs with the loft and softness of the undercoat.

Wool that has never been used is known as virgin wool and can be mixed with wool that has been recovered from rags. ""Shoddy"" is the term for recovered wool that is not matted, while ""mungo"" comes from felted wool. Extract is recovered chemically from mixed cotton/wool fabrics.

The fleece is shorn in one piece from the sheep. Ideally, the wool is cut as close to the skin as possible to maximise fibre length. Going over the same spot twice produces small fibres that will produce pills in finished fabric, something that skilled shearers are usually able to avoid. This is then skirted to remove the soiled wool from around the legs and anus, graded, and baled. Grading is done on quality as well as length of the fibres. Long wool fibres can be up to 15 in, but anything over 2.5 inches is suitable for combing into worsteds. Fibres less than that form short wool and are described as clothing or carding wool, and are best suited for the jumbled arrangement of woolens.

At the mill the wool is scoured in a detergent to remove grease (the yolk) and impurities. This is done mechanically in the opening machine. Vegetable matter can be removed chemically using sulphuric acid (carbonising). Washing uses a solution of soap and sodium carbonate. The wool is oiled before carding or combing.

The processes in silk production are similar to those of cotton but take account that reeled silk is a continuous fibre. The terms used are different.

Both wool and silk require farmland. Whereas silkworms require mulberry leaves, sheep eat grass, clover, forbs and other pasture plants. Sheep, like all ruminants emit CO2 via their digestive system.[44] Also, their pastures may sometimes be fertilised which further increases emissions.[45]

Synthetic fibres are the result of extensive development by scientists to improve upon the naturally occurring animal and plant fibres. In general, synthetic fibres are created by forcing, or extruding, fibre forming materials through holes (called spinnerets) into the air, thus forming a thread. Before synthetic fibres were developed, cellulose fibres were made from natural cellulose, which comes from plants.

The first artificial fibre, known as art silk from 1799 onwards, became known as viscose around 1894, and finally rayon in 1924. A similar product known as cellulose acetate was discovered in 1865. Rayon and acetate are both artificial fibres, but not truly synthetic, being made from wood. Although these artificial fibres were discovered in the mid-nineteenth century, successful modern manufacture began much later in the 1930s. Nylon, the first synthetic fibre, made its debut in the United States as a replacement for silk, and was used for parachutes and other military uses. [citation needed]

The techniques used to process these fibres in yarn are essentially the same as with natural fibres, modifications have to be made as these fibres are of great length, and have no texture such as the scales in cotton and wool that aid meshing.[citation needed]

Unlike natural fibres, produced by plants, animals or insects, synthetic fibres are made from fossil fuels, and thus require no farmland.[46]

 Media related to Textile manufacturing at Wikimedia Commons
"
Accessory Manufacturing,"In fashion, an accessory is an item used to contribute, in a secondary manner, to an individual's outfit. Accessories are often chosen to complete an outfit and complement the wearer's look.[1] They have the capacity to further express an individual's identity and personality. Accessories come in different shapes, sizes, hues, etc. The term came into use in the 16th century.[2]

Fashion accessories may be loosely categorized into two general areas: carried accessories and worn accessories. Carried accessories include purses and handbags, hand fans, parasols and umbrellas, wallets, canes, and ceremonial swords. Worn accessories include cravats, ties, hats, bonnets, belts and suspenders, gloves, muffs, necklaces, bracelets, watches,[a] eyewear, sashes, shawls, scarves, lanyards, socks, pins, piercings, rings, stockings and hair ties.[1]

Shoes, boots, sneakers, and all types of footwear are not accessories but 'wear for the foot'. The type of accessory that an individual chooses to wear or carry to complement their outfit can be determined by several factors, including the specific context of where the individual is going. For example, if an individual is going to work their choice of accessory would differ from someone who is going out to drinks or dinner; thus depending on work or play different accessories would be chosen. Similarly, an individual's economic status, religious and cultural background would also be a contributing factor.[4]

In Victorian fashion accessories such as fans, parasols and gloves held significance for how women experienced gender, race, and class. In this era, there was a trend for women to adopt, or aspire to, a more leisurely lifestyle. Consequently, gloves were often used by women to cover their hands and mask any signs of labour.[5] One fashion accessory commonly worn by women in Victorian England was the slide bracelet. Slide bracelets were worn after the wrist watch came into fashion. [6]

During the early 16th century, in Italy hat badges were worn by civilian men of higher social status as a decorative item, in imitation of the cap badges worn by the invading military. Hat badges were often worn in conjunction with a decorative sword and hilt. Hat badges were fashioned after plaquettes and often depicted a scene with personal relevance to the wearer.[7]
"
Children's Clothing Manufacturing,"Children's clothing or kids' clothing is clothing for children who have not yet grown to full height. Children's clothing is often more casual than adult clothing, fit for play and rest.

In the early 21st century, however, childrenswear became heavily influenced by trends in adult fashion. Grandma bait is a retail industry term for expensive children's clothing.[1]  Due to the rise of social media platforms such as Instagram, celebrities and fashion bloggers have been using their accounts to post pictures of their children wearing luxury ""street style"" clothing, thus inspiring parents to dress their children as they would dress themselves. Good quality, well-designed garments are a priority for some parents, and children's clothing is getting a prime place in top-label stores and high-end fashion retail outlets. Clothes are also getting separately designed for boys and girls at a very early age.[2]

Childhood is distinct from certain parameters in all societies, from infancy to adolescence; societal expectations about children's abilities, limitations, and how they look are present at all stages of their development. In every era, clothing plays an important role in the ""look"" of childhood. An overview of the history of children's clothing reveals changes.

Children, regardless of gender, shared styles and cuts before the twentieth century. From the sixteenth to the twentieth centuries, both men and women wore gowns, tunics, and robes. The gown became a thing for women, newborns, and toddlers only after men's attire evolved into two-piece clothes, shirts, and breeches. Children's styles evolved from gowns to adult garments as they grew older.[3]

Swaddling was popular for a short period, but ankle-length white frocks and slip skirts for babies and crawling toddlers were popular around the 1500s since women wore ultra-fine muslin and chemise dresses in the 1700s, which looked remarkably similar to the attire worn by young children since the middle of the century. Children's gowns were given additional accessories like waist belts as kid's fashion grew.

Another look that gave the ideal example of women's neoclassical fashion was this one. Most women, girls, and toddler boys wore garments made of lightweight fabrics like silk or cotton during the 1800s.[4]


While men's clothes changed dramatically as they grew older in the nineteenth century, girls' dresses stayed relatively unchanged. Women's attire did not alter much in terms of cut or stylistic detail, from their birth robes to the skirted outfits they wore their entire lives. The fundamental distinction between children's and women's fashion was that the length of the dress steadily increased, eventually reaching the floor by mid-adolescence.[2][5]
Function and design must meet at the right proportions in children's clothes for them to be popular and accepted. Fabric choices, openings and fastenings, fit and ease, and trimmings used  are all major considerations when designing children's wear. Some other factors a designer designing for children's clothing should focus on are the changing shape of the growing kid and different proportions of the different parts of the body.

Leisurewear and sportswear are two very prominent design styles in children's clothing. 

School uniforms are another type of specialized clothes for children. 

American sizes for baby clothes are usually based on the child's weight.  European sizes are usually based on the child's height. These may be expressed as an estimated age of the child, e.g., size 6 months (or 3–6 months) is expected to fit a child 61 to 67 centimetres (24 to 26 in) in height and 5.7 to 7.5 kilograms (13 to 17 lb) in weight.[5]

Children's clothing is also sometimes worn by adults who are very short.[4]

Gender-specific clothing for young children was rare throughout most of history, for purely practical reasons.[6]  This included both the type of clothing and the color.[6]  Before birth, the parents did not know whether the baby would be a boy or girl; white clothes were easier to clean; and less clothing was needed, since all of it could be used for any baby, instead of parents feeling like they should buy or make a full set of pink dresses for one baby and then a full set of blue overalls for the next baby.[6]

Before the 1940s, young boys and girls alike wore short dresses.[6]  In the US, during the 1940s and 1950s, boys were dressed like their fathers, which meant shirts and trousers and the same colors that their fathers wore.[6]  From the mid-1960s through the mid-1980s, the fashion for American girls was unisex clothing, such as jeans and T-shirts.[6]  Thus American fashion transitioned from having both boys and girls frequently wear dresses in the 19th century to having neither always wearing dresses by the 1970s.[6]

Gender-specific colors emerged in the middle of the 20th century.[6]  Clothing was expensive, and white clothes could be bleached when they became dirty.[6]  Consequently, colored clothes were not used for babies until the Victorian era, and at that time, any child could be dressed in any color.[6]  The color code of using pink primarily for girls' clothes and blue primarily for boys' clothes did not appear until the 1940s onwards.[6][7]  In fact, in the first part of the 20th century, light blue clothes were recommended for American girls, as being the more ""delicate and dainty"" color, and pink was recommended for boys because it was the ""stronger color"".[6]  Other color-coding schemes recommended that the colors be selected to coordinate with the child's hair or eye color, with blue being recommended for blond hair or blue eyes.

Children's clothing in the English-speaking world has become increasingly segregated, with young girls especially being expected to wear pink. Peggy Orenstein writes in her book, Cinderella Ate My Daughter, that pink-coloured and princess-themed clothes are almost ubiquitous for young girls in shops in the United States. She sees this as problematic because it limits girls to not only one colour, but also to one spectrum of experience, and it ""firmly fuses girls' identity to appearance.""[8] In reaction to this situation, a campaign group Pinkstinks was formed in the UK in 2008[9] to raise awareness of the gender stereotyping of children.[10] Further, clothing companies have started to sell clothes that are unisex or gender-neutral, such as the Swedish company Polarn O. Pyret,[11] while others have been founded specifically to offer such items, such as Tootsa MacGinty.[12][13]
"
Window Treatment Manufacturing,"Window coverings are considered any type of materials used to cover a window to manage sunlight, privacy, additional weatherproofing or for purely decorative purposes.

Window coverings comprise materials used to cover a window to manage sunlight, privacy, additional weatherproofing or for purely decorative purposes. Window coverings are typically used on the interior side of windows, but exterior solutions are also available.

Window coverings may be used to manage overheating and glare issues due to sunlight. Designers may consider these variables in the context of visual comfort indices, which include light quantity (sufficient light to perform tasks), direct sunlight (which may lead to overheating), light uniformity (light distribution over the task plane), and glare.[1] Window coverings can provide a sense of privacy. It is important to consider both privacy in terms of the view from the outside in and from the inside out. Poor privacy is often caused when windows are located on the ground floor, or face nearby neighboring high-rise buildings.[2] Nonetheless, if occupants are not satisfied with the privacy the window provides, window coverings will be used to avoid it. Window coverings can also be applied temporarily to protect windows in storm conditions (such as hurricane shutters) or for extra thermal performance in winter to protect against heat loss through windows (such as insulated blinds or window inserts). Window coverings may be selected by building usage and occupant activity in the room. The amount of daylight needed in the room will differ depending on room type. For information on daylighting metrics that may be affected by window covering use, please reference the daylighting page. Aesthetic qualities of window coverings should also be considered, such as how the color, material, and style match the rest of the interior space.

Window coverings can also affect view quality through the windows. View quality can be expressed through three main variables: content (what an occupant sees), access (how much view can be seen from the occupant's position), and clarity (how clearly an occupant can see the content).[3][4] Partially closing curtains or pulling shades down part-way would affect view quality by limiting view access. Using small aperture shades such as fabric roller shades would primarily affect view quality through a change in view clarity. View clarity can be assessed by visual acuity, contrast sensitivity, and color perception.[3] The color, as well as aperture size, known as Openness Factor, or OF, are two variables of fabric shades that can change view clarity perception. Darker fabric shades with larger openness factors generally achieve higher view clarity.[5]

The impact of window coverings on view quality is of interest for designers, manufacturers, and researchers. However, there is limited research available on the topic of view quality, “due to its complex nature, insufficient funding, and a lack of coordinated effort to move the field forward.”[6]

Types of coverings include:

Window coverings can be static or dynamic. Static window coverings are fixed in place while dynamic window coverings can change their status manually or automatically. Dynamic window coverings can control daylight and solar energy entering the building. Dynamic window coverings are effective in adapting to changing outdoor and indoor conditions. Optimal control of window coverings can increase occupant comfort (visual and thermal comfort) while saving building energy use (lighting, cooling, and heating energy).[7] Typical dynamic window coverings include automated blinds and automatic shades.

Window blinds and shades can be controlled to avoid glare while introducing daylight to the building. The height of the blind and the angle of the slat can be determined by daylighting demand and solar positions (azimuth, altitude). By controlling the height of shades or blinds, the optimal amount of sunlight can be introduced to meet the target horizontal illuminance (work plane illuminance). The optimal angle of the blind slats can protect occupants from direct sunlight (cut-off angle strategy). Glare can also be controlled by glare index-based models, such as discomfort glare probability (DGP).[8]

Window coverings can be controlled to minimize overheating. When the sunlight is strong, fully closed window coverings can decrease the cooling load while maintaining occupants’ thermal comfort. When the outdoor temperature is extremely low, fully closed drapes or curtains can reduce the heating load.[9]

Control of window coverings can also be determined by occupants’ personal preferences. The occupant may prefer to close the window coverings due to privacy issues. Indoor conditions, such as furniture configuration or occupant position, may also affect the shade or blind control.

Dynamic window coverings are important in the area of building automation.
"
Canvas Manufacturing,"Canvas is an extremely durable plain-woven fabric used for making sails, tents, marquees, backpacks, shelters, as a support for oil painting and for other items for which sturdiness is required, as well as in such fashion objects as handbags, electronic device cases, and shoes. It is popularly used by artists as a painting surface, typically stretched across a wooden frame.

Although historically made from hemp, modern canvas is usually made of cotton, linen, or sometimes polyvinyl chloride (PVC),  It differs from other heavy cotton fabrics, such as denim, in being plain weave rather than twill weave. Canvas comes in two basic types: plain and duck. The threads in duck canvas are more tightly woven. The term duck comes from the Dutch word for cloth, doek.  In the United States, canvas is classified in two ways: by weight (ounces per square yard) and by a graded number system. The numbers run in reverse of the weight so a number 10 canvas is lighter than number 4.

The word ""canvas"" is derived from the 13th century Anglo-French canevaz and the Old French canevas. Both may be derivatives of the Vulgar Latin cannapaceus for ""made of hemp"", originating from the Greek κάνναβις (cannabis).[2][3]

Canvas has become a common support medium for  oil painting, acrylic painting, pour paint, watercolor, etc., replacing wooden panels. One of the earliest surviving oils on canvas is a French Madonna with angels from around 1410 in the Gemäldegalerie, Berlin. Its use in Saint George and the Dragon by Paolo Uccello in about 1470,[4] and Sandro Botticelli's Birth of Venus in the 1480s was still unusual for the period.  Large paintings for country houses were more likely to be on canvas, as it  was less expensive  than panel painting.  [5]  Another common category of paintings on lighter cloth such as linen was in distemper or glue, often used for banners to be carried in procession. This is a less durable medium, and surviving examples such as Dirk Bouts' Entombment, in distemper on linen (1450s, National Gallery) are rare, and often rather faded in appearance.

Panel painting remained more common until the 16th century in Italy and the 17th century in Northern Europe. Mantegna and Venetian artists were among those leading the change; Venetian sail canvas was readily available and regarded as the best quality.

Canvas is usually stretched across a wooden frame called a stretcher and may be coated with gesso prior to being used to prevent oil paint from coming into direct contact with the canvas fibers which would eventually cause the canvas to decay. A traditional and flexible chalk gesso is composed of lead carbonate and linseed oil, applied over a rabbit skin glue ground; a variation using titanium white pigment and calcium carbonate is rather brittle and susceptible to cracking. As lead-based paint is poisonous, care has to be taken in using it. Various alternative and more flexible canvas primers are commercially available, the most popular being a synthetic latex paint composed of titanium dioxide and calcium carbonate, bound with a thermo-plastic emulsion.

Many artists have painted onto unprimed canvas, such as Jackson Pollock,[6] Kenneth Noland, Francis Bacon, Helen Frankenthaler, Dan Christensen, Larry Zox, Ronnie Landfield, Color Field painters, Lyrical Abstractionists and others. Staining acrylic paint into the fabric of cotton duck canvas was more benign and less damaging to the fabric of the canvas than the use of oil paint. In 1970, artist Helen Frankenthaler commented about her use of staining:

When I first started doing the stain paintings, I left large areas of canvas unpainted, I think, because the canvas itself acted as forcefully and as positively as paint or line or color. In other words, the very ground was part of the medium, so that instead of thinking of it as background or negative space or an empty spot, that area did not need paint because it had paint next to it. The thing was to decide where to leave it and where to fill it and where to say this doesn't need another line or another pail of colors. It's saying it in space.[7]
Early canvas was made of linen, a sturdy brownish fabric of considerable strength. Linen is particularly suitable for the use of oil paint. In the early 20th century, cotton canvas, often referred to as ""cotton duck"", came into use. Linen is composed of higher quality material, and remains popular with many professional artists, especially those who work with oil paint. Cotton duck, which stretches more fully and has an even, mechanical weave, offers a more economical alternative. The advent of acrylic paint has greatly increased the popularity and use of cotton duck canvas. Linen and cotton derive from two entirely different plants, the flax plant and the cotton plant, respectively.

Gessoed canvases on stretchers are also available. They are available in a variety of weights: light-weight is about 4 oz/sq yd (140 g/m2) or 5 oz/sq yd (170 g/m2); medium-weight is about 7 oz/sq yd (240 g/m2) or 8 oz/sq yd (270 g/m2); heavy-weight is about 10 oz/sq yd (340 g/m2) or 12 oz/sq yd (410 g/m2). They are prepared with two or three coats of gesso and are ready for use straight away. Artists desiring greater control of their painting surface may add a coat or two of their preferred gesso. Professional artists who wish to work on canvas may prepare their own canvas in the traditional manner.

One of the most outstanding differences between modern painting techniques and those of the Flemish and Dutch Masters is in the preparation of the canvas. ""Modern"" techniques take advantage of both the canvas texture as well as those of the paint itself. Renaissance masters took extreme measures to ensure that none of the texture of the canvas came through. This required a painstaking, months-long process of layering the raw canvas with (usually) lead-white paint, then polishing the surface, and then repeating.[8] The final product had little resemblance to fabric, but instead had a glossy, enamel-like finish.

With a properly prepared canvas, the painter will find that each subsequent layer of color glides on in a ""buttery"" manner, and that with the proper consistency of application (fat over lean technique), a painting entirely devoid of brushstrokes can be achieved. A warm iron is applied over a piece of wet cotton to flatten the wrinkles.

Canvas can also be printed on using offset or specialist digital printers to create canvas prints. This process of digital inkjet printing is popularly referred to as Giclée. After printing, the canvas can be wrapped around a stretcher and displayed.

Canvas is a popular base fabric for embroidery such as cross-stitch and Berlin wool work.[9]  Some specific types of embroidery canvases are Aida cloth (also called Java canvas[10]), Penelope canvas, Chess canvas, and Binca canvas.[11][12][13] Plastic canvas is a stiffer form of Binca canvas.[14]

From the 13th century onwards, canvas was used as a covering layer on pavise shields. The canvas was applied to the wooden surface of the pavise, covered with multiple layers of gesso and often richly painted in tempera technique. Finally, the surface was sealed with a transparent varnish. While the gessoed canvas was a perfect painting surface, the primary purpose of the canvas application may have been the strengthening of the wooden shield corpus in a manner similar to modern glass-reinforced plastic.

Splined canvases differ from traditional side-stapled canvas in that canvas is attached with a spline at the rear of the frame. This allows the artist to incorporate painted edges into the artwork itself without staples at the sides, and the artwork can be displayed without a frame. Splined canvas can be restretched by adjusting the spline.

Stapled canvases stay stretched tighter over a longer period of time, but are more difficult to re-stretch when the need arises.

Canvas boards are made of canvas stretched over and glued to a cardboard backing, and sealed on the backside. The canvas is typically linen primed for a certain type of paint. They are primarily used by artists for quick studies.

Understanding the mechanical properties of art canvases is necessary for art conservation, especially when deciding on transporting paintings, conservation treatments and environmental specifications inside museums.[15] Canvases are layered structures made from weaving fibers together, where each layer responds differently to changes in humidity, resulting in localized stresses that cause deformation, cracking, and delamination.[15] There are two directions to the canvas: the warp direction (threads run vertically) and the weft direction (threads run horizontally). Researchers performed tensile testing to determine the effects of humidity on the strength of canvases and observed that increasing humidity decreased the effective elastic modulus (combined modulus of the weft and warp directions). For example, the effective modulus at 30% relative humidity is 180 MPa, which drops to 13 MPa at 90% relative humidity, suggesting that canvas is becoming more flexible and susceptible to deformation.[15] There is an inherent anisotropy to the elastic modulus measured in the weft and warp direction as evidenced in the strain vs. load behavior of the canvas. The canvas exhibits a 0.1 strain in the weft direction and 0.2 strain in the warp direction before failing (thread ripping apart).[15] Though, tensile testing provides an explicit measure of material strength, conservators are unable to tare a piece of painting to create the samples (required length of 250 mm), therefore the traditional methods of assessing mechanical properties have been visual cues and pH values.[16]

Art conservators have recently adopted a new method called zero-span strength analysis, nanoindentation, and numerical modelling to quantitatively evaluate the mechanical properties of painting canvases.[16][17][18] Zero-span strength analysis measures the tensile strength of materials, such as paper and yarns, by reducing the clamping distance to 0.1 mm and applying load to a particular point on the yarn.[16][19] This minimizes effects from material geometry and accurately assesses intrinsic fiber strength. This also reduces the amount of material needed for samples to 60 mm.[16] Using zero-span strength analysis, conservators measured tensile strength of flax, commonly used canvas material in historical paintings and correlated tensile strength to the degree of cellulose depolymerization -- cellulose is a component of flax.[16] Another method for assessing canvas quality is nanoindentation utilizing a millimeter-sized cantilever with a microsphere at its end and measuring local viscoelastic properties.[18] However, with the nanoindentation method, conservators can probe the composite behavior of the layers of paint on top of the canvas, not the actual strength of the canvas itself. Lastly, conservators are using finite element modeling (FEM) and extended-FEM (XFEM) on canvases undergoing desiccation (removal of moisture) to visualize the global and local stresses.[17]
"
Rope Production Services,"

A rope is a group of yarns, plies, fibres, or strands that are twisted or braided together into a larger and stronger form. Ropes have tensile strength and so can be used for dragging and lifting. Rope is thicker and stronger than similarly constructed cord, string, and twine.



Rope may be constructed of any long, stringy, fibrous material (e.g., rattan, a natural material), but generally is constructed of certain natural or synthetic fibres.[1][2][3] Synthetic fibre ropes are significantly stronger than their natural fibre counterparts, they have a higher tensile strength, they are more resistant to rotting than ropes created from natural fibres, and they can be made to float on water.[4] But synthetic ropes also possess certain disadvantages, including slipperiness, and some can be damaged more easily by UV light.[5]

Common natural fibres for rope are Manila hemp, hemp, linen, cotton, coir, jute, straw, and sisal. Synthetic fibres in use for rope-making include polypropylene, nylon, polyesters (e.g. PET, LCP, Vectran), polyethylene (e.g. Dyneema and Spectra), Aramids (e.g. Twaron, Technora and Kevlar) and acrylics (e.g. Dralon). Some ropes are constructed of mixtures of several fibres or use co-polymer fibres. Wire rope is made of steel or other metal alloys.  Ropes have been constructed of other fibrous materials such as silk, wool, and hair, but such ropes are not generally available. Rayon is a regenerated fibre used to make decorative rope.

The twist of the strands in a twisted or braided rope serves not only to keep a rope together, but enables the rope to more evenly distribute tension among the individual strands. Without any twist in the rope, the shortest strand(s) would always be supporting a much higher proportion of the total load.

Because rope has a long history, many systems have been used to specify the size of a rope. In systems that use the inch (Imperial and US customary measurement systems), large ropes over 1 inch (25.4 mm) diameter – such as those used on ships – are measured by their circumference in inches; smaller ropes have a nominal diameter based on the circumference divided by three (as a rough approximation of pi). In the metric system of measurement, the nominal diameter is given in millimetres. The current preferred international standard for rope sizes is to give the mass per unit length, in kilograms per metre. However, even sources otherwise using metric units may still give a ""rope number"" for large ropes, which is the circumference in inches.[6]

Rope has been used since prehistoric times.[7] It is of paramount importance in fields as diverse as construction, seafaring, exploration, sports, theatre, and communications. Many types of knots have been developed to fasten with rope, join ropes, and utilize rope to generate mechanical advantage. Pulleys can redirect the pulling force of a rope in another direction, multiply its lifting or pulling power, and distribute a load over multiple parts of the same rope to increase safety and decrease wear.

Winches and capstans are machines designed to pull ropes.

Knotted ropes have historically been used for measurement and mathematics. For example, Ancient Egyptian rope stretchers used knotted ropes to measure distances, Middle Age European shipbuilders and architects performed calculations using arithmetic ropes, and some pre-colonial South American cultures used quipu for numerical record-keeping.

The use of ropes for hunting, pulling, fastening, attaching, carrying, lifting, and climbing dates back to prehistoric times. It is likely that the earliest ""ropes"" were naturally occurring lengths of plant fibre, such as vines, followed soon by the first attempts at twisting and braiding these strands together to form the first proper ropes in the modern sense of the word. The earliest evidence of suspected rope is a very small fragment of three-ply cord from a Neanderthal site dated 50,000 years ago.[8][9] This item was so small, it was only discovered and described with the help of a high power microscope.  It is slightly thicker than the average thumb-nail, and would not stretch from edge-to-edge across a little finger-nail. There are other ways fibres can twist in nature, without deliberate construction.[10]

A tool dated between 35,000 and 40,000 years found in the Hohle Fels cave in south-western Germany has been identified as a means for making rope.[11] It is a 20 cm (8 in) strip of mammoth ivory with four holes drilled through it. Each hole is lined with precisely cut spiral incisions. The grooves on three of the holes spiral in a clockwise direction from each side of the strip. The grooves on one hole spiral clockwise on one side, but counter-clockwise from the other side.[12] Plant fibres have been found on it that could have come from when they fed through the holes and the tool twisted, creating a single ply yarn. Fiber-making experiments with a replica found that the perforations served as effective guides for raw fibers, making it easier to make a strong, elastic rope than simply twisting fibers by hand spiral incisions would have tended to keep the fibres in place.[11][13] But the incisions cannot impart any twist to the fibres pulled through the holes.[14] Other 15,000-year-old objects with holes with spiral incisions, made from reindeer antler, found across Europe are thought to have been used to manipulate ropes, or perhaps some other purpose.[15] They were originally named ""batons"", and thought possibly to have been carried as badges of rank.[13][16]

Impressions of cordage found on fired clay provide evidence of string and rope-making technology in Pavlov I, Moravia,  dating back between 24,000 and 26,000 years.[17] Fossilized fragments of ""probably two-ply laid rope of about 7 mm [0.28 in] diameter"" were found in one of the caves at Lascaux, dating to approximately 15,000 BC.[18]

The ancient Egyptians were probably the first civilization to develop special tools to make rope. Egyptian rope dates back to 4000 to 3500 BC and was generally made of water reed fibres.[19] Other rope in antiquity was made from the fibres of date palms, flax, grass, papyrus, leather, or animal hair. The use of such ropes pulled by thousands of workers allowed the Egyptians to move the heavy stones required to build their monuments. Starting from approximately 2800 BC, rope made of hemp fibres was in use in China. Rope and the craft of rope making spread throughout Asia, India, and Europe over the next several thousand years.

From the Middle Ages until the 18th century, in Europe ropes were constructed in ropewalks, very long buildings where strands the full length of the rope were spread out and then laid up or twisted together to form the rope. The cable length was thus set by the length of the available rope walk. This is related to the unit of length termed cable length. This allowed for long ropes of up to 300 yards (270 m) long or longer to be made. These long ropes were necessary in shipping as short ropes would require splicing to make them long enough to use for sheets and halyards. The strongest form of  splicing is the short splice, which doubles the cross-sectional area of the rope at the area of the splice, which would cause problems in running the line through pulleys. Any splices narrow enough to maintain smooth running would be less able to support the required weight.[citation needed]  Rope intended for naval use would have a coloured yarn, known as the ""rogue's yarn"", included in the layup. This enabled the source to be identified and to detect pilfering.[20]

Leonardo da Vinci drew sketches of a concept for a ropemaking machine, but it was never built. Remarkable feats of construction were accomplished using rope but without advanced technology: In 1586, Domenico Fontana erected the 327 ton obelisk on Rome's Saint Peter's Square with a concerted effort of 900 men, 75 horses, and countless pulleys and meters of rope. By the late 18th century several working machines had been built and patented.

Some rope is still made from natural fibres, such as coir and sisal, despite the dominance of synthetic fibres such as nylon and polypropylene, which have become increasingly popular since the 1950s.

Nylon was discovered in the late 1930s and was first introduced into fiber ropes during World War II. Indeed, the first synthetic fiber ropes were small braided parachute cords and three-strand tow ropes for gliders, made of nylon during World War II.[21]

Laid rope, also called twisted rope, is historically the prevalent form of rope, at least in modern Western history. Common twisted rope generally consists of three strands and is normally right-laid, or given a final right-handed twist.  The ISO 2 standard uses the uppercase letters S and Z to indicate the two possible directions of twist, as suggested by the direction of slant of the central portions of these two letters.  The handedness of the twist is the direction of the twists as they progress away from an observer. Thus Z-twist rope is said to be right-handed, and S-twist to be left-handed.

Twisted ropes are built up in three steps. First, fibres are gathered and spun into yarns. A number of these yarns are then formed into strands by twisting. The strands are then twisted together to lay the rope. The twist of the yarn is opposite to that of the strand, and that in turn is opposite to that of the rope.  It is this counter-twist, introduced with each successive operation, which holds the final rope together as a stable, unified object.[22]

Traditionally, a three strand laid rope is called a plain- or hawser-laid, a four strand rope is called shroud-laid, and a larger rope formed by counter-twisting three or more multi-strand ropes together is called cable-laid.[23] Cable-laid rope is sometimes clamped to maintain a tight counter-twist rendering the resulting cable virtually waterproof. Without this feature, deep water sailing (before the advent of steel chains and other lines) was largely impossible, as any appreciable length of rope for anchoring or ship to ship transfers, would become too waterlogged – and therefore too heavy – to lift, even with the aid of a capstan or windlass.

One property of laid rope is partial untwisting when used.[24] This can cause spinning of suspended loads, or stretching, kinking, or hockling of the rope itself.  An additional drawback of twisted construction is that every fibre is exposed to abrasion numerous times along the length of the rope. This means that the rope can degrade to numerous inch-long fibre fragments, which is not easily detected visually.[citation needed]

Twisted ropes have a preferred direction for coiling. Normal right-laid rope should be coiled clockwise, to prevent kinking. Coiling this way imparts a twist to the rope.  Rope of this type must be bound at its ends by some means to prevent untwisting.

While rope may be made from three or more strands,[25] modern braided rope consists of a braided (tubular) jacket over strands of fibre (these may also be braided). Some forms of braided rope with untwisted cores have a particular advantage; they do not impart an additional twisting force when they are stressed. The lack of added twisting forces is an advantage when a load is freely suspended, as when a rope is used for rappelling or to suspend an arborist. Other specialized cores reduce the shock from arresting a fall when used as a part of a personal or group safety system.

Braided ropes are generally made from nylon, polyester, polypropylene or high performance fibres such as high modulus polyethylene (HMPE) and aramid. Nylon is chosen for its strength and elastic stretch properties. However, nylon absorbs water and is 10–15% weaker when wet. Polyester is about 90% as strong as nylon but stretches less under load and is not affected by water. It has somewhat better UV resistance, and is more abrasion resistant. Polypropylene is preferred for low cost and light weight (it floats on water) but it has limited resistance to ultraviolet light, is susceptible to friction and has a poor heat resistance.[citation needed]

Braided ropes (and objects like garden hoses, fibre optic or coaxial cables, etc.) that have no lay (or inherent twist) uncoil better if each alternate loop is twisted in the opposite direction, such as in figure-eight coils, where the twist reverses regularly and essentially cancels out.

Single braid consists of an even number of strands, eight or twelve being typical, braided into a circular pattern with half of the strands going clockwise and the other half going anticlockwise. The strands can interlock with either twill or panama (Basked) or seldom plain weave. Kyosev introduced the German notation in English, where the floating length (German: Flechtigkeit) and the number of yarns in a group (German: Fädigkeit) in more natural way for braiding process are used, instead of the pattern names in weaving.[25] The central void may be large or small; in the former case the term hollow braid is sometimes preferred.

Double braid, also called braid on braid, consists of an inner braid filling the central void in an outer braid, that may be of the same or different material. Often the inner braid fibre is chosen for strength while the outer braid fibre is chosen for abrasion resistance.

In a solid braid, (square braid, gasket, or form braid[26] there are at least three or more groups of yarns, interlacing in complex (interlocking) structure. This construction is popular for gaskets and general purpose utility rope but rare in specialized high performance line.


Kernmantle rope has a core (kern) of long twisted fibres in the center, with a braided outer sheath or mantle of woven fibres. The kern provides most of the strength (about 70%), while the mantle protects the kern and determines the handling properties of the rope (how easy it is to hold, to tie knots in, and so on). In dynamic climbing line, core fibres are usually twisted to make the rope more elastic. Static kernmantle ropes are made with untwisted core fibres and tighter braid, which causes them to be stiffer in addition to limiting the stretch.

Plaited rope is made by braiding twisted strands, and is also called square braid.[27] It is not as round as twisted rope and coarser to the touch. It is less prone to kinking than twisted rope and, depending on the material, very flexible and therefore easy to handle and knot. This construction exposes all fibres as well, with the same drawbacks as described above. Brait rope is a combination of braided and plaited, a non-rotating alternative to laid three-strand ropes. Due to its excellent energy-absorption characteristics, it is often used by arborists. It is also a popular rope for anchoring and can be used as mooring warps. This type of construction was pioneered by Yale Cordage.

Endless winding rope is made by winding single strands of high-performance yarns around two end terminations until the desired break strength or stiffness has been reached. This type of rope (often specified as cable to make the difference between a braided or twined construction) has the advantage of having no construction stretch as is the case with above constructions. Endless winding is pioneered by SmartRigging and FibreMax.

The sport of rock climbing uses what is termed ""dynamic"" rope, an elastic rope which stretches under load to absorb the energy generated in arresting a fall without creating forces high enough to injure the climber. Such ropes are of kernmantle construction, as described below.

Conversely, ""static"" ropes have minimal stretch and are not designed to arrest free falls. They are used in caving, rappelling, rescue applications, and industries such as window washing. 

The UIAA, in concert with the CEN, sets climbing-rope standards and oversees testing. Any rope bearing a GUIANA or CE certification tag is suitable for climbing. Climbing ropes cut easily when under load. Keeping them away from sharp rock edges is imperative.  Previous falls arrested by a rope, damage to its sheath, and contamination by dirt or solvents all weaken a rope and can render it unsuitable for further sport use.

Rock climbing ropes are designated as suitable for single, double or twin use. A single rope is the most common, and is intended to be used by itself. These range in thickness from roughly 9 to 11 mm (0.35 to 0.43 in). Smaller diameter ropes are lighter, but wear out faster. 

Double ropes are thinner than single, usually 9 mm (0.35 in) and under, and are intended for use in pairs. These offer a greater margin of safety against cutting, since it is unlikely that both ropes will be cut, but complicate both belaying and leading. Double ropes may be clipped into alternating pieces of protection, allowing each to stay straighter and reduce both individual and total rope drag.

Twin ropes are thin ropes which must be clipped into the same piece of protection, in effect being treated as a single strand. This adds security in situations where a rope may get cut. However new lighter-weight ropes with greater safety have virtually replaced this type of rope.[citation needed]

The butterfly and alpine coils are methods of coiling a rope for carrying.

Rope made from hemp, cotton or nylon is generally stored in a cool dry place for proper storage. To prevent kinking it is usually coiled. To prevent fraying or unravelling, the ends of a rope are bound with twine (whipping), tape, or heat shrink tubing. The ends of plastic fibre ropes are often melted and fused solid; however, the rope and knotting expert Geoffrey Budworth warns against this practice thus:[28]

Sealing rope ends this way is lazy and dangerous. A tugboat operator once sliced the palm of his hand open down to the sinews after the hardened (and obviously sharp) end of a rope that had been heat-sealed pulled through his grasp. There is no substitute for a properly made whipping.

If a load-bearing rope gets a sharp or sudden jolt or the rope shows signs of deteriorating, it is recommended that the rope be replaced immediately and should be discarded or only used for non-load-bearing tasks.[29][30]

The average rope life-span is 5 years. Serious inspection should be given to line after that point.[citation needed] However, the use to which a rope is put affects frequency of inspection. Rope used in mission-critical applications, such as mooring lines or running rigging, should be regularly inspected on a much shorter timescale than this, and rope used in life-critical applications such as mountain climbing should be inspected on a far more frequent basis, up to and including before each use.

Avoid stepping on climbing rope, as this might force tiny pieces of rock through the sheath, which can eventually deteriorate the core of the rope.

Ropes may be flemished into coils on deck for safety,  presentation, and tidiness.

Many types of filaments in ropes are weakened by corrosive liquids, solvents, and high temperatures. Such damage is particularly treacherous because it is often invisible to the eye.[31]

Shock loading should be avoided with general use ropes, as it can damage them.[32] All ropes should be used within a safe working load, which is much less than their breaking strength.

A rope under tension – particularly if it has a great deal of elasticity – can be dangerous if parted. Care should be taken around lines under load.

""Rope"" is a material, and a tool. When it is assigned  a specific function it is often referred to as a ""line"", especially in nautical usage. A line may get a further distinction, for example sail control lines are known as ""sheets""  (e.g. A jib sheet).

A halyard is a line used to raise and lower a sail, typically  with a shackle on its sail end. Other maritime examples of ""lines"" include anchor line, mooring line, fishing line, marline. Common items include clothesline and a chalk line.

In some marine uses the term rope is retained, such as man rope, bolt rope, and bell rope.
"
Tent Manufacturing Services,"A tent is a shelter consisting of sheets of fabric or other material draped over or attached to a frame of poles or a supporting rope. While smaller tents may be free-standing or attached to the ground, large tents are usually anchored using guy ropes tied to stakes or tent pegs. First used as portable homes by nomads, tents are now more often used for recreational camping and as temporary shelters.

Tents range in size from ""bivouac"" structures, just big enough for one person to sleep in, up to huge circus tents capable of seating thousands of people. Tents for recreational camping fall into two categories. Tents intended to be carried by backpackers are the smallest and lightest type. Small tents may be sufficiently light that they can be carried for long distances on a touring bicycle, a boat, or when backpacking. The second type are larger, heavier tents which are usually carried in a car or other vehicle. Depending on tent size and the experience of the person or people involved, such tents can usually be assembled (pitched) in between 5 and 25 minutes; disassembly (striking) takes a similar length of time. Some very specialised tents have spring-loaded poles and can be pitched in seconds, but take somewhat longer to strike (take down and pack).

Over the past decade, tents have also been increasingly linked with homelessness crises in the United States, Canada, and other regions. Places of multiple homeless people living in tents closely pitched or plotted near each other are often referred to as tent cities.

A form of tent called a teepee or tipi, noted for its cone shape and peak smoke hole, was also used by Native American tribes and Aboriginal Canadians of the Plains Indians since ancient times, variously estimated from 10,000 to 4,000 years BC.[1][2]

Tents were used at least as far back as the early Iron Age.[3] They are mentioned in the Bible; for example, in Genesis 4:20 Jabal is described as ""the first to live in tents and raise sheep and goats"". The Roman Army used leather tents, copies of which have been used successfully by modern re-enactors.[4] Various styles developed over time, some derived from traditional nomadic tents, such as the yurt.

Most military tents throughout history were of a simple ridge design. The major technological advance was the use of linen or hemp canvas for the canopy versus leather for the Romans. The primary use of tents was still to provide portable shelter for a small number of men in the field.

By World War I larger designs were being deployed in rear areas to provide shelter for support activities and supplies.
Four types of tents which can be characterized by their unique shapes are A-Frame tents, Pyramid tents, Hoop tents , and Dome tents. tents tend not to be very spacious, given their ground surface area.

Tents are used as habitation by nomads, recreational campers, soldiers, and disaster victims. Pole marquees, a type of large tent are typically used as overhead shelter for festivals, weddings, backyard parties, corporate events, excavation (construction) covers, and industrial shelters.

Tents have traditionally been used by nomadic people all over the world, such as Native Americans, Mongolian, Turkic and Tibetan Nomads, and the Bedouin.

Armies all over the world have long used tents as part of their working life. Tents are preferred by the military for their relatively quick setup and take down times, compared to more traditional shelters. One of the world's largest users of tents is the U.S. Department of Defense. The U.S. DoD has strict rules on tent quality and tent specifications. The most common tent uses for the military are temporary sleeping quarters (barracks); dining facilities (DFACs); field headquarters; morale, welfare, and recreation (MWR) facilities; and security checkpoints. One of the most popular military designs currently fielded is the TEMPER Tent, an acronym for Tent Expandable Modular PERsonnel. The United States Army is beginning to use a more modern tent called the deployable rapid assembly shelter or DRASH, a collapsible tent with provisions for air conditioning and heating.[5]

Camping is a popular form of recreation which often involves the use of tents. A tent is economical and practical because of its portability and low environmental impact. These qualities are necessary when used in the wilderness or backcountry.

Tents are often used in humanitarian emergencies, such as war, earthquakes and fire. The primary choice of tents in humanitarian emergencies are canvas tents,[citation needed] because a cotton canvas tent allows functional breathability while serving the purpose of temporary shelter. Tents distributed by organisations such as UNHCR are made by various manufacturers, depending on the region where the tents are deployed, as well as depending on the purpose.

At times, however, these temporary shelters become a permanent or semi-permanent home, especially for displaced people living in refugee camps or shanty towns who can not return to their former home and for whom no replacement homes are made available.

Tents have been increasingly used as shelter for homeless people in the U.S., especially California, Oregon, and Washington. Encampments spiked in the mid-to-late 2010s. These tent cities housing many homeless and travelers/vagabonds have also, are also commonly found in major cities in the South, including Austin, Texas, which had passed a restriction on homeless encampments in May 2021.[6]

Tents are also often used as sites and symbols of protest over time. In 1968 Resurrection City saw hundreds of tents set up by anti-poverty campaigners in Washington D.C. In the 1970s and 1980s anti-nuclear peace camps spread across Europe and North America, with the largest women's-only camp to date set up outside the RAF Greenham Common United States airbase in Newbury, England to protest the deployment there of cruise missiles during the Cold War. The 1990s saw environmental protest camps as part of the campaign for the Clayoquot Sound in Canada and the roads protests in the UK. The first No Border Network camp was held in Strasbourg in 2002, becoming the first in a series of international camps that continue to be organised today. Other international camps of the 2000s include summit counter-mobilisations like Horizone at the Gleneagles G8 gathering in 2005 and the start of Camp for Climate Action in 2006. Since September 2011, the tent has been used as a symbol of the Occupy movement,[citation needed] an international protest movement which is primarily directed against economic and social inequality. Occupy protesters use tents to create camps in public places wherein they can form communities of open discussion and democratic action.[citation needed]

Generally, the interior of an enclosed tent is about 10 degrees Fahrenheit warmer than the outside environment (not accounting for wind chill), due to the retention of body heat and (to a lesser extent) radiation.[7]

Tent fabric may be made of many materials including cotton (canvas), nylon, felt and polyester. Cotton absorbs water, so it can become very heavy when wet, but the associated swelling tends to block any minute holes so that wet cotton is more waterproof than dry cotton. Cotton tents were often treated with paraffin to enhance water resistance. Nylon and polyester are much lighter than cotton and do not absorb much water; with suitable coatings they can be very waterproof, but they tend to deteriorate over time due to a slow chemical breakdown caused by ultraviolet light. The most common treatments to make fabric waterproof are silicone impregnation or polyurethane coating. Since stitching makes tiny holes in a fabric seams are often sealed or taped to block these holes and maintain waterproofness, though in practice a carefully sewn seam can be waterproof.

Rain resistance is measured and expressed as hydrostatic head in millimetres (mm).[8] This indicates the pressure of water needed to penetrate a fabric. Heavy or wind-driven rain has a higher pressure than light rain. Standing on a groundsheet increases the pressure on any water underneath. Fabric with a hydrostatic head rating of 1000 mm or less is best regarded as shower resistant, with 1500 mm being usually suitable for summer camping. Tents for year-round use generally have at least 2000 mm; expedition tents intended for extreme conditions are often rated at 3000 mm. Where quoted, groundsheets may be rated for 5000 mm or more.

Many tent manufacturers indicate capacity by such phrases as ""3 berth"" or ""2 person"". These numbers indicate how many people the manufacturer thinks can use the tent, though these numbers do not always allow for any personal belongings, such as luggage, inflatable mattresses, camp beds, cots, etc., nor do they always allow for people who are of above average height. Checking the quoted sizes of sleeping areas reveals that several manufacturers consider that a width of 150 cm (4.9 ft) is enough for three people; snug is the operative word.[original research?] Experience indicates that camping may be more comfortable if the actual number of occupants is one or even two less than the manufacturer's suggestion, though different manufacturers have different standards for space requirement and there is no accepted standard.

Tent used in areas with biting insects often have their vent and door openings covered with fine-mesh netting.

Tents can be improvised using waterproof fabric, string, and sticks.

There are three basic configurations of tents, each of which may appear with many variations:

Single skin (USA: single wall): Only one waterproof layer of fabric is used, comprising at least roof and walls. To minimize condensation on the inside of the tent, some expedition tents use waterproof/breathable fabrics.

Single skin with flysheet: A waterproof flysheet or rain fly is suspended over and clear of the roof of the tent; it often overlaps the tent roof slightly, but does not extend down the sides or ends of the tent.

Double skin (USA: double wall): The outer tent is a waterproof layer which extends down to the ground all round. One or more 'inner tents' provide sleeping areas. The outer tent may be just a little larger than the inner tent, or it may be a lot larger and provide a covered living area separate from the sleeping area(s). An inner tent is not waterproof, but allows water vapour to pass through so that condensation occurs only on the exterior side. The double layer may also provide some thermal insulation. Either the outer skin or the inner skin may be the structural component, carrying the poles; the structural skin is always pitched first, though some tents are built with the outer and inner linked so that they are both pitched at the same time.

Components:

Many factors affect tent design, including:

Shelters are not normally used for sleeping. Instead they may act as a store or provide shelter from sun, rain, or dew.

With modern materials, tent manufacturers have great freedom to vary types and styles and shapes of tents.

Many tents which use rigid steel poles are free-standing and do not require guy ropes, though they may require pegs around the bottom edge of the fabric. These tents are usually so heavy (25 to 80 kg) that it takes a rather strong wind to blow them away.

Flexible poles used for tents in this section are typically between 3 and 6 metres (9.8 and 19.7 ft) long. Cheap poles are made of tubes of fibreglass with an external diameter less than 1 cm (1⁄3 in), whereas more expensive aluminium alloys are the material of choice for added strength and durability. For ease of transportation, these poles are made in sections some 30 to 60 cm (0.98 to 1.97 ft) long, with one end of each section having a socket into which the next section can fit. For ease of assembly, the sections for each pole are often connected by an internal elastic cord running the entire length of the pole.

Inflatable pole supports, also known as airbeams, serve as rigid structural supports when inflated but are soft and pliable when deflated. Tents using such technology are neither commonly used nor widely accepted and are available from a very limited number of suppliers.

Much like a bicycle tube and tire, airbeams are often composed of a highly dimensionally stable (i.e. no stretch) fabric sleeve and an air-holding inner bladder. However, other airbeam constructions consist of coated fabrics that are cut and manufactured to its intended shape by a method such as thermal welding. Depending on the desired tent size, airbeams can be anywhere from 2-40 inches in diameter, inflated to different pressures.[13]  High pressure airbeams (40-80 psi) that are filled by compressors are most often used in larger shelters, whereas low pressure beams (5-7 psi) are preferred for recreational use.[14] The relatively low pressure enables the use of a manual pump to inflate the airbeam to the desired level. Airbeams have the unique quality of bending, rather than breaking, when overloaded. Tents that use inflatable airbeams are structured almost identically to those that use flexible poles.

Most of these tent styles are no longer generally available. Most of these are single-skin designs, with optional fly sheets for the ridge tents.

All the tents listed here had a canvas fabric and most used a substantial number of guy ropes (8 to 18). The guys had to be positioned and tensioned fairly precisely in order to pitch the tent correctly, so some training and experience were needed. Pup tents might use wooden or metal poles, but all the other styles mentioned here used wooden poles.

These larger tents are seldom used for sleeping.

Tent design has influenced many large modern buildings. These buildings have in turn influenced the next generation of tent design. Tent-style tensile structures are used to cover large public areas such as entertainment venues, arenas and retail areas (example: The O2) or sports stadiums (example: Munich Olympic Stadium) and airports (example: Denver International Airport). The Sami Parliament of Norway is inspired by the lavvu, a tent traditionally used by the Sami people.
"
Wood Product Manufacturing,"

The wood industry or timber industry (sometimes lumber industry -- when referring mainly to sawed boards) is the industry concerned with forestry, logging, timber trade, and the production of primary forest products and wood products (e.g. furniture) and secondary products like wood pulp for the pulp and paper industry. Some of the largest producers are also among the biggest owners of forest. The wood industry has historically been and continues to be an important sector in many economies.

In the narrow sense of the terms, wood, forest, forestry and timber/lumber industry appear to point to different sectors, in the industrialized, internationalized world, there is a tendency toward huge integrated businesses that cover the complete spectrum from silviculture and forestry in private primary or secondary forests or plantations via the logging process up to wood processing and trading and transport (e.g. timber rafting, forest railways, logging roads).[citation needed]

Processing and products differs especially with regard to the distinction between softwood and hardwood.[1][2][3][4][5] While softwood primarily goes into the production of wood fuel and pulp and paper, hardwood is used mainly for furniture, floors, etc.. Both types can be of use for building and (residential) construction purposes (e.g. log houses, log cabins, timber framing).[citation needed]

Lumber and wood products, including timber for framing, plywood, and woodworking, are created in the wood industry from the trunks and branches of trees through several processes, commencing with the selection of appropriate logging sites and concluding with the milling and treatment processes of the harvested material. In order to determine which logging sites and milling sites are responsibly producing environmental, social and economic benefits, they must be certified under the Forest Stewardship Council Forests For All Forever (FSC) Certification that ensures these qualities.[6]

Mature trees are harvested from both plantations and native forests. Trees harvested at a younger age produce smaller logs, and these can be turned into lower-value products. Factors such as location, climate conditions, species, growth rate, and silviculture can affect the size of a mature tree.[7]

The native hardwood saw-milling industry originally consisted of small family-owned mills, but has recently changed to include a small number of larger mills. Mills produce large volumes of material and aim to ensure delivery of a high quality standard of product. Their goal is to do this efficiently and safely, at low cost, with rapid production time and high output.[7]

Once the timber has been manipulated in the required fashion, it can be shipped out for usage. There are many different purposes for wood including plywood, veneer, pulp, paper, particleboard, pallets, craft items, toys, instrument-making, furniture production, packing cases, wine barrels, cardboard, firewood, garden mulch, fibre adhesives, packaging and pet litter. Western Australia has a unique substance called ‘biochar’, which is made from jarrah and pine and sometimes from crop and forestry residues, along with the former materials. Biochar can be used to manufacture silicone and as a soil additive.

Softwoods, such as the Australian eucalyptus, are highly valued, a are used mainly for construction, paper making, and cladding. The term 'round wood' describes all the wood removed from forests in log form and used for purposes other than fuel. Wood manufacturing residues, such as sawdust and chippings, are collectively known as ""pulp"".[7] The United States industrial production index hit a 13-year high during the COVID-19 pandemic, according to a report from the Board of Governors of the Federal Reserve System.[8][9]

Originally, trees were felled from native forests using axes and hand-held cross-cut saws – a slow process involving significant manual labor. Since sawmills were traditionally located within forests, milled timber had to be transported over long distances via rough terrain or waterways to reach its destination. Logs were later transported via train and tram lines, first by steam-powered log haulers then by steam-powered locomotives, and finally diesel and petrol-powered locomotives. Even in the modern era, timber is dried in kilns. When the first steam railway in Australia opened in Melbourne in 1854, timber transportation changed dramatically. Trains made the transportation of lumber quicker and more affordable, making it possible for the Australian sawmill industry to move inland.[7]

Wood is transported by a variety of methods, typically by road vehicle and log driving over shorter distances. For longer journeys, wood is transported by sea on timber carriers, subject to the IMO TDC Code.[10]

As of 2019, the top timberland owners in the US were structured as real-estate investment trusts and include:[11]

In 2008 the largest lumber and wood producers in the US were[12]

As these companies are often publicly traded, their ultimate owners are a diversified group of investors. There are also timber-oriented real-estate investment trusts.

According to sawmilldatabase, the world top producers of sawn wood in 2007 were:[14]

Workers within the forestry and logging industry sub-sector fall within the agriculture, forestry, fishing, and hunting (AFFH) industry sector as characterized by the North American Industry Classification System (NAICS).[17] The National Institute for Occupational Safety and Health (NIOSH) has taken a closer look at the AFFH industry's noise exposures and prevalence of hearing loss. While the overall industry sector had a prevalence of hearing loss lower than the overall prevalence of noise-exposed industries (15% v. 19%), workers within forestry and logging exceeded 21%.[18] Thirty-six percent of workers within forest nurseries and gathering of forest products, a sub-sector within forestry and logging, experienced hearing loss, the most of any AFFH sub-sector. Workers within forest nurseries and gathering of forest products are tasked with growing trees for reforestation and gathering products such as rhizomes and barks. Comparatively, non-noise-exposed workers have only a 7% prevalence of hearing loss.[19]

Worker noise exposures in the forestry and logging industry have been found to be up to 102 dBA.[20] NIOSH recommends that a worker have an 8-hour time-weighted average of noise exposure of 85 dBA.[21] Excessive noise puts workers at an increased risk of developing hearing loss. If a worker were to develop a hearing loss as a result of occupational noise exposures, it would be classified as occupational hearing loss. Noise exposures within the forestry and logging industry can be reduced by enclosing engines and heavy equipment, installing mufflers and silencers, and performing routine maintenance on equipment.[20] Noise exposures can also be reduced through the hierarchy of hazard controls where removal or replacement of noisy equipment serves as the best method of noise reduction.[citation needed]

The Bureau of Labor Statistics (BLS) has found that fatalities of forestry and logging workers have increased from 2013 to 2016, up from 81 to 106 per year. In 2016, there were 3.6 cases of injury and illness per 100 workers within this industry.[22]

Illegal logging is the harvest, transportation, purchase, or sale of timber in violation of laws. The harvesting procedure itself may be illegal, including using corrupt means to gain access to forests; extraction without permission, or from a protected area; the cutting down of protected species; or the extraction of timber in excess of agreed limits. Illegal logging is a driving force for a number of environmental issues such as deforestation, soil erosion and biodiversity loss which can drive larger-scale environmental crises such as climate change and other forms of environmental degradation.

Illegality may also occur during transport, such as illegal processing and export (through fraudulent declaration to customs); the avoidance of taxes and other charges, and fraudulent certification.[23] These acts are often referred to as ""wood laundering"".[24]

The existence of a wood economy, or more broadly, a forest economy (in many countries a bamboo economy predominates), is a prominent matter in many developing countries as well as in many other nations with a temperate climate and especially in those with low temperatures. These are generally the countries with greater forested areas so conditions allow for development of local forestry to harvest wood for local uses. The uses of wood in furniture, buildings, bridges, and as a source of energy are widely known. Additionally, wood from trees and bushes, can be used in a variety of products, such as wood pulp, cellulose in paper, celluloid in early photographic film, cellophane, and rayon (a substitute for silk).[citation needed]

At the end of their normal usage, wood products can be burnt to obtain thermal energy or can be used as a fertilizer. The potential environmental damage that a wood economy could occasion include a reduction of biodiversity due to monoculture forestry (the intensive cultivation of very few trees types); and CO2 emissions. However, forests can aid in the reduction of atmospheric carbon dioxide and thus limit climate change.[25]

Paper is today the most used wood product.[citation needed]

The wood economy was the starting point of the civilizations worldwide, since eras preceding the Paleolithic[clarification needed] and the Neolithic. It necessarily preceded ages of metals by many millennia, as the melting of metals was possible only through the discovery of techniques to light fire (usually obtained by the scraping of two very dry wooden rods) and the building of many simple machines and rudimentary tools, as canes, club handles, bows, arrows, lances. One of the most ancient handmade articles ever found is a polished wooden spear tip (Clacton Spear) 250,000 years old (third interglacial period), that was buried under sediments in England, at Clacton-on-Sea.[26][27]

The main source of the lumber used in the world is forests, which can be classified as virgin, semivirgin and plantations. Much timber is removed for firewood by local populations in many countries, especially in the third world, but this amount can only be estimated, with wide margins of uncertainty.[citation needed]

In 1998, the worldwide production of ""Roundwood"" (officially counted wood not used as firewood), was about 1,500,000,000 cubic metres (2.0×109 cu yd), amounting to around 45% of the wood cultivated in the world. Cut logs and branches destined to become elements for building construction accounted for approximately 55% of the world's industrial wood production. 25% became wood pulp (including wood powder and broccoli) mainly destined for the production of paper and paperboard, and approximately 20% became panels in plywood and valuable wood for furniture and objects of common use (FAO 1998).[28]

By 2001 the rainforest areas of Brazil were reduced by a fifth (respect of 1970), to around 4,000,000 km2; the ground cleared was mainly destined for cattle pasture—Brazil is the world's largest exporter of beef with almost 200,000,000 head of cattle.[29] The booming Brazilian ethanol economy based upon sugar cane cultivation, is likewise reducing forests area. Canadian forest was reduced by almost 30% to 3,101,340 km2 over the same period.[30]

Regarding the problem of climate change, it is known that burning forests increase CO2 in the atmosphere, while intact virgin forest or plantations act as sinks for CO2, for these reasons wood economy fights greenhouse effect. The amount of CO2 absorbed depends on the type of trees, lands and the climate of the place where trees naturally grow or are planted. Moreover, by night plants do not photosynthesize, and produce CO2, eliminated the successive day. Paradoxically in summer oxygen created by photosynthesis in forests near to cities and urban parks, interacts with urban air pollution (from cars, etc.) and is transformed by solar beams in ozone (molecule of three oxygen atoms), that while in high atmosphere constitutes a filter against ultraviolet beams, in the low atmosphere is a pollutant, able to provoke respiratory disturbances.[31][32]

In a low-carbon economy, forestry operations will be focused on low-impact practices and regrowth.  Forest managers will make sure that they do not disturb soil-based carbon reserves too much. Specialized tree farms will be the main source of material for many products.  Quick maturing tree varieties will be grown on short rotations to maximize output.[33]

Brazil has a long tradition in the harvesting of several types of trees with specific uses. Since the 1960s, imported species of pine tree and eucalyptus have been grown mostly for the plywood and paper pulp industries. Currently high-level research is being conducted, to apply the enzymes of sugar cane fermentation to cellulose in wood, to obtain methanol, but the cost is much higher when compared with ethanol derived from corn costs.[34]

There is a close relation in the forestry economy between these countries; they have many tree genera in common, and Canada is the main producer of wood and wooden items destined to the US, the biggest consumer of wood and its byproducts in the world. The water systems of the Great Lakes, Erie Canal, Hudson River and Saint Lawrence Seaway to the east coast and the Mississippi River to the central plains and Louisiana allows transportation of logs at very low costs. On the west coast, the basin of the Columbia River has plenty of forests with excellent timber.[citation needed]

The agency Canada Wood Council calculates that in the year 2005 in Canada, the forest sector employed 930,000 workers (1 job in every 17), making around $108 billion of value in goods and services. For many years products derived from trees in Canadian forests had been the most important export items of the country. In 2011, exports around the world totaled some $64.3 billion – the single largest contributor to Canadian trade balance.[30][36][better source needed]

Canada is the world leader in sustainable forest management practices. Only 120,000,000 hectares (1,200,000 km2; 463,320 sq mi) (28% of Canadian forests) are currently managed for timber production while an estimated 32,000,000 hectares (320,000 km2; 123,550 sq mi) are protected from harvesting by the current legislation.[37][better source needed]

The Canadian timber industry has led to environmental conflict with Indigenous people protecting their land from logging. For example, the Asubpeeschoseewagong First Nation set up the Grassy Narrows road blockade for twenty years beginning in 2002 to prevent clearcutting of their land.[38][39]

Wood obtained from Nigeria's wood industry undergoes processing in various wood processing sectors, including furniture manufacturing, sawmill operations, plywood mills, pulp and paper facilities, and particleboard mills. As of 2010, workers are typically not given any safety training.[47]

Poplar: in Italy is the most important species for tree plantations, is used for several purposes as plywood manufacture, packing boxes, paper, matches, etc. It needs good quality grounds with good drainage, but can be used to protect the cultivations if disposed in windbreak lines. More than 70% of Italian poplar cultivations are located in the pianura Padana. Constantly the extension of the cultivation is being reduced, from 650 km2 in the 1980s to current 350 km2. The yield of poplars is about 1,500 t/km2 of wood every year.[48] The production from poplars is around 45–50% of the total Italian wood production.[49]

In Sweden, Finland and to an extent Norway, much of the land area is forested, and the pulp and paper industry is one of the most significant industrial sectors. Chemical pulping produces an excess of energy, since the organic matter in black liquor, mostly lignin and hemicellulose breakdown products, is burned in the recovery boiler. Thus, these countries have high proportions of renewable energy use (25% in Finland, for instance). Considerable effort is directed towards increasing the value and usage of forest products by companies and by government projects.[citation needed]

A forest product is any material derived from forestry for direct consumption or commercial use, such as lumber, paper, or fodder for livestock. Wood, by far the dominant product of forests, is used for many purposes, such as wood fuel (e.g. in form of firewood or charcoal) or the finished structural materials used for the construction of buildings, or as a raw material, in the form of wood pulp, that is used in the production of paper.  All other non-wood products derived from forest resources, comprising a broad variety of other forest products, are collectively described as non-timber forest products (NTFP).[52][53][54] Non-timber forest products are viewed to have fewer negative effects on forest ecosystem when providing income sources for local community.[55]

Globally, about 1,150,000,000 ha (2.8×109 acres) of forest is managed primarily for the production of wood and non-wood forest products. In addition, 749,000,000 ha (1.85×109 acres) is designated for multiple use, which often includes production.[56]

Worldwide, the area of forest designated primarily for production has been relatively stable since 1990, but the area of multiple-use forest has decreased by about 71,000,000 ha (180,000,000 acres).[56]

Combustion of wood is linked to the production of micro-environmental pollutants, as carbon dioxide (CO2), carbon monoxide (CO) (an invisible gas able to provoke irreversible saturation of blood's hemoglobine), as well as nanoparticles.[58]

Charcoal is the dark grey residue consisting of impure carbon obtained by removing water and other volatile constituents from animal and vegetation substances.  Charcoal is usually produced by slow pyrolysis, the heating of wood or other substances in the absence of oxygen. Charcoal can then be used as a fuel with a higher combustion temperature.[citation needed]

Wood gas generator (gasogen): is a bulky and heavy device (but technically simple) that transforms burning wood in a mix of molecular hydrogen (H2), carbon monoxide (CO), carbon dioxide (CO2), molecular nitrogen (N2) and water vapor (H2O). This gas mixture, known as ""wood gas"", ""poor gas"" or ""syngas"" is obtained after the combustion of dry wood in a reductive environment (low in oxygen) with a limited amount of atmospheric air, at temperatures of 900 °C, and can fuel an internal combustion engine.[59]

Wood is relatively light in weight, because its specific weight is less than 500 kg/m3, this is an advantage, when compared against 2,000–2,500 kg/m3 for reinforced concrete or 7,800 kg/m3 for steel.[citation needed]

Wood is strong, because the efficiency of wood for structural purposes has qualities that are similar to steel.[citation needed]

Wood is used to build bridges (as the Magere bridge in Amsterdam), as well as water and air mills, and microhydro generators for electricity.[citation needed]

Hardwood is used as a material in wooden houses, and other structures with a broad range of dimensions. In traditional homes wood is preferred for ceilings, doors, floorings and windows. Wooden frames were traditionally used for home ceilings, but they risk collapse during fires.[citation needed]

The development of energy efficient houses including the ""passive house"" has revamped the importance of wood in construction, because wood provides acoustic and thermal insulation, with much better results than concrete.[citation needed]

In Japan, ancient buildings, of relatively high elevation, like pagodas, historically had shown to be able to resist earthquakes of high intensity, thanks to the traditional building techniques, employing elastic joints, and to the excellent ability of wooden frames to elastically deform and absorb severe accelerations and compressive shocks.[citation needed]

In 2006, Italian scientists from CNR patented[60] a building system that they called ""SOFIE"",[61] a seven-storey wooden building, 24 meters high, built by the ""Istituto per la valorizzazione del legno e delle specie arboree"" (Ivalsa) of San Michele all'Adige. In 2007 it was tested with the hardest Japanese antiseismic test for civil structures: the simulation of Kobe's earthquake (7.2 Richter scale), with the building placed over an enormous oscillating platform belonging to the NIED-Institute, located in Tsukuba science park, near the city of Miki in Japan. This Italian project, employed very thin and flexible panels in glued laminated timber, and according to CNR researchers could lead to the construction of much more safe houses in seismic areas.[62]

One of the most enduring materials is the lumber from virginian southern live oak and white oak, specially live oak is 60% stronger than white oak and more resistant to moisture. As an example, the main component in the structure of battle ship USS Constitution, the world's oldest commissioned naval vessel afloat (launched in 1797) is white oak.[63]

Woodworking

Woodworking is the activity or skill of making items from wood, and includes cabinet making (cabinetry and furniture), wood carving, joinery, carpentry, and woodturning. Millions of people make a livelihood on woodworking projects.[citation needed]
"
Window and Door Manufacturing,"

A window is an opening in a wall, door, roof, or vehicle that allows the exchange of light and may also allow the passage of sound and sometimes air. Modern windows are usually glazed or covered in some other transparent or translucent material, a sash set in a frame[1] in the opening; the sash and frame are also referred to as a window.[2] Many glazed windows may be opened, to allow ventilation, or closed to exclude inclement weather. Windows may have a latch or similar mechanism to lock the window shut or to hold it open by various amounts.

Types include the eyebrow window, fixed windows, hexagonal windows, single-hung, and double-hung sash windows, horizontal sliding sash windows, casement windows, awning windows, hopper windows, tilt, and slide windows (often door-sized), tilt and turn windows, transom windows, sidelight windows, jalousie or louvered windows, clerestory windows, lancet windows, skylights, roof windows, roof lanterns, bay windows, oriel windows, thermal, or Diocletian, windows, picture windows, rose windows, emergency exit windows, stained glass windows, French windows, panel windows, double/triple-paned windows, and witch windows.

The English language-word window originates from the Old Norse vindauga, from vindr 'wind' and auga 'eye'.[3] In Norwegian, Nynorsk, and Icelandic, the Old Norse form has survived to this day (in Icelandic only as a less used word for a type of small open ""window"", not strictly a synonym for gluggi, the Icelandic word for 'window'[4]). In Swedish, the word vindöga remains as a term for a hole through the roof of a hut, and in the Danish language vindue and Norwegian Bokmål vindu, the direct link to eye is lost, just as for window. The Danish (but not the Bokmål) word is pronounced fairly similarly to window.

Window is first recorded in the early 13th century, and originally referred to an unglazed hole in a roof. Window replaced the Old English eagþyrl, which literally means 'eye-hole', and eagduru 'eye-door'. Many Germanic languages, however, adopted the Latin word fenestra to describe a window with glass, such as standard Swedish fönster, or German Fenster. The use of window in English is probably because of the Scandinavian influence on the English language by means of loanwords during the Viking Age. In English, the word fenester was used as a parallel until the mid-18th century. Fenestration is still used to describe the arrangement of windows within a façade, as well as defenestration, meaning 'to throw out of a window'.

The Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt, in Alexandria c. 100 AD[citation needed]. Presentations of windows can be seen in ancient Egyptian wall art and sculptures from Assyria. Paper windows were economical and widely used in ancient China, Korea, and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century. In the 19th century American west, greased paper windows came to be used by pioneering settlers. Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were fully perfected.

In the 13th century BC, the earliest windows were unglazed openings in a roof to admit light during the day.[citation needed] Later,[when?] windows were covered with animal hide, cloth, or wood. Shutters that could be opened and closed came next.[when?] Over time, windows were built that both protected the inhabitants from the elements and transmitted light, using multiple small pieces of translucent material, such as flattened pieces of translucent animal horn, paper sheets, thin slices of marble (such as fengite), or pieces of glass, set in frameworks of wood, iron or lead. In the Far East, paper was used to fill windows.[1]
The Romans were the first known users of glass for windows, exploiting a technology likely first developed in Roman Egypt. Specifically, in Alexandria c. 100 CE, cast-glass windows, albeit with poor optical properties, began to appear, but these were small thick productions, little more than blown-glass jars (cylindrical shapes) flattened out into sheets with circular striation patterns throughout. It would be over a millennium before window glass became transparent enough to see through clearly, as we expect now. In 1154, Al-Idrisi described glass windows as a feature of the palace belonging to the king of the Ghana Empire.[5][6]

Over the centuries techniques were developed to shear through one side of a blown glass cylinder and produce thinner rectangular window panes from the same amount of glass material. This gave rise to tall narrow windows, usually separated by a vertical support called a mullion. Mullioned glass windows were the windows of choice[when?] among the European well-to-do, whereas paper windows were economical and widely used in ancient China, Korea, and Japan. In England, glass became common in the windows of ordinary homes only in the early-17th century, whereas windows made up of panes of flattened animal horn were used as early as the 14th century.[7]

Modern-style floor-to-ceiling windows became possible only after the industrial plate glass-making processes were perfected in the late 19th century [8] Modern windows are usually filled using glass, although transparent plastic is also used.[1]

The introduction of lancet windows into Western European church architecture from the 12th century CE built on a tradition of arched windows [9] inserted between columns,[10] and led not only to tracery and elaborate stained-glass windows but also to a long-standing motif of pointed or rounded window-shapes in ecclesiastical buildings, still seen in many churches today.

Peter Smith discusses overall trends in early-modern rural Welsh window architecture:

Up to about 1680 windows tended to be horizontal in proportion, a shape suitable for lighting the low-ceilinged rooms that had resulted from the insertion of the upper floor into the hall-house. After that date vertically proportioned windows came into fashion, partly at least as a response to the Renaissance taste for the high ceiling. Since 1914 the wheel has come full circle and a horizontally proportioned window is again favoured.[11]

The spread of plate-glass technology made possible the introduction of picture windows (in Levittown, Pennsylvania,[12] founded 1951–1952[clarification needed]).

Many modern day windows may have a window screen or mesh, often made of aluminum or fibreglass, to keep bugs out when the window is opened. Windows are primarily designed to facilitate a vital connection with the outdoors, offering those within the confines of the building visual access to the everchanging events occurring outside. The provision of this connection serves as an integral safeguard for the health and well-being of those inhabiting buildings, lest they experience the detrimental effects of enclosed buildings devoid of windows. Among the myriad criteria for the design of windows, several pivotal criteria have emerged in daylight standards: location, time, weather, nature, and people. Of these criteria, windows that are designed to provide views of nature are considered to be the most important by people.[13]

A cross-window is a rectangular window usually divided into four lights by a mullion and transom that form a Latin cross.[14]

The term eyebrow window is used in two ways: a curved top window in a wall or an eyebrow dormer; and a row of small windows usually under the front eaves such as the James-Lorah House in Pennsylvania.[15]

A fixed window is a window that cannot be opened,[16] whose function is limited to allowing light to enter (unlike an unfixed window, which can open and close). Clerestory windows in church architecture are often fixed. Transom windows may be fixed or operable. This type of window is used in situations where light or vision alone is needed as no ventilation is possible in such windows without the use of trickle vents or overglass vents.

A single-hung sash window is a window that has one sash that is movable (usually the bottom one) and the other fixed. This is the earlier form of sliding sash window and is also cheaper.[1]

A sash window is the traditional style of window in the United Kingdom, and many other places that were formerly colonized by the UK, with two parts (sashes) that overlap slightly and slide up and down inside the frame. The two parts are not necessarily the same size; where the upper sash is smaller (shorter) it is termed a cottage window. Currently, most new double-hung sash windows use spring balances to support the sashes, but traditionally, counterweights held in boxes on either side of the window were used. These were and are attached to the sashes using pulleys of either braided cord or, later, purpose-made chain. Three types of spring balances are called a tape or clock spring balance; channel or block-and-tackle balance, and a spiral or tube balance.

Double-hung sash windows were traditionally often fitted with shutters. Sash windows can be fitted with simplex hinges that let the window be locked into hinges on one side, while the rope on the other side is detached—so the window can be opened for fire escape or cleaning.

A foldup has two equal sashes similar to a standard double-hung but folds upward allowing air to pass through nearly the full-frame opening. The window is balanced using either springs or counterbalances, similar to a double-hung. The sashes can be either offset to simulate a double-hung, or in-line. The inline versions can be made to fold inward or outward. The inward swinging foldup windows can have fixed screens, while the outward swinging ones require movable screens. The windows are typically used for screen rooms, kitchen pass-throughs, or egress.

A horizontal sliding sash window has two or more sashes that overlap slightly but slide horizontally within the frame. In the UK, these are sometimes called Yorkshire sash windows, presumably because of their traditional use in that county.

A casement window is a window with a hinged sash that swings in or out like a door comprising either a side-hung, top-hung (also called ""awning window""; see below), or occasionally bottom-hung sash or a combination of these types, sometimes with fixed panels on one or more sides of the sash.[2] In the US, these are usually opened using a crank, but in parts of Europe, they tend to use projection friction stays and espagnolette locking. Formerly, plain hinges were used with a casement stay. Handing applies to casement windows to determine direction of swing; a casement window may be left-handed, right-handed, or double. The casement window is the dominant type now found in modern buildings in the UK and many other parts of Europe.

An awning window is a casement window that is hung horizontally, hinged on top, so that it swings outward like an awning. In addition to being used independently, they can be stacked, several in one opening, or combined with fixed glass. They are particularly useful for ventilation.[17]

A hopper window is a bottom-pivoting casement window that opens by tilting vertically, typically to the inside, resembling a hopper chute.[18]

A pivot window is a window hung on one hinge on each of two opposite sides which allows the window to revolve when opened. The hinges may be mounted top and bottom (Vertically Pivoted) or at each jamb (Horizontally Pivoted). The window will usually open initially to a restricted position for ventilation and, once released, fully reverse and lock again for safe cleaning from inside. Modern pivot hinges incorporate a friction device to hold the window open against its weight and may have restriction and reversed locking built-in. In the UK, where this type of window is most common, they were extensively installed in high-rise social housing.

A tilt and slide window is a window (more usually a door-sized window) where the sash tilts inwards at the top similar to a hopper window and then slides horizontally behind the fixed pane.

A tilt and turn window can both tilt inwards at the top or open inwards from hinges at the side. This is the most common type of window in Germany, its country of origin. It is also widespread in many other European countries. In Europe, it is usual for these to be of the ""turn first"" type. i.e. when the handle is turned to 90 degrees the window opens in the side hung mode. With the handle turned to 180 degrees the window opens in bottom hung mode. Most usually in the UK the windows will be ""tilt first"" i.e. bottom hung at 90 degrees for ventilation and side hung at 180 degrees for cleaning the outer face of the glass from inside the building.[19]

A transom window is a window above a door. In an exterior door the transom window is often fixed, in an interior door, it can open either by hinges at top or bottom, or rotate on hinges. It provided ventilation before forced air heating and cooling. A fan-shaped transom is known as a fanlight, especially in the British Isles.

Windows beside a door or window are called side-, wing-, margen-lights, and flanking windows.[20]

Also known as a louvered window, the jalousie window consists of parallel slats of glass or acrylic that open and close like a Venetian blind, usually using a crank or a lever. They are used extensively in tropical architecture. A jalousie door is a door with a jalousie window.

A clerestory window is a window set in a roof structure or high in a wall, used for daylighting.

A skylight is a window built into a roof structure.[21] This type of window allows for natural daylight and moonlight.

A roof window is a sloped window used for daylighting, built into a roof structure. It is one of the few windows that could be used as an exit. Larger roof windows meet building codes for emergency evacuation.

A roof lantern is a multi-paned glass structure, resembling a small building, built on a roof for day or moon light. Sometimes includes an additional clerestory. May also be called a cupola.

A bay window is a multi-panel window, with at least three panels set at different angles to create a protrusion from the wall line.[2]

An oriel window is a form of bay window. This form most often appears in Tudor-style houses and monasteries. It projects from the wall and does not extend to the ground. Originally a form of porch, they are often supported by brackets or corbels.

Thermal, or Diocletian, windows are large semicircular windows (or niches) which are usually divided into three lights (window compartments) by two mullions. The central compartment is often wider than the two side lights on either side of it.

A picture window is a large fixed window in a wall, typically without glazing bars, or glazed with only perfunctory glazing bars (muntins) near the edge of the window. Picture windows provide an unimpeded view, as if framing a picture.[22]

A multi-lite window is a window glazed with small panes of glass separated by wooden or lead glazing bars, or muntins, arranged in a decorative glazing pattern often dictated by the building's architectural style. Due to the historic unavailability of large panes of glass, the multi-lit (or lattice window) was the most common window style until the beginning of the 20th century, and is still used in traditional architecture.

An emergency exit window is a window big enough and low enough so that occupants can escape through the opening in an emergency, such as a fire. In many countries, exact specifications for emergency windows in bedrooms are given in many building codes. Specifications for such windows may also allow for the entrance of emergency rescuers. Vehicles, such as buses, aircraft, and trains frequently have emergency exit windows as well.[23]

A stained glass window is a window composed of pieces of colored glass, transparent, translucent or opaque, frequently portraying persons or scenes. Typically the glass in these windows is separated by lead glazing bars. Stained glass windows were popular in Victorian houses and some Wrightian houses, and are especially common in churches.[24]

A French door[25] has two columns of upright rectangular glass panes (lights) extending its full length; and two of these doors on an exterior wall and without a mullion separating them, that open outward with opposing hinges to a terrace or porch, are referred to as a French window.[26] Sometimes these are set in pairs or multiples thereof along the exterior wall of a very large room, but often, one French window is placed centrally in a typically sized room, perhaps among other fixed windows flanking the feature. French windows are known as porte-fenêtre in France and portafinestra in Italy, and frequently are used in modern houses.

Double-paned windows have two parallel panes (slabs of glass) with a separation of typically about 1 cm; this space is permanently sealed and filled at the time of manufacture with dry air or other dry nonreactive gas. Such windows provide a marked improvement in thermal insulation (and usually in acoustic insulation as well) and are resistant to fogging and frosting caused by temperature differential. They are widely used for residential and commercial construction in intemperate climates. In the UK, double-paned and triple-paned are referred to as double-glazing and triple-glazing. Triple-paned windows are now a common type of glazing in central to northern Europe. Quadruple glazing is now being introduced in Scandinavia.

A hexagonal window is a hexagon-shaped window, resembling a bee cell or crystal lattice of graphite. The window can be vertically or horizontally oriented, openable or dead. It can also be regular or elongately-shaped and can have a separator (mullion). Typically, the cellular window is used for an attic or as a decorative feature, but it can also be a major architectural element to provide the natural lighting inside buildings.

A guillotine window is a window that opens vertically. Guillotine windows have more than one sliding frame, and open from bottom to top or top to bottom.

EN 12519 is the European standard that describes windows terms officially used in EU Member States. The main terms are:

The United States NFRC Window Label lists the following terms:

The European harmonised standard hEN 14351–1, which deals with doors and windows, defines 23 characteristics (divided into essential and non essential). Two other, preliminary European Norms that are under development deal with internal pedestrian doors (prEN 14351–2), smoke and fire resisting doors, and openable windows (prEN 16034).[29]

Windows can be a significant source of heat transfer.[30] Therefore, insulated glazing units consist of two or more panes to reduce the transfer of heat.

These are the pieces of framing that separate a larger window into smaller panes. In older windows, large panes of glass were quite expensive, so muntins let smaller panes fill a larger space. In modern windows, light-colored muntins still provide a useful function by reflecting some of the light going through the window, making the window itself a source of diffuse light (instead of just the surfaces and objects illuminated within the room). By increasing the indirect illumination of surfaces near the window, muntins tend to brighten the area immediately around a window and reduce the contrast of shadows within the room.

Frames and sashes can be made of the following materials:

Composites (also known as Hybrid Windows) are start since early 1998 and combine materials like aluminium + pvc or wood to obtain aesthetics of one material with the functional benefits of another.

A special class of PVC window frames, uPVC window frames, became widespread since the late 20th century, particularly in Europe: there were 83.5 million installed by 1998[33] with numbers still growing as of 2012.[34]

Low-emissivity coated panes reduce heat transfer by radiation, which, depending on which surface is coated, helps prevent heat loss (in cold climates) or heat gains (in warm climates).

High thermal resistance can be obtained by evacuating or filling the insulated glazing units with gases such as argon or krypton, which reduces conductive heat transfer due to their low thermal conductivity. Performance of such units depends on good window seals and meticulous frame construction to prevent entry of air and loss of efficiency.

Modern double-pane and triple-pane windows often include one or more low-e coatings to reduce the window's U-factor (its insulation value, specifically its rate of heat loss). In general, soft-coat low-e coatings tend to result in a lower solar heat gain coefficient (SHGC) than hard-coat low-e coatings.

Modern windows are usually glazed with one large sheet of glass per sash, while windows in the past were glazed with multiple panes separated by glazing bars, or muntins, due to the unavailability of large sheets of glass. Today, glazing bars tend to be decorative, separating windows into small panes of glass even though larger panes of glass are available, generally in a pattern dictated by the architectural style at use. Glazing bars are typically wooden, but occasionally lead glazing bars soldered in place are used for more intricate glazing patterns.

Many windows have movable window coverings such as blinds or curtains to keep out light, provide additional insulation, or ensure privacy.
Windows allow natural light to enter, but too much can have negative effects such as glare and heat gain. Additionally, while windows let the user see outside, there must be a way to maintain privacy on in the inside.[35] Window coverings are practical accommodations for these issues.

Historically, windows are designed with surfaces parallel to vertical building walls. Such a design allows considerable solar light and heat penetration due to the most commonly occurring incidence of sun angles. In passive solar building design, an extended eave is typically used to control the amount of solar light and heat entering the window(s).

An alternative method is to calculate an optimum window mounting angle that accounts for summer sun load minimization, with consideration of actual latitude of the building. This process has been implemented, for example, in the Dakin Building in Brisbane, California—in which most of the fenestration is designed to reflect summer heat load and help prevent summer interior over-illumination and glare, by canting windows to nearly a 45 degree angle.

Photovoltaic windows not only provide a clear view and illuminate rooms, but also convert sunlight to electricity for the building.[36] In most cases, translucent photovoltaic cells are used.

Passive solar windows allow light and solar energy into a building while minimizing air leakage and heat loss. Properly positioning these windows in relation to sun, wind, and landscape—while properly shading them to limit excess heat gain in summer and shoulder seasons, and providing thermal mass to absorb energy during the day and release it when temperatures cool at night—increases comfort and energy efficiency. Properly designed in climates with adequate solar gain, these can even be a building's primary heating system.

A window covering is a shade or screen that provides multiple functions. Some coverings, such as drapes and blinds provide occupants with privacy. Some window coverings control solar heat gain and glare. There are external shading devices and internal shading devices.[37] Low-e window film is a low-cost alternative to window replacement to transform existing poorly-insulating windows into energy-efficient windows. For high-rise buildings, smart glass can provide an alternative.
"
Cabinetry Manufacturing,"Kitchen cabinets are the built-in furniture installed in many kitchens for storage of food, cooking equipment, and often silverware and dishes for table service.  Appliances such as refrigerators, dishwashers, and ovens are often integrated into kitchen cabinetry. There are many options for cabinets available at present.[1]

As commonly used today, the term kitchen cabinet denotes a built-in kitchen installation of either a floor or a wall cabinet. Typically, multiple floor cabinets are covered by a single counter, and floors and walls are not accessible behind and under the cabinets. Kitchen cabinets per se were invented in the 20th century.  A precursor, not built-in, was the Hoosier cabinet of the 1910s, a single piece of furniture incorporating storage and work surfaces, of which over 2 million were sold by 1920.[2]

Considering that North Americans spend a large proportion of their lives indoors, it’s clear why this is a key issue in designing healthy spaces. Additionally, air quality is not a stand-alone problem; rather, every other component of the home can affect air quality. Air quality can be compromised by off-gassing from cabinetry, countertops, flooring, wall coverings or fabrics; by cooking by-products released into the air, and by mold caused by excess moisture or poor ventilation.[4]

Cabinets consist of six-sided wooden boxes or ""carcass"" closed on five sides with a door or drawers on the sixth.

Cabinets may be either face-frame or frameless in construction.  Each option provides features and drawbacks.

 In Europe, there is a DIN standard dimension where the height of a base unit is 720mm + 150mm for the bottom plinth and 40mm for worktop thickness, providing a working surface height of 910mm with an area of 600mm x 600 mm. This ""60cm square"" horizontal area (or its multiples) accommodates many standard floor-standing electrical appliances.  

Cabinet doors may feature a variety of materials such as wood, metal or glass.  Wood may be solid wood (""breadboard"" construction) or engineered wood or may be mixed (e.g. engineered wood panel in a solid wood frame)

A functional design objective for cabinet interiors involves maximization of useful space and utility in the context of the kitchen workflow.  Drawers and trays in lower cabinets permit access from above and avoid uncomfortable or painful crouching.

In face-frame construction, a drawer or tray must clear the face-frame stile and is 2 inches (51 mm) narrower than the available cabinet interior space.  The loss of 2 inches is particularly noticeable and significant for kitchens including multiple narrow [15-inch (380 mm)  or less] cabinets.

In frameless construction, drawer boxes may be sized nearly to the interior opening of the cabinet providing better use of the available space.

However, the same is not true for trays. Even in the case of frameless construction doors and their hinges when open block a portion of the interior cabinet width.  Since trays are mounted behind the door, trays are typically significantly narrower than drawers.  Special hinges are available that can permit trays of similar width as drawers but they have not come into wide use.

Shelves provide in all cases more storage space than drawers or trays, but are less accessible.

Stock wall-oven cabinets may be adapted to built-in ovens, coffee-makers, or other appliances by removing portions of the cabinet and adding trim panels to achieve a flush installation.

Frameless cabinets provide for wall oven front panel widths equal to the cabinet width (see above).  In such an installation the oven front panel occupies a similar profile as a cabinet door.  Accordingly, frameless installations for wall-oven make most efficient use of the available wall space in a kitchen.

This effect is difficult to achieve in typical face-frame cabinet installations, as it requires modification to the face-frame (essentially eliminating the face-frame at the oven cut-out).

Cabinets may be finished with opaque paint, opaque lacquer and transparent finishes such as lacquer or varnish. Decorative finishes include distressing, glazing, and toning. The choice of finish can affect the cabinet's color, sheen (from flat to high gloss), and feel.

Hardware is the term used for metal fittings incorporated into a cabinet extraneous of the wood or engineered wood substitute and the countertop.  The most basic hardware consists of hinges and drawer/door pulls, although only hinges are an absolute necessity for a cabinet since pulls can be fashioned of wood or plastic, and drawer slides were traditionally fashioned of wood.  In a modern kitchen it is highly unusual to use wood for a drawer slides owing to the much superior quality of metal drawer slides/sides.

Drawers and trays make it easier to access a cabinet's contents. They are a substantial benefit because they reduce bending and squatting. The only drawback is slightly less usable space which is taken up by the slides as well as door clearances. A typical drawer is 5 inches (130 mm) narrower than a comparable shelf. A drawer can usually hold about 75 to 100 lb (34 to 45 kg) for ordinary use. Using slides, mounted on the side (reducing width slightly) or bottom (completely out of sight), a drawer or tray can be extended considerably with a smooth, linear motion using minimum effort.

Drawer extension is the exposed proportion of a fully extended drawer. Traditional drawers with wood-on-wood runners can only be extended about three-quarters; however, modern runners enable full-extension drawers. A slide's design parameters are its height, depth, extension, weight rating, durability, serviceability, and smoothness of operation.

New varieties of hinges and slides have been developed that enhance the action and usability of doors and drawers. 

There is a large variety of specialty hardware for kitchen cabinets. Special hardware for corner and other blind cabinets makes their contents more easily accessible. They may be in the form of lazy susans with or without a wedge cut out or of tray slides which enable the hidden corner space to be occupied with trays that slide both laterally and forwards/backwards. Sponge drawers use special hinges that fit between the cabinet front and the sink.

Before buying cabinets, precise measurements are essential otherwise there may be un-utilized space, cabinets may not fit, or there may be interference between various elements of the kitchen, such as doors and drawers. Note that European cabinets typically have different sizes than ones in North America, and are typically built in multiples of 100mm, with 600mm wide being a common size.

Buyers can buy pre-built ""stock"" cabinets for fast delivery which usually arrive in a week or less. In contrast, custom-made cabinets can have longer delivery times, such as four weeks.

Cabinet dimensions are specified with width first, height second, depth last. The width–height–depth is a generally accepted convention. A 18x36x12 cabinet is therefore 18 inches (460 mm) wide, 36 inches (910 mm) tall, and 12 inches (300 mm) deep. Sometimes upper cabinets are presumed to be 12 inches (300 mm) deep, so only the width and height are given. For example, a ""W1836"" label means wall-mounted cabinet [12 inches (300 mm) deep] is 18 inches (460 mm) wide and 36 inches (910 mm) high.

Custom cabinetry, while expensive, can fit the available space attractively, and can fit into walls which aren't exactly flat or straight. They can combine more than one opening and eliminate unsightly doubled stiles in face-frame installations as well as bring aesthetic appeal using unusual woods or finishes. Custom cabinets sometimes offer inset cabinet doors, and can match existing or period furniture styles. It's sometimes possible to mix custom and stock cabinetry which have identical finishes.

Cabinets can be purchased from specialty retailers, kitchen remodelers, home centers, on-line retailers, and ready-to-assemble furniture manufacturers. Some installers offer a package deal from measurement, to construction, to installation.

Cabinets are sometimes delivered in fully assembled form. Carcasses should be inspected carefully before installation, since defects are difficult to repair after installation. Ready-to-assemble furniture cabinets are lower-in-cost and are delivered in a flat box. Some courses teach homeowners how to build their own cabinets.[6]
"
Trim Manufacturing,"Trim or trimming in clothing and home decorating is applied ornament, such as gimp, passementerie, ribbon, ruffles, or, as a verb, to apply such ornament.

Before the Industrial Revolution, all trim was made and applied by hand, thus making heavily trimmed furnishings and garments expensive and high-status.  Machine-woven trims and sewing machines put these dense trimmings within the reach of even modest dressmakers and home sewers, and an abundance of trimming is a characteristic of mid-Victorian fashion.[1] As a predictable reaction, high fashion came to emphasize exquisiteness of cut and construction over denseness of trimming, and applied trim became a signifier of mass-produced clothing by the 1930s.[2] The iconic braid and gold button trim of the Chanel suit are a notable survival of trim in high fashion.[3]

In home decorating, the 1980s and 1990s saw a fashion for dense, elaborately layered trimmings on upholstered furniture and drapery.[4]

Today, most trimmings are commercially manufactured.[citation needed] Scalamandré is known for elaborate trim for home furnishings, and Wrights is a leading manufacturer of trim for home sewing and crafts.[citation needed] Conso is another leading manufacturer.[5]



This textile arts article is a stub. You can help Wikipedia by expanding it."
Pallet Manufacturing,"A pallet (also called a skid) is a flat transport structure, which supports goods in a stable fashion while being lifted by a forklift, a pallet jack, a front loader, a jacking device, or an erect crane. Many pallets can handle a load of 1,000 kg (2,200 lb). While most pallets are wooden, pallets can also be made of plastic, metal, paper, and recycled materials.

A pallet is the structural foundation of a unit load, which allows handling and storage efficiencies. Goods in shipping containers are often placed on a pallet secured with strapping, stretch wrap or shrink wrap and shipped. In addition, pallet collars can be used to support and protect items shipped and stored on pallets.

Containerization for transport has spurred the use of pallets because shipping containers have the smooth, level surfaces needed for easy pallet movement. Since its invention in the twentieth century, its use has dramatically supplanted older forms of crating like the wooden box and the wooden barrel, as it works well with modern packaging like corrugated boxes and intermodal containers commonly used for bulk shipping. In 2020 about half a billion pallets are made each year and about two billion pallets are in use across the United States alone.[1] Organizations using standard pallets for loading and unloading can have much lower costs for handling and storage, with faster material movement than businesses that do not. The exceptions are establishments that move small items such as jewelry or large items such as cars. But even they can be improved. For instance, the distributors of costume jewelry normally use pallets in their warehouses and car manufacturers use pallets to move components and spare parts. Pallets make it easier to move heavy stacks. Loads with pallets under them can be hauled by forklift trucks of different sizes, or even by hand-pumped and hand-drawn pallet jacks. Movement is easy on a wide, strong, flat floor: concrete is excellent. The greatest investment needed for economical pallet use is in the construction of commercial or industrial buildings. Ability to pass through standard doors and buildings make handling more convenient. For this reason, some modern pallet standards are designed to pass through standard doorways, for example the europallet (800 mm × 1,200 mm) and the U.S. military 35 in × 45.5 in (890 mm × 1,160 mm).

The lack of a single international standard for pallets causes substantial continuing expense in international trade. A single standard is difficult because of the wide variety of needs a standard pallet would have to satisfy: passing doorways, fitting in standard containers, and bringing low labor costs. For example, organizations already handling large pallets often see no reason to pay the higher handling cost of using smaller pallets that can fit through doors. Heavy-duty pallets are a form of reusable packaging and are designed to be used multiple times. Lightweight pallets are designed for a single use. In the EU, government legislation based on the Waste Framework Directive requires the reuse of packaging items in preference to recycling and disposal.

Skids date back to Ancient Egypt and Ancient Mesopotamia, at least as far back as the 1st millennium B.C.[2]

The development of the forklift and the needs of World War II logistics operations led to substantial use of pallets.[3]

References to the early modern pallets types are slim with a string of patents showing parts of the development. The earliest may be a U.S. patent on a skid from 1924 describing Howard T. Hallowell's ""Lift Truck Platform"".[4] In the late 1930s, pallets became more commonplace with the newer forklift types. George G. Raymond and William C. House filed for a patent in 1937 (granted US Patent 2178646 in 1939) for a pallet designed to complement a new pallet jack design;[5] the essential features of both are still in common use today. A 1939 patent from Carl Clark shows a type of pallet with steel stringers. Wartime developments were often patented just after the war, so there is a patent from Robert Braun on a four-way pallet in 1945, and a patent from Norman Cahners (a U.S. Navy supply officer) shows a disposable pallet in 1949. The principle of a modern four-way pallet is described by Darling Graeme in 1949.[6]

The production of pallets accounts for 43% of hardwood and 15% of softwood usage in the U.S.[7]

The cheapest pallets are made of softwood and are often considered expendable, to be discarded as trash along with other wrapping elements, at the end of the trip. These pallets are simple stringer pallets, and able to be lifted from two sides.

Slightly more complex, hardwood block pallets, plastic pallets and metal pallets can be lifted from all four sides.  These costlier pallets usually require a deposit and are returned to the sender or resold as used.  Many ""four way"" pallets are color-coded according to the loads they can bear, and other attributes. Wood pallets can pose serious bio-hazard risks as they are susceptible to bacterial and chemical contamination, such as E. coli problems in food and produce transportation,[citation needed] and even insect infestation, and thus the need for ISPM 15.

Wooden pallet construction specifications can depend on the pallet's intended use: general, FDA, storage, chemical, export; the expected load weight; type of wood desired: recycled, hard, soft, kiln dried or combo (new and recycled); and even the type of fasteners desired to hold the pallet together: staples or nails.

The price of wooden pallets reached a record high during the COVID-19 pandemic, due to increases in the prices of supplies and labor.[8]

Although pallets come in all sizes and configurations, all pallets fall into two very broad categories: ""stringer"" and ""block"" pallets.  Various software packages exist to assist the pallet maker in designing an appropriate pallet for a specific load and evaluating wood options to reduce costs. 

Stringer pallets are one of the original models of wooden pallets. They use a frame of three or more parallel pieces of timber (called stringers).  The top deckboards are then affixed to the stringers to create the pallet structure. Stringer pallets can have a notch cut into them allowing ""four-way"" entry. Forklifts can lift a stringer pallet from all four directions, though lifting by the stringers is more secure. Stringer pallets can be made of both wood and plastic.

Block pallets utilize both parallel and perpendicular stringers to better facilitate efficient handling. A block pallet is also known as a ""four-way"" pallet, since a pallet-jack may be used from any side to move it.

Carrier blocks are specialized pallets for lumber carriers

Flush pallets are pallets with deck boards that are flush with the stringers and stringer boards along the ends and sides of the pallet.[citation needed]

All stringer and some block pallets have ""unidirectional bases"", i.e. bottom boards oriented in one direction. While automated handling equipment can be designed for this, often it can operate faster and more effectively if the bottom edges of a pallet have bottom boards oriented in both directions. For example, it may not need to turn a pallet to rack it, and operation is less sensitive to pallet orientation.

The least expensive way to improve a pallet is usually to specify better nails. With non-wood pallets, a controlled coefficient of friction is often helpful to prevent the pallet from slipping from forks and racks. Stiffer pallets are more durable, and are handled more easily by automated equipment. If a pallet does not need to be lifted from all four sides, two-way pallets with unnotched stringers may be used, with the additional benefits of added rigidity and strength. Specifying tolerances on flatness and water content may help the supplier meet target requirements. Inspection of pallets, whether in person or by a third-party (such as ""SPEQ"" inspected pallets) offer additional assurance of quality.

The main processes that are used to manufacture wooden pallets:[citation needed]

Due to the International Plant Protection Convention (abbreviated IPPC), most pallets shipped across national borders must be made of materials that are incapable of being a carrier of invasive species of insects and plant diseases. The standards for these pallets are specified in ISPM 15.

Pallets made of raw, untreated wood are not compliant with ISPM 15. To be compliant the pallets (or other wood packaging material) must meet debarked standards,[9] and must be treated by either of the following means under the supervision of an approved agency:

Treated wood pallets must be stamped on two opposite sides, indicating either HT for heat treated or MB for methyl bromide treatment.

Pallets made of non-wood materials such as steel, aluminum, plastic, or engineered wood products, such as plywood, oriented strand board, or corrugated fiberboard do not need IPPC approval, and are considered to be exempt from ISPM 15 regulations.

Plastic pallets are often made of new HDPE or recycled PET (drink bottles). They are usually extremely durable, lasting for a hundred trips or more,[11][full citation needed] and resist weathering, rot, chemicals and corrosion.  The benefits of plastic pallets over wood pallets include the ability to be easily sanitized, resistance to odor, fire retardancy, longer service life span, durability and better product protection, non-splintering, and lighter weight, thus saving on transportation and labor costs and making them safer and more environmentally friendly. They often stack. Plastic pallets are exempt by inspection for biosafety concerns, and easily sanitize for international shipping. HDPE is impervious to most acids, and toxic chemicals clean from them more easily. Some plastic pallets can collapse from plastic creep if used to store heavy loads for long periods. Plastic pallets cannot easily be repaired, and can be ten times as expensive as hardwood,[11] so they are often used by logistics service providers who can profit from their durability and stackability. The large supply chains have increased the use of plastic pallets as many organisations seek to reduce costs through waste, transport, and health and safety. Pallets and dollies can be combined (for example the Pally),[12] eliminating pallet instability and the need for additional lifting equipment along with creating valuable space in busy operating environments. They also deliver significant time and cost savings by reducing supply chain handling. Plastics' reusability has contributed to an increase in usage of plastic pallets.[13]

Plastic pallets are produced and used widely in the U.S. and Europe, spurred by the adoption of the ISPM 15. A full comparison of wood vs plastic can be made by a life cycle analysis.[14] Plastic pallets can cost 10 times as much as hardwood pallets[11] and even more expensive compared to cheap expendable softwood pallets. RFID chips can be molded into the pallets to monitor locations and track inventory.[15]

There are six main types of plastic processes that are used to manufacture pallets:[16]

Paper pallets, also referred to as ""ecopallets"", are often used for light loads, but engineered paper pallets are increasingly used for loads that compare with wood. Paper pallets are also used where recycling and easy disposal is important. New designs of ecopallets have been made from just two flat pieces of corrugated board (no glue/staples) and weigh just 4.5 kg (9.9 lb), offering dramatic freight savings. Ecopallets are also ISPM 15 exempt, negating fumigation and barrier ""slip"" sheets. They are cleaner, safer, and provide a cost-saving eco-friendly alternative to other pallet materials. Some engineered corrugated pallets offer a significantly reduced height, providing substantial freight cost reduction. Low-profile hand pallet trucks allow picking up pallets as low as 25 mm.

Steel pallets are strong and are used for heavy loads, high-stacking loads, long term dry storage, and loads moved by abusive logistic systems. They are often used for military ammunition.[17] Metal pallets make up less than 1% of the market. Materials include carbon steel, stainless steel, and aluminum. Of these, carbon steel offers excellent durability at the lowest cost. Stainless steel does not require a paint coating, and is preferred for such applications as clean room environments. Carbon steel units are expensive compared to wood, and stainless and aluminum cost about 2–3 times that of carbon steel. Long term costs, however, can be lower than wood. General advantages of metal pallets are high strength and stiffness, excellent durability, resistance to vermin, the absence of splinters, sanitization, and recyclability. Disadvantages include a higher initial price, significant weight, low friction, and susceptibility to rusting (in the case of carbon steel). Metal is primarily used in captive or closed loop environments where durability and product protection are key performance requirements. Metal units today are increasingly price competitive and lighter in weight. Primary industries that use metal pallets include automotive, pharmaceutical, lawn tractors, motorcycles, and tires.[18][full citation needed]

Aluminum pallets are stronger than wood or plastic, lighter than steel, and resist weather, rotting, plastic creep and corrosion.  They are sometimes used for air-freight, long-term outdoor or at-sea storage, or military transport.

Wooden pallets typically consist of three or four stringers that support several deckboards, on top of which goods are placed. In a pallet measurement, the first number is the stringer length and the second is the deckboard length. Square or nearly square pallets help a load resist tipping.

Pallet users want pallets to pass easily through buildings, to stack and fit in racks, to be accessible to forklifts and pallet jacks and to function in automated warehouses. To avoid shipping air, pallets should also pack tightly inside intermodal containers and vans.

Though some major standards exist, there are no universally accepted standards for pallet dimensions. Companies and organizations utilize hundreds of different pallet sizes around the globe.[19]  While no single dimensional standard governs pallet production, a few different sizes are widely used.

The standard 48×40 North American pallet, or GMA pallet, has stringers of 48 inches and deckboards of 40 inches, and was standardized by the Grocery Manufacturers Association (GMA). 

Lightweight plastic pallets can weigh as little as 3 to 15 pounds (1.4 to 6.8 kg), while heavier models may weigh up to 300 pounds (140 kg). Standard GMA pallets can hold up to 460 pounds (210 kg). 

Heavy duty International Plant Protection Convention (IPPC) Pallets are approximately 44 inches (1,118 mm) wide by 48 inches (1,219 mm) long, have three wood stringers that are a nominal 4 inches (102 mm) high by 3 inches (76 mm) wide timber, and weigh about 135 pounds (61 kg). Their deck is fully covered by 30 mm (1.18 in) plywood, and is painted in blue in European and Russian countries.

Two-way pallets are designed to be lifted by the deckboards. 

Four-way pallets, or pallets for heavy loads  (or general-purpose systems that might have heavy loads) are best lifted by their more rigid stringers. These pallets are usually heavier, bigger and more durable than two-way pallets.

The International Organization for Standardization (ISO) sanctions six pallet dimensions, detailed in ISO Standard 6780:2003 Flat pallets for intercontinental materials handling — Principal dimensions and tolerances, which was reviewed and confirmed in 2014:[20]

ISO container

Of the top pallets used in North America, the most commonly used by far is the Grocery Manufacturers Association (GMA) pallet, which accounts for 30% of all new wood pallets produced in the United States.[21] The ISO also recognizes the GMA pallet footprint as one of its six standard sizes.

The Australian standard pallet is a pallet size commonly found in Australia but rarely found elsewhere. It is a square pallet originally made of hardwood 1,165 mm × 1,165 mm (45.9 in × 45.9 in) in size which fits perfectly in the RACE container of the Australian Railway. They are ill-suited for the standard 20-foot (6.1 m) and 40-foot (12 m) ISO shipping containers used around the globe. Australian standard pallets are usually manufactured in hardwood, but 1165 × 1165 mm pallets can also be manufactured using lighter timber suitable for use as disposable pallets using 16 millimeter boards. Extensively used in storage and warehousing, they are popular pallets for racking, with the right shape and size to be removed from transport and directly onto warehouse racking for storage.

The Australian standard pallet dates back to World War II, while ISO containers date to the late 1950s. Although the pallet's dimensions pre-date the ISO containers, it requires less dunnage, is square, and leaves less wasted space than other pallets, including the GMA pallet. In 2010, Australia adopted the globally accepted ISPM 15 wood packaging material regulations (before this time it was hardwood and more expensive).[24]

A number of different organizations and associations around the world work towards establishing and promulgating standards for pallets. Some strive to develop universal standards for pallet dimensions, types of material used in construction, performance standards, and testing procedures. Other organizations choose to focus on pallet standards for a specific industry (such as groceries) or type of material (such as wood).

ISO Technical Committee 51 (Pallets for unit load method of materials handling) states its scope of work entailing the ""standardization of pallets in general use in the form of platforms or trays on which goods may be packed to form unit loads for handling by mechanical devices"".[25] The Technical Committee works in conjunction with other Technical Committees focused on transportation infrastructure to develop interrelated standards. TC 51 is responsible for developing ISO Standard 6780: Flat pallets for intercontinental materials handling—Principal dimensions and tolerances as well as sixteen other standards related to pallet construction and testing.

The National Wood Pallet and Container Association (NWPCA) is a trade organization based in the United States representing the interests of wood pallet and container manufacturers.[26]

The U.S. Department of Defense, Department of the Navy, Naval Sea Systems Command maintains MIL-STD-1660, the standard description of palletized unit loads for the U.S. Military and some allies.[27] DOD Unit loads generally use 40 in × 48 in (1,016 mm × 1,219 mm) pallets, are less than 4,000 lb (1,814 kg), weatherproof, and stack 16 ft (4.88 m) high.  They often use steel pallets, steel straps with notched seals, outdoor plywood, and plastic film. The standard describes tests for stacking, transport, sling, forklift and pallet jack, impact, drop tests, tip, water-retention, and disassembly.

In addition to the other standards it publishes, the European Committee for Standardization, also known as the Comité Européen de Normalisation (CEN), produces standards for pallets.  While the standards are voluntary in nature, many companies and organizations involved in transportation have adopted them. The major standard for pallets produced by CEN is ICS: 55.180.20 General purpose pallets[28]

Both wood and plastic pallets are possible fire hazards. In the USA, the National Fire Protection Association requires that both types ""shall be stored outside or in a detached structure"" unless protected by fire sprinklers.[29]

Items made from pallet wood are likely to be durable and demonstrate good weather resistance due to these treatments. However, close contact with pallet wood or inhalation of dusts from sanding or sawing can be a source of exposure to pesticide and fungicide chemicals. It is likely that the January 2010 recall of Johnson and Johnson Tylenol and other drugs was due to their being stored on wooden pallets that had been treated with the fungicide/pesticide 2,4,6-tribromophenol. This chemical can be degraded by molds to produce 2,4,6-tribromoanisole whose strong, musty odor caused consumers to complain.[30] There is no acute or chronic health data on 2,4,6-tribromoanisole,[31] but it is believed that the contaminated drugs caused nausea and other health effects in some people.

An air cargo pallet is a detachable and interchangeable airworthy floor panel. It can be loaded and unloaded on aircraft to transport air cargo Most are less than 1 in (25 mm) thick, for a cargo aircraft. Many civilian types have evolved over 6 decades from the thicker military 463L Master Pallet. When combined with a cargo net, an air cargo pallet becomes an aircraft Unit Load Device and must be inspected for integrity before use in an aircraft. It is handled on dedicated roller or ball-mat ground equipment and vehicles (fork-lifting is prohibited especially when the pallet is loaded).[relevant?]

Pallet boxes are pallets with four pallet walls on top and possibly a lid. Unlike pallets, pallet boxes can be stacked when they are fully loaded. They may also be fitted with a lid during stacking for stability reasons. By stacking pallet boxes on each other space can be used more efficiently, thus they are able to save warehouse space and truck load capacity. Another advantage of pallet boxes is that the goods are stored safely and are not easily damaged during logistic movements and transport. Furthermore, there are collapsible pallet boxes from which the pallet walls can be laid flat on the pallet. In this way, less space is required during empty return transport.

Bulk boxes are closely related;  they are usually large separate boxes shipped on pallets.

Old and discarded wooden pallets can be used in pallet crafts and various furniture pieces.[32]

Discarded wooden pallets should not be used for firewood or crafts unless it has been determined that the wood in these pallets has not been treated with wood preservatives, fungicides and/or pesticides. Various pyrethrins and propiconazole are common treatments for wooden pallets. In addition, imported palletized goods are routinely fumigated with highly toxic pesticides. During use, harmful materials or chemicals also may spill on the pallet wood and be absorbed.

Craft publications have advised readers to use pallets to build a skateboarding obstacle called a manual pad, barricades during amateur paintball games, or other sport-related items. Other publications have suggested using pallet wood for small animal cages or fences. Pallet wood has been recycled for use as furniture wood by at least one company.

The acoustic guitar maker Taylor Guitars once produced a high quality ""pallet guitar""[33] made from pallet wood, in order to demonstrate the importance of construction technique versus expensive exotic woods.

I-Beam Design, an architecture and interior design firm based in New York City, won an award in a 1999 competition sponsored by Architecture for Humanity for their submission of ""The Pallet House"", a design solution to house the returning refugees of Kosovo. Full-scale prototypes of the Pallet House were featured in the ""Casa per Tutti"" Exhibit at the Milan Triennale and the Earth Awards in Prince Charles' Royal Gardens as part of The Prince's Charities Foundation's Conference on a Sustainable Future organized in collaboration with IBM and the Financial Times. The Pallet House is an affordable transitional home that can become permanent over time. It can be used as refugee housing or as affordable housing as well. Due to the nature of the pallet module, the wall cavity can be insulated with a variety of materials that are accessible to the user.

The two Austrian students Andreas Claus Schnetzer and Gregor Pils from the University of Vienna created a home entitled Pallet house; as the name suggests, they reused pallets to form a modular, energy efficient and affordable housing. The idea was born in 2008 during a competition and the Pallet house has been exhibited in several European cities including Venice, Vienna, Linz and Grenoble.[34]

In 2014, Denver, Colorado was host to an Inaugural Pallet-Fest festival, which showcased the versatility of pallets and included large art structures made with upcycled materials, a pallet maze, a pallet amphitheater with live musical performances, sustainable living demonstrations, an upcycled fashion show, vendors and artists selling upcycled goods, and a parkour course. The event was crowdfunded and organized by Upcycle Events.[35]

In Northern Ireland, wooden pallets are used to construct bonfires during celebrations of the Eleventh Night, the night before the Twelfth of July. They are built in the weeks ahead and are lit in loyalist neighbourhoods, often accompanied by street parties and marching bands. There is some contention to this practice, especially when they involve the burning of flags or effigies.[36]
"
Furniture Manufacturing,"Furniture refers to objects intended to support various human activities such as seating (e.g., stools, chairs, and sofas), eating (tables), storing items, working, and sleeping (e.g., beds and hammocks). Furniture is also used to hold objects at a convenient height for work (as horizontal surfaces above the ground, such as tables and desks), or to store things (e.g., cupboards, shelves, and drawers). Furniture can be a product of design and can be considered a form of decorative art. In addition to furniture's functional role, it can serve a symbolic or religious purpose. It can be made from a vast multitude of materials, including metal, plastic, and wood. Furniture can be made using a variety of woodworking joints which often reflects the local culture.

People have been using natural objects, such as tree stumps, rocks and moss, as furniture since the beginning of human civilization and continues today in some households/campsites. Archaeological research shows that from around 30,000 years ago, people started to construct and carve their own furniture, using wood, stone, and animal bones. Early furniture from this period is known from artwork such as a Venus figurine found in Russia, depicting the goddess on a throne. The first surviving extant furniture is in the homes of Skara Brae in Scotland, and includes cupboards, dressers and beds all constructed from stone. Complex construction techniques such as joinery began in the early dynastic period of ancient Egypt. This era saw constructed wooden pieces, including stools and tables, sometimes decorated with valuable metals or ivory. The evolution of furniture design continued in ancient Greece and ancient Rome, with thrones being commonplace as well as the klinai, multipurpose couches used for relaxing, eating, and sleeping. The furniture of the Middle Ages was usually heavy, oak, and ornamented. Furniture design expanded during the Italian Renaissance of the fourteenth and fifteenth century. The seventeenth century, in both Southern and Northern Europe, was characterized by opulent, often gilded Baroque designs. The nineteenth century is usually defined by revival styles. The first three-quarters of the twentieth century are often seen as the march towards Modernism. One unique outgrowth of post-modern furniture design is a return to natural shapes and textures.[1]

The English word furniture is derived from the French word fourniture,[2] the noun form of fournir, which means to supply or provide.[3] Thus fourniture in French means supplies or provisions.[4] The English usage, referring specifically to household objects, is specific to that language;[5] French and other Romance languages as well as German use variants of the word meubles, which derives from Latin mobilia, meaning ""moveable goods"".[6]

The practice of using natural objects as rudimentary pieces of furniture likely dates to the beginning of human civilization.[7] Early humans are likely to have used tree stumps as seats, rocks as rudimentary tables, and mossy areas for sleeping.[7] During the late Paleolithic or early Neolithic period, from around 30,000 years ago, people began constructing and carving their own furniture, using wood, stone and animal bones.[8] The earliest evidence for the existence of constructed furniture is a Venus figurine found at the Gagarino site in Russia, which depicts the goddess in a sitting position, on a throne.[9] A similar statue of a seated woman was found in Çatalhöyük in Turkey, dating to between 6000 and 5500 BCE.[7] The inclusion of such a seat in the figurines implies that these were already common artefacts of that age.[9]

A range of unique stone furniture has been excavated in Skara Brae, a Neolithic village in Orkney, Scotland The site dates from 3100 to 2500 BCE and due to a shortage of wood in Orkney, the people of Skara Brae were forced to build with stone, a readily available material that could be worked easily and turned into items for use within the household. Each house shows a high degree of sophistication and was equipped with an extensive assortment of stone furniture, ranging from cupboards, dressers, and beds to shelves, stone seats, and limpet tanks. The stone dresser was regarded as the most important as it symbolically faces the entrance in each house and is therefore the first item seen when entering, perhaps displaying symbolic objects, including decorative artwork such as several Neolithic carved stone balls also found at the site.

Ancient furniture has been excavated from the 8th-century BCE Phrygian tumulus, the Midas Mound, in Gordion, Turkey. Pieces found here include tables and inlaid serving stands. There are also surviving works from the 9th–8th-century BCE Assyrian palace of Nimrud. The earliest surviving carpet, the Pazyryk Carpet was discovered in a frozen tomb in Siberia and has been dated between the 6th and 3rd century BCE.

Civilization in ancient Egypt began with the clearance and irrigation of land along the banks of the River Nile,[10] which began in about 6000 BCE. By that time, society in the Nile Valley was already engaged in organized agriculture and the construction of large buildings.[11]  At this period, Egyptians in the southwestern corner of Egypt were herding cattle and also constructing large buildings. Mortar was in use by around 4000 BCE The inhabitants of the Nile Valley and delta were self-sufficient and were raising barley and emmer (an early variety of wheat) and stored it in pits lined with reed mats.[12] They raised cattle, goats and pigs and they wove linens and baskets.[12] Evidence of furniture from the predynastic period is scarce, but samples from First Dynasty tombs indicate an already advanced use of furnishings in the houses of the age.[13]

During the Dynastic Period, which began in around 3200 BCE, Egyptian art developed significantly, and this included furniture design.[14] Egyptian furniture was primarily constructed using wood, but other materials were sometimes used, such as leather,[15] and pieces were often adorned with gold, silver, ivory and ebony, for decoration.[15] Wood found in Egypt was not suitable for furniture construction, so it had to be imported into the country from other places,[14] particularly Phoenicia.[16] The scarcity of wood necessitated innovation in construction techniques. The use of scarf joints to join two shorter pieces together and form a longer beam was one example of this,[17] as well as construction of veneers in which low quality cheap wood was used as the main building material, with a thin layer of expensive wood on the surface.[18]

The earliest used seating furniture in the dynastic period was the stool, which was used throughout Egyptian society, from the royal family down to ordinary citizens.[19] Various different designs were used, including stools with four vertical legs, and others with crossed splayed legs; almost all had rectangular seats, however.[19] Examples include the workman's stool, a simple three legged structure with a concave seat, designed for comfort during labour,[20] and the much more ornate folding stool, with crossed folding legs,[21] which were decorated with carved duck heads and ivory,[21] and had hinges made of bronze.[19] Full chairs were much rarer in early Egypt, being limited to only wealthy and high ranking people, and seen as a status symbol; they did not reach ordinary households until the 18th dynasty.[22] Early examples were formed by adding a straight back to a stool, while later chairs had an inclined back.[22] Other furniture types in ancient Egypt include tables, which are heavily represented in art, but almost nonexistent as preserved items – perhaps because they were placed outside tombs rather than within,[23] as well as beds and storage chests.[24][25]

Historical knowledge of Greek furniture is derived from various sources, including literature, terracotta, sculptures, statuettes, and painted vases.[26] Some pieces survive to this day, primarily those constructed from metals, including bronze, or marble.[26] Wood was an important and common material in Greek furniture, both domestic and imported.[26] A common technique was to construct the main sections of the furniture with cheap solid wood, then apply a veneer using an expensive wood, such as maple or ebony.[26] Greek furniture construction also made use of dowels and tenons for joining the wooden parts of a piece together.[26] Wood was shaped by carving, steam treatment, and the lathe, and furniture is known to have been decorated with ivory, tortoise shell, glass, gold or other precious materials.[27]

The modern word ""throne"" is derived from the ancient Greek thronos (Greek singular: θρόνος), which was a seat designated for deities or individuals of high status/hierarchy or honor.[28] The colossal chryselephantine statue of Zeus at Olympia, constructed by Phidias and lost in antiquity, featured the god Zeus seated on an elaborate throne, which was decorated with gold, precious stones, ebony and ivory, according to Pausanias.[29] Other Greek seats included the klismos, an elegant Greek chair with a curved backrest and legs whose form was copied by the Romans and is now part of the vocabulary of furniture design,[30] the backless stool (diphros), which existed in most Greek homes,[31] and folding stool.[32] The kline, used from the late seventh century BCE,[33] was a multipurpose piece used as a bed, but also as a sofa and for reclining during meals.[34] It was rectangular and supported on four legs, two of which could be longer than the other, providing support for an armrest or headboard.[35] Mattresses, rugs, and blankets may have been used, but there is no evidence for sheets.[34]

In general, Greek tables were low and often appear in depictions alongside klinai.[36] The most common type of Greek table had a rectangular top supported on three legs, although numerous configurations exist, including trapezoid and circular.[37] Tables in ancient Greece were used mostly for dining purposes – in depictions of banquets, it appears as though each participant would have used a single table, rather than a collective use of a larger piece.[38] Tables also figured prominently in religious contexts, as indicated in vase paintings, for example, the wine vessel associated with Dionysus, dating to around 450 BCE and now housed at the Art Institute of Chicago.[39] Chests were used for storage of clothes and personal items and were usually rectangular with hinged lids.[37] Chests depicted in terracotta show elaborate patterns and design, including the Greek fret.[34]

Roman furniture was based heavily on Greek furniture, in style and construction. Rome gradually superseded Greece as the foremost culture of Europe, leading eventually to Greece becoming a province of Rome in 146 BC. Rome thus took over production and distribution of Greek furniture, and the boundary between the two is blurred. The Romans did have some limited innovation outside of Greek influence, and styles distinctly their own.[40]

Roman furniture was constructed principally using wood, metal and stone, with marble and limestone used for outside furniture. Very little wooden furniture survives intact, but there is evidence that a variety of woods were used, including maple, citron, beech, oak, and holly. Some imported wood such as satinwood was used for decoration. The most commonly used metal was bronze, of which numerous examples have survived, for example, headrests for couches and metal stools. Similar to the Greeks, Romans used tenons, dowels, nails, and glue to join wooden pieces together, and also practised veneering.[40]

The 1738 and 1748 excavations of Herculaneum and Pompeii revealed Roman furniture, preserved in the ashes of the AD 79 eruption of Vesuvius.

In contrast to the ancient civilizations of Egypt, Greece, and Rome, there is comparatively little evidence of furniture from the 5th to the 15th century.[41] Very few extant pieces survive, and evidence in literature is also scarce.[41] It is likely that the style of furniture prevalent in late antiquity persisted throughout the Middle Ages.[41] For example, a throne similar to that of Zeus is depicted in a sixth-century diptych,[41] while the Bayeux tapestry shows Edward the Confessor and Harold seated on seats similar to the Roman sella curulis.[42] The furniture of the Middle Ages was usually heavy, oak, and ornamented with carved designs.

The Hellenistic influence upon Byzantine furniture can be seen through the use of acanthus leaves, palmettes, bay and olive leaves as ornaments. Oriental influences manifest through rosettes, arabesques and the geometric stylisation of certain vegetal motifs. Christianity brings symbols in Byzantine ornamentation: the pigeon, fishes, the lamb and vines.[43] The furniture from Byzantine houses and palaces was usually luxurious, highly decorated and finely ornamented. Stone, marble, metal, wood and ivory are used. Surfaces and ornaments are gilded, painted plychrome, plated with sheets of gold, emailed in bright colors, and covered in precious stones. The variety of Byzantine furniture is pretty big: tables with square, rectangle or round top, sumptuous decorated, made of wood sometimes inlaid, with bronze, ivory or silver ornaments; chairs with high backs and with wool blankets or animal furs, with coloured pillows, and then banks and stools; wardrobes were used only for storing books; cloths and valuable objects were kept in chests, with iron locks; the form of beds imitated the Roman ones, but have different designs of legs.[44]

The main ornament of Gothic furniture and all applied arts is the ogive. The geometric rosette accompanies the ogive many times, having a big variety of forms. Architectural elements are used at furniture, at the beginning with purely decorative reasons, but later as structure elements. Besides the ogive, the main ornaments are: acanthus leaves, ivy, oak leaves, haulms, clovers, fleurs-de-lis, knights with shields, heads with crowns and characters from the Bible. Chests are the main type of Gothic furniture used by the majority of the population. Usually, the locks and escutcheon of chests have also an ornamental scope, being finely made.[45]

Along with the other arts, the Italian Renaissance of the fourteenth and fifteenth century marked a rebirth in design, often inspired by the Greco-Roman tradition. A similar explosion of design, and renaissance of culture in general occurred in Northern Europe, starting in the fifteenth century.

The 17th century, in both Southern and Northern Europe, was characterized by opulent, often gilded Baroque designs that frequently incorporated a profusion of vegetal and scrolling ornament. Starting in the eighteenth century, furniture designs began to develop more rapidly. Although there were some styles that belonged primarily to one nation, such as Palladianism in Great Britain or Louis Quinze in French furniture, others, such as the Rococo and Neoclassicism were perpetuated throughout Western Europe.

During the 18th century, the fashion was set in England by the French art. In the beginning of the century Boulle cabinets were at the peak of their popularity and Louis XIV was reigning in France. In this era, most of the furniture had metal and enamelled decorations in it and some of the furniture was covered in inlays of marbles lapis lazuli, and porphyry and other stones. By mid-century this Baroque style was displaced by the graceful curves, shining ormolu, and intricate marquetry of the Rococo style, which in turn gave way around 1770 to the more severe lines of Neoclassicism, modeled after the architecture of ancient Greece and Rome.[52] Creating a mass market for furniture, the distinguished London cabinet maker Thomas Chippendale's The Gentleman and Cabinet Maker's Director (1754) is regarded as the ""first comprehensive trade catalogue of its kind"".[53]

There is something so distinct in the development of taste in French furniture, marked out by the three styles to which the three monarchs have given the name of ""Louis Quatorze"", ""Louis Quinze"", and ""Louis Seize"". This will be evident to anyone who will visit, first the Palace of Versailles, then the Grand Trianon, and afterwards the Petit Trianon.[54]

The nineteenth century is usually defined by concurrent revival styles, including Gothic, Neoclassicism, and Rococo. The design reforms of the late century introduced the Aesthetic movement and the Arts and Crafts movement. Art Nouveau was influenced by both of these movements. Shaker-style furniture became popular during this time in North America as well.

This design was in many ways rooted in necessity and emphasizes both form and materials. Early British Colonial American[vague] chairs and tables are often constructed with turned spindles and chair backs often constructed with steaming to bend the wood. Wood choices tend to be deciduous hardwoods with a particular emphasis on the wood of edible or fruit bearing trees such as cherry or walnut.[citation needed]

The first three-quarters of the 20th century is seen as the march towards Modernism. The furniture designers of Art Deco, De Stijl, Bauhaus, Jugendstil, Wiener Werkstätte, and Vienna Secession all worked to some degree within the Modernist motto. 

Born from the Bauhaus and Streamline Moderne came the post-World War II style ""Mid-Century Modern"". Mid-Century Modern materials developed during the war including laminated plywood, plastics, and fiberglass. Prime examples include furniture designed by George Nelson Associates, Charles and Ray Eames, Paul McCobb, Florence Knoll, Harry Bertoia, Eero Saarinen, Harvey Probber, Vladimir Kagan and Danish modern designers including Finn Juhl and Arne Jacobsen.

Industrialisation, Post-Modernism, and the Internet have allowed furniture design to become more accessible to a wider range of people than ever before. There are many modern styles of furniture design, each with roots in Classical, Modernist, and Post-Modern design and art movements. The growth of Maker Culture across the Western sphere of influence has encouraged higher participation and development of new, more accessible furniture design techniques. One unique outgrowth of this post-modern furniture design trajectory is Live Edge, which incorporates the natural surface of a tree as part of a furniture object, heralding a resurgence of these natural shapes and textures within the home.[1] Additionally, the use of Epoxy Resin has become more prevalent in DIY furniture styles.

Great efforts from individuals, governments, and companies has led to the manufacturing of products with higher sustainability known as Ecodesign. This new line of furniture is based on environmentally friendly design. Its use and popularity are increasing each year.[69]

Postmodern design, intersecting the Pop art movement, gained steam in the 1960s and 70s, promoted in the 80s by groups such as the Italy-based Memphis movement. Transitional furniture is intended to fill a place between Traditional and Modern tastes.[citation needed]

Asian furniture has a quite distinct history. The traditions out of India, China, Korea, Pakistan, Indonesia (Bali and Java) and Japan are some of the best known, but places such as Mongolia, and the countries of South East Asia have unique facets of their own.

The use of uncarved wood and bamboo and the use of heavy lacquers are well known Chinese styles. It is worth noting that Chinese furniture varies dramatically from one dynasty to the next. Chinese ornamentation is highly inspired by paintings, with floral and plant life motifs including bamboo trees, chrysanthemums, waterlilies, irises, magnolias, flowers and branches of cherry, apple, apricot and plum, or elongated bamboo leaves; animal ornaments include lions, bulls, ducks, peacocks, parrots, pheasants, roosters, ibises and butterflies. The dragon is the symbol of earth fertility, and of the power and wisdom of the emperor. Lacquers are mostly populated with princesses, various Chinese people, soldiers, children, ritually and daily scenes. Architectural features tend toward geometric ornaments, like meanders and labyrinths. The interior of a Chinese house was simple and sober. All Chinese furniture is made of wood, usually ebony, teak, or rosewood for heavier furniture (chairs, tables and benches) and bamboo, pine and larch for lighter furniture (stools and small chairs).[70]

Traditional Japanese furniture is well known for its minimalist style, extensive use of wood, high-quality craftsmanship and reliance on wood grain instead of painting or thick lacquer. Japanese chests are known as Tansu, known for elaborate decorative iron work, and are some of the most sought-after of Japanese antiques. The antiques available generally date back to the Tokugawa and Meiji periods. Both the technique of lacquering and the specific lacquer (resin of Rhus vernicifera) originated in China, but the lacquer tree also grows well in Japan. The recipes of preparation are original to Japan: resin is mixed with wheat flour, clay or pottery powder, turpentine, iron powder or wood coal. In ornamentation, the chrysanthemum, known as kiku, the national flower, is a very popular ornament, including the 16-petal chrysanthemum symbolizing the Emperor. Cherry and apple flowers are used for decorating screens, vases and shōji. Common animal ornaments include dragons, carps, cranes, gooses, tigers, horses and monkeys; representations of architecture such as houses, pavilions, towers, torii gates, bridges and temples are also common. The furniture of a Japanese house consists of tables, shelves, wardrobes, small holders for flowers, bonsais or for bonkei, boxes, lanterns with wooden frames and translucent paper, neck and elbow holders, and jardinieres.[71]

Seating is amongst the oldest known furniture types, and authors including Encyclopædia Britannica regard it as the most important.[2] In addition to the functional design, seating has had an important decorative element from ancient times to the present day. This includes carved and sculpted pieces intended as works of art, as well as the styling of seats to indicate social importance, with senior figures or leaders granted the use of specially designed seats.[2]

The simplest form of seat is the chair,[72] which is a piece of furniture designed to allow a single person to sit down, which has a back and legs, as well as a platform for sitting.[73] Chairs often feature cushions made from various fabrics.[74]

All different types of woods have unique signature marks that can help in easy identification of the type. Hardwood and softwood are the two main categories for wood. Both hardwoods and softwoods are used in furniture manufacturing, and each has its own specific uses. Deciduous trees, which have broad leaves that change color periodically throughout the year, are the source of hardwood. Coniferous trees, also known as cone-bearing trees, have small leaves or needles that stay on the tree throughout the year.[75][76] Common softwoods used include pine, redwood and yew. Higher quality furniture tends to be made out of hardwood, including oak, maple, mahogany, teak, walnut, cherry and birch. Highest quality wood will have been air dried to rid it of its moisture.[77]

A popular furniture hardwood is American black cherry. Cherry is a light reddish brown to brown color that intensifies into a rich color as it ages, and grows mostly in the eastern United States. Cherry has a tighter grain than birch and is softer. Much cherry lumber is narrow, and it has been utilized to make many lovely classic furniture pieces.[75]

Birch is a sturdy, durable, even-textured hardwood that is common in the United States and Canada. The wood appears white or creamy yellow to light brown with a crimson tinge in its natural state. Birch is frequently stained to complement other types of wood in furniture. Birch is used to make a lot of transparent, cabinet-grade plywood because it absorbs stain well and finishes beautifully. Birch is frequently used to construct interior doors and cupboards in addition to furniture.[75]

Restoring a piece of furniture may imply attempting to repair and revive the original finish in some way. More often than not, this entails removing the existing treatment and preparing the raw wood for a new finish. Methods for repair depend on what kind of wood it is: solid or veneered, hardwood or softwood, open grained or closed grained. These variables can sometimes decide if a piece of furniture is worth repairing, as well as the type of repairs and finish it will require if it is restored. The 3 methods of restoring furniture are rejuvenate, repair, and refinish.

Rejuvenate The piece can easily be restored by just cleaning and waxing the surface while preserving the current finish. It works on wooden furniture that is still in good shape and is the simplest way to clean it.

Repair This process can fix dents and cracks by touching up some worn-out areas without removing the surface with this technique, the finish can be maintained while repairing the object with specialized products.

Refinish Remove anything that is left for example any paint with a finish-stripper product or lightly sanding the area down and then applying wood finish like oil wax in order to protect the secure the wood.[75]

Cleaning Remove dirt, dust, and grime from the furniture using a mild soap or specialized furniture cleaner.




"
Bedding Manufacturing,"Bedding, also called bedclothes[1] or bed linen, is the materials laid above the mattress of a bed for hygiene, warmth, protection of the mattress, and decorative effect. Bedding is the removable and washable portion of a human sleeping environment. Multiple sets of bedding for each bed are often washed in rotation and/or changed seasonally to improve sleep comfort at varying room temperatures. Most standardized measurements for bedding are rectangular, but there are also some square-shaped sizes, which allows the user to put on bedding without having to consider its lengthwise orientation (e.g. a 220 cm × 220 cm (87 in × 87 in) duvet).

In American English, the word bedding generally does not include the mattress,[2] while in British English it often does.[3] In Australian and New Zealand English, bedding is often called manchester,[4] especially in shops. Manchester was a center of the cotton industry in the late 18th and the 19th century, and into the 20th century, and so cotton goods (principally sheets and towels) were given the name  'Manchester goods', which later was simplified to 'manchester'.

A set of bedding generally consists of at least flat or fitted bed sheet that covers the mattress; a flat top sheet; either a blanket, a quilt, or a duvet. Sometimes with a duvet cover is to be used in addition to or instead of – the top sheet; and a number of pillows with pillowcases, also referred to as pillow shams. (See § Elements for more info on all these terms.) Additional blankets, etc. may be added to ensure the necessary insulation in cold sleeping areas. A common practice for children and some adults is to decorate a bed with plush stuffed animals, dolls, and other soft toys. These are not included under the designation of bedding, although they may provide additional warmth to the sleeper.

Lightweight white, solid-color or printed plain weave, satin weave, or flannel cotton or cotton/polyester blends are the most common types of sheeting, although linen and silk may also be used, including in combination. Goose or duck down and other feathers are frequently used as a warm and lightweight filling in duvets, comforters and quilts. But such fill can protrude in part even from tightly woven fabric, and be an irritant for many people, particularly those with allergies. Natural and synthetic down alternatives are marketed. Cotton, wool or polyester batting is commonly used as fill in quilts and down alternative comforters. These are less expensive and more easily laundered than natural down or feathers. Synthetic fibers are best in the form of thermofused (where fibers cross) batting. Thick-woven or knitted wool, cotton, acrylic or other microfiber synthetics, or blends of these, are typically used for blankets. The fabric produced from the cotton warp and weft, cotton warp and lyocell weft has a significant improvement in all manner and is best suited for making bed linen.[5]

Among the earliest discovered examples of bedding are remnants found in a Paleolithic structure at Ohalo II, Israel. Dating back 23,000 years, these remnants consist of partially charred stems and leaves positioned on the floor surrounding a central hearth. Potential earlier evidence of bedding, dating from the Middle Paleolithic, is evident in Spain's Esquilleu cave, displaying the gathering of grass near a hearth.[6]

Around 3400 BC Egyptian pharaohs had their beds moved off the ground and slept on a raised surface. Bed linen was widely evolved in Egypt. It was seen as a symbol of light and purity, as well as a symbol of prosperity. The Egyptian mummies were often wrapped in bed linen. The complexity of applications has increased with research and developments in the area of bed linen materials over the years.[5]

Roman Empire mattresses were stuffed with wool, feather, reeds or hay. The beds were decorated with paint, bronze, silver, jewels and gold. It was rare for a Roman couple to spend the night together. It was more common for each spouse to have a separate room. Researchers believe that the Roman bed was definitely less comfortable than today.[7]

During the Renaissance, mattresses were stuffed with straw and feathers and then covered with silks, velvets or satin material. Embroidered canopies and ornamental hangings as well as the advent of the featherbed led to beds becoming extremely expensive, often willed down from generation to generation.[8]

In the 18th century, Europeans began to use bed frames made from cast iron, and mattresses that were made of cotton. Until that time, assorted vermin were simply accepted as a component of even the most royal beds.[9]

In the 19th century the bed spring was invented, also called the box spring.[10]

In the 20th century United States, consumers bought the inner spring mattress, followed in the 1960s by the water bed (originating on the West Coast), and adoption of Japanese-style futons, air mattresses, and foam rubber mattresses and pillows.

Bedding sizes are made with consideration of the dimensions of the bed and mattress for which it is to be used. Bed sizes vary around the world, with countries having their own standards and terminology. If you are located in the US, custom size bedding is available.[13]
"
Paper Production Services,"
The pulp and paper industry comprises companies that use wood, specifically pulpwood, as raw material and produce pulp, paper, paperboard, and other cellulose-based products. 
In the manufacturing process, pulp is introduced into a paper machine where it is shaped into a paper web and water is extracted through pressing and drying stages.

Pressing involves removing water from the sheet by applying force. This process employs a specialized type of felt, distinct from traditional felt, to absorb the water. In contrast, hand-made paper uses a blotter sheet for this purpose.
Drying involves eliminating water from the paper sheets through air or heat. Historically, this was achieved by hanging the sheets to dry, similar to laundry. In modern papermaking, various heated drying mechanisms are employed, with the steam-heated can dryer being the most prevalent on paper machines.[1][2][3]

Papermaking as a craft is ancient, and for centuries it used various fibers, mainly grasses (cereal straws and others), or rags from old clothing made from them, in various preindustrial times and places. The commercial planting of domesticated mulberry trees to make pulp for papermaking is attested as early as the 6th century.[4] Due to advances in printing technology, the Chinese paper industry continued to grow under the Song dynasty to meet the rising demand for printed books. Demand for paper was also stimulated by the Song government, which needed a large supply of paper to print paper money and exchange certificates.[5]

An example of an enterprising paper mill during the late phase of the preindustrial era is the mill by William Rittenhouse and sons at what is now preserved as Historic Rittenhouse Town in Pennsylvania. 

The first mechanized paper machine was installed at Frogmore Paper Mill, Apsley, Hertfordshire in 1803, followed by another in 1804.[6] The site operates currently as a museum.[7]

During the 19th and 20th centuries, the paper chemical technologies for making the pulp from wood rather than grasses underwent some major industrial-era upgrades, as first the soda pulping process and then the Kraft process helped reduce the unit cost of paper manufacture. This made paper newly abundant, and along with continual advancements in printing press technologies, as well as in transport technologies (for distribution), during these same centuries, led to greatly increased sales and circulation of newspapers, other periodicals, and books of every kind.

The pulp and paper industry has been criticized by environmental groups like the Natural Resources Defense Council for unsustainable deforestation and clearcutting of old-growth forest.[8] The industry trend is to expand globally to countries like Russia, China and Indonesia with low wages and low environmental oversight.[9] According to Greenpeace, farmers in Central America illegally rip up vast tracts of native forest for cattle and soybean production without any consequences,[10] and companies who buy timber from private land owners contribute to massive deforestation of the Amazon Rainforest.[11] On the other hand, the situation is quite different where forest growth has been on the increase for a number of years. It is estimated for instance that since 1990 forests have grown in Europe by 17 million hectares,[12] which has been supported through the practice of sustainable forest management by the industry. In Sweden, for every tree that is felled, two are planted.[13]

The pulp and paper industry consumes a significant amount of water and energy and produces wastewater with a high concentration of chemical oxygen demand (COD), among other contaminants.[14] Recent studies underline coagulation as an appropriate pre-treatment of pulp and paper industrial wastewater and as a cost-effective solution for the removal of COD and the reduction of pressures on the aquatic environment.[15]

The industry is dominated by North American (United States and Canada), northern European (Finland, Sweden, and North-West Russia) and East Asian countries (such as East Siberian Russia, China, Japan, and South Korea). Australasia and Brazil also have significant pulp and paper enterprises. The industry also has a significant presence in a number of European countries including Germany, Portugal, Italy, the Netherlands and Poland. The United States had been the world's leading producer of paper until it was overtaken by China in 2009.[16]

According to data from Statista, 
China produced 110 million metric tons in 2018 followed by the US with 72 million.[17]

According to statistic data by RISI, main producing countries of paper and paperboard, not including pulp, in the world are as follows:[18]



The world's main paper and paperboard company groups are as follows. (Some figures are estimates.):[19]

In 2008, the top 10 forest, paper and packaging products companies were, according to a report by PricewaterhouseCoopers:[20]

Leading manufacturers of capital equipment with over $1 billion in annual revenue for the pulp and paper industry include:
"
Stationery Manufacturing,"

Stationery refers to writing materials, including cut paper, envelopes, continuous form paper, and other office supplies.[1] Stationery usually specifies materials to be written on by hand (e.g., letter paper) or by equipment such as computer printers.

Originally, the term 'stationery' referred to all products sold by a stationer, whose name indicated that his book shop was on a fixed spot. This was usually somewhere near a university, and permanent, while medieval trading was mainly carried on by itinerant peddlers (including chapmen, who sold books) and others (such as farmers and craftsmen) at markets and fairs. It was a unique term used between the 13th and 15th centuries in the manuscript culture. Stationers' shops were places where books were bound, copied, and published. These shops often loaned books to nearby university students for a fee. The books were loaned out in sections, allowing students to study or copy them, and the only way to get the next part of the book was to return the previous section.[2]

In some cases, stationers' shops became the preferred choice for scholars to find books, instead of university libraries due to stationers' shops' wider collection of books.[3] The Stationers' Company formerly held a monopoly over the publishing industry in England and was responsible for copyright regulations.

Printing is the process of applying a colouring agent to a surface to create a body of text or illustrations. This is often achieved through printing technology, but can be done by hand using more traditional methods. The earliest form of printing is wood blocking.

Letterpress is a process of printing several identical copies that presses words and designs onto the page. The print may be inked or blind, but is typically done in a single color. Motifs or designs may be added as many letterpress machines use movable plates that must be hand-set. Letterpress printing remained the primary method of printing until the 19th century.

When a single document needs to be produced, it may be handwritten or printed, typically by a computer printer. Several copies of one original paper can be produced by some printers using multipart stationery. Typing with a typewriter is largely obsolete, having been superseded for most purposes by preparing a document with a word processor and then printing it.

Thermographic printing is a process that involves several stages but can be implemented in a low-cost manufacturing process. The process involves printing the desired designs or text with an ink that remains wet, rather than drying on contact with the paper. The paper is then dusted with a powdered polymer that adheres to the ink. The paper is vacuumed or agitated, mechanically or by hand, to remove excess powder, and then heated to near combustion. The wet ink and polymer bond and dry, resulting in a raised print surface similar to the result of an engraving process.

Embossing is a printing technique used to create raised surfaces in the converted paper stock. The process relies upon mated dies that press the paper into a shape that can be observed on both the front and back surfaces. Two things are required during the process of embossing: a die and a stock. The result is a three-dimensional (3D) effect that emphasizes a particular area of the design. 

Engraving is a process that requires a design to be cut into a plate made of relatively hard material. The metal plate is first polished so that the design cut can be easily visible to the person. This technology has a long history and requires a significant amount of skill, experience, and expertise. The finished plate is usually covered in ink, and then the ink is removed from all of the un-etched portions of the plate. The plate is then pressed into paper under substantial pressure. The result is a design that is slightly raised on the surface of the paper and covered in ink. Due to the cost of the process and expertise required, many consumers opt for thermographic printing, a process that results in a similarly raised print surface, but through different means at less cost.

Many shops that sell stationery also sell other school supplies for students in primary and secondary education, including pocket calculators, display boards, compasses and protractors, set squares, lunch boxes, and related items.[4][5]

This section contains an incomplete list of famous brands, manufacturers and retailers of stationery worldwide.

In US and Canada, Office Depot and Staples are two major retailers of stationery.

Notable stationery brands in Europe include LAMY, MOLESKINE, Staedtler, and Faber-Castell.

In Japan, major manufacturers of stationery include Kokuyo, Maruman, Lihit Lab, King Jim, MUJI and Tombow. MUJI also has about 800 retail stores worldwide.

In mainland China, 晨光文具 (Chén guāng wén jù) is a major manufacturer and retailer of stationery, and MUJI is a popular retailer in larger cities.
"
Publishing Services,"

Publishing is the activity of making information, literature, music, software, and other content available to the public for sale or free of charge.[1] Traditionally, the term refers to the creation and distribution of printed works, such as books, comic books, newspapers, and magazines. With the advent of digital information systems, the scope has expanded to include digital publishing such as e-books, digital magazines, websites, social media, music, and video game publishing.

The commercial publishing industry ranges from large multinational conglomerates such as News Corp, Pearson, Penguin Random House, and Thomson Reuters[2] to major retail brands and thousands of small independent publishers. It has various divisions such as trade/retail publishing of fiction and non-fiction, educational publishing, and academic and scientific publishing.[3] Publishing is also undertaken by governments, civil society, and private companies for administrative or compliance requirements, business, research, advocacy, or public interest objectives.[4] This can include annual reports, research reports, market research, policy briefings, and technical reports. Self-publishing has become very common.

Publishing has evolved from a small, ancient form limited by law or religion to a modern, large-scale industry disseminating all types of information.[5]

""Publisher"" can refer to a publishing company, organization, or an individual who leads a publishing company, imprint, periodical, or newspaper.

The publishing process covering most magazine, journal, and book publishers includes: (Different stages are applicable to different types of publishers)[6]

Newspapers or news websites are publications of current reports, articles, and features written by journalists. They are free, sometimes with a premium edition, or paid for, either individually or through a subscription. They are filled with photographs or other media and usually are subsidized with advertising. Typically, they cover local, national, and international news or feature a particular industry. Some organizations charge premium fees if they have the expertise and exclusive knowledge. The news industry is meant to serve the public interest, hold people and businesses to account, and promote freedom of information and expression.[7] Editors manage the tone of voice of their publication; for example, negative versus positive articles can affect the reader's perspective.[8]

A journal is an academic or technical publication also available in digital and(or) print format, containing articles written by researchers, professors, and individuals with professional expertise. These publications are specific to a particular field and often push the boundaries established in these fields. They usually have peer review processes before publishing to test the validity and quality of the content.[9]

A magazine is a periodical published at regular intervals. It features creative layouts, photography, and illustrations that cover a particular subject or interest. Magazines are available in print or digital formats and can be purchased on apps or websites like Readly or accessed free of charge on apps or websites like Issuu.

The global book publishing industry consists of books categorized as fiction or non-fiction and print, e-book, or audiobook. The book market is huge, with around 1.5 billion people speaking English.[10] Translation services are also available to make these texts accessible in other languages. Self-publishing makes publishing widely accessible through small print-run digital printing or online self-publishing platforms. E-reader screen technology continues to improve with increased contrast and resolution making them more comfortable to read. Each book has a registered ISBN to identify it.

Directories contain searchable indexed data about businesses, products, and services. They were printed in the past but are now mostly online. Directories are available as searchable lists, on a map, as a sector-specific portal, as a review site (expert or consumer), or as a comparison site. Although some businesses may not consider themselves publishers, the way the data is displayed is published.

A textbook is an educational book, or e-book, that contains information on a particular subject and is used by people studying that subject.[11] The need for textbook publishing continues due to the global need for education.[12][13] Textbooks from major publishers are being integrated with online learning platforms for expert knowledge and access to a library of books with digital content.[14] A university press is an academic publisher run by a university. Oxford University Press is the largest in the world and specializes in research, education, and English language teaching internationally.[15]

A catalog is a visual directory or list of a large range of products that allow you to browse and buy from a particular company.[16] In print, this is usually in the format of a softback book or directory. Smaller visual catalogs can be known as brochures. With the Internet, they have evolved into searchable databases of products known under the term e-commerce. Interactive catalogs and brochures like IKEA[17] and Avon[18] allow customers to browse a full range if they have not decided on their purchase. Responsive web and app design will allow further integration between interactive catalog visuals and searchable product databases.

Until recently, physical books were the primary source of recording knowledge. For accessibility and global reach, this content can be repurposed for the web. The British Library, for example, holds more than 170 million items with 3 million new additions each year.[19] With consent, content can be published online through e-books, audio books, CMS-based websites, online learning platforms, videos, or mobile apps. On the Internet, writers and copy editors are known as content writers and content editors, although their roles vary from their print-based counterparts.

Advertising can provide income or a subsidized income for publishers. If the advertising has a return on investment (ROI), the publisher can boost income exponentially by increasing the spending. An ROI of up to £10 per £1 invested is possible, as seen in the John Lewis & Partners Christmas campaigns.[20][21] Likewise, any cost savings that harm the customer/consumer experience can impact a brand in the long term. Multichannel marketing can be more cost-effective in creating an immersive experience that cannot be replicated with one channel. For example, when considering marketing spend, a shop with a small margin (or none at all) compared to a website is very cost-effective because it acts as a huge billboard that offers a browsing experience that enables consumers to make purchasing decisions. It gives them a feel for the brand, has a presence in the community, and creates jobs. Also, using social media publishing to advertise has a good ROI if trending, high-quality content is created that reflects positively on the brand.

Film, television, radio, and advertisements publish information to their audiences. Computer games, streaming apps, and social media publish content in various ways that can keep audiences more engaged. Marketing additional products closely related to a major film, such as Star Wars, is an example of tie-in publishing. These products include but are not limited to spin-off books, graphic novels, soundtrack albums, computer games, models and toys, social media posts, and promotional publications. Examples of tie-in publishing based on books are the Harry Potter and James Bond franchises.

The publishing landscape is continually evolving.  Currently there are four major types of publishers in book publishing:[22]

These companies traditionally produce hardcopy books in large print runs. They have established networks which distribute those books to bricks-and-mortar stores and libraries.  

When a mainstream publisher accepts a book for publication, they require the author to sign a contract surrendering some rights to the publisher.  In exchange, the publisher will take care of all aspects of publishing the book at the publisher's cost.  They rely entirely on sales of the book to recoup those costs and make a profit. The author receives a royalty on each sale (and sometimes an advance on royalties when the book is accepted[23]).  Because of the financial risk, mainstream publishers are extremely selective in what they will publish, and reject most manuscripts submitted to them.[24]

In 2013, Penguin (owned by Pearson) and Random House (owned by Bertelsmann) merged, narrowing the mainstream publishing industry to a handful of big publishers as it adapted to digital media.[25] The merger created the largest consumer book publisher globally, with a global market share of more than 25 percent.[26] As of 2022[update], approximately 80% percent of the United States trade market for books was controlled by the ""Big Five"" publishing houses: Penguin Random House, Hachette, HarperCollins, Simon & Schuster, and Macmillan.[27]

In November 2020, ViacomCBS agreed to sell Simon & Schuster, the third largest book publisher in the United States, to Penguin Random House in a deal that, if it had gone through, would have formed the largest publishing company in the world.[25] On November 2, 2021, the United States Department of Justice filed a lawsuit (U.S. v. Bertelsmann SE & CO. KGaA, et al.) to block the merger on antitrust grounds,[28] and on October 31, 2022, the D.C. District Court ruled in favor of the Department of Justice, filing a permanent injunction on the merger.[29]

Although newspaper and magazine companies still often own printing presses and binderies, book publishers rarely do.[citation needed] Similarly, the trade usually sells the finished products through a distributor who stores and distributes the publisher's wares for a percentage fee or sells on a sale or return basis.

Some major publishers have entire divisions devoted to a single franchise, e.g., Ballantine Del Rey LucasBooks has the exclusive rights to Star Wars in the United States; Random House UK (Bertelsmann)/Century LucasBooks holds the same rights in the United Kingdom. The video game industry self-publishes through BL Publishing/Black Library (Warhammer) and Wizards of the Coast (Dragonlance, Forgotten Realms, etc.). The BBC has its own publishing division that does very well with long-running series such as Doctor Who. These multimedia works are cross-marketed aggressively, and sales frequently outperform the average stand-alone published work, making them a focus of corporate interest.[30]

The advent of the Internet has provided an alternative mode of book distribution and most mainstream publishers also offer their books in ebook format. Preparing a book for e-book publication is the same as print publication, with only minor variations in the process to account for the different publishing mediums; E-book publication also eliminates some costs like the discount given to retailers (usually around 45 percent).[31]

Small publishers, also called independent or indie publishers,[32] operate on a traditional model (i.e. the author surrenders some rights in exchange for the publisher bearing all costs of publishing), but their precise terms can vary greatly.[33]  Often, they do not pay an advance on royalties.

A hybrid publisher shares the costs of publication (and therefore the risks) with the author.  Because of this financial risk, they are selective in what they publish.  The contract varies according to what is negotiated between author and company, but will always include the surrender of some rights to the publisher.[34]  Hybrid publishing is the source of debate in the publishing industry, due to the  tendency of vanity presses to masquerade as hybrids.

A vanity press will publish any book.  In return, the author must cover all the costs of publication, surrender some rights to the publisher, and pay royalties on sales.  Vanity presses often engage in deceptive practices or offer costly, poor-quality services with limited recourse available to the writer. In the US, these practices have been cited by the Better Business Bureau as unfavorable reports by consumers.[35]  Given the bad reputation of vanity publishing, many vanity presses brand themselves as hybrid publishers. The Society of Authors (SoA) and the Writers' Guild of Great Britain (WGGB) have called for reform of the paid-for publishing sector. These unions, representing 14,800 authors, jointly published a report to expose widespread bad practices among companies that charge writers to publish their work while taking away their rights.

When an author self-publishes a book, they retain all rights and assume responsibility for all stages of preparing, publishing and distributing the book. The author may hire professionals on a fee-for-service basis as needed, (e.g. an editor, cover designer, proofreader) or engage a company to provide an integrated package.[36]

Accessible publishing uses the digitization of books to mark them up into XML and produce multiple formats to sell to customers, often targeting those who experience difficulty reading. Formats include a variety of larger print sizes, specialized print formats for dyslexia,[37] eye tracking problems, and macular degeneration, as well as Braille, DAISY, audiobooks, and e-books.[38]

Green publishing means adapting the publishing process to minimize environmental impact. One example is the concept of on-demand printing, using digital or print-on-demand technology. This cuts down the need to ship books since they are manufactured close to the customer on a just-in-time basis.[39]

A further development is the growth of online publishing, where no physical books are produced. The author creates an e-book and uploads it to a website, from which anyone can download and read it.

An increasing number of authors are using niche marketing online to sell more books by engaging with their readers online.[40]

Refer to the ISO divisions of ICS 01.140.40 and 35.240.30 for further information.[41][42]

Publication is the distribution of copies or content to the public.[43][44] The Berne Convention requires that this can only be done with the consent of the copyright holder, which initially is always the author.[43] In the Universal Copyright Convention, ""publication"" is defined in Article VI as ""the reproduction in tangible form and the general distribution to the public of copies of a work from which it can be read or otherwise visually perceived.""[44]

Privishing (private publishing, but not to be confused with self-publishing) is a modern term for publishing a book but printing so few copies or with such lack of marketing, advertising, or sales support that it effectively does not reach the public.[45] The book, while nominally published, is almost impossible to obtain through normal channels such as bookshops, often cannot be ordered specially, and has a notable lack of support from its publisher, including refusal to reprint the title. A book that is privished may be referred to as ""killed."" Depending on the motivation, privishing may constitute a breach of contract, censorship,[46] or good business practice (e.g., not printing more books than the publisher believes will sell in a reasonable length of time).

Publishing became possible with the invention of writing and became more practical upon the introduction of printing. Before printing, distributed works were copied manually by scribes. Due to printing, publishing progressed hand-in-hand with the development of books.

The Chinese inventor Bi Sheng made a movable type of earthenware c. 1045, but there are no known surviving examples of his work. The Korean civil servant Ch'oe Yun-ŭi, who lived during the Goryeo Dynasty, invented the first metal moveable type in 1234–1250 AD.[47]

In what is commonly regarded as an independent invention, Johannes Gutenberg developed movable type in Europe around 1450, along with innovations in casting the type based on a matrix and hand mould. The invention of the printing press gradually made books less expensive to produce and more widely available.

Early printed books, single sheets, and images created before 1501 in Europe are known as incunables or incunabula. ""A man born in 1453, the year of the fall of Constantinople, could look back from his fiftieth year on a lifetime in which about eight million books had been printed, more perhaps than all the scribes of Europe had produced since Constantine founded his city in A.D. 330.""[48]

The history of modern newspaper publishing started in Germany in 1609, with the publication of magazines following in 1663.

Historians describe the last third of the eighteenth century of the German book trade as the Sturm und Drang period, German for ""storm and stress.""[49]

Missionaries brought printing presses to sub-Saharan Africa in the mid-18th century.[50]

Historically, publishing has been handled by publishers, although some authors self-published.[51] The establishment of the World Wide Web in 1989 soon propelled the website into a dominant publishing medium. Wikis and blogs soon developed, followed by online books, online newspapers, and online magazines. This also facilitated the technological convergence of commercial and self-published content and the convergence of publishing and production into online production through the development of multimedia content.

A U.S.-based study in 2016 that surveyed 34 publishers found that straight, able-bodied, white females overwhelmingly represent the publishing industry in the US.[52] Salon described the situation as a ""lack of diversity behind the scenes in book world.""[53] A survey in 2020 by the same group found there has been no significant statistical change in the lack of diversity since the 2016 survey.[54] Lack of diversity in the American publishing industry has been an issue for years. Within the industry, the least amount of diversity was in higher-level editorial positions.[55]

According to the report The Global Publishing Industry in 2022, published by the World Intellectual Property Organization, there is data available for 24 countries about the number of publications done in educational and trade sector:[56]

Publishing on specific contexts

Publishing tools
"
Printing Services,"

Printing is a process for mass reproducing text and images using a master form or template. The earliest non-paper products involving printing include cylinder seals and objects such as the Cyrus Cylinder and the Cylinders of Nabonidus. The earliest known form of printing evolved from ink rubbings made on paper or cloth from texts on stone tablets, used during the sixth century.[1][a] Printing by pressing an inked image onto paper (using woodblock printing) appeared later that century.[3] Later developments in printing technology include the movable type invented by Bi Sheng around 1040[4][5] and the printing press invented by Johannes Gutenberg in the 15th century. The technology of printing played a key role in the development of the Renaissance and the Scientific Revolution and laid the material basis for the modern knowledge-based economy and the spread of learning to the masses.[6]

Woodblock printing is a technique for printing text, images or patterns that was used widely throughout East Asia. It originated in China in antiquity as a method of printing on textiles and later on paper.[3]

The earliest examples of ink-squeeze rubbings and potential stone printing blocks appear in the mid-sixth century in China. A type of printing called mechanical woodblock printing on paper started during the 7th century in the Tang dynasty,[3][1] and subsequently spread throughout East Asia. Nara Japan printed the Hyakumantō Darani en masse around 770, and distributed them to temples throughout Japan. In Korea, an example of woodblock printing from the eighth century was discovered in 1966. A copy of the Buddhist Dharani Sutra called the Pure Light Dharani Sutra (Korean: 무구정광대다라니경; Hanja: 無垢淨光大陀羅尼經; RR: Mugu jeonggwang dae darani-gyeong), discovered in Gyeongju, in a Silla dynasty pagoda that was repaired in 751,[8] was undated but must have been created sometime before the reconstruction of the Shakyamuni Pagoda of Bulguk Temple, Kyongju Province in  751.[9][10][11][12][13] The document is estimated to have been created no later than 704.[8]

By the ninth century, printing on paper had taken off, and the first completely surviving printed book is the Diamond Sutra (British Library) of 868, uncovered from Dunhuang.[14] By the tenth century, 400,000 copies of some sutras and pictures were printed, and the Confucian classics were in print. A skilled printer could print up to 2,000 double-page sheets per day.[15]

Printing spread early to Korea and Japan, which also used Chinese logograms, but the technique was also used in Turpan and Vietnam using a number of other scripts. This technique then spread to Persia and Russia.[16] This technique was transmitted to Europe by around 1400 and was used on paper for old master prints and playing cards.[17]

Block printing, called tarsh in Arabic, developed in Arabic Egypt during the ninth and tenth centuries, mostly for prayers and amulets. There is some evidence to suggest that these print blocks were made from non-wood materials, possibly tin, lead, or clay. The techniques employed are uncertain. Block printing later went out of use during the Timurid Renaissance.[18] The printing technique in Egypt was embraced by reproducing texts on paper strips and supplying them in different copies to meet the demand.[19][20]

Block printing first came to Europe as a method for printing on cloth, where it was common by 1300. Images printed on cloth for religious purposes could be quite large and elaborate. When paper became relatively easily available, around 1400, the technique transferred very quickly to small woodcut religious images and playing cards printed on paper. These prints were produced in very large numbers from about 1425 onward.

Around the mid-fifteenth-century, block-books, woodcut books with both text and images, usually carved in the same block, emerged as a cheaper alternative to manuscripts and books printed with movable type. These were all short, heavily illustrated works, the bestsellers of the day, repeated in many different block-book versions: the Ars moriendi and the Biblia pauperum were the most common. There is still some controversy among scholars as to whether their introduction preceded or, in the majority view, followed the introduction of movable type, with the estimated range of dates being between about 1440 and 1460.[21]

Movable type is the system of printing and typography using movable pieces of metal type, made by casting from matrices struck by letterpunches. Movable type allowed for much more flexible processes than hand copying or block printing.

Around 1040, the first known movable type system was created in China by Bi Sheng out of porcelain.[5] Bi Sheng used clay type, which broke easily, but Wang Zhen by 1298 had carved a more durable type from wood. He also developed a complex system of revolving tables and number-association with written Chinese characters that made typesetting and printing more efficient. Still, the main method in use there remained woodblock printing (xylography), which ""proved to be cheaper and more efficient for printing Chinese, with its thousands of characters"".[22]

Copper movable type printing originated in China at the beginning of the 12th century. It was used in large-scale printing of paper money
issued by the Northern Song dynasty. Movable type spread to Korea during the Goryeo dynasty.

Around 1230, Koreans invented a metal type movable printing using bronze. The Jikji, published in 1377, is the earliest known metal printed book. Type-casting was used, adapted from the method of casting coins. The character was cut in beech wood, which was then pressed into a soft clay to form a mould, and bronze poured into the mould, and finally the type was polished.[23] Eastern metal movable type was spread to Europe between the late 14th and early 15th centuries.[24][25][26][27][28] The Korean form of metal movable type was described by the French scholar Henri-Jean Martin as ""extremely similar to Gutenberg's"".[29] Authoritative historians Frances Gies and Joseph Gies claimed that ""The Asian priority of invention movable type is now firmly established, and that Chinese-Korean technique, or a report of it traveled westward is almost certain.""[30]

Around 1450, Johannes Gutenberg introduced the first movable type printing system in Europe. He advanced innovations in casting type based on a matrix and hand mould, adaptations to the screw-press, the use of an oil-based ink, and the creation of a softer and more absorbent paper.[31] Gutenberg was the first to create his type pieces from an alloy of lead, tin, antimony, copper and bismuth – the same components still used today.[32] Johannes Gutenberg started work on his printing press around 1436, in partnership with Andreas Dritzehen – whom he had previously instructed in gem-cutting – and Andreas Heilmann, the owner of a paper mill.[33][page needed]

Compared to woodblock printing, movable type page setting and printing using a press was faster and more durable. Also, the metal type pieces were sturdier and the lettering more uniform, leading to typography and fonts. The high quality and relatively low price of the Gutenberg Bible (1455) established the superiority of movable type for Western languages. The printing press rapidly spread across Europe, leading up to the Renaissance, and later all around the world.[34]

Time Life magazine called Gutenberg's innovations in movable type printing the most important invention of the second millennium.[35]

The steam-powered rotary printing press, invented in 1843 in the United States by Richard M. Hoe,[36] ultimately allowed millions of copies of a page in a single day. Mass production of printed works flourished after the transition to rolled paper, as continuous feed allowed the presses to run at a much faster pace. Hoe's original design operated at up to 2,000 revolutions per hour where each revolution deposited 4 page images, giving the press a throughput of 8,000 pages per hour.[37] By 1891, The New York World and Philadelphia Item were operating presses producing either 90,000 4-page sheets per hour or 48,000 8-page sheets.[38]

The rotary printing press uses impressions curved around a cylinder to print on long continuous rolls of paper or other substrates. Rotary drum printing was later significantly improved by William Bullock. There are multiple types of rotary printing press technologies that are still used today: sheetfed offset, rotogravure, and flexographic printing.[39]

The table lists the maximum number of pages which various press designs could print per hour.

All printing process are concerned with two kinds of areas on the final output:

After the information has been prepared for production (the prepress step), each printing process has definitive means of separating the image from the non-image areas.

Conventional printing has four types of process:

To print an image without a blank area around the image, the non-printing areas must be trimmed after printing. Crop marks can be used to show the printer where the printing area ends, and the non-printing area begins.[45] The part of the image which is trimmed off is called bleed.

Letterpress printing is a technique of relief printing. A worker composes and locks movable type into the bed of a press, inks it, and presses paper against it to transfer the ink from the type which creates an impression on the paper. 
There is different paper for different works the quality of paper shows different ink to use.

Letterpress printing was the normal form of printing text from its invention by Johannes Gutenberg in the mid-15th century and remained in wide use for books and other uses until the second half of the 20th century, when offset printing was developed. More recently, letterpress printing has seen a revival in an artisanal form.

Offset printing is a widely used modern printing process. This technology is best described as when a positive (right-reading) image on a printing plate is inked and transferred (or ""offset"") from the plate to a rubber blanket. The blanket image becomes a mirror image of the plate image. An offset transfer moves the image to a printing substrate (typically paper), making the image right-reading again. Offset printing uses a lithographic process which is based on the repulsion of oil and water. The offset process employs a flat (planographic) image carrier (plate) which is mounted on a press cylinder. The image to be printed obtains ink from ink rollers, while the non-printing area attracts an (acidic) film of water, keeping the non-image areas ink-free. Most offset presses use three cylinders: Plate, blanket, impression. Currently, most books and newspapers are printed using offset lithography.

Gravure printing is an intaglio printing technique, where the image being printed is made up of small depressions in the surface of the printing plate. The cells are filled with ink, and the excess is scraped off the surface with a doctor blade. Then a rubber-covered roller presses paper onto the surface of the plate and into contact with the ink in the cells. The printing cylinders are usually made from copper plated steel, which is subsequently chromed, and may be produced by diamond engraving; etching, or laser ablation.

Gravure printing is known for its ability to produce high-quality, high-resolution images with accurate color reproduction and using viscosity control equipment during production.
Ink evaporation control affects the change in the color of the printed image.

Gravure printing is used for long, high-quality print runs such as magazines, mail-order catalogues, packaging and printing onto fabric and wallpaper. It is also used for printing postage stamps and decorative plastic laminates, such as kitchen worktops.

Flexography is a type of relief printing. The relief plates are typically made from photopolymers.
The process is used for flexible packaging, corrugated board, labels, newspapers and more. In this market it competes with gravure printing by holding 80% of the market in US, 50% in Europe but only 20% in Asia.[46]

The other significant printing techniques include:



It is estimated that following the innovation of Gutenberg's printing press, the European book output rose from a few million to around one billion copies within a span of less than four centuries.[47]

Samuel Hartlib, who was exiled in Britain and enthusiastic about social and cultural reforms, wrote in 1641 that ""the art of printing will so spread knowledge that the common people, knowing their own rights and liberties, will not be governed by way of oppression"".[48][49]

In the Muslim world, printing, especially in Arabic scripts, was strongly opposed throughout the early modern period, partially due to the high artistic renown of the art of traditional calligraphy. However, printing in Hebrew or Armenian script was often permitted. Thus, the first movable type printing in the Ottoman Empire was in Hebrew in 1493, after which both religious and non-religious texts were able to be printed in Hebrew.[50] According to an imperial ambassador to Istanbul in the middle of the sixteenth century, it was a sin for the Turks, particularly Turkish Muslims, to print religious books. In 1515, Sultan Selim I issued a decree under which the practice of printing would be punishable by death. At the end of the sixteenth century, Sultan Murad III permitted the sale of non-religious printed books in Arabic characters, yet the majority were imported from Italy. Ibrahim Muteferrika established the first press for printing in Arabic in the Ottoman Empire, against opposition from the calligraphers and parts of the Ulama. It operated until 1742, producing altogether seventeen works, all of which were concerned with non-religious, utilitarian matters. Printing did not become common in the Islamic world until the 19th century.[51]

Hebrew language printers were banned from printing guilds in some Germanic states; as a result, Hebrew printing flourished in Italy, beginning in 1470 in Rome, then spreading to other cities including Bari, Pisa, Livorno, and Mantua. Local rulers had the authority to grant or revoke licenses to publish Hebrew books,[52] and many of those printed during this period carry the words 'con licenza de superiori' (indicating their printing having been officially licensed) on their title pages.

It was thought that the introduction of printing 'would strengthen religion and enhance the power of monarchs.'[53] The majority of books were of a religious nature, with the church and crown regulating the content. The consequences of printing 'wrong' material were extreme. Meyrowitz[53] used the example of William Carter who in 1584 printed a pro-Catholic pamphlet in Protestant-dominated England. The consequence of his action was hanging.

Print gave a broader range of readers access to knowledge and enabled later generations to build directly on the intellectual achievements of earlier ones without the changes arising within verbal traditions. Print, according to Acton in his 1895 lecture On the Study of History, gave ""assurance that the work of the Renaissance would last, that what was written would be accessible to all, that such an occultation of knowledge and ideas as had depressed the Middle Ages would never recur, that not an idea would be lost"".[48]

Print was instrumental in changing the social nature of reading.

Elizabeth Eisenstein identifies two long-term effects of the invention of printing. She claims that print created a sustained and uniform reference for knowledge and allowed comparisons of incompatible views.[54]

Asa Briggs and Peter Burke identify five kinds of reading that developed in relation to the introduction of print:

The invention of printing also changed the occupational structure of European cities. Printers emerged as a new group of artisans for whom literacy was essential, while the much more labour-intensive occupation of the scribe naturally declined. Proof-correcting arose as a new occupation, while a rise in the numbers of booksellers and librarians naturally followed the explosion in the numbers of books.

Gutenberg's printing press had profound impacts on universities as well. Universities were influenced in their ""language of scholarship, libraries, curriculum, [and] pedagogy""[55]

Before the invention of the printing press, most written material was in Latin. However, after the invention of printing the number of books printed expanded as well as the vernacular. Latin was not replaced completely, but remained an international language until the eighteenth century.[55]

At this time, universities began establishing accompanying libraries. ""Cambridge made the chaplain responsible for the library in the fifteenth century but this position was abolished in 1570 and in 1577 Cambridge established the new office of university librarian. Although, the University of Leuven did not see a need for a university library based on the idea that professor were the library. Libraries also began receiving so many books from gifts and purchases that they began to run out of room. However, the issue was solved in 1589 by a man named Merton who decided books should be stored on horizontal shelves rather than lecterns.[55]

The printed press changed university libraries in many ways. Professors were finally able to compare the opinions of different authors rather than being forced to look at only one or two specific authors. Textbooks themselves were also being printed in different levels of difficulty, rather than just one introductory text being made available.[55]

> 30,000 (A3 trim size, web-fed)[57]

By 2005, digital printing accounted for approximately 9% of the 45 trillion pages printed annually around the world.[62]

Printing at home, an office, or an engineering environment is subdivided into:

Some of the more common printing technologies are:

Vendors typically stress the total cost to operate the equipment, involving complex calculations that include all cost factors involved in the operation as well as the capital equipment costs, amortization, etc. For the most part, toner systems are more economical than inkjet in the long run, even though inkjets are less expensive in the initial purchase price.

Professional digital printing (using toner) primarily uses an electrical charge to transfer toner or liquid ink to the substrate onto which it is printed. Digital print quality has steadily improved from early color and black and white copiers to sophisticated colour digital presses such as the Xerox iGen3, the Kodak Nexpress, the HP Indigo Digital Press series, and the InfoPrint 5000. The iGen3 and Nexpress use toner particles and the Indigo uses liquid ink. The InfoPrint 5000 is a full-color, continuous forms inkjet drop-on-demand printing system. All handle variable data, and rival offset in quality. Digital offset presses are also called direct imaging presses, although these presses can receive computer files and automatically turn them into print-ready plates, they cannot insert variable data.

Small press and fanzines generally use digital printing. Prior to the introduction of cheap photocopying, the use of machines such as the spirit duplicator, hectograph, and mimeograph was common.

3D printing is a form of manufacturing technology where physical objects are created from three-dimensional digital models using 3D printers. The objects are created by laying down or building up many thin layers of material in succession. The technique is also known as additive manufacturing, rapid prototyping, or fabricating.[63]

In the 1980s, 3D printing techniques were considered suitable only for the production of functional or aesthetic prototypes, and a more appropriate term for it at the time was rapid prototyping.[64][65] As of 2019[update], the precision, repeatability, and material range of 3D printing have increased to the point that some 3D printing processes are considered viable as an industrial-production technology, whereby the term additive manufacturing can be used synonymously with 3D printing.[66][67][68] One of the key advantages of 3D printing[69][70] is the ability to produce very complex shapes or geometries that would be otherwise infeasible to construct by hand, including hollow parts or parts with internal truss structures to reduce weight. Fused deposition modeling (FDM), which uses a continuous filament of a thermoplastic material, is the most common 3D printing process in use as of 2020[update].[71][72]





The classic manual of early hand-press technology is


"
Gas Manufacturing Services,"Industrial gases  are the gaseous materials that are manufactured for use in industry. The principal gases provided are nitrogen, oxygen, carbon dioxide, argon, hydrogen, helium and acetylene, although many other gases and mixtures are also available in gas cylinders. The industry producing these gases is also known as industrial gas, which is seen as also encompassing the supply of equipment and technology to produce and use the gases.[1] Their production is a part of the wider chemical Industry (where industrial gases are often seen as ""specialty chemicals"").

Industrial gases are used in a wide range of industries, which include oil and gas, petrochemicals, chemicals, power, mining, steelmaking, metals, environmental protection, medicine, pharmaceuticals, biotechnology, food, water, fertilizers, nuclear power, electronics and aerospace. Industrial gas is sold to other industrial enterprises; typically comprising large orders to corporate industrial clients, covering a size range from building a process facility or pipeline down to cylinder gas supply.

Some trade scale business is done, typically through tied local agents who are supplied wholesale. This business covers the sale or hire of gas cylinders and associated equipment to tradesmen and occasionally the general public. This includes products such as balloon helium, dispensing gases for beer kegs, welding gases and welding equipment, LPG and medical oxygen.

Retail sales of small scale gas supply are not confined to just the industrial gas companies or their agents. A wide variety of hand-carried small gas containers, which may be called cylinders, bottles, cartridges, capsules or canisters are available to supply LPG, butane, propane, carbon dioxide or nitrous oxide.  Examples are whipped-cream chargers, powerlets, campingaz and sodastream.

 The first gas from the natural environment used by humans was almost certainly air when it was discovered that blowing on or fanning a fire made it burn brighter. Humans also used the warm gases from a fire to smoke foods and steam from boiling water to cook foods.

Carbon dioxide has been known from ancient times as the byproduct of fermentation, particularly for beverages, which was first documented dating from 7000 to 6600 B.C. in Jiahu, China.[2] Natural gas was used by the Chinese in about 500 B.C. when they discovered the potential to transport gas seeping from the ground in crude pipelines of bamboo to where it was used to boil sea water.[3] Sulfur dioxide was used by the Romans in winemaking as it had been discovered that burning candles made of sulfur [4] inside empty wine vessels would keep them fresh and prevent them gaining a vinegar smell.[5]

Early understanding consisted of empirical evidence and the protoscience of alchemy; however with the advent of scientific method[6]  and the science of chemistry, these gases became positively identified and understood.

The history of chemistry tells us that a number of gases were identified and either discovered or first made in relatively pure form during the Industrial Revolution of the 18th and 19th centuries by notable chemists in their laboratories. The timeline of attributed discovery for various gases are carbon dioxide (1754),[7] hydrogen (1766),[8][9] nitrogen (1772),[8] nitrous oxide (1772),[10] oxygen (1773),[8][11][12] ammonia (1774),[13]  chlorine (1774),[8] methane (1776),[14]  hydrogen sulfide (1777),[15] carbon monoxide (1800),[16] hydrogen chloride (1810),[17] acetylene (1836),[18] helium (1868) [8][19] fluorine (1886),[8] argon (1894),[8] krypton, neon and xenon (1898) 
[8] and radon (1899).[8]

Carbon dioxide, hydrogen, nitrous oxide, oxygen, ammonia, chlorine, sulfur dioxide and manufactured fuel gas were already being used during the 19th century, and mainly had uses in food, refrigeration, medicine, and for fuel and gas lighting.[20] For example, carbonated water was being made from 1772 and commercially from 1783, chlorine was first used to bleach textiles in 1785 [21] and nitrous oxide was first used for dentistry anaesthesia in 1844.[10] At this time gases were often generated for immediate use by chemical reactions. A notable example of a generator is Kipps apparatus which was invented in 1844 [22] and could be used to generate gases such as hydrogen, hydrogen sulfide, chlorine, acetylene and carbon dioxide by simple gas evolution reactions. Acetylene was manufactured commercially from 1893 and acetylene generators were used from about 1898 to produce gas for gas cooking and gas lighting, however electricity took over as more practical for lighting and once LPG was produced commercially from 1912, the use of acetylene for cooking declined.[20]

Once gases had been discovered and produced in modest quantities, the process of industrialisation spurred on innovation and invention of technology to produce larger quantities of these gases. Notable developments in the industrial production of gases include the electrolysis of water to produce hydrogen (in 1869) and oxygen (from 1888), the Brin process for oxygen production which was invented in the 1884, the chloralkali process to produce chlorine in 1892 and the Haber Process to produce ammonia in 1908.[23]

The development of uses in refrigeration also enabled advances in air conditioning and the liquefaction of gases. Carbon dioxide was first liquefied in 1823. The first Vapor-compression refrigeration cycle using ether was invented by Jacob Perkins in 1834 and a similar cycle using ammonia was invented in 1873 and another with sulfur dioxide in 1876.[20] Liquid oxygen and Liquid nitrogen were both first made in 1883; Liquid hydrogen was first made in 1898 and liquid helium in 1908. LPG was first made in 1910. A patent for LNG was filed in 1914 with the first commercial production in 1917.[24]

Although no one event marks the beginning of the industrial gas industry, many would take it to be the 1880s with the construction of the first high pressure gas cylinders.[20] Initially cylinders were mostly used for carbon dioxide in carbonation or dispensing of beverages. 
In 1895 refrigeration compression cycles were further developed to enable the liquefaction of air,[25] most notably by Carl von Linde[26] allowing larger quantities of oxygen production and in 1896 the discovery that large quantities of acetylene could be dissolved in acetone and rendered nonexplosive allowed the safe bottling of acetylene.[27]

A particularly important use was the development of welding and metal cutting done with oxygen and acetylene from the early 1900s.
As production processes for other gases were developed many more gases came to be sold in cylinders without the need for a gas generator.

Air separation plants refine air in a separation process and so allow the bulk production of nitrogen and argon in addition to oxygen - these three are often also produced as cryogenic liquid. To achieve the required low distillation temperatures, an Air Separation Unit (ASU) uses a refrigeration cycle that operates by means of the Joule–Thomson effect. 
In addition to the main air gases, air separation is also the only practical source for production of the rare noble gases neon, krypton and xenon.

Cryogenic technologies also allow the liquefaction  of natural gas, hydrogen  and helium. In natural-gas processing, cryogenic technologies are used to remove nitrogen from natural gas in a Nitrogen Rejection Unit; a process that can also be used to produce helium from natural gas where natural gas fields contain sufficient helium to make this economic. The larger industrial gas companies have often invested in extensive patent libraries in all fields of their business, but particularly in cryogenics.

The other principal production technology in the industry is Reforming. Steam reforming is a chemical process used to convert natural gas and steam into a syngas containing hydrogen and carbon monoxide with carbon dioxide as a byproduct. Partial oxidation and autothermal reforming are similar processes but these also require oxygen from an ASU. Synthesis gas is often a precursor to the chemical synthesis of ammonia or methanol. The carbon dioxide produced is an acid gas and is most commonly removed by amine treating. This separated carbon dioxide can potentially be sequestrated to a carbon capture reservoir or used for Enhanced oil recovery.

Air Separation and hydrogen reforming technologies are the cornerstone of the industrial gases industry and also form part of the technologies required for many fuel gasification ( including IGCC), cogeneration and Fischer-Tropsch gas to liquids schemes. Hydrogen has many production methods and may be almost a carbon neutral alternative fuel if produced by water electrolysis (assuming the electricity is produced in nuclear or other low carbon footprint power plant instead of reforming natural gas which is by far dominant method). One example of displacing the use of hydrocarbons is Orkney;[28] see hydrogen economy for more information on hydrogen's uses.
Liquid hydrogen is used by NASA in the Space Shuttle as a rocket fuel. 

Simpler gas separation technologies, such as membranes or molecular sieves used in pressure swing adsorption or vacuum swing adsorption are also used to produce low purity air gases in nitrogen generators and oxygen plants. Other examples producing smaller amounts of gas are  chemical oxygen generators or oxygen concentrators.

In addition to the major gases produced by air separation and syngas reforming, the industry provides many other gases. Some gases are simply byproducts from other industries and others are sometimes bought from other larger chemical producers, refined and repackaged; although a few have their own production processes. Examples are hydrogen chloride produced by burning hydrogen in chlorine, nitrous oxide produced by thermal decomposition of ammonium nitrate when gently heated, electrolysis for the production of fluorine, chlorine and hydrogen, and electrical corona discharge to produce ozone from air or oxygen.

Related services and technology can be supplied such as vacuum, which is often provided in hospital gas systems; purified compressed air; or refrigeration. Another unusual system is the inert gas generator. Some industrial gas companies may also supply related chemicals, particularly liquids such as bromine, hydrogen fluoride and ethylene oxide.

Most materials that are gaseous at ambient temperature and pressure are supplied as compressed gas. A gas compressor is used to compress the gas into storage pressure vessels (such as gas canisters, gas cylinders or tube trailers) through piping systems.  Gas cylinders are by far the most common gas storage [29] and large numbers are produced at a ""cylinder fill"" facility.

However, not all industrial gases are supplied in the gaseous phase. A few gases are vapors that can be liquefied at ambient temperature under pressure alone, so they can also be supplied as a liquid in an appropriate container.  This phase change also makes these gases useful as ambient refrigerants and the most significant industrial gases with this property are ammonia (R717), propane (R290), butane (R600), and sulfur dioxide (R764). Chlorine also has this property but is too toxic, corrosive and reactive to ever have been used as a refrigerant. Some other gases exhibit this phase change if the ambient temperature is low enough; this includes ethylene (R1150), carbon dioxide (R744), ethane (R170), nitrous oxide (R744A), and sulfur hexafluoride; however, these can only be liquefied under pressure if kept below their critical temperatures which are 9 °C for C2H4 ; 31 °C for CO2 ; 32 °C for C2H6 ; 36 °C for N2O ; 45 °C for SF6.[30] All of these substances are also provided as a gas (not a vapor) at the 200 bar pressure in a gas cylinder because that pressure is above their critical pressure.[30]

Permanent gases (those with a critical temperature below ambient) can only be supplied as liquid if they are also cooled. All gases can potentially be used as a refrigerant around the temperatures at which they are liquid; for example nitrogen (R728) and methane (R50) are used as refrigerant at cryogenic temperatures.[25]

Exceptionally carbon dioxide can be produced as a cold solid known as dry ice, which sublimes as it warms in ambient conditions, the properties of carbon dioxide are such that it cannot be liquid at a pressure below its triple point of 5.1 bar.[30]

Acetylene is also supplied differently. Since it is so unstable and explosive, this is supplied as a gas dissolved in acetone within a packing mass in a cylinder. Acetylene is also the only other common industrial gas that sublimes at atmospheric pressure.[30]

The major industrial gases can be produced in bulk and delivered to customers by pipeline, but can also be packaged and transported.

Most gases are sold in gas cylinders and some sold as liquid in appropriate containers (e.g. Dewars) or as bulk liquid delivered by truck. The industry originally supplied gases in cylinders to avoid the need for local gas generation; but for large customers such as steelworks or oil refineries, a large gas production plant may be built nearby (typically called an ""on-site"" facility) to avoid using large numbers of cylinders manifolded together. Alternatively, an industrial gas company may supply the plant and equipment to produce the gas rather than the gas itself. An industrial gas company may also offer to act as plant operator under an operations and maintenance contract for a gases facility for a customer, since it usually has the experience of running such facilities for the production or handling of gases for itself.

Some materials are dangerous to use as a gas; for example, fluorine is highly reactive and industrial chemistry requiring fluorine often uses hydrogen fluoride (or hydrofluoric acid) instead. Another approach to overcoming gas reactivity is to generate the gas as and when required, which is done, for example, with ozone.

The delivery options are therefore local gas generation, pipelines, bulk transport (truck, rail, ship), and packaged gases in gas cylinders or other containers.[1]

Bulk liquid gases are often transferred to end user storage tanks. Gas cylinders (and liquid gas containing vessels) are  often used by end users for their own small scale distribution systems. Toxic or flammable gas cylinders are often stored by end users in gas cabinets for protection from external fire or from any leak.

Despite attempts at standardization to facilitate user and first responders' safety, no universal coding exists for cylinders with industrial gases, therefore several color coding standards are in usage. In most developed countries of the world, notably countries of European union and United Kingdom, EN 1089-3 is used, with cylinders of liquefied petroleum gas being an exception.

In United States of America, no official regulation of color coding for gas cylinders exists and none is enforced.[31]

Industrial gas is a group of materials that are specifically manufactured for use in industry and are also gaseous at ambient temperature and pressure.  They are chemicals which can be an elemental gas or a chemical compound that is either organic or inorganic, and tend to be low molecular weight molecules. They could also be a mixture of individual gases. They have value as a chemical; whether as a feedstock, in process enhancement, as a useful end product, or for a particular use; as opposed to having value as a ""simple"" fuel.

The term “industrial gases” [32] is sometimes narrowly defined as just the major gases sold, which are: nitrogen, oxygen, carbon dioxide, argon, hydrogen, acetylene and helium.[33]  Many names are given to gases outside of this main list by the different industrial gas companies, but generally the gases fall into the categories ""specialty gases"", “medical gases”, “fuel gases” or “refrigerant gases”. However gases can also be known by their uses or industries that they serve, hence ""welding gases"" or ""breathing gases"", etc.; or by their source, as in ""air gases""; or by their mode of supply as in ""packaged gases"". The major gases might also be termed ""bulk gases"" or ""tonnage gases"".

In principle any gas or gas mixture sold by the ""industrial gases industry"" probably has some industrial use and might be termed an ""industrial gas"". In practice, ""industrial gases"" are likely to be a pure compound or a mixture of precise chemical composition, packaged or in small quantities, but with high purity or tailored to a specific use (e.g. oxyacetylene).
Lists of the more significant gases are listed in ""The Gases"" below.

There are cases when a gas is not usually termed an ""industrial gas""; principally where the gas is processed for later use of its energy rather than manufactured for use as a chemical substance or preparation.

The oil and gas industry is seen as distinct. So, whilst it is true that natural gas is a ""gas"" used in ""industry"" - often as a fuel, sometimes as a feedstock, and in this generic sense is an ""industrial gas""; this term is not generally used by industrial enterprises for hydrocarbons produced by the petroleum industry directly from natural resources or in an oil refinery. Materials such as LPG and LNG are complex mixtures often without precise chemical composition that often also changes whilst stored.

The petrochemical industry is also seen as distinct. So petrochemicals (chemicals derived from petroleum) such as ethylene are also generally not described as ""industrial gases"".

Sometimes the chemical industry is thought of as distinct from industrial gases; so materials such as ammonia and chlorine might be considered ""chemicals"" (especially if supplied as a liquid) instead of or sometimes as well as ""industrial gases"".

Small scale gas supply of hand-carried containers is sometimes not considered to be industrial gas as the use is considered personal rather than industrial; and suppliers are not always gas specialists.

These demarcations are based on perceived boundaries of these industries (although in practice there is some overlap), and an exact scientific definition is difficult. To illustrate ""overlap"" between industries:

Manufactured fuel gas (such as town gas) would historically have been considered an industrial gas. Syngas is often considered to be a petrochemical; although its production is a core industrial gases technology. Similarly, projects harnessing Landfill gas or biogas, Waste-to-energy schemes, as well as Hydrogen Production all exhibit overlapping technologies.

Helium is an industrial gas, even though its source is from natural gas processing.

Any gas is likely to be considered an industrial gas if it is put in a gas cylinder (except perhaps if it is used as a fuel)

Propane would be considered an industrial gas when used as a refrigerant, but not when used as a refrigerant in LNG production, even though this is an overlapping technology.

The known chemical elements which are, or can be obtained from natural resources (without transmutation) and which are gaseous are hydrogen, nitrogen, oxygen, fluorine, chlorine, plus the noble gases; and are collectively referred to by chemists as the ""elemental gases"".[34] These elements are all primordial apart from the noble gas radon which is a trace radioisotope which occurs naturally since all isotopes are radiogenic nuclides from radioactive decay. These elements are all nonmetals.

(Synthetic elements have no relevance to the industrial gas industry; however for scientific completeness, note that it has been suggested, but not scientifically proven, that metallic elements 112 (Copernicium) and 114 (Flerovium) are gases.[35])

The elements which are stable two atom homonuclear molecules at standard temperature and pressure (STP), are hydrogen (H2), nitrogen (N2) and oxygen (O2), plus the halogens fluorine (F2) and chlorine (Cl2). The noble gases are all monatomic.

In the industrial gases industry the term ""elemental gases"" (or sometimes less accurately ""molecular gases"") is used to distinguish these gases from molecules that are also chemical compounds.

Radon is chemically stable, but it is radioactive and does not have a stable isotope. Its most stable isotope, 222Rn, has a half-life of 3.8 days. Its uses are due to its radioactivity rather than its chemistry and it requires specialist handling outside of industrial gas industry norms. It can however be produced as a by-product of uraniferous ores processing. Radon is a trace naturally occurring radioactive material (NORM) encountered in the air processed in an ASU.

Chlorine is the only elemental gas that is technically a vapor since STP is below its critical temperature; whilst 
bromine and mercury are liquid at STP, and so their vapor exists in equilibrium with their liquid at STP.

This list shows the other most common gases sold by industrial gas companies.[1]

There are many gas mixtures possible.

This list shows the most important liquefied gases:[1]

The uses of industrial gases are diverse.

The following is a small list of areas of use:
"
Ink Production Services,"Ink is a gel, sol, or solution that contains at least one colorant, such as a dye or pigment, and is used to color a surface to produce an image, text, or design. Ink is used for drawing or writing with a pen, brush, reed pen, or quill. Thicker inks, in paste form, are used extensively in letterpress and lithographic printing.

Ink can be a complex medium, composed of solvents, pigments, dyes, resins, lubricants, solubilizers, surfactants, particulate matter, fluorescents, and other materials. The components of inks serve many purposes; the ink's carrier, colorants, and other additives affect the flow and thickness of the ink and its dry appearance.

 Many ancient cultures around the world have independently discovered and formulated inks due to the need to write and draw. The recipes and techniques for the production of ink are derived from archaeological analyses or from written texts itself. The earliest inks from all civilizations are believed to have been made with lampblack, a kind of soot, easily collected as a by-product of fire.[4]

Ink was used in Ancient Egypt for writing and drawing on papyrus from at least the 26th century BC.[5] Egyptian red and black inks included iron and ocher as pigments, in addition to phosphate, sulfate, chloride, and carboxylate ions, with lead also used as a drier.[6]

The earliest Chinese inks may date to[7] four millennia ago,[8] to the Chinese Neolithic Period. These included plant, animal, and mineral inks, based on such materials as graphite; these were ground with water and applied with ink brushes. Direct evidence for the earliest Chinese inks, similar to modern inksticks, is found around 256 BC, in the end of the Warring States period; being produced from soot and animal glue.[9] The preferred inks for drawing or painting on paper or silk are produced from the resin of the pine trees between 50 and 100 years old. The Chinese inkstick is produced with a fish glue, whereas Japanese glue (膠 nikawa) is from cow or stag.[10]

India ink was invented in China,[11][12] though materials were often traded from India, hence the name.[11][12] The traditional Chinese method of making the ink was to grind a mixture of hide glue, carbon black, lampblack, and bone black pigment with a pestle and mortar, then pour it into a ceramic dish to dry.[11] To use the dry mixture, a wet brush would be applied until it reliquified.[11] The manufacture of India ink was well-established by the Cao Wei dynasty (220–265 AD).[13] Indian documents written in Kharosthi with ink have been unearthed in Xinjiang.[14] The practice of writing with ink and a sharp pointed needle was common in early South India.[2] Several Buddhist and Jain sutras in India were compiled in ink.[3]

Cephalopod ink, known as sepia, turns from dark blue-black to brown on drying, and was used as an ink in the Graeco-Roman period and subsequently. Black atramentum was also used in ancient Rome; in an article for The Christian Science Monitor, Sharon J. Huntington describes these other historical inks:

About 1,600 years ago, a popular ink recipe was created. The recipe was used for centuries. Iron salts, such as ferrous sulfate (made by treating iron with sulfuric acid), were mixed with tannin from gallnuts (they grow on trees) and a thickener. When first put to paper, this ink is bluish-black. Over time it fades to a dull brown.
Scribes in medieval Europe (about AD 800 to 1500) wrote principally on parchment or vellum. One 12th century ink recipe called for hawthorn branches to be cut in the spring and left to dry. Then the bark was pounded from the branches and soaked in water for eight days. The water was boiled until it thickened and turned black. Wine was added during boiling. The ink was poured into special bags and hung in the sun. Once dried, the mixture was mixed with wine and iron salt over a fire to make the final ink.[15]
The reservoir pen, which may have been the first fountain pen, dates back to 953, when Ma'ād al-Mu'izz, the caliph of Egypt, demanded a pen that would not stain his hands or clothes, and was provided with a pen that held ink in a reservoir.[16]

In the 15th century, a new type of ink had to be developed in Europe for the printing press by Johannes Gutenberg.[17] According to Martyn Lyons in his book Books: A Living History, Gutenberg's dye was indelible, oil-based, and made from the soot of lamps (lamp-black) mixed with varnish and egg white.[18] Two types of ink were prevalent at the time: the Greek and Roman writing ink (soot, glue, and water) and the 12th century variety composed of ferrous sulfate, gall, gum, and water.[19] Neither of these handwriting inks could adhere to printing surfaces without creating blurs. Eventually an oily, varnish-like ink made of soot, turpentine, and walnut oil was created specifically for the printing press.

Ink formulas vary, but commonly involve two components:

Inks generally fall into four classes:[20]

Pigment inks are used more frequently than dyes because they are more color-fast, but they are also more expensive, less consistent in color, and have less of a color range than dyes.[20]
Pigments are solid, opaque particles suspended in ink to provide color.[20] Pigment molecules typically link together in crystalline structures that are 0.1–2 μm in size and comprise 5–30 percent of the ink volume.[20] Qualities such as hue, saturation, and lightness vary depending on the source and type of pigment.Solvent-based inks are widely used for high-speed printing and applications that require quick drying times. And the inclusion of TiO2 powder provides superior coverage and vibrant colors.[21]

Dye-based inks are generally much stronger than pigment-based inks and can produce much more color of a given density per unit of mass. However, because dyes are dissolved in the liquid phase, they have a tendency to soak into paper, potentially allowing the ink to bleed at the edges of an image.

To circumvent this problem, dye-based inks are made with solvents that dry rapidly or are used with quick-drying methods of printing, such as blowing hot air on the fresh print. Other methods include harder paper sizing and more specialized paper coatings. The latter is particularly suited to inks used in non-industrial settings (which must conform to tighter toxicity and emission controls), such as inkjet printer inks. Another technique involves coating the paper with a charged coating. If the dye has the opposite charge, it is attracted to and retained by this coating, while the solvent soaks into the paper. Cellulose, the wood-derived material most paper is made of, is naturally charged, and so a compound that complexes with both the dye and the paper's surface aids retention at the surface. Such a compound is commonly used in ink-jet printing inks.

An additional advantage of dye-based ink systems is that the dye molecules can interact with other ink ingredients, potentially allowing greater benefit as compared to pigmented inks from optical brighteners and color-enhancing agents designed to increase the intensity and appearance of dyes.

Dye-based inks can be used for anti-counterfeit purposes and can be found in some gel inks, fountain pen inks, and inks used for paper currency.[22] These inks react with cellulose to bring about a permanent color change.[22] Dye based inks are used to color hair.

There is a misconception that ink is non-toxic even if swallowed. Once ingested, ink can be hazardous to one's health. Certain inks, such as those used in digital printers, and even those found in a common pen can be harmful. Though ink does not easily cause death, repeated skin contact or ingestion can cause effects such as severe headaches, skin irritation, or nervous system damage.[23] These effects can be caused by solvents, or by pigment ingredients such as p-Anisidine, which helps create some inks' color and shine.

Three main environmental issues with ink are:

Some regulatory bodies[specify] have set standards for the amount of heavy metals in ink.[24] There is a trend toward vegetable oils rather than petroleum oils in recent years in response to a demand for better environmental sustainability performance.

Ink uses up non-renewable oils and metals, which has a negative impact on the environment.[25]

Carbon inks were commonly made from lampblack or soot and a binding agent such as gum arabic or animal glue. The binding agent keeps carbon particles in suspension and adhered to paper. Carbon particles do not fade over time even when bleached or when in sunlight. One benefit is that carbon ink does not harm paper. Over time, the ink is chemically stable and therefore does not threaten the paper's strength. Despite these benefits, carbon ink is not ideal for permanence and ease of preservation. Carbon ink tends to smudge in humid environments and can be washed off surfaces. The best method of preserving a document written in carbon ink is to store it in a dry environment (Barrow 1972).

Recently, carbon inks made from carbon nanotubes have been successfully created. They are similar in composition to traditional inks in that they use a polymer to suspend the carbon nanotubes. These inks can be used in inkjet printers and produce electrically conductive patterns.[26]

Iron gall inks became prominent in the early 12th century; they were used for centuries and were widely thought to be the best type of ink. However, iron gall ink is corrosive and damages paper over time (Waters 1940). Items containing this ink can become brittle and the writing fades to brown. The original scores of Johann Sebastian Bach are threatened by the destructive properties of iron gall ink. The majority of his works are held by the German State Library, and about 25% of those are in advanced stages of decay (American Libraries 2000). The rate at which the writing fades is based on several factors, such as proportions of ink ingredients, amount deposited on the paper, and paper composition (Barrow 1972:16). Corrosion is caused by acid catalyzed hydrolysis and iron(II)-catalysed oxidation of cellulose (Rouchon-Quillet 2004:389).

Treatment is a controversial subject. No treatment undoes damage already caused by acidic ink. Deterioration can only be stopped or slowed. Some[who?] think it best not to treat the item at all for fear of the consequences. Others believe that non-aqueous procedures are the best solution. Yet others think an aqueous procedure may preserve items written with iron gall ink. Aqueous treatments include distilled water at different temperatures, calcium hydroxide, calcium bicarbonate, magnesium carbonate, magnesium bicarbonate, and calcium hyphenate. There are many possible side effects from these treatments. There can be mechanical damage, which further weakens the paper. Paper color or ink color may change, and ink may bleed. Other consequences of aqueous treatment are a change of ink texture or formation of plaque on the surface of the ink (Reibland & de Groot 1999).

Iron gall inks require storage in a stable environment, because fluctuating relative humidity increases the rate that formic acid, acetic acid, and furan derivatives form in the material the ink was used on. Sulfuric acid acts as a catalyst to cellulose hydrolysis, and iron (II) sulfate acts as a catalyst to cellulose oxidation. These chemical reactions physically weaken the paper, causing brittleness.[27]

Indelible means ""un-removable"". Some types of indelible ink have a very short shelf life because of the quickly evaporating solvents used. India, Mexico, Indonesia, Malaysia and other developing countries have used indelible ink in the form of electoral stain to prevent electoral fraud. Election ink based on silver nitrate was first applied in the 1962 Indian general election, after being developed at the National Physical Laboratory of India.

The election commission in India has used indelible ink for many elections. Indonesia used it in its election in 2014.[28] In Mali, the ink is applied to the fingernail. The technique is not infallible and can itself be used in other types of fraud, as rather than bolstering one's own votes it can be used to eliminate opponent voters by marking them before they have chances to cast their votes. There are also reports of ""indelible"" ink washing off voters' fingers in Afghanistan.[29]
"
Plastic Manufacturing,"

Plastics are a wide range of synthetic or semisynthetic materials that use polymers as a main ingredient. Their plasticity makes it possible for plastics to be molded, extruded, or pressed into solid objects of various shapes. This adaptability, combined with a wide range of other properties, such as being lightweight, durable, flexible, nontoxic, and inexpensive to produce, has led to their widespread use around the world.[1] Most plastics are derived from natural gas and petroleum, and a small fraction from renewable materials, with one such material being polylactic acid.[2]

Between 1950 and 2017, 9.2 billion metric tons of plastic are estimated to have been made, with more than half of this amount being produced since 2004. In 2023 alone, preliminary figures indicate that over 400 million metric tons of plastic were produced worldwide.[3] If global trends in plastic demand continue, it is projected that annual global plastic production will exceed 1.3 billion tons by 2060.[3] The primary uses for plastic include packaging, which makes up about 40% of its usage, and building and construction, which makes up about 20% of its usage.[1]

The success and dominance of plastics since the early 20th century has had major benefits for mankind, ranging from medical devices to light-weight construction materials.  The sewage systems in many countries relies on the resiliency and adaptability of polyvinyl chloride.  It is also true that plastics are the basis of widespread environmental concerns,[4] due to their slow decomposition rate in natural ecosystems. Most plastic produced has not been reused.  Some is unsuitable for reuse.  Much is captured in landfills or as plastic pollution.  Particular concern focuses on microplastics. Marine plastic pollution, for example, creates garbage patches. Of all the plastic discarded so far, some 14% has been incinerated and less than 10% has been recycled.[5]

In developed economies, about a third of plastic is used in packaging and roughly the same in buildings in applications such as piping, plumbing or vinyl siding.[6] Other uses include automobiles (up to 20% plastic[6]), furniture, and toys.[6] In the developing world, the applications of plastic may differ; 42% of India's consumption is used in packaging.[6] Worldwide, about 50 kg of plastic is produced annually per person, with production doubling every ten years.

The world's first fully synthetic plastic was Bakelite, invented in New York in 1907, by Leo Baekeland,[7] who coined the term ""plastics"".[8] Dozens of different types of plastics are produced today, such as polyethylene, which is widely used in product packaging, and polyvinyl chloride (PVC), used in construction and pipes because of its strength and durability. Many chemists have contributed to the materials science of plastics, including Nobel laureate Hermann Staudinger, who has been called ""the father of polymer chemistry"", and Herman Mark, known as ""the father of polymer physics"".[9]

The word plastic derives from the Ancient Greek πλαστικός (plastikos), meaning ""capable of being shaped or molded."" In turn, this derives from πλαστός (plastos), meaning ""molded.""[10] As a noun, the word plastic most commonly refers to the solid products of petrochemical-derived manufacturing.[11]

The word plasticity, as a noun, specifically refers to the deformability of the materials used in the manufacture of plastics. Plasticity allows molding, extrusion, or compression into a variety of shapes, including films, fibers, plates, tubes, bottles, and boxes, among many others. In materials science, plasticity also has a more technical definition, describing the nonreversible change in form of solid substances when subjected to external forces. However, this definition extends beyond the scope of this article.

Most plastics contain organic polymers.[12] The vast majority of these polymers are formed from chains of carbon atoms, with or without the attachment of oxygen, nitrogen or sulfur atoms. These chains comprise many repeating units formed from monomers. Each polymer chain consists of several thousand repeating units. The backbone is the part of the chain that is on the main path, linking together a large number of repeat units. To customize the properties of a plastic, different molecular groups called side chains hang from this backbone; they are usually attached to the monomers before the monomers themselves are linked together to form the polymer chain. The structure of these side chains influences the properties of the polymer.

Plastics are usually classified by the chemical structure of the polymer's backbone and side chains. Important groups classified in this way include the acrylics, polyesters, silicones, polyurethanes, and halogenated plastics. Plastics can be classified by the chemical process used in their synthesis, such as condensation, polyaddition, and cross-linking.[13] They can also be classified by their physical properties, including hardness, density, tensile strength, thermal resistance, and glass transition temperature. Plastics can additionally be classified by their resistance and reactions to various substances and processes, such as exposure to organic solvents, oxidation, and ionizing radiation.[14] Other classifications of plastics are based on qualities relevant to manufacturing or product design for a particular purpose. Examples include thermoplastics, thermosets, conductive polymers, biodegradable plastics, engineering plastics and elastomers.

One important classification of plastics is the degree to which the chemical processes used to make them are reversible or not.

Thermoplastics do not undergo chemical change in their composition when heated and thus can be molded repeatedly. Examples include polyethylene (PE), polypropylene (PP), polystyrene (PS), and polyvinyl chloride (PVC).[15]

Thermosets, or thermosetting polymers, can melt and take shape only once: after they have solidified, they stay solid and retain their shape permanently.[16] If reheated, thermosets decompose rather than melt. Examples of thermosets include epoxy resin, polyimide, and Bakelite. The vulcanization of rubber is an example of this process. Before heating in the presence of sulfur, natural rubber (polyisoprene) is a sticky, slightly runny material, and after vulcanization, the product is dry and rigid.

Approximately 80% of global plastic production includes commodity plastics, a type of plastics primarily chosen for their low cost and ease of manufacturing. These plastics are mass-produced and used in everyday applications such as packaging, food containers, and household products. Most commodity plastics are identifiable by their Resin Identification Codes (RICs), a standardized numbering system developed by ASTM International.

Beyond the six most widely recognized listed above, there are more commodity plastics that are also mass-produced and commonly used, such as polyurethanes (PURs). PURs are a class of plastics also designated as commodity plastics due to their low cost, ease of manufacturing, and versatility. However, they lack RICs because they encompass many chemically diverse formulations such as foams and adhesives.

Packaging represents the largest application of commodity plastics, consuming 146 million metric tons (36% of global production) in 2015 alone. Beyond packaging, however, these plastics are critical in various other fields such as agriculture, construction, consumer goods, and healthcare.

Although many traits such as durability and resistance to biodegradability are desirable in various applications, they have led to significant environmental issues. An estimated 8 to 12 million tons of plastic enter oceans annually, primarily from mismanaged packaging waste. Commodity plastics account for the majority of this pollution, as their recycling rates remain low (e.g., only ~9% of all plastics are recycled globally). Microplastics derived from their degradation further threaten ecosystems and human health.

A huge number of plastics exist beyond the commodity plastics, with many having exceptional properties.

Engineering plastics are more robust and are used to manufacture products such as vehicle parts, building and construction materials, and some machine parts. In some cases, they are polymer blends formed by mixing different plastics together (ABS, HIPS etc.). Engineering plastics can replace metals in vehicles, lowering their weight and improving fuel efficiency by 6–8%. Roughly 50% of the volume of modern cars is made of plastic, but this only accounts for 12–17% of the vehicle weight.[20]

High-performance plastics are a category of polymers exhibiting superior properties compared to commodity and engineering plastics. These plastics can withstand high temperatures, often above 302°F (150°C), are highly resistant to chemical corrosion and degradation, have excellent mechanical and electric properties, and are lightweight and extremely versatile.

Many plastics are completely amorphous, meaning they lack a highly ordered molecular structure.[22] Crystalline plastics exhibit a pattern of more regularly spaced atoms, such as high-density polyethylene (HDPE), polybutylene terephthalate (PBT), and polyether ether ketone (PEEK). However, some plastics are partially amorphous and partially crystalline in molecular structure, giving them both a melting point and one or more glass transitions (the temperature above which the extent of localized molecular flexibility is substantially increased). These so-called semi-crystalline plastics include polyethylene, polypropylene, polyvinyl chloride, polyamides (nylons), polyesters and some polyurethanes.

Intrinsically conducting polymers (ICPs) are organic polymers that conduct electricity. While a conductivity of up to 80 kilosiemens per centimeter (kS/cm) in stretch-oriented polyacetylene[23] has been achieved, it does not approach that of most metals. For example, copper has a conductivity of several hundred kS/cm.[24]

Biodegradable plastics are plastics that degrade (break down) upon exposure to biological factors, such as sunlight, ultra-violet radiation, moisture, bacteria, enzymes, or wind abrasion. Attacks by insects, such as waxworms and mealworms, can also be considered forms of biodegradation. Aerobic degradation requires the plastic to be exposed at the surface, whereas anaerobic degradation would be effective in landfill or composting systems. Some companies produce biodegradable additives to further promote biodegradation. Although starch powder can be added as a filler to facilitate degradation of some plastics, such treatment does not lead to complete breakdown. Some researchers have genetically engineered bacteria to synthesize completely biodegradable plastics, such as polyhydroxybutyrate (PHB); however, As of 2021,[update] these were still relatively expensive.[25]

While most plastics are produced from petrochemicals, bioplastics are made substantially from renewable plant materials like cellulose and starch.[26] Due both to the finite limits of fossil fuel reserves and to rising levels of greenhouse gases caused primarily by the burning of those fuels, the development of bioplastics is a growing field.[27][28] Global production capacity for bio-based plastics is estimated at 327,000 tonnes per year. In contrast, global production of polyethylene (PE) and polypropylene (PP), the world's leading petrochemical-derived polyolefins, was estimated at over 150 million tonnes in 2015.[29]

The plastic industry includes the global production, compounding, conversion and sale of plastic products. Although the Middle East and Russia produce most of the required petrochemical raw materials, the production of plastic is concentrated in the global East and West. The plastic industry comprises a huge number of companies and can be divided into several sectors:

Between 1950 and 2017, 9.2 billion tonnes of plastic are estimated to have been made, with more than half of this having been produced since 2004. Since the birth of the plastic industry in the 1950s, global production has increased enormously, reaching 400 million tonnes a year in 2021; this is up from 381 million metric tonnes in 2015 (excluding additives).[5][17] From the 1950s, rapid growth occurred in the use of plastics for packaging, in building and construction, and in other sectors.[5] If global trends on plastic demand continue, it is estimated that by 2050 annual global plastic production will exceed 1.1-billion tonnes annually.[5]

Plastics are produced in chemical plants by the polymerization of their starting materials (monomers); which are almost always petrochemical in nature. Such facilities are normally large and are visually similar to oil refineries, with sprawling pipework running throughout. The large size of these plants allows them to exploit economies of scale. Despite this, plastic production  is not particularly monopolized, with about 100 companies accounting for 90% of global production.[30] This includes a mixture of private and state-owned enterprises. Roughly half of all production takes place in East Asia, with China being the largest single producer. Major international producers include:

Historically, Europe and North America have dominated global plastics production. However, since 2010 Asia has emerged as a significant producer, with China accounting for 31% of total plastic resin production in 2020.[31] Regional differences in the volume of plastics production are driven by user demand, the price of fossil fuel feedstocks, and investments made in the petrochemical industry. For example, since 2010 over US$200 billion has been invested in the United States in new plastic and chemical plants, stimulated by the low cost of raw materials. In the European Union (EU), too, heavy investments have been made in the plastics industry, which employs over 1.6-million people with a turnover of more than 360 billion euros per year. In China in 2016 there were over 15,000 plastic manufacturing companies, generating more than US$366 billion in revenue.[5]

In 2017, the global plastics market was dominated by thermoplastics– polymers that can be melted and recast. Thermoplastics include polyethylene (PE), polyethylene terephthalate (PET), polypropylene (PP), polyvinyl chloride (PVC), polystyrene (PS) and synthetic fibers, which together represent 86% of all plastics.[5]

Plastic is not sold as a pure unadulterated substance, but is instead mixed with various chemicals and other materials, which are collectively known as additives. These are added during the compounding stage and include substances such as stabilizers, plasticizers and dyes, which are intended to improve the lifespan, workability or appearance of the final item. In some cases, this can involve mixing different types of plastic together to form a polymer blend, such as high impact polystyrene. Large companies may do their own compounding prior to production, but some producers have it done by a third party. Companies that specialize in this work are known as Compounders.

The compounding of thermosetting plastic is relatively straightforward; as it remains liquid until it is cured into its final form. For thermosoftening materials, which are used to make the majority of products, it is necessary to melt the plastic in order to mix-in the additives. This involves heating it to anywhere between 150–320 °C (300–610 °F). Molten plastic is viscous and exhibits laminar flow, leading to poor mixing. Compounding is therefore done using extrusion equipment, which is able to supply the necessary heat and mixing to give a properly dispersed product.

The concentrations of most additives are usually quite low, however high levels can be added to create Masterbatch products. The additives in these are concentrated but still properly dispersed in the host resin. Masterbatch granules can be mixed with cheaper bulk polymer and will release their additives during processing to give a homogeneous final product. This can be cheaper than working with a fully compounded material and is particularly common for the introduction of color.

Converters (sometimes known as processors) are companies or specialists that fabricate finished plastic products from raw materials, often in the form of resins, pellets, or films.

For thermosetting materials, the process is slightly different, as the plastics are liquid to begin with and but must be cured to give solid products, but much of the equipment is broadly similar.

The most commonly produced plastic consumer products include packaging made from LDPE (e.g. bags, containers, food packaging film), containers made from HDPE (e.g. milk bottles, shampoo bottles, ice cream tubs), and PET (e.g. bottles for water and other drinks). Together these products account for around 36% of plastics use in the world. Most of them (e.g. disposable cups, plates, cutlery, takeaway containers, carrier bags) are used for only a short period, many for less than a day. The use of plastics in building and construction, textiles, transportation and electrical equipment also accounts for a substantial share of the plastics market. Plastic items used for such purposes generally have longer life spans. They may be in use for periods ranging from around five years (e.g. textiles and electrical equipment) to more than 20 years (e.g. construction materials, industrial machinery).[5]


Plastic consumption differs among countries and communities, with some form of plastic having made its way into most people's lives. North America (i.e. the North American Free Trade Agreement or NAFTA region) accounts for 21% of global plastic consumption, closely followed by China (20%) and Western Europe (18%). In North America and Europe, there is high per capita plastic consumption (94 kg and 85 kg/capita/year, respectively). In China, there is lower per capita consumption (58 kg/capita/year), but high consumption nationally because of its large population.[5]
The largest application for plastics is as packaging materials, but they are used in a wide range of other sectors, including: construction (pipes, gutters, door and windows), textiles (stretchable fabrics, fleece), consumer goods (toys, tableware, toothbrushes), transportation (headlights, bumpers, body panels, wing mirrors), electronics (phones, computers, televisions) and as machine parts.[17] In optics, plastics are used to manufacture aspheric lenses.[33]



Additives are chemicals blended into plastics to improved their performance or appearance.[34][35] Additives are therefore one of the reasons why plastic is used so widely.[36] Plastics are composed of chains of polymers. Many different chemicals are used as plastic additives. A randomly chosen plastic product generally contains around 20 additives. The identities and concentrations of additives are generally not listed on products.[5]

In the EU, over 400 additives are used in high volumes.[37][5] In a global market analysis, 5,500 additives were found.[38] At a minimum, all plastic contains some polymer stabilizers which permit them to be melt-processed (molded) without suffering polymer degradation.Additives in polyvinyl chloride (PVC), used widely for sanitary plumbing, can constitute up to 80% of the total volume.[5] Unadulterated plastic (barefoot resin) is rarely sold.[citation needed]

Additives may be weakly bound to the polymers or react in the polymer matrix. Although additives are blended into plastic they remain chemically distinct from it and can gradually leach back out during normal use, when in landfills, or following improper disposal in the environment.[39] Additives may also degrade to form other compounds that could be more benign or more toxic. Plastic fragmentation into microplastics and nanoplastics can allow chemical additives to move in the environment far from the point of use. Once released, some additives and derivatives may persist in the environment and bioaccumulate in organisms. They can have adverse effects on human health and biota. A recent review by the United States Environmental Protection Agency (US EPA) revealed that out of 3,377 chemicals potentially associated with plastic packaging and 906 likely associated with it, 68 were ranked by ECHA as ""highest for human health hazards"" and 68 as ""highest for environmental hazards"".[5]

As additives change the properties of plastics they have to be considered during recycling. Presently, almost all recycling is performed by simply remelting and fabricating used plastic into new items. Additives present risks in recycled products due to their difficulty to remove. When plastic products are recycled, it is highly likely that the additives will be integrated into the new products. Plastic waste, even if it is all of the same polymer type, will contain varying types and amounts of additives. Mixing these together can give a material with inconsistent properties, which can be unappealing to industry. For example, mixing different colored plastics with different plastic colorants together can produce a discolored or brown material and for this reason plastic is usually sorted both by polymer type and color prior to recycling.[5]

Lack of transparency and reporting across the value chain often results in lack of knowledge concerning the chemical profile of the final products. For example, products containing brominated flame retardants have been incorporated into new plastic products. Flame retardants are a group of chemicals used in electronic and electrical equipment, textiles, furniture and construction materials which should not be present in food packaging or child care products. A recent study found brominated dioxins as unintentional contaminants in toys made from recycled plastic electronic waste that contained brominated flame retardants. Brominated dioxins have been found to exhibit toxicity similar to that of chlorinated dioxins. They can have negative developmental effects and negative effects on the nervous system and interfere with mechanisms of the endocrine system.[5]

Plastics have proliferated in part because they are relatively benign.  They are not acutely toxic, in large part because they are insoluble and or indigestible owing to their large molecular weight.  Their degradation products also are rarely toxic.  The same cannot be said about some additives, which tend to be lower molecular weight.

Controversies associated with plastics often relate to their additives, some of which are potentially harmful.[40][41][34] For example, some flame retardants, such as octabromodiphenyl ether and pentabromodiphenyl ether, are unsuitable for food packaging. Other harmful additives include cadmium, chromium, lead and mercury (regulated under the Minamata Convention on Mercury), which have previously been used in plastic production, are banned in many jurisdictions. However, they are still routinely found in some plastic packaging, including for food.[citation needed]

Additives can also be problematic if waste is burned, especially when burning is uncontrolled or takes place in low-technology incinerators, as is common in many developing countries. Incomplete combustion can cause emissions of hazardous substances such as acid gases and ash, which can contain persistent organic pollutants (POPs) such as dioxins.[5]

A number of additives identified as hazardous to humans and/or the environment are regulated internationally. The Stockholm Convention on Persistent Organic Pollutants is a global treaty to protect human health and the environment from chemicals that remain intact in the environment for long periods, become widely distributed geographically, accumulate in the fatty tissue of humans and wildlife, and have harmful impacts on human health or on the environment.[5] The use of bisphenol A (BPA) in plastic baby bottles is banned in many parts of the world but is not restricted in some low-income countries.[5]

In 2023, plasticosis, a new disease caused by the ingestion of plastic waste, was discovered in seabirds. Birds affected with this disease were found to have scarred and inflamed digestive tracts, which can impair their ability to digest food.[42]  ""When birds ingest small pieces of plastic, they found, it inflames the digestive tract. Over time, the persistent inflammation causes tissues to become scarred and disfigured, affecting digestion, growth and survival.""[43]

Plastics per se have low toxicity due to their insolubility in water and because they have a large molecular weight.  They are biochemically inert. Additives in plastic products can be more problemative.[45] For example, plasticizers like adipates and phthalates are often added to brittle plastics like PVC to make them pliable. Traces of these compounds can leach out of the product. Owing to concerns over the effects of such leachates, the EU has restricted the use of DEHP (di-2-ethylhexyl phthalate) and other phthalates in some applications, and the US has limited the use of DEHP, DPB, BBP, DINP, DIDP, and DnOP in children's toys and child-care articles through the Consumer Product Safety Improvement Act. Some compounds leaching from polystyrene food containers have been proposed to interfere with hormone functions and are suspected human carcinogens (cancer-causing substances).[46] Other chemicals of potential concern include alkylphenols.[41]

While a finished plastic may be non-toxic, the monomers used in the manufacture of its parent polymers may be toxic. In some cases, small amounts of those chemicals can remain trapped in the product unless suitable processing is employed. For example, the World Health Organization's International Agency for Research on Cancer (IARC) has recognized vinyl chloride, the precursor to PVC, as a human carcinogen.[46]

Some plastic products degrade to chemicals with estrogenic activity.[47] The primary building block of polycarbonates, bisphenol A (BPA), is an estrogen-like endocrine disruptor that may leach into food.[46] Research in Environmental Health Perspectives finds that BPA leached from the lining of tin cans, dental sealants and polycarbonate bottles can increase the body weight of lab animals' offspring.[48] A more recent animal study suggests that even low-level exposure to BPA results in insulin resistance, which can lead to inflammation and heart disease.[49] As of January 2010, the Los Angeles Times reported that the US Food and Drug Administration (FDA) is spending $30 million to investigate indications of BPA's link to cancer.[50] Bis(2-ethylhexyl) adipate, present in plastic wrap based on PVC, is also of concern, as are the volatile organic compounds present in new car smell. The EU has a permanent ban on the use of phthalates in toys. In 2009, the US government banned certain types of phthalates commonly used in plastic.[51]

Because the chemical structure of most plastics renders them durable, they are resistant to many natural degradation processes. Much of this material may persist for centuries or longer, given the demonstrated persistence of structurally similar natural materials such as amber.[clarification needed]

Estimates differ as to the amount of plastic waste produced in the last century. By one estimate, one billion tons of plastic waste have been discarded since the 1950s.[52] Others estimate a cumulative human production of 8.3-billion tons of plastic, of which 6.3-billion tons is waste, with only 9% getting recycled.[53]

It is estimated that this waste is made up of 81% polymer resin, 13% polymer fibers and 32% additives. In 2018 more than 343 million tons of plastic waste were generated, 90% of which was composed of post-consumer plastic waste (industrial, agricultural, commercial and municipal plastic waste). The rest was pre-consumer waste from resin production and manufacturing of plastic products (e.g. materials rejected due to unsuitable color, hardness, or processing characteristics).[5]

The Ocean Conservancy reported that China, Indonesia, Philippines, Thailand, and Vietnam dump more plastic into the sea than all other countries combined.[54] The rivers Yangtze, Indus, Yellow, Hai, Nile, Ganges, Pearl, Amur, Niger, and Mekong ""transport 88% to 95% of the global [plastics] load into the sea.""[55][56][verify quote punctuation]

The presence of plastics, particularly microplastics, within the food chain is increasing. In the 1960s microplastics were observed in the guts of seabirds, and since then have been found in increasing concentrations.[57] The long-term effects of plastics in the food chain are poorly understood. In 2009 it was estimated that 10% of modern waste was plastic,[58] although estimates vary according to region.[57] Meanwhile, 50% to 80% of debris in marine areas is plastic.[57] Plastic is often used in agriculture. There is more plastic in the soil than in the oceans. The presence of plastic in the environment hurts ecosystems and human health.[59]

Research on the environmental impacts has typically focused on the disposal phase. However, the production of plastics is also responsible for substantial environmental, health and socioeconomic impacts.[60]

Prior to the Montreal Protocol, CFCs had been commonly used in the manufacture of the plastic polystyrene, the production of which had contributed to depletion of the ozone layer.

Efforts to minimize environmental impact of plastics may include lowering of plastics production and use, waste- and recycling-policies, and the proactive development and deployment of alternatives to plastics such as for sustainable packaging.

Microplastics are ""synthetic solid particles or polymeric matrices, with regular or irregular shape and with size ranging from 1 μm to 5 mm, of either primary or secondary manufacturing origin, which are insoluble in water.""[61] Microplastics are dangerous to human health and the environment because they contain harmful chemicals which leak into the air, water, and food.

Microplastics cause pollution by entering natural ecosystems from a variety of sources, including cosmetics, clothing, construction, renovation, food packaging, and industrial processes.

The term microplastics is used to differentiate from larger, non-microscopic plastic waste. Two classifications of microplastics are currently recognized. Primary microplastics include any plastic fragments or particles that are already 5.0 mm in size or less before entering the environment. These include microfibers from clothing, microbeads, plastic glitter[62] and plastic pellets (also known as nurdles).[63][64][65] Secondary microplastics arise from the degradation (breakdown) of larger plastic products through natural weathering processes after entering the environment. Such sources of secondary microplastics include water and soda bottles, fishing nets, plastic bags, microwave containers, tea bags and tire wear.[66][65][67][68]

Both types are recognized to persist in the environment at high levels, particularly in aquatic and marine ecosystems, where they cause water pollution.[69]

Approximately 35% of all ocean microplastics come from textiles/clothing, primarily due to the erosion of polyester, acrylic, or nylon-based clothing, often during the washing process.[70] Microplastics also accumulate in the air and terrestrial ecosystems. Airborne microplastics have been detected in the atmosphere, as well as indoors and outdoors. 

Because plastics degrade slowly (often over hundreds to thousands of years),[71][72] microplastics have a high probability of ingestion, incorporation into, and accumulation in the bodies and tissues of many organisms. The toxic chemicals that come from both the ocean and runoff can also biomagnify up the food chain.[73][74] In terrestrial ecosystems, microplastics have been demonstrated to reduce the viability of soil ecosystems.[75][76] As of 2023, the cycle and movement of microplastics in the environment was not fully known. Microplastics in surface sample ocean surveys might have been underestimated as deep layer ocean sediment surveys in China found that plastics are present in deposition layers far older than the invention of plastics.

Plastics  degrade by a variety of processes, the most significant of which is usually photo-oxidation. Their chemical structure determines their fate. Polymers' marine degradation takes much longer as a result of the saline environment and cooling effect of the sea, contributing to the persistence of plastic debris in certain environments.[57] Recent studies have shown, however, that plastics in the ocean decompose faster than had been previously thought, due to exposure to the sun, rain, and other environmental conditions, resulting in the release of toxic chemicals such as bisphenol A. However, due to the increased volume of plastics in the ocean, decomposition has slowed down.[78] The Marine Conservancy has predicted the decomposition rates of several plastic products: It is estimated that a foam plastic cup will take 50 years, a plastic beverage holder will take 400 years, a disposable diaper will take 450 years, and fishing line will take 600 years to degrade.[79]

Microbial species capable of degrading plastics are known to science, some of which are potentially useful for disposal of certain classes of plastic waste.

Plastic recycling is the processing of plastic waste into other products.[100][101][102] Recycling can reduce dependence on landfills, conserve resources and protect the environment from plastic pollution and greenhouse gas emissions.[103][104][105] Recycling rates lag behind those of other recoverable materials, such as aluminium, glass and paper. From the start of plastic production through to 2015, the world produced around 6.3 billion tonnes of plastic waste, only 9% of which has been recycled and only ~1% has been recycled more than once.[106] Of the remaining waste, 12% was incinerated and 79% was either sent to landfills or lost to the environment as pollution.[106]

Almost all plastic is non-biodegradable and without recycling, spreads across the environment[107][108] where it causes plastic pollution. For example, as of 2015, approximately 8 million tonnes of waste plastic enters the oceans annually, damaging oceanic ecosystems and forming ocean garbage patches.[109]

Almost all recycling is mechanical and involves the melting and reforming of plastic into other items. This can cause polymer degradation at the molecular level, and requires that waste be sorted by colour and polymer type before processing, which is often complicated and expensive. Errors can lead to material with inconsistent properties, rendering it unappealing to industry.[110] Though filtration in mechanical recycling reduces microplastic release, even the most efficient filtration systems cannot prevent the release of microplastics into wastewater.[111][112]

In feedstock recycling, waste plastic is converted into its starting chemicals, which can then become fresh plastic. This involves higher energy and capital costs. Alternatively, plastic can be burned in place of fossil fuels in energy recovery facilities, or biochemically converted into other useful chemicals for industry.[113] In some countries, burning is the dominant form of plastic waste disposal, particularly where landfill diversion policies are in place.

Plastic recycling is low in the waste hierarchy, meaning that reduction and reuse are more favourable and long-term solutions for sustainability. 

By heating to above 500 °C (932 °F) in the absence of oxygen (pyrolysis), plastics can be broken down into simpler hydrocarbons, which can be used as feedstocks for the fabrication of new plastics.[120] These hydrocarbons can also be used as fuels.[121]

According to the Organisation for Economic Co-operation and Development, plastic contributed greenhouse gases in the equivalent of 1.8 billion tons of carbon dioxide (CO2) to the atmosphere in 2019, 3.4% of global emissions.[122] They say that by 2060, plastic could emit 4.3 billion tons of greenhouse gas a year. The effect of plastics on global warming is mixed. Plastics are generally made from fossil gas or petroleum; thus, the production of plastics creates further fugitive emissions of methane when the fossil gas or petroleum is produced. Additionally, much of the energy used in plastic production is not sustainable energy; for example, high temperature from burning fossil gas. However, plastics can also limit methane emissions; for example, packaging to reduce food waste.[123]

A study from 2024 found that compared to glass and aluminum, plastic may actually have less of a negative effect on the environment and therefore might be the best option for must food packaging and other common uses.[124] The study found that, ""replacing plastics with alternatives is worse for greenhouse gas emissions in most cases."" and that the study involving European researchers found, ""15 of the 16 applications a plastic product incurs fewer greenhouse gas emissions than their alternatives.""[124]

Production of plastics from crude oil requires 7.9 to 13.7 kWh/lb (taking into account the average efficiency of US utility stations of 35%). Producing silicon and semiconductors for modern electronic equipment is even more energy consuming: 29.2 to 29.8 kWh/lb for silicon, and about 381 kWh/lb for semiconductors.[125] This is much higher than the energy needed to produce many other materials. For example, to produce iron (from iron ore) requires 2.5-3.2 kWh/lb of energy; glass (from sand, etc.) 2.3–4.4 kWh/lb; steel (from iron) 2.5–6.4 kWh/lb; and paper (from timber) 3.2–6.4 kWh/lb.[126]

Quickly burning plastics at very high temperatures breaks down many toxic components, such as dioxins and furans. This approach is widely used in municipal solid waste incineration. Municipal solid waste incinerators also normally treat the flue gas to decrease pollutants further, which is needed because uncontrolled incineration of plastic produces carcinogenic polychlorinated dibenzo-p-dioxins.[127] Open-air burning of plastic occurs at lower temperatures and normally releases such toxic fumes.

In the European Union, municipal waste incineration is regulated by the Industrial Emissions Directive,[128] which stipulates a minimum temperature of 850 °C for at least two seconds.[129]

The bacterium Blaptica dubia is claimed to help degradation of commercial polysterene. This biodegradation seems to occur in some plastic degrading bacteria inhabiting the gut of cockroaches. The biodegradation products have been found in their feces too.[130]

The development of plastics has evolved from the use of naturally plastic materials (e.g., gums and shellac) to the use of the chemical modification of those materials (e.g., natural rubber, cellulose, collagen, and milk proteins), and finally to completely synthetic plastics (e.g., bakelite, epoxy, and PVC). Early plastics were bio-derived materials such as egg and blood proteins, which are organic polymers. In around 1600 BC, Mesoamericans used natural rubber for balls, bands, and figurines.[6] Treated cattle horns were used as windows for lanterns in the Middle Ages.[citation needed] Materials that mimicked the properties of horns were developed by treating milk proteins with lye. In the nineteenth century, as chemistry developed during the Industrial Revolution, many materials were reported. The development of plastics accelerated with Charles Goodyear's 1839 discovery of vulcanization to harden natural rubber.

Parkesine, invented by Alexander Parkes in 1855 and patented the following year,[131] is considered the first man-made plastic. It was manufactured from cellulose (the major component of plant cell walls) treated with nitric acid as a solvent. The output of the process (commonly known as cellulose nitrate or pyroxilin) could be dissolved in alcohol and hardened into a transparent and elastic material that could be molded when heated.[132] By incorporating pigments into the product, it could be made to resemble ivory. Parkesine was unveiled at the 1862 International Exhibition in London and garnered for Parkes the bronze medal.[133]

In 1893, French chemist Auguste Trillat discovered the means to insolubilize casein (milk proteins) by immersion in formaldehyde, producing material marketed as galalith.[134] In 1897, mass-printing press owner Wilhelm Krische of Hanover, Germany, was commissioned to develop an alternative to blackboards.[134] The resultant horn-like plastic made from casein was developed in cooperation with the Austrian chemist (Friedrich) Adolph Spitteler (1846–1940). Although unsuitable for the intended purpose, other uses would be discovered.[134]

The world's first fully synthetic plastic was Bakelite, invented in New York in 1907 by Leo Baekeland,[7] who coined the term plastics.[8] Many chemists have contributed to the materials science of plastics, including Nobel laureate Hermann Staudinger, who has been called ""the father of polymer chemistry"", and Herman Mark, known as ""the father of polymer physics"".[9] After World War I, improvements in chemistry led to an explosion of new forms of plastics, with mass production beginning in the 1940s and 1950s.[58] Among the earliest examples in the wave of new polymers were polystyrene (first produced by BASF in the 1930s)[6] and polyvinyl chloride (first created in 1872 but commercially produced in the late 1920s).[6] In 1923, Durite Plastics, Inc., was the first manufacturer of phenol-furfural resins.[135] In 1933, polyethylene was discovered by Imperial Chemical Industries (ICI) researchers Reginald Gibson and Eric Fawcett.[6]

The discovery of polyethylene terephthalate (PETE) is credited to employees of the Calico Printers' Association in the UK in 1941; it was licensed to DuPont for the US and ICI otherwise, and as one of the few plastics appropriate as a replacement for glass in many circumstances, resulting in widespread use for bottles in Europe.[6] In 1954 polypropylene was discovered by Giulio Natta and began to be manufactured in 1957.[6] Also in 1954 expanded polystyrene (used for building insulation, packaging, and cups) was invented by Dow Chemical.[6] Since the 1960s, plastic production has surged with the advent of polycarbonate and HDPE, widely used in various products.[136] In the 1980s and 1990s, plastic recycling and the development of biodegradable plastics began to flourish to mitigate environmental impacts.[137][138] From 2000 to the present, bioplastics from renewable sources and awareness of microplastics have spurred extensive research and policies to control plastic pollution.[139]

Work is currently underway to develop a global treaty on plastic pollution. On March 2, 2022, UN Member States voted at the resumed fifth UN Environment Assembly (UNEA-5.2) to establish an Intergovernmental Negotiating Committee (INC) with the mandate of advancing a legally-binding international agreement on plastics.[140] The resolution is entitled ""End plastic pollution: Towards an international legally binding instrument."" The mandate specifies that the INC must begin its work by the end of 2022 with the goal of ""completing a draft global legally binding agreement by the end of 2024.""[141]

Plastic in the sense of malleable
"
Cosmetic Manufacturing,"The cosmetic industry describes the industry that manufactures and distributes cosmetic products.  These include colour cosmetics, like foundation and mascara, skincare such as moisturisers and cleansers, haircare such as shampoos, conditioners and hair colours, and toiletries such as bubble bath and soap.  The manufacturing industry is dominated by a small number of multinational corporations that originated in the early 20th century, but the distribution and sale of cosmetics is spread among a wide range of different businesses. Cosmetics must be safe when customers use them in accordance with the label's instructions or in the conventional or expected manner. One measure a producer may take to guarantee the safety of a cosmetic product is product testing. FDA occasionally does testing as part of its research program or when looking into potential safety issues with a product. Both the cosmetics business and consumers can benefit from the FDA's resources on product testing.  

The largest cosmetic companies are L'Oreal, Estée Lauder, Coty, Nivea, Shiseido and Chanel.[1] The market volume of the cosmetics industry in Europe and the United States is about EUR €70 billion per year, according to a 2005 publication.[2] The worldwide cosmetics and perfume industry currently generates an estimated annual turnover of US$170 billion (according to Eurostaf – May 2007). Europe is the leading market, representing approximately €63 billion[as of?].

Within the United States, the state of California has the largest concentration of beauty establishments in America at 25.5%, followed by New Jersey at 8.1% of American beauty establishments.[3] Since 2016, the number of cosmetic stores rises between 3 and 4% each year and employment in this division is rising each year 13-16%.[citation needed]

California has the largest concentration due to social media marketing from celebrities and ‘beauty gurus’. For example, Kylie Jenner's company Kylie Cosmetics, is valued at $800 million and sold an estimated $330 million worth of makeup in 2017. YouTube and social media influencers Jeffree Star (Jeffree Star Cosmetics), and Michelle Phan (Ipsy), and pop-star Rihanna with her Fenty Beauty line of cosmetics, have also contributed to California's rising popularity in the beauty industry.[citation needed]

Because the US dollar is so valuable to other countries around the world, it has become extremely expensive for most countries to import American beauty products and to export their own products to America. However, there are a few countries with beauty products that are in popular demand in America due to their quality and value including France, Canada, Italy, and the United Kingdom.

As of 2018, “These four countries account for 45.2% of all industry imports as domestic consumers demand luxury products”.[3] South Korean and Japanese skincare products which are designed to be more gentle and innovative, are also becoming more popularized in the United States due to their quality and affordability. The U.S. has held the same regulation over this industry since 1938 under the FD&C Act, which has gained additional amendments over time.[4]

Canada is a big influencer in the American beauty industry due to its proximity to the United States and because it falls under the North American Free Trade Agreement (NAFTA). This agreement “eliminates most tariffs on products traded between Canada, Mexico, and the United States,""[5] and allows Canada to trade superfluously. Canada accounts for an estimated 13.6% of imports and 19.9% of cosmetic exports in 2018.[3] [citation needed]Because Mexico also benefits from the NAFTA trade agreement, they represent another top cosmetic importer and exporter for lower-priced beauty products which are manufactured in mass and sold in large drug store chains in America.[citation needed]

Cosmetic sales in France reached €6.5 billion in 2006, according to FIPAR (Fédération des Industries de la Parfumerie – the French federation for the perfume industry).[6] France is another country in which the cosmetic industry plays an important role, both nationally and internationally. Most products with a label, ""Made in France"" are valued on the international market. According to data from 2008, the cosmetic industry has grown constantly in France for 40 consecutive years. Famous cosmetic brands produced in France include Vichy, Yves Saint Laurent, Yves Rocher, Bioderma and L'Oreal. L'Oreal is known for its mass production of hair and makeup products which are produced in mass and sold in drug stores in America as well as internationally. L'Oreal has gained popularity especially due to its app Makeup Genius which allows users to try on makeup using their phone camera in addition to working with Los Angeles-based NYX Cosmetics and Estee Lauder's MAC Cosmetics, L'Oreal is one of the leading cosmetic brands in the United States. According to the company's latest financial report of 2017, North America accounted for 13.6% of the company's global cosmetic sales.[7]

In Germany, the cosmetic industry generated €12.6 billion of retail sales in 2008,[7] which, at the time, made the German cosmetic industry the third largest in the world after Japan and the United States. It has been shown that Germany's cosmetic  industry grew nearly 5 percent in one year, from 2007 to 2008. German exports in this industry reached €5.8 billion in 2008, whereas imports of cosmetics totaled €3 billion.[7]  Germany gains most of its imported cosmetics from France, Switzerland, the United States and Italy.[citation needed]

In Italy, the cosmetics industry makes a significant contribution in the national economy, known by its commitment to innovation, quality, and the status enjoyed by the ""Made in Italy"" label.  According to a 2022 Cosmetics Europe study, Italy ranks third among national markets for cosmetics, with a market value of €11.5 billion, trailing only behind Germany and France.[8] The sector's value was reported at $17.5 billion in 2017.[9]

The cosmetic sector demonstrates steady growth, exhibited by brands like Gucci Beauty, Armani Beauty, Dolce&Gabbana Beauty, among others.[10][11] For instance, in a September 2021 sales report, Gucci's perfumes and makeup were significant contributors to Coty's Prestige Division, which reported revenues of $870.7 million.[12] Additionally, in 2022, Dolce&Gabbana Beauty established itself as a standalone entity with a valuation of €1 billion, as it became the first Italian company to take direct control over its beauty operations, from production through to sales and marketing.[10][13] Furthermore, Italian brands Armani and Valentino have been instrumental in increasing the sales of the L'Oreal luxury division, their French licensee, in 2022.[14]

The Belarusian cosmetics market consists of Belarusian companies. There are more than 20 cosmetic companies in Belarus - Belita, Vitex, Lux Visage, Mastige, Modum, Rekish Cosmetics. The volume of the cosmetic market is more than 300 million euros per year.[citation needed]

The cosmetic industry in Asia is mainly dominated by regional cosmetic brands. Shiseido Co. Ltd, a popular cosmetic brand based in Japan, has 82.1% of its sales in Asia.[15] No other Western company in the top 10 match these kinds of regional sales. Furthermore, geographic dispersion of sales by Asian cosmetic companies in Asia accounted for 92.42% of sales, while geographic dispersion of assets of Asian cosmetic companies in Asia was 87.05%.[15] Western cosmetic companies often have failed to gain footholds in various countries. For example, in Japan, many advertisement campaigns that find success in the West, such as celebrity appearances and references to science, did not sway a sample group of respondents to purchase the foreign brands.[16] However, despite the lack of Western cosmetic presence, the growing trend of “fair skin”, or whiting one's skin, can expose consumers skin to harmful chemicals when using “fairness creams”.[17]

While there has been significant economic growth in many Asian markets, regulation pertaining to chemicals in cosmetic products has been concurrently lacking. SK-II, a cosmetic product owned by P&G, was found to contain banned heavy metals in China in 2006.[18] Another study found that women who had recently moved to Vancouver, Canada from East and South Asia had higher levels of lead in their blood than South and East Asian immigrants who had been living in Canada for longer.[19] One of sources of lead was determined to be some facial powders marketed in various regions of Asia.[20]

Due to the popularity of cosmetics, especially fragrances and perfumes, many designers who are not necessarily involved in the cosmetic industry came up with different perfumes carrying their names. Moreover, some actors and singers have their own perfume line (such as Celine Dion). Designer perfumes are, like any other designer products, the most expensive in the industry as the consumer pays not only for the product but also for the brand.[citation needed]

The cosmetic industry worldwide seems to be continuously developing, now more than ever with the advent of the Internet companies. Many famous companies sell their cosmetic products online also, in countries in which they do not have representatives.

Research on the email marketing of cosmetics to consumers suggests they are goal-oriented with email content that is seen as useful, motivating recipients to visit a store to test the cosmetics or talk to sales representatives. Useful content included special sales offerings and new product information rather than information about makeup trends.[21]

Many companies advertise white or light skin as not only a cosmetic change, but a lifestyle change. White beauty implies a lifestyle of “sophistication, beauty, power, and wealth.” Mass advertising and marketing from the US and Europe, as well as multiple mass media forms are used to reach other cultures to influence their purchasing habits. Many of the skin lightening products sold usually have celebrity endorsements, further increasing sales and the desire for lighter skin. These products can cause serious damage to skin and pose a health risk to the consumer.[22]

In a study done by S. S. Agrawal and Pallavi Sharma on eleven skin lightening products sold in India, it was found that “mercury was detected in all the samples of the skin lightening creams in the range of 0.14–0.36 ppm.” This study also observed that none of the brands tested include mercury as an ingredient on the packaging, which may mislead consumers regarding health risks. In a report by the World Health Organization, it was stated that “skin whitening products can cause leukemia, liver and kidney cancer and could also result in severe skin conditions.”[23] Though these health risks exist, women of color in many parts of the world are purchasing skin lightening creams.

Choma and Prusaczyk's survey of women of color in the US and India “show[s] that chronic surveillance of skin tone predicts skin tone dissatisfaction and skin bleaching.” Some companies in the cosmetic industry have capitalized off of the cultural pressure and standards for having lighter skin.  This study concluded that “skin bleaching is not merely a physical or aesthetic change, but one with potentially wide-ranging implications on psychological well-being and, more broadly, the perpetuation of racist ideologies and beauty standards.”[24]

Some components found in cosmetics, as well as their production, have been found to have negative environmental impact.[25][26][27][28] For example, Palm oil is found in lipstick, and shampoo. Palm oil is connected to the destruction of forests and habitats of endangered species, including orangutans, tigers, elephants, and rhinos.[29][30]

Animal testing has been a large controversy in the cosmetic industry. Animal tests performed include the Draize eye irritancy test, where test chemicals are applied to rabbits’ eyes and left on for several days,[31] and toxicity tests such as LD50, where a substance's toxicity is tested by determining the concentration at which it will kill 50% of the test animals.[32]

Due to public outcry as well as financial and temporal considerations, animal testing in cosmetics has steadily been decreasing over time and replaced with non-animal tests.[33] One of these non-animal tests is the Environmental Protection Agency’s ToxCast which has a similar accuracy compared to animal tests but achieves results with fewer economic costs and less time.[33]

An EU ban of marketing cosmetic products which have been tested on animals became effective in 2013.[34] There are a few exceptions to this law. Animal testing data for cosmetics can be used if the data for the ingredient used in the cosmetic originated from testing for non-cosmetic products.[34] Secondly, if a country outside of the EU requires animal testing and the cosmetic product was tested in that country, the ban also does not apply.[34] Cosmetic tests on animals are still legal in several countries, such as the US, Japan, Russia,[33] and China.

There are a number of cosmetics companies that claim they are “cruelty-free,” such as Bath & Body Works, Aveda, ELF, and Milani,[35] but some argue that because the label of “cruelty-free” both lacks regulation and standardized third-party certification, it has little real weight.[31] Brands that claim to be cruelty free often advertize this with a logo of a rabbit on their products.

In some cases, cosmetic testing is done on humans, which also leads to a related discussion of best practices and ethics.[36]

A number of studies have focused on social issues such as labor rights and sociology of the body.[37][38][39] Research has found that some retail stores have discriminatory hiring practices and prefer to hire class-privileged workers to promote their upscale image.[38]
"
Soap Production Services,"

Soap is a salt of a fatty acid (sometimes other carboxylic acids) used for cleaning and lubricating products as well as other applications.[1] In a domestic setting, soaps, specifically ""toilet soaps"", are surfactants usually used for washing, bathing, and other types of housekeeping. In industrial settings, soaps are used as thickeners, components of some lubricants, emulsifiers, and catalysts.

Soaps are often produced by mixing fats and oils with a base.[2] Humans have used soap for millennia; evidence exists for the production of soap-like materials in ancient Babylon around 2800 BC.[3]

In a domestic setting, ""soap"" usually refers to what is technically called a toilet soap, used for household and personal cleaning. Toilet soaps are salts of fatty acids with the general formula (RCO2−)M+, where M is Na (sodium) or K (potassium).[4]

When used for cleaning, soap solubilizes particles and grime, which can then be separated from the article being cleaned. The insoluble oil/fat ""dirt"" become associated inside micelles, tiny spheres formed from soap molecules with polar hydrophilic (water-attracting) groups on the outside and encasing a lipophilic (fat-attracting) pocket, which shields the oil/fat molecules from the water, making them soluble. Anything that is soluble will be washed away with the water. In hand washing, as a surfactant, when lathered with a little water, soap kills microorganisms by disorganizing their membrane lipid bilayer and denaturing their proteins.[citation needed] It also emulsifies oils, enabling them to be carried away by running water.[5]

When used in hard water, soap does not lather well but forms soap scum (related to metallic soaps, see below).[6]

So-called metallic soaps are key components of most lubricating greases and thickeners.[4] A commercially important example is lithium stearate. Greases are usually emulsions of calcium soap or lithium soap and mineral oil. Many other metallic soaps are also useful, including those of aluminium, sodium, and mixtures thereof. Such soaps are also used as thickeners to increase the viscosity of oils. In ancient times, lubricating greases were made by the addition of lime to olive oil, which would produce calcium soaps.[7] Metal soaps are also included in modern artists' oil paints formulations as a rheology modifier.[8] Metal soaps can be prepared by neutralizing fatty acids with metal oxides:

A cation from an organic base such as ammonium can be used instead of a metal; ammonium nonanoate is an ammonium-based soap that is used as an herbicide.[9]

Another class of non-toilet soaps are resin soaps, which are produced in the paper industry by the action of tree rosin with alkaline reagents used to separate cellulose from raw wood.  A major component of such soaps is the sodium salt of abietic acid.  Resin soaps are used as emulsifiers.[10]

The production of toilet soaps usually entails saponification of triglycerides, which are vegetable or animal oils and fats. An alkaline solution (often lye) induces saponification whereby the triglyceride fats first hydrolyze into salts of fatty acids. Glycerol (glycerin) is liberated. The glycerin is sometimes left in the soap product as a softening agent, although it is sometimes separated.[11][12] Handmade soap can differ from industrially made soap in that an excess of fat or coconut oil beyond that needed to consume the alkali is used (in a cold-pour process, this excess fat is called ""superfatting""), and the glycerol left in acts as a moisturizing agent. However, the glycerine also makes the soap softer. The addition of glycerol and processing of this soap produces glycerin soap. Superfatted soap is more skin-friendly than one without extra fat, although it can leave a ""greasy"" feel. Sometimes, an emollient is added, such as jojoba oil or shea butter.[13] Sand or pumice may be added to produce a scouring soap. The scouring agents serve to remove dead cells from the skin surface being cleaned. This process is called exfoliation.

To make antibacterial soap, compounds such as triclosan or triclocarban can be added. There is some concern that use of antibacterial soaps and other products might encourage antimicrobial resistance in microorganisms.[14]

The type of alkali metal used determines the kind of soap product. Sodium soaps, prepared from sodium hydroxide (soda lye), are firm, whereas potassium soaps, derived from potassium hydroxide (potash lye), are softer or often liquid. Historically, potassium hydroxide was extracted from the ashes of bracken or other plants. Lithium soaps also tend to be hard. These are used exclusively in greases.

For making toilet soaps, triglycerides (oils and fats) are derived from coconut, olive, or palm oils, as well as tallow.[15] Triglyceride is the chemical name for the triesters of fatty acids and glycerin. Tallow, i.e., rendered fat, is the most available triglyceride from animals. Each species offers quite different fatty acid content, resulting in soaps of distinct feel. The seed oils give softer but milder soaps. Soap made from pure olive oil, sometimes called Castile soap or Marseille soap, is reputed for its particular mildness. The term ""Castile"" is also sometimes applied to soaps from a mixture of oils with a high percentage of olive oil.



Proto-soaps, which mixed fat and alkali and were used for cleansing, are mentioned in Sumerian, Babylonian and Egyptian texts.[16][17]

The earliest recorded evidence of the production of soap-like materials dates back to around 2800 BC in ancient Babylon.[18] A formula for making a soap-like substance was written on a Sumerian clay tablet around 2500 BC. This was produced by heating a mixture of oil and wood ash, the earliest recorded chemical reaction, and used for washing woolen clothing.[19]

The Ebers papyrus (Egypt, 1550 BC) indicates the ancient Egyptians used a soap-like product as a medicine and created this by combining animal fats or vegetable oils with a soda ash substance called trona.[19] Egyptian documents mention a similar substance was used in the preparation of wool for weaving.[citation needed]

In the reign of Nabonidus (556–539 BC), a recipe for a soap-like substance consisted of uhulu [ashes], cypress [oil] and sesame [seed oil] ""for washing the stones for the servant girls"".[20]

True soaps, which we might recognise as soaps today, were different to proto-soaps. They foamed, were made deliberately, and could be produced in a hard or soft form because of an understanding of lye sources.[17] It is uncertain as to who was the first to invent true soap.[16][21]

Knowledge of how to produce true soap emerged at some point between early mentions of proto-soaps and the first century AD.[16][17] Alkali was used to clean textiles such as wool for thousands of years[22] but soap only forms when there is enough fat, and experiments show that washing wool does not create visible quantities of soap.[16] Experiments by Sally Pointer show that the repeated laundering of materials used in perfume-making lead to noticeable amounts of soap forming. This fits with other evidence from Mesopotamian culture.[16]

Pliny the Elder, whose writings chronicle life in the first century AD, describes soap as ""an invention of the Gauls"".[23] The word sapo, Latin for soap, has  connected to a mythical Mount Sapo, a hill near the River Tiber where animals were sacrificed.[24] But in all likelihood, the word was borrowed from an early Germanic language and is cognate with Latin sebum, ""tallow"". It first appears in Pliny the Elder's account,[25] Historia Naturalis, which discusses the manufacture of soap from tallow and ashes. There he mentions its use in the treatment of scrofulous sores, as well as among the Gauls as a dye to redden hair which the men in Germania were more likely to use than women.[26][27] The Romans avoided washing with harsh soaps before encountering the milder soaps used by the Gauls around 58 BC.[28] Aretaeus of Cappadocia, writing in the 2nd century AD, observes among ""Celts, which are men called Gauls, those alkaline substances that are made into balls [...] called soap"".[29] The Romans' preferred method of cleaning the body was to massage oil into the skin and then scrape away both the oil and any dirt with a strigil.[30] The standard design is a curved blade with a handle, all of which is made of metal.[31]

The 2nd-century AD physician Galen describes soap-making using lye and prescribes washing to carry away impurities from the body and clothes. The use of soap for personal cleanliness became increasingly common in this period. According to Galen, the best soaps were Germanic, and soaps from Gaul were second best. Zosimos of Panopolis, circa 300 AD, describes soap and soapmaking.[32]

In the Southern Levant, the ashes from barilla plants, such as species of Salsola, saltwort (Seidlitzia rosmarinus) and Anabasis, were used to make potash.[33][34] Traditionally, olive oil was used instead of animal lard throughout the Levant, which was boiled in a copper cauldron for several days.[35] As the boiling progresses, alkali ashes and smaller quantities of quicklime are added and constantly stirred.[35] In the case of lard, it required constant stirring while kept lukewarm until it began to trace. Once it began to thicken, the brew was poured into a mold and left to cool and harden for two weeks. After hardening, it was cut into smaller cakes. Aromatic herbs were often added to the rendered soap to impart their fragrance, such as yarrow leaves, lavender, germander, etc.

A detergent similar to soap was manufactured in ancient China from the seeds of Gleditsia sinensis.[36] Another traditional detergent is a mixture of pig pancreas and plant ash called zhuyizi (simplified Chinese: 猪胰子; traditional Chinese: 豬胰子; pinyin: zhūyízǐ). Soap made of animal fat did not appear in China until the modern era.[37] Soap-like detergents were not as popular as ointments and creams.[36]

Hard toilet soap with a pleasant smell was produced in the Middle East during the Islamic Golden Age, when soap-making became an established industry. Recipes for soap-making are described by Muhammad ibn Zakariya al-Razi (c. 865–925), who also gave a recipe for producing glycerine from olive oil. In the Middle East, soap was produced from the interaction of fatty oils and fats with alkali. In Syria, soap was produced using olive oil together with alkali and lime. Soap was exported from Syria to other parts of the Muslim world and to Europe.[38]

A 12th-century document describes the process of soap production.[39] It mentions the key ingredient, alkali, which later became crucial to modern chemistry, derived from al-qaly or ""ashes"".

By the 13th century, the manufacture of soap in the Middle East had become a major cottage industry, with sources in Nablus, Fes, Damascus, and Aleppo.[citation needed]

Soapmakers in Naples were members of a guild in the late sixth century (then under the control of the Eastern Roman Empire),[40] and in the eighth century, soap-making was well known in Italy and Spain.[41] The Carolingian capitulary De Villis, dating to around 800, representing the royal will of Charlemagne, mentions soap as being one of the products the stewards of royal estates are to tally. The lands of Medieval Spain were a leading soapmaker by 800, and soapmaking began in the Kingdom of England about 1200.[42] Soapmaking is mentioned both as ""women's work"" and as the produce of ""good workmen"" alongside other necessities, such as the produce of carpenters, blacksmiths, and bakers.[43]

In Europe, soap in the 9th century was produced from animal fats and had an unpleasant smell. This changed when olive oil began to be used in soap formulas instead, after which much of Europe's soap production moved to the Mediterranean olive-growing regions.[44] Hard toilet soap was introduced to Europe by Arabs and gradually spread as a luxury item. It was often perfumed.[38][44]

By the 15th century, the manufacture of soap in Christendom often took place on an industrial scale, with sources in Antwerp, Castile, Marseille, Naples and Venice.[41]

In France, by the second half of the 16th century, the semi-industrialized professional manufacture of soap was concentrated in a few centers of Provence—Toulon, Hyères, and Marseille—which supplied the rest of France.[45] In Marseilles, by 1525, production was concentrated in at least two factories, and soap production at Marseille tended to eclipse the other Provençal centers.[46]

English manufacture tended to concentrate in London.[47] The demand for high-quality hard soap was significant enough during the Tudor period that barrels of ashes were imported for the manufacture of soap.[17]

Finer soaps were later produced in Europe from the 17th century, using vegetable oils (such as olive oil) as opposed to animal fats. Many of these soaps are still produced, both industrially and by small-scale artisans. Castile soap is a popular example of the vegetable-only soaps derived from the oldest ""white soap"" of Italy. In 1634 Charles I granted the newly formed Society of Soapmakers a monopoly in soap production who produced certificates from 'foure Countesses, and five Viscountesses, and divers other Ladies and Gentlewomen of great credite and quality, besides common Laundresses and others', testifying that 'the New White Soap washeth whiter and sweeter than the Old Soap'.[48]

During the Restoration era (February 1665 – August 1714) a soap tax was introduced in England, which meant that until the mid-1800s, soap was a luxury, used regularly only by the well-to-do. The soap manufacturing process was closely supervised by revenue officials who made sure that soapmakers' equipment was kept under lock and key when not being supervised. Moreover, soap could not be produced by small makers because of a law that stipulated that soap boilers must manufacture a minimum quantity of one imperial ton at each boiling, which placed the process beyond the reach of the average person. The soap trade was boosted and deregulated when the tax was repealed in 1853.[49][50][51]

Industrially manufactured bar soaps became available in the late 18th century, as advertising campaigns in Europe and America promoted popular awareness of the relationship between cleanliness and health.[52] In modern times, the use of soap has become commonplace in industrialized nations due to a better understanding of the role of hygiene in reducing the population size of pathogenic microorganisms.[53]

Until the Industrial Revolution, soapmaking was conducted on a small scale and the product was rough. In 1780, James Keir established a chemical works at Tipton, for the manufacture of alkali from the sulfates of potash and soda, to which he afterwards added a soap manufactory. The method of extraction proceeded on a discovery of Keir's. In 1790, Nicolas Leblanc discovered how to make alkali from common salt.[28] Andrew Pears started making a high-quality, transparent soap, Pears soap, in 1807 in London.[54] His son-in-law, Thomas J. Barratt, became the brand manager (the first of its kind) for Pears in 1865.[55] In 1882, Barratt recruited English actress and socialite Lillie Langtry to become the poster-girl for Pears soap, making her the first celebrity to endorse a commercial product.[56][57]

William Gossage produced low-priced, good-quality soap from the 1850s. Robert Spear Hudson began manufacturing a soap powder in 1837, initially by grinding the soap with a mortar and pestle. American manufacturer Benjamin T. Babbitt introduced marketing innovations that included the sale of bar soap and distribution of product samples. William Hesketh Lever and his brother, James, bought a small soap works in Warrington in 1886 and founded what is still one of the largest soap businesses, formerly called Lever Brothers and now called Unilever. These soap businesses were among the first to employ large-scale advertising campaigns.

Liquid soap was invented in the nineteenth century; in 1865, William Sheppard patented a liquid version of soap.[58] In 1898, B.J. Johnson developed a soap derived from palm and olive oils; his company, the B.J. Johnson Soap Company, introduced ""Palmolive"" brand soap that same year.[59] This new brand of soap became popular rapidly, and to such a degree that B.J. Johnson Soap Company changed its name to Palmolive.[60]

In the early 1900s, other companies began to develop their own liquid soaps. Such products as Pine-Sol and Tide appeared on the market, making the process of cleaning things other than skin, such as clothing, floors, and bathrooms, much easier.

Liquid soap also works better for more traditional or non-machine washing methods, such as using a washboard.[61]
"
Chemical Manufacturing,"The chemical industry comprises the companies and other organizations that develop and produce industrial, specialty and other chemicals. Central to the modern world economy, the chemical industry converts raw materials (oil, natural gas, air, water, metals, and minerals) into commodity chemicals for industrial and consumer products. It includes industries for petrochemicals such as polymers for plastics and synthetic fibers; inorganic chemicals such as acids and alkalis; agricultural chemicals such as fertilizers, pesticides and herbicides; and other categories such as industrial gases, speciality chemicals and pharmaceuticals.

Various professionals are involved in the chemical industry including chemical engineers, chemists and lab technicians.

Although chemicals were made and used throughout history, the birth of the heavy chemical industry (production of chemicals in large quantities for a variety of uses) coincided with the beginnings of the Industrial Revolution.

One of the first chemicals to be produced in large amounts through industrial processes was sulfuric acid. In 1736 pharmacist Joshua Ward developed a process for its production that involved heating sulfur with saltpeter, allowing the sulfur to oxidize and combine with water. It was the first practical production of sulphuric acid on a large scale. John Roebuck and Samuel Garbett were the first to establish a large-scale factory in Prestonpans, Scotland, in 1749, which used leaden condensing chambers for the manufacture of sulfuric acid.[1][2]

In the early 18th century, cloth was bleached by treating it with stale urine or sour milk and exposing it to sunlight for long periods of time, which created a severe bottleneck in production. Sulfuric acid began to be used as a more efficient agent as well as lime by the middle of the century, but it was the discovery of bleaching powder by Charles Tennant that spurred the creation of the first great chemical industrial enterprise. His powder was made by reacting chlorine with dry slaked lime and proved to be a cheap and successful product. He opened the St Rollox Chemical Works, north of Glasgow, and production went from just 52 tons in 1799 to almost 10,000 tons just five years later.[3]

Soda ash was used since ancient times in the production of glass, textile, soap, and paper, and the source of the potash had traditionally been wood ashes in Western Europe. By the 18th century, this source was becoming uneconomical due to deforestation, and the French Academy of Sciences offered a prize of 2400 livres for a method to produce alkali from sea salt (sodium chloride). The Leblanc process was patented in 1791 by Nicolas Leblanc who then built a Leblanc plant at Saint-Denis.[4] He was denied his prize money because of the French Revolution.[5]

In Britain, the Leblanc process became popular.[5] William Losh built the first soda works in Britain at the Losh, Wilson and Bell works on the River Tyne in 1816, but it remained on a small scale due to large tariffs on salt production until 1824. When these tariffs were repealed, the British soda industry was able to rapidly expand. James Muspratt's chemical works in Liverpool and Charles Tennant's complex near Glasgow became the largest chemical production centres anywhere. By the 1870s, the British soda output of 200,000 tons annually exceeded that of all other nations in the world combined.

These huge factories began to produce a greater diversity of chemicals as the Industrial Revolution matured. Originally, large quantities of alkaline waste were vented into the environment from the production of soda, provoking one of the first pieces of environmental legislation to be passed in 1863. This provided for close inspection of the factories and imposed heavy fines on those exceeding the limits on pollution. Methods were devised to make useful byproducts from the alkali.

The Solvay process was developed by the Belgian industrial chemist Ernest Solvay in 1861. In 1864, Solvay and his brother Alfred constructed a plant in Charleroi Belgium. In 1874, they expanded into a larger plant in Nancy, France. The new process proved more economical and less polluting than the Leblanc method, and its use spread. In the same year, Ludwig Mond visited Solvay to acquire the rights to use his process, and he and John Brunner formed Brunner, Mond & Co., and built a Solvay plant at Winnington, England. Mond was instrumental in making the Solvay process a commercial success. He made several refinements between 1873 and 1880 that removed byproducts that could inhibit the production of sodium carbonate in the process.

The manufacture of chemical products from fossil fuels began at scale in the early 19th century. The coal tar and ammoniacal liquor residues of coal gas manufacture for gas lighting began to be processed in 1822 at the Bonnington Chemical Works in Edinburgh to make naphtha, pitch oil (later called creosote), pitch, lampblack (carbon black) and sal ammoniac (ammonium chloride).[6] Ammonium sulphate fertiliser, asphalt road surfacing, coke oil and coke were later added to the product line.

The late 19th century saw an explosion in both the quantity of production and the variety of chemicals that were manufactured. Large chemical industries arose in Germany and later in the United States.

Production of artificial manufactured fertilizer for agriculture was pioneered by Sir John Lawes at his purpose-built Rothamsted Research facility. In the 1840s he established large works near London for the manufacture of superphosphate of lime. Processes for the vulcanization of rubber were patented by Charles Goodyear in the United States and Thomas Hancock in England in the 1840s. The first synthetic dye was discovered by William Henry Perkin in London. He partly transformed aniline into a crude mixture which, when extracted with alcohol, produced a substance with an intense purple colour. He also developed the first synthetic perfumes. German industry quickly began to dominate the field of synthetic dyes. The three major firms BASF, Bayer, and Hoechst produced several hundred different dyes. By 1913, German industries produced almost 90% of the world's supply of dyestuffs and sold approximately 80% of their production abroad.[7] In the United States, Herbert Henry Dow's use of electrochemistry to produce chemicals from brine was a commercial success that helped to promote the country's chemical industry.[8]

The petrochemical industry can be traced back to the oil works of Scottish chemist James Young, and Canadian Abraham Pineo Gesner. The first plastic was invented by Alexander Parkes, an English metallurgist. In 1856, he patented Parkesine, a celluloid based on nitrocellulose treated with a variety of solvents.[9] This material, exhibited at the 1862 London International Exhibition, anticipated many of the modern aesthetic and utility uses of plastics. The industrial production of soap from vegetable oils was started by William Lever and his brother James in 1885 in Lancashire based on a modern chemical process invented by William Hough Watson that used glycerin and vegetable oils.[10]

By the 1920s, chemical firms consolidated into large conglomerates; IG Farben in Germany, Rhône-Poulenc in France and Imperial Chemical Industries in Britain. Dupont became a major chemicals firm in the early 20th century in America.

Polymers and plastics such as polyethylene, polypropylene, polyvinyl chloride, polyethylene terephthalate, polystyrene and polycarbonate comprise about 80% of the industry's output worldwide.[11] Chemicals are used in many different consumer goods, and are also used in many different sectors. This includes agriculture manufacturing, construction, and service industries.[11] Major industrial customers include rubber and plastic products, textiles, apparel, petroleum refining, pulp and paper, and primary metals. Chemicals are nearly a $5 trillion global enterprise, and the EU and U.S. chemical companies are the world's largest producers.[12]

Sales of the chemical business can be divided into a few broad categories, including basic chemicals (about 35% – 37% of dollar output), life sciences (30%), specialty chemicals (20% – 25%) and consumer products (about 10%).[13]

Basic chemicals, or ""commodity chemicals"" are a broad chemical category including polymers, bulk petrochemicals and intermediates, other derivatives and basic industrials, inorganic chemicals, and fertilizers.

Polymers are the largest revenue segment and includes all categories of plastics and human-made fibers. The major markets for plastics are packaging, followed by home construction, containers, appliances, pipe, transportation, toys, and games. 

Principal raw materials for polymers are bulk petrochemicals like ethylene, propylene and benzene.

Petrochemicals and intermediate chemicals are primarily made from liquefied petroleum gas (LPG), natural gas and crude oil fractions. Large volume products include ethylene, propylene, benzene, toluene, xylenes, methanol, vinyl chloride monomer (VCM), styrene, butadiene, and ethylene oxide. These basic or commodity chemicals are the starting materials used to manufacture many polymers and other more complex organic chemicals particularly those that are made for use in the specialty chemicals category.

Other derivatives and basic industrials include synthetic rubber, surfactants, dyes and pigments, turpentine, resins, carbon black, explosives, and rubber products and contribute about 20 percent of the basic chemicals' external sales.

Inorganic chemicals (about 12% of the revenue output) make up the oldest of the chemical categories. Products include salt, chlorine, caustic soda, soda ash, acids (such as nitric acid, phosphoric acid, and sulfuric acid), titanium dioxide, and hydrogen peroxide.

Fertilizers are the smallest category (about 6 percent) and include phosphates, ammonia, and potash chemicals.

Life sciences (about 30% of the dollar output of the chemistry business) include differentiated chemical and biological substances, pharmaceuticals, diagnostics, animal health products, vitamins, and pesticides. While much smaller in volume than other chemical sectors, their products tend to have high prices – over ten dollars per pound – growth rates of 1.5 to 6 times GDP, and research and development spending at 15 to 25% of sales. Life science products are usually produced with high specifications and are closely scrutinized by government agencies such as the Food and Drug Administration. Pesticides, also called ""crop protection chemicals"", are about 10% of this category and include herbicides, insecticides, and fungicides.[13]

Specialty chemicals are a category of relatively high-valued, rapidly growing chemicals with diverse end product markets. Typical growth rates are one to three times GDP with prices over a dollar per pound. They are generally characterized by their innovative aspects. Products are sold for what they can do rather than for what chemicals they contain. Products include electronic chemicals, industrial gases, adhesives and sealants as well as coatings, industrial and institutional cleaning chemicals, and catalysts. In 2012, excluding fine chemicals, the $546 billion global specialty chemical market was 33% Paints, Coating and Surface Treatments, 27% Advanced Polymer, 14% Adhesives and Sealants, 13% additives, and 13% pigments and inks.[14]

Speciality chemicals are sold as effect or performance chemicals. Sometimes they are mixtures of formulations, unlike ""fine chemicals"", which are almost always single-molecule products.

Consumer products include direct product sales of chemicals such as soaps, detergents, and cosmetics. Typical growth rates are 0.8 to 1.0 times GDP.[citation needed]

Consumers rarely come into contact with basic chemicals. Polymers and specialty chemicals are materials that they encounter everywhere daily. Examples are plastics, cleaning materials, cosmetics, paints and coatings, electronics, automobiles and the materials used in home construction.[14] These specialty products are marketed by chemical companies to the downstream manufacturing industries as pesticides, specialty polymers, electronic chemicals, surfactants, construction chemicals, industrial cleaners, flavours and fragrances, specialty coatings, printing inks, water-soluble polymers, food additives, paper chemicals, oil field chemicals, plastic adhesives, adhesives and sealants, cosmetic chemicals, water management chemicals, catalysts, and textile chemicals. Chemical companies rarely supply these products directly to the consumer.

Annually the American Chemistry Council tabulates the US production volume of the top 100 chemicals. In 2000, the aggregate production volume of the top 100 chemicals totaled 502 million tons, up from 397 million tons in 1990. Inorganic chemicals tend to be the largest volume but much smaller in dollar revenue due to their low prices. The top 11 of the 100 chemicals in 2000 were sulfuric acid (44 million tons), nitrogen (34), ethylene (28), oxygen (27), lime (22), ammonia (17), propylene (16), polyethylene (15), chlorine (13), phosphoric acid (13) and diammonium phosphates (12).[citation needed]

The largest chemical producers today are global companies with international operations and plants in numerous countries. Below is a list of the top 25 chemical companies by chemical sales in 2015. (Note: Chemical sales represent only a portion of total sales for some companies.)

Top chemical companies by chemical sales in 2015.[15]

London, United Kingdom

From the perspective of chemical engineers, the chemical industry involves the use of chemical processes such as chemical reactions and refining methods to produce a wide variety of solid, liquid, and gaseous materials. Most of these products serve to manufacture other items, although a smaller number go directly to consumers. Solvents, pesticides, lye, washing soda, and portland cement provide a few examples of products used by consumers.

The industry includes manufacturers of inorganic- and organic-industrial chemicals, ceramic products, petrochemicals, agrochemicals, polymers and rubber (elastomers), oleochemicals (oils, fats, and waxes), explosives, fragrances and flavors. Examples of these products are shown in the Table below.

Related industries include petroleum, glass, paint, ink, sealant, adhesive, pharmaceuticals and food processing.

Chemical processes such as chemical reactions operate in chemical plants to form new substances in various types of reaction vessels. In many cases, the reactions take place in special corrosion-resistant equipment at elevated temperatures and pressures with the use of catalysts. The products of these reactions are separated using a variety of techniques including distillation especially fractional distillation, precipitation, crystallization, adsorption, filtration, sublimation, and drying.

The processes and products or products are usually tested during and after manufacture by dedicated instruments and on-site quality control laboratories to ensure safe operation and to assure that the product will meet required specifications. More organizations within the industry are implementing chemical compliance software to maintain quality products and manufacturing standards.[16] The products are packaged and delivered by many methods, including pipelines, tank-cars, and tank-trucks (for both solids and liquids), cylinders, drums, bottles, and boxes. Chemical companies often have a research-and-development laboratory for developing and testing products and processes. These facilities may include pilot plants and such research facilities may be located at a site separate from the production plant(s).

The scale of chemical manufacturing tends to be organized from largest in volume (petrochemicals and commodity chemicals), to specialty chemicals, and the smallest, fine chemicals.

The petrochemical and commodity chemical manufacturing units are on the whole single product continuous processing plants. Not all petrochemical or commodity chemical materials are made in one single location, but groups of related materials often are to induce industrial symbiosis as well as material, energy and utility efficiency and other economies of scale.

Those chemicals made on the largest of scales are made in a few manufacturing locations around the world, for example in Texas and Louisiana along the Gulf Coast of the United States, on Teesside (United Kingdom), and in Rotterdam in the Netherlands. The large-scale manufacturing locations often have clusters of manufacturing units that share utilities and large-scale infrastructure such as power stations, port facilities, and road and rail terminals. To demonstrate the clustering and integration mentioned above, some 50% of the United Kingdom's petrochemical and commodity chemicals are produced by the Northeast of England Process Industry Cluster on Teesside.

Specialty chemical and fine chemical manufacturing are mostly made in discrete batch processes. These manufacturers are often found in similar locations but in many cases, they are to be found in multi-sector business parks.

In the U.S. there are 170 major chemical companies.[17] They operate internationally with more than 2,800 facilities outside the U.S. and 1,700 foreign subsidiaries or affiliates operating. The U.S. chemical output is $750 billion a year. The U.S. industry records large trade surpluses and employs more than a million people in the United States alone. The chemical industry is also the second largest consumer of energy in manufacturing and spends over $5 billion annually on pollution abatement.

In Europe, the chemical, plastics, and rubber sectors are among the largest industrial sectors.[18] Together they generate about 3.2 million jobs in more than 60,000 companies. Since 2000 the chemical sector alone has represented 2/3 of the entire manufacturing trade surplus of the EU.

In 2012, the chemical sector accounted for 12% of the EU manufacturing industry's added value. Europe remains the world's biggest chemical trading region with 43% of the world's exports and 37% of the world's imports, although the latest data shows that Asia is catching up with 34% of the exports and 37% of imports.[19] Even so, Europe still has a trading surplus with all regions of the world except Japan and China where in 2011 there was a chemical trade balance. Europe's trade surplus with the rest of the world today amounts to 41.7 billion Euros.[20]

Over the 20 years between 1991 and 2011, the European Chemical industry saw its sales increase from 295 billion Euros to 539 billion Euros, a picture of constant growth. Despite this, the European industry's share of the world chemical market has fallen from 36% to 20%. This has resulted from the huge increase in production and sales in emerging markets like India and China.[21] The data suggest that 95% of this impact is from China alone. In 2012 the data from the European Chemical Industry Council shows that five European countries account for 71% of the EU's chemicals sales. These are Germany, France, the United Kingdom, Italy and the Netherlands.[22]

The chemical industry has seen growth in China, India, Korea, the Middle East, South East Asia, Nigeria and Brazil.  The growth is driven by changes in feedstock availability and price, labor and energy costs, differential rates of economic growth and environmental pressures.

Just as companies emerge as the main producers of the chemical industry, we can also look on a more global scale at how industrialized countries rank, with regard to the billions of dollars worth of production a country or region could export. Though the business of chemistry is worldwide in scope, the bulk of the world's $3.7 trillion chemical output is accounted for by only a handful of industrialized nations. The United States alone produced $689 billion, 18.6 percent of the total world chemical output in 2008.[23]
"
Asphalt Production Services,"Asphalt concrete (commonly called asphalt,[1] blacktop, or pavement in North America, and tarmac or bitumen macadam in the United Kingdom and the Republic of Ireland) is a composite material commonly used to surface roads, parking lots, airports, and the core of embankment dams.[2] Asphalt mixtures have been used in pavement construction since the nineteenth century.[3] It consists of mineral aggregate bound together with bitumen (a substance also independently known as asphalt, pitch, or tar), laid in layers, and compacted.

The American English terms asphalt (or asphaltic) concrete, bituminous asphalt concrete, and bituminous mixture are typically used only in engineering and construction documents, which define concrete as any composite material composed of mineral aggregate adhered with a binder. The abbreviation, AC, is sometimes used for asphalt concrete but can also denote asphalt content or asphalt cement, referring to the liquid asphalt portion of the composite material.

Natural asphalt (Ancient Greek: ἄσφαλτος (ásphaltos)) has been known of and used since antiquity, in Mesopotamia, Phoenicia, Egypt, Babylon, Greece, Carthage, and Rome, to waterproof temple baths, reservoirs, aqueducts, tunnels, and moats, as a masonry mortar, to cork vessels, and surface roads.[4] The Procession Street of Babylonian King Nabopolassar, c. 625 BC, leading north from his palace through the city's wall, being described as being constructed from burnt brick and asphalt.[5] Natural asphalt covered and bonded cobbles were used from 1824, in France, as a means to construct roads. In 1829 natural Seyssel asphalt mixed with 7% aggregate, to create an asphat-mastic surface was used for a footpath at Pont Morand, Lyons, France,  the technique spreading to Paris in 1835, London, England, in 1836, and the Philadelphia, USA, in 1838.[6] In 1834, John Henry Cassell & Company of Poplar, London, a pitch and varnish supplier, obtained an English patent for a method to surface roads with a layer of tar, covered by a layer of macadam, and sealed with a layer of tar and sand, and marketed the surface ""lava stone for paving and waterproofing""; soon after being contracted to surface the approach road to Vauxhall bridge, and a road in Millwall, London.[7] In 1837, Richard Tappin Claridge obtained a similar English patent (GB patent 1837 #7849), substituting Seyssel asphalt as the binder, having seen it employed in France and Belgium, he would subsequently form the Claridge's Patent Asphalte Company, in 1838.[7] A two mile stretch of a gravel constructed road, running out of Nottingham, and Huntingdon High Street, were experimentally covered is natural asphalt during the 1840s.[8] A Macadam road surfaced with asphalt was constructed in 1852, between Paris and Perpignan, France, using Swiss Val de Travers rock asphalt (natural asphalt covered limestone aggregate).[9][5] In 1869, Threadneedle Street, in London, England was resurfaced with Swiss Val de Travers rock asphalt.[6] A process to surface a packed sand road through application of heated natural asphalt mixed with sand, in a ratio of 1:5, rolling, and hardened through the application of natural asphalt mixed with a petroleum oil, was invented by Belgian-American chemist Edward De Smedt, at Columbia University, in 1870, obtaining a pair of U.S. patents for the material and method of hardening.[10][11] Civil Engineer, Surveyor, and an English county Highway board member, Edgar Purnell Hooley created a process and engine to combine a synthetic, refined petroleum tar, and resin, with Macadam aggregates (gravel, portland cement, crushed rocks, and blast furnace slag) in a steam heated mixer, at 212 °F (100 °C), and through a heated reservoir, conduits, and meshes, create a machine and material that can be applied to form a road surface, filing a UK patent, in 1902, for his improvement.[12][13] Hooley founding a UK company to market the technology, where the term tar macadam, shortened to tarmac was coined, after the name of his companyTar Macadam (Purnell Hooley's Patent) Syndicate Limited, derived from the combination of tar and Macadam gravel composite mixtures. [14]

Mixing of asphalt and aggregate is accomplished in one of several ways:[15]

In addition to the asphalt and aggregate, additives, such as polymers, and antistripping agents[clarification needed] may be added to improve the properties of the final product.

Areas paved with asphalt concrete—especially airport aprons—have been called ""the tarmac"" at times, despite not being constructed using the tarmacadam process.[24]

A variety of specialty asphalt concrete mixtures have been developed to meet specific needs, such as stone-matrix asphalt, which is designed to ensure a strong wearing surface, or porous asphalt pavements, which are permeable and allow water to drain through the pavement for controlling storm water.

Different types of asphalt concrete have different performance characteristics in roads in terms of surface durability, tire wear, braking efficiency and roadway noise. In principle, the determination of appropriate asphalt performance characteristics must take into account the volume of traffic in each vehicle category, and the performance requirements of the friction course. In general, the viscosity of asphalt allows it to conveniently form a convex surface, and a central apex to streets and roads to drain water to the edges. This is not, however, in itself an advantage over concrete, which has various grades of viscosity and can be formed into a convex road surface. Rather, it is the economy of asphalt concrete that renders it more frequently used. Concrete is found on interstate highways where maintenance is highly crucial.

Asphalt concrete generates less roadway noise than a Portland cement concrete surface, and is typically less noisy than chip seal surfaces.[25][26] Because tire noise is generated through the conversion of kinetic energy to sound waves, more noise is produced as the speed of a vehicle increases. The notion that highway design might take into account acoustical engineering considerations, including the selection of the type of surface paving, arose in the early 1970s.[25][26]

With regard to structural performance, the asphalt behaviour depends on a variety of factors including the material, loading and environmental condition. Furthermore, the performance of pavement varies over time. Therefore, the long-term behaviour of asphalt pavement is different from its short-term performance. The LTPP is a research program by the FHWA,  which is specifically focusing on long-term pavement behaviour.[27][28]

Asphalt deterioration can include crocodile cracking, potholes, upheaval, raveling[clarification needed], bleeding, rutting, shoving, stripping,[clarification needed] and grade depressions. In cold climates, frost heaves can crack asphalt even in one winter. Filling the cracks with bitumen is a temporary fix, but only proper compaction and drainage can slow this process.

Factors that cause asphalt concrete to deteriorate over time mostly fall into one of three categories: construction quality, environmental considerations, and traffic loads. Often, damage results from combinations of factors in all three categories.

Construction quality is critical to pavement performance. This includes the construction of utility trenches and appurtenances that are placed in the pavement after construction. Lack of compaction in the surface of the asphalt, especially on the longitudinal joint, can reduce the life of a pavement by 30 to 40%. Service trenches in pavements after construction have been said to reduce the life of the pavement by 50%,[29] mainly due to the lack of compaction in the trench, and because of water intrusion through improperly sealed joints.

Environmental factors include heat and cold, the presence of water in the subbase or subgrade soil underlying the pavement, and frost heaves.

High temperatures soften the asphalt binder, allowing heavy tire loads to deform the pavement into ruts. Paradoxically, high heat and strong sunlight also cause the asphalt to oxidize, becoming stiffer and less resilient, leading to crack formation. Cold temperatures can cause cracks as the asphalt contracts. Cold asphalt is also less resilient and more vulnerable to cracking.

Water trapped under the pavement softens the subbase and subgrade, making the road more vulnerable to traffic loads. Water under the road freezes and expands in cold weather, causing and enlarging cracks. In spring thaw, the ground thaws from the top down, so water is trapped between the pavement above and the still-frozen soil underneath. This layer of saturated soil provides little support for the road above, leading to the formation of potholes. This is more of a problem for silty or clay soils than sandy or gravelly soils. Some jurisdictions pass frost laws to reduce the allowable weight of trucks during the spring thaw season and protect their roads.

The damage a vehicle causes is roughly proportional to the axle load raised to the fourth power, so doubling the weight an axle carries actually causes 16 times as much damage.[30] Wheels cause the road to flex slightly, resulting in fatigue cracking, which often leads to crocodile cracking. Vehicle speed also plays a role. Slowly moving vehicles stress the road over a longer period of time, increasing ruts, cracking, and corrugations in the asphalt pavement.

Other causes of damage include heat damage from vehicle fires, or solvent action from chemical spills.

The life of a road can be prolonged through good design, construction and maintenance practices. During design, engineers measure the traffic on a road, paying special attention to the number and types of trucks. They also evaluate the subsoil to see how much load it can withstand. The pavement and subbase thicknesses are designed to withstand the wheel loads. Sometimes, geogrids are used to reinforce the subbase and further strengthen the roads. Drainage, including ditches, storm drains and underdrains are used to remove water from the roadbed, preventing it from weakening the subbase and subsoil.[31]

Sealcoating asphalt is a maintenance measure that helps keep water and petroleum products out of the pavement.

Maintaining and cleaning ditches and storm drains will extend the life of the road at low cost. Sealing small cracks with bituminous crack sealer prevents water from enlarging cracks through frost weathering, or percolating down to the subbase and softening it.

For somewhat more distressed roads, a chip seal or similar surface treatment may be applied. As the number, width and length of cracks increases, more intensive repairs are needed. In order of generally increasing expense, these include thin asphalt overlays, multicourse overlays, grinding off the top course and overlaying, in-place recycling, or full-depth reconstruction of the roadway.

It is far less expensive to keep a road in good condition than it is to repair it once it has deteriorated. This is why some agencies place the priority on preventive maintenance of roads in good condition, rather than reconstructing roads in poor condition. Poor roads are upgraded as resources and budget allow. In terms of lifetime cost and long term pavement conditions, this will result in better system performance. Agencies that concentrate on restoring their bad roads often find that by the time they have repaired them all, the roads that were in good condition have deteriorated.[32]

Some agencies use a pavement management system to help prioritize maintenance and repairs.

Asphalt concrete is a recyclable material that can be reclaimed and reused both on-site and in asphalt plants.[33] The most common recycled component in asphalt concrete is reclaimed asphalt pavement (RAP). RAP is recycled at a greater rate than any other material in the United States.[34] Many roofing shingles also contain asphalt, and asphalt concrete mixes may contain reclaimed asphalt shingles (RAS). Research has demonstrated that RAP and RAS can replace the need for up to 100% of the virgin aggregate and asphalt binder in a mix,[35] but this percentage is typically lower due to regulatory requirements and performance concerns. In 2019, new asphalt pavement mixtures produced in the United States contained, on average, 21.1% RAP and 0.2% RAS.[34]

Recycled asphalt components may be reclaimed and transported to an asphalt plant for processing and use in new pavements, or the entire recycling process may be conducted in-place.[33] While in-place recycling typically occurs on roadways and is specific to RAP, recycling in asphalt plants may utilize RAP, RAS, or both. In 2019, an estimated 97.0 million tons of RAP and 1.1 million tons of RAS were accepted by asphalt plants in the United States.[34]

RAP is typically received by plants after being milled on-site, but pavements may also be ripped out in larger sections and crushed in the plant. RAP millings are typically stockpiled at plants before being incorporated into new asphalt mixes. Prior to mixing, stockpiled millings may be dried and any that have agglomerated in storage may have to be crushed.[33]

RAS may be received by asphalt plants as post-manufacturer waste directly from shingle factories, or they may be received as post-consumer waste at the end of their service life.[34] Processing of RAS includes grinding the shingles and sieving the grinds to remove oversized particles. The grinds may also be screened with a magnetic sieve to remove nails and other metal debris. The ground RAS is then dried, and the asphalt cement binder can be extracted.[36] For further information on RAS processing, performance, and associated health and safety concerns, see Asphalt Shingles.

In-place recycling methods allow roadways to be rehabilitated by reclaiming the existing pavement, remixing, and repaving on-site. In-place recycling techniques include rubblizing, hot in-place recycling, cold in-place recycling, and full-depth reclamation.[33][37] For further information on in-place methods, see Road Surface.

During its service life, the asphalt cement binder, which makes up about 5–6% of a typical asphalt concrete mix,[38] naturally hardens and becomes stiffer.[39][40][33] This aging process primarily occurs due to oxidation, evaporation, exudation, and physical hardening.[33] For this reason, asphalt mixes containing RAP and RAS are prone to exhibiting lower workability and increased susceptibility to fatigue cracking.[35][36] These issues are avoidable if the recycled components are apportioned correctly in the mix.[39][35] Practicing proper storage and handling, such as by keeping RAP stockpiles out of damp areas or direct sunlight, is also important in avoiding quality issues.[35][33] The binder aging process may also produce some beneficial attributes, such as by contributing to higher levels of rutting resistance in asphalts containing RAP and RAS.[40][41]

One approach to balancing the performance aspects of RAP and RAS is to combine the recycled components with virgin aggregate and virgin asphalt binder. This approach can be effective when the recycled content in the mix is relatively low,[39] and has a tendency to work more effectively with soft virgin binders.[40] A 2020 study found that the addition of 5% RAS to a mix with a soft, low-grade virgin binder significantly increased the mix's rutting resistance while maintaining adequate fatigue cracking resistance.[41]

In mixes with higher recycled content, the addition of virgin binder becomes less effective, and rejuvenators may be used.[39] Rejuvenators are additives that restore the physical and chemical properties of the aged binder.[40] When conventional mixing methods are used in asphalt plants, the upper limit for RAP content before rejuvenators become necessary has been estimated at 50%.[35] Research has demonstrated that the use of rejuvenators at optimal doses can allow for mixes with 100% recycled components to meet the performance requirements of conventional asphalt concrete.[35][39]

Beyond RAP and RAS, a range of waste materials can be re-used in place of virgin aggregate, or as rejuvenators. Crumb rubber, generated from recycled tires, has been demonstrated to improve the fatigue resistance and flexural strength of asphalt mixes that contain RAP.[42][43] In California, legislative mandates require the Department of Transportation to incorporate crumb rubber into asphalt paving materials.[44] Other recycled materials that are actively included in asphalt concrete mixes across the United States include steel slag, blast furnace slag, and cellulose fibers.[34]

Further research has been conducted to discover new forms of waste that may be recycled into asphalt mixes. A 2020 study conducted in Melbourne, Australia presented a range of strategies for incorporating waste materials into asphalt concrete. The strategies presented in the study include the use of plastics, particularly high-density polyethylene, in asphalt binders, and the use of glass, brick, ceramic, and marble quarry waste in place of traditional aggregate.[45]

Rejuvenators may also be produced from recycled materials, including waste engine oil, waste vegetable oil, and waste vegetable grease.[39]

Recently, discarded face masks have been incorporated into stone mastic.[46]

[1]"
Rubber Manufacturing,"Rubber Technology is the subject dealing with the transformation of rubbers or elastomers into useful products, such as automobile tires, rubber mats and, exercise rubber stretching bands.  The materials includes latex, natural rubber, synthetic rubber and other polymeric materials, such as thermoplastic elastomers. Rubber processed through such methods are components of a wide range of items.

Rubber products can be categorized into two main categories. 

Most rubber products are vulcanized, a process which involves heating with a small quantity of sulphur (or equivalent cross-linking agent) so as to stabilise the polymer chains, over a wide range of temperature. This discovery was made by Charles Goodyear in the 1844, but is a process restricted to polymer chains having a double bond in the backbone.[1] Such materials include natural rubber and polybutadiene. The range of materials available is much wider however, since all polymers become elastomeric above their glass transition temperature. However, the elastomeric state is unstable because chains can slip past one another resulting in creep or stress relaxation under static or dynamic load conditions. Chemical cross links add the stability to the network that is needed for most practical applications.[citation needed]

Methods for processing rubber include mastication and various operations like mixing, calendering, extrusion, all processes being essential to bring crude rubber into a state suitable for shaping the final product. The former breaks down the polymer chains, and lowers their molecular mass so that viscosity is low enough for further processing. After this has been achieved, various additions can be made to the material ready for cross-linking. Rubber may be masticated on a two-roll mill or in an industrial mixer, which come in different types.[2]

Rubber is first compounded with additives like sulfur, carbon black and accelerators. It is converted into a dough-like mixture which is called ""compound"" then milled into sheets of desired thickness. Rubber may then be extruded or molded before being cured.
"
Plastic Signage Production,"Signage is the design or use of signs and symbols to communicate a message.[1][2] Signage also means signs collectively or being considered as a group.[3] The term signage is documented to have been popularized in 1975 to 1980.[2]

Signs are any kind of visual graphics created to display information to a particular audience. This is typically manifested in the form of wayfinding information in places such as streets or on the inside and outside buildings. Signs vary in form and size based on location and intent, from more expansive banners, billboards, and murals, to smaller street signs, street name signs, sandwich boards and lawn signs. Newer signs may also use digital or electronic displays.

The main purpose of signs is to communicate, to convey information designed to assist the receiver with decision-making based on the information provided. Alternatively, promotional signage may be designed to persuade receivers of the merits of a given product or service.  Signage is distinct from labeling, which conveys information about a particular product or service.

The term, 'sign' comes from the old French signe (noun), signer (verb), meaning a gesture or a motion of the hand. This, in turn, stems from Latin 'signum' indicating an""identifying mark, token, indication, symbol; proof; military standard, ensign; a signal, an omen; sign in the heavens, constellation.""[4] In the English, the term is also associated with a flag or ensign. In France, a banner not infrequently took the place of signs or sign boards in the Middle Ages. Signs, however, are best known in the form of painted or carved advertisements for shops, inns, cinemas, etc. They are one of various emblematic methods for publicly calling attention to the place to which they refer.

The term, 'signage' appears to have come into use in the 20th century as a collective noun used to describe a class of signs, especially advertising and promotional signs which came to prominence in the first decades of the twentieth century.[5] The Oxford Dictionary defines the term, signage, as ""Signs collectively, especially commercial or public display signs.""[6]

Some of the earliest signs were used informally to denote the membership of specific groups. Early Christians used the sign or a cross or the Ichthys (i.e. fish) to denote their religious affiliations, whereas the sign of the sun or the moon would serve the same purpose for pagans.[7]

The use of commercial signage has a very ancient history. Retail signage and promotional signs appear to have developed independently in the East and the West. In antiquity, the ancient Egyptians, Romans and Greeks were known to use signage.  In ancient Rome, signboards were used for shop fronts as well as to announce public events.[8] Roman signboards were usually made from stone or terracotta. Alternatively, they were whitened areas, known as albums on the outer walls of shops, forums and marketplaces. Many Roman examples have been preserved; among them the widely recognized bush to indicate a tavern, from which is derived the proverb, ""A good wine needs no bush"".[9] Apart from the bush, certain identifiable trade signs that survive into modern times include the three balls of pawnbrokers and the red and white barber's pole. Of the signs identified with specific trades, some of these later evolved into trademarks. This suggests that the early history of commercial signage is intimately tied up with the history of branding and labelling.[10]

Recent research suggests that China exhibited a rich history of early retail signage systems.[11] One well-documented, early example of a highly developed brand associated with retail signage is that of the White Rabbit brand of sewing needles, from China's Song dynasty period (960–1127 CE).[12][13] A copper printing plate used to print posters contained message, which roughly translates as: ""Jinan Liu's Fine Needle Shop: We buy high quality steel rods and make fine quality needles, to be ready for use at home in no time.""[14] The plate also includes a trademark in the form of a white rabbit which signified good luck and was particularly relevant to the primary purchasers, women with limited literacy. Details in the image show a white rabbit crushing herbs, and included advice to shoppers to look for the stone white rabbit in front of the maker's shop. Thus, the image served as an early form of brand recognition.[15] Eckhart and Bengtsson have argued that during the Song dynasty, Chinese society developed a consumerist culture, where a high level of consumption was attainable for a wide variety of ordinary consumers rather than just the elite. The rise of a consumer culture prompted the commercial investment in carefully managed company image, retail signage, symbolic brands, trademark protection and sophisticated brand concepts.[13]

During the Medieval period, the use of signboards was generally optional for traders. However, publicans were on a different footing. As early as the 14th century, English law compelled innkeepers and landlords to exhibit signs. In 1389, King Richard II of England compelled landlords to erect signs outside their premises. The legislation stated ""Whosoever shall brew ale in the town with intention of selling it must hang out a sign, otherwise he shall forfeit his ale.""[16] Legislation was intended to make public houses easily visible to passing inspectors of the quality of the ale they provided (during this period, drinking water was not always good to drink and ale was the usual replacement). In 1393 a publican was prosecuted for failing to display signs. The practice of using signs spread to other types of commercial establishments throughout the Middle Ages.[17] Similar legislation was enacted in Europe. For instance, in France edicts were issued 1567 and 1577, compelling innkeepers and tavern-keepers to erect signs.[18]

Large towns, where many premises practiced the same trade, and especially, where these congregated in the same street, a simple trade sign was insufficient to distinguish one house from another. Thus, traders began to employ a variety of devices to differentiate themselves. Sometimes the trader used a rebus on his own name (e.g. two cocks for the name of Cox); sometimes he adopted a figure of an animal or other object, or portrait of a well-known person, which he considered likely to attract attention. Other signs used the common association of two heterogeneous objects, which (apart from those representing a rebus) were in some cases merely a whimsical combination, but in others arose from a popular misconception of the sign itself (e.g. the combination of the leg and star may have originated in a representation of the insignia of the garter), or from corruption in popular speech (e.g. the combination goat and compasses is said by some to be a corruption of God encompasses).

Around this time, some manufacturers began to adapt the coats of arms or badges of noble families as a type of endorsement. These would be described by the people without consideration of the language of heraldry, and thus such signs as the Red Lion, the Green Dragon, etc., have become familiar, especially as pub signs. By the 17th and 18th centuries, the number of commercial houses actively displaying the royal arms on their premises, packaging and labelling had increased, but many claims of royal endorsement were fraudulent. By 1840, the rules surrounding the display of royal arms were tightened to prevent false claims. By the early 19th century, the number of royal warrants granted rose rapidly when Queen Victoria granted some 2,000 royal warrants during her reign of 64 years.[19]

Since the object of signboards was to attract the public, they were often of an elaborate character. Not only were the signs themselves large and sometimes of great artistic merit (especially in the 16th and 17th centuries, when they reached their greatest vogue) but the posts or metal supports protruding from the houses over the street, from which the signs were swung, were often elaborately worked, and many beautiful examples of wrought-iron supports survive both in England and continental Europe.

Exterior signs were a prominent feature of the streets of London from the 16th century. Large overhanging signs became a danger and a nuisance in the narrow ways as the city streets became more congested with vehicular traffic. Over time, authorities were forced to regulate the size and placement of exterior signage. In 1669, a French royal order prohibited the excessive size of sign boards and their projection too far over the streets. In Paris in 1761, and in London, about 1762–1773, laws were introduced which gradually compelled sign boards to be removed or fixed flat against the wall.

For the most part, signs only survived in connection with inns, for which some of the greatest artists of the time painted sign boards, usually representing the name of the inn. With the gradual abolition of sign boards, the numbering of houses began to be introduced in the early 18th century in London. It had been attempted in Paris as early as 1512, and had become almost universal by the close of the 18th century, though not enforced until 1805.  Another important factor was that during the Middle Ages a large percentage of the population was illiterate and so pictures were more useful as a means of identifying a public house. For this reason there was often no reason to write the establishment's name on the sign and inns opened without a formal written name—the name being derived later from the illustration on the public house's sign. In this sense, a pub sign can be thought of as an early example of visual branding.[20]

During the 19th century, some artists specialized in the painting of signboards, such as the Austro-Hungarian artist Demeter Laccataris. Pending this development, houses which carried on trade at night (e.g. coffee houses, brothels, etc.) had various specific arrangements of lights, and these still survive to some extent, as in the case of doctors' surgeries, and chemists' dispensaries.

Several developments in the early 20th century provided the impetus for widespread commercial adoption of exterior signage. The first, spectaculars, erected in Manhattan in 1892, became commonplace in the first decade of the 20th century and by 1913, ""the skies were awash with a blaze of illuminated, animated signs.""[21] In the 1920s, the newly developed neon sign was introduced to the United States. Its flexibility and visibility led to widespread commercial adoption and by the 1930s, neon signs were a standard feature of modern building around the world.[22] Privilege signs, which employed the manufacturer's brand as a form of retail endorsement, were common on retail stores during the 20th century, but their use has waned as retailers gained increasing power in the late 20th century. A small number of privilege signs are still present, but most have become abandoned ghost signs.[23][24][25]

An early computer generated hard copy of various size metal printed characters for displays was introduced and patented in 1971, Patent US3596285A, may have been the first data driven printed example of signage in the USA.

Historic retail sign boards

In general, signs perform the following roles or functions:

Signs may be used in exterior spaces or on-premises locations. Signs used on the exterior of a building are often designed to encourage people to enter and on the interior to encourage people to explore the environment and participate in all that the space has to offer. Any given sign may perform multiple roles simultaneously. For example, signage may provide information, but may also serve to assist customers navigate their way through a complex service or retail environment.[28][29]

Pictograms are images commonly used to convey the message of a sign. In statutory signage, pictograms follow specific sets of colour, shape and sizing rules based on the laws of the country in which the signage is being displayed. For example, In UK and EU signage, the width of a sign's pictogram must be 80% the height of the area it is printed to. In the US, to comply with the ADA Accessibility Guidelines, the same pictogram must be located within its own defined field, with raised characters and braille located beneath the field.

For a pictogram to be successful it must be recognizable across cultures and languages, even if there is no text present. Following standard color and shape conventions increases the likelihood that the pictogram and sign will be universally understood.

The shape of a sign can help to convey its message. Shape can be brand- or design-based, or can be part of a set of signage conventions used to standardize sign meaning. Usage of particular shapes may vary by country and culture.

Some common signage shape conventions are as follows:

Below is a list of commonly used materials in signmaking shops.

Below is a list of commonly used processes in signmaking shops.

Signs frequently use lighting as a means of conveying their information or as a way to increase visibility.

Neon signs, introduced in 1910 at the Paris Motor Show, are produced by the craft of bending glass tubing into shapes. A worker skilled in this craft is known as a glass bender, neon or tube bender.

Light-emitting diodes (LEDs ) are frequently used in signs for both general illumination, display of alphanumeric characters with animation effects, or as part of multi-pixel video displays.  LED signs became common at sport venues, businesses, churches, schools, and government buildings. Brightness of LED signs can vary, leading to some municipalities in the United States banning their use due to issues such as light pollution.[30]

Signage is one of the most common visual elements of a wayfinding system. In order for the system to be successful in helping its users navigate within the space and seek out a desired destination, signage must use appropriate language and symbols, have a high degree of visibility, and be appropriately placed within the environment. 

There are four major types of signage in wayfinding systems.[31][32] They are:
"
Media Production Services,"A production company, production house or production studio is a studio that creates works in the fields of performing arts, new media art, film, television, radio, comics, interactive arts, video games, websites, music, and video. These groups consist of technical staff and members to 
produce the media, and are often incorporated as a commercial publisher.

Generally the term refers to all individuals responsible for the technical aspects of creating a particular product, regardless of where in the process their expertise is required, or how long they are involved in the project.  For example, in a theatrical performance, the production team has not only the running crew, but also the theatrical producer, designers, and theatrical direction.

The production company may be directly responsible for fundraising the production or may accomplish this through a parent company, partner, or private investor. It handles budgeting, scheduling, scripting, the supply with talent and resources, the organization of staff, the production itself, post-production, distribution, and marketing.[1]

Production companies are often either owned or under contract with a media conglomerate, film studio, record label, video game publisher, or entertainment company, due to the concentration of media ownership, who act as the production company's partner or parent company. This has become known as the ""studio system"". Independent studios usually prefer production house (see Lionsgate), production studio (see Amazon Studios), or production team (see Rooster Teeth). In the case of television, a production company would serve under a television network. Production companies can work together in co-productions. In music, the term production team typically refers to a group of individuals filling the role of ""record producer"" usually reserved for one individual. Some examples of musical production teams include Matmos and D-Influence.

The aforementioned publishing conglomerates distribute said creative works, but it is not uncommon for production companies to act as a publication. For example, The Walt Disney Company and Nintendo act as publisher for Walt Disney Animation Studios and Nintendo Entertainment Planning & Development respectively. Self-publishing, in this case, should not be confused with a self-publishing vanity press which are paid services. Both large and small production studios have an editorial board or editor-in-chief, along with other forms of a command hierarchy.

Entertainment companies operate as mini conglomerate, operating many divisions or subsidiaries in many different industries. Warner Bros. Entertainment and Lionsgate Entertainment are two companies with this corporate structure. It allows for a single company to maintain control over seemingly unrelated companies that fall within the ranges of entertainment, which increases and centralises the revenue into one company (example: a film production company, TV production company, video game company, and comic book company are all owned by a single entertainment company). A motion picture company, such as Paramount Pictures, specializing ""only"" in motion pictures is only connected with its other counterpart industries through its parent company. Instead of performing a corporate reorganization, many motion picture companies often have sister companies they collaborate with in other industries that are subsidiaries owned by their parent company and is often not involved in the making of products that are not motion picture related. A film production company can either operate as an affiliate (under a contract) or as a subsidiary for an entertainment company, motion picture company, television network, or all, and are generally smaller than the company they are partnered with.

A book to film unit is a unit of a book publishing company for the purposes of getting books that they published adapted into film.

Films have been using books as a prime source for films for years. In 2012, six out of the nine best picture Oscar nominees were originally books. Previously, publishers did not develop their books into movie nor receive any of the profits. Neither Scholastic or Little Brown, get any box office revenue from the Harry Potter and The Twilight Saga movies just through book sales. As the publishers faced decreasing revenue due to increased competition from self-published e-books, or Amazon.com moving into the publishing field, publishers have started to enter the film and TV production business to boost their net income[2] with Amazon attempting to compete there too. More screenwriters are turning to book publishers to get their screenplay published as a book, so as to have a boost in their attempt to have the screenplay turned into a movie, given that it is a known product after the book.[3][4]

Publisher Simon & Schuster, though not involved with film and TV, shares possible film and TV deals with CBS (S&S is owned by Paramount Global).[5] Alloy Entertainment while not a unit of a publisher started using a book packaging to film model of film and TV development by developing the property in-house, hire authors for the books and films, so as to own the property. Random House was the first big six book publisher to establish a book to film unit, Random House Films, in 2005 with a Focus Features deal under a development and co-finance plan.[2]

Macmillan Films was launched by Thomas Dunne Books in October 2010 under the packaging model similar to Alloy while also moving to get film rights from Dunne's published author.[6] Also that year, Random House changed their strategy to film development and packaging only.[2]

Condé Nast Entertainment was started by Magazine publisher Conde Nast in October 2012.[2] In 2013, Macmillan Films became Macmillan Entertainment with an expansion to look at other divisions' book for possible films.[7][8]

A production company is usually run by a producer or director, but can also be run by a career executive. In entertainment, a production company relies highly on talent or a well known entertainment franchise to raise the value of an entertainment project and draw out larger audiences. This gives the entertainment industry a democratized power structure to ensure that both the companies and talent receive their fair share of pay and recognition for work done on a production.

The entertainment industry is centered on funding (investments from studios, investment firms, or individuals either from earnings from previous productions or personal wealth), projects (scripts and entertainment franchises), and talent (actors, directors, screenwriters, and crew). Production companies are judged and ranked based on the amount of funding it has, as well the productions it has completed or been involved with in the past. If a production company has major funding either through earnings, studio investors, or private investors, and has done or been involved with big budget productions in the past, it is considered to be a major production company. These companies often work with well-known and expensive talent. If a production company does not have much funding and has not done or been involved with any big budget productions, it is considered to be a small production company. These companies often work with up and coming talent.

Small production companies will either grow to become a major production company, a subsidiary completely owned by another company, remain small, or fail. The success of an entertainment production company is centered on the projects it produces, the talent it can acquire, and the performance of the talent. Marketing is also a major factor. All films, as a tradition, are often marketed around the image and the performance of the actors; with an option of marketing the behind the scenes crew such as the directors and screenwriters. Unlike many other businesses, a production company does not rely on an ongoing revenue stream, they operate on ongoing investments; this often requires a parent company or a private corporate investment entity (see Legendary Pictures). Their only source of profit comes from the productions they produce. Because entertainment and media are currently in ""high demand"", a production company can profit if its management is capable of using its resources to supply good quality products and services to the public. Many entertainment production companies brand their entertainment projects. An entertainment project can either become a ""one time hit"" or an ongoing ""entertainment franchise"" that can be continued, remade, rebooted, or expanded into other sister industries; such as the video game industry (see Star Wars, Star Trek). Entertainment projects can be either an original or an adaptation from another industry.

In rare occasional cases, a few troubled major studios would also shed their distribution and/or marketing staffs, mainly due to reduced resources, and resort to co-investing and/or co-distributing film projects with larger studios, operating as virtual, production-only movie studios. Notable examples include legendary studio Metro-Goldwyn-Mayer, which, after many years of box office flops (mostly with low budgets), bad management and distribution, and bankruptcy, was restructured at the end of 2010 under new management and currently struck deals with some of the Big Six studios (most notably the Sony Pictures Motion Picture Group and Warner Bros.); Miramax, which was downsized by former owner Disney into a smaller division.

Because a production company is only operational when a production is being produced and most of the talent and crew are freelancers, many production companies are only required to hire management staff that helps to oversee the company's daily activities. In some cases, a production company can be run by only a handful of people. The company's funds are mainly committed towards employing talent, crew, and acquiring new updated production equipment on a regular basis. Many productions often require at least one to two cameras and lighting equipment for on location shooting. Production equipment is either leased or purchased from another production company or directly from the manufacturer. In the entertainment industry, in order to secure experienced professional talent and crew, production companies often become a signatory company to that talent or crew members ""guild"". By becoming a signatory company, it agrees to abide by the guild regulations. All big budget guild productions are exclusive to guild members and non guild members are not allowed to participate in these productions unless authorized by the guild.  Productions with smaller budgets are allowed to use both guild talent and talent from the public. The majority of the talent and crew working in the entertainment industry are members of their professions guild. Most productions in the entertainment industry are guild productions.

A production company is responsible for the development and filming of a specific production or media broadcast. In entertainment, the production process begins with the development of a specific project. Once a final script has been produced by the screenwriters, the production enters into the pre-production phase, most productions never reach this phase for financing or talent reasons. In pre-production, the actors are signed on and prepared for their roles, crew is signed on, shooting locations are found, sets are built or acquired, and the proper shooting permits are acquired for on location shooting. Actors and crew are hand picked by the producer, director, and casting director, who often use collaborators or referenced personnel to prevent untrusted or unwelcomed people from gaining access to a specific production and compromising the entire production through leaks. Once a production enters into principal photography, it begins filming. Productions are almost never cancelled once they reach this phase. Codenames are often used on bigger productions during filming to conceal the production's shooting locations for both privacy and safety reasons. In many cases, the director, producers, and the leading actors are often the only people with access to a full or majority of a single script. Supporting actors, background actors, and crew often never receive a full copy of a specific script to prevent leaks. Productions are often shot in secured studios, with limited to no public access, but they are also shot on location on secured sets or locations. Due to the exposure, when shooting in public locations, major productions often employ security to ensure the protection of the talent and crew working on a specific production. After filming is completed, the production enters into post production, which is handled by a post production company and overseen by the production company. The editing, musical score, visual effects, re-recording of the dialog, and sound effects are ""mixed"" to create the final film, which is then screened at the final screening. Marketing is also launched during this phase, such as the release of trailers and posters. Once a final film has been approved, the film is taken over by the distributors, who then release the film.

For legal reasons, it is common within the entertainment industry for production companies not to accept unsolicited materials from any other company, talent, or the general public. It is also common for filmmakers or producers to become entrepreneurs and open their own production companies so that they can have more control over their careers and pay, while acting as an ""in-house"" creative and business driving force for their company but continuing to freelance as an artist for other companies, if desired.
"
Software Manufacturing,"A software factory is a structured collection of related software assets that aids in producing computer software applications or software components according to specific, externally defined end-user requirements through an assembly process.[1] A software factory applies manufacturing techniques and principles to software development to mimic the benefits of traditional manufacturing.  Software factories are generally involved with outsourced software creation.

In software engineering and enterprise software architecture, a software factory is a software product line that configures extensive tools, processes, and content using a template based on a schema to automate the development and maintenance of variants of an archetypical product by adapting, assembling, and configuring framework-based components.[2]

Since coding requires a software engineer (or the parallel in traditional manufacturing, a skilled craftsman) it is eliminated from the process at the application layer, and the software is created by assembling predefined components instead of using traditional IDEs. Traditional coding is left only for creating new components or services. As with traditional manufacturing, the engineering is left to creation of the components and the requirements gathering for the system.  The end result of manufacturing in a software factory is a composite application.

Software–factory–based application development addresses the problem of traditional application development where applications are developed and delivered without taking advantage of the knowledge gained and the assets produced from developing similar applications. Many approaches, such as training, documentation, and frameworks, are used to address this problem; however, using these approaches to consistently apply the valuable knowledge previously gained during development of multiple applications can be an inefficient and error-prone process.

Software factories address this problem by encoding proven practices for developing a specific style of application within a package of integrated guidance that is easy for project teams to adopt. Developing applications using a suitable software factory can provide many benefits, such as improved productivity, quality and evolution capability.[1]

Software factories are unique and therefore contain a unique set of assets designed to help build a specific type of application. In general, most software factories contain interrelated assets of the following types:

Building a product using a software factory involves the following activities:

Developing applications using a software factory can provide many benefits when compared to conventional software development approaches. These include the following:

These benefits can provide value to several different teams in the following ways:

Business tasks can be simplified which can significantly increase user productivity. This is achieved through using common and consistent user interfaces that reduce the need for end-user training. Easy deployment of new and updated functionality and flexible user interfaces also allows end users to perform tasks in a way that follows the business workflow. Data quality improvements reduce the need for data exchange between application parts through the ALT+TAB and copy and paste techniques.[3]

Software factories can be used by architects to design applications and systems with improved quality and consistency. This is achieved through the ability to create a partial implementation of a solution that includes only the most critical mechanisms and shared elements. Known as the baseline architecture, this type of implementation can address design and development challenges, expose architectural decisions and mitigate risks early in the development cycle. Software factories also enable the ability to create a consistent and predictable way of developing, packaging, deploying and updating business components to enforce architectural standards independent of business logic.[3]

Developers can use software factories to increase productivity and incur less ramp-up time. This is achieved through creating a high-quality starting point (baseline) for applications which includes code and patterns. This enables projects to begin with a higher level of maturity than traditionally developed applications. Reusable assets, guidance and examples help address common scenarios and challenges and automation of common tasks allows developers to easily apply guidance in consistent ways. Software factories provide a layer of abstraction that hides application complexity and separates concerns, allowing developers to focus on different areas such as business logic, the user interface (UI) or application services without in-depth knowledge of the infrastructure or baseline services. Abstraction of common developer tasks and increased reusability of infrastructure code can help boost productivity and maintainability.[3]

Applications built with software factories result in a consolidation of operational efforts. This provides easier deployment of common business elements and modules, resulting in consistent configuration management across a suite of applications. Applications can be centrally managed with pluggable architecture which allows operations teams to control basic services.[3]

There are several approaches that represent contrasting views on software factory concepts, ranging from tool oriented to process oriented initiatives. The following approaches cover Japanese, European, and North American initiatives.[4]

Under this approach, software produced in the software factory is primarily used for control systems, nuclear reactors, turbines, etc. The main objectives of this approach are quality matched with productivity, ensuring that the increased costs do not weaken competitiveness. There is also the additional objective of creating an environment in which design, programming, testing, installation and maintenance can be performed in a unified manner.

The key in improving quality and productivity is the reuse of software. Dominant traits of the organizational design include a determined effort to make operating work routine, simple and repetitive and to standardize work processes.

A representative of this approach would be Toshiba's software factory concept, denoting the company's software division and procedures as they were in 1981 and 1987 respectively.

This approach was funded under the Eureka program and called the Eureka Software Factory. Participants in this project are large European companies, computer manufacturers, software houses, research institutes and universities. The aim of this approach is to provide the technology, standards, organizational support and other necessary infrastructures in order for software factories to be constructed and tailored from components marketed by independent suppliers.

The objective of this approach is to produce an architecture and framework for integrated development environments. The generic software factory develops components and production environments that are part of software factories together with standards and guidance for software components.

The experienced-based component factory is developed at the Software Engineering Laboratory at the NASA Goddard Space Flight Center. The goals of this approach are to ""understand the software process in a production environment, determine the impact of available technologies and infuse identified/refined methods back into the development process"". The approach has been to experiment with new technologies in a production environment, extract and apply experiences and data from experiments and to measure the impact with respect to cost, reliability and quality.

This approach puts a heavy emphasis on continuous improvement through understanding the relationship between certain process characteristics and product qualities. The software factory is used to collect data about strengths and weaknesses to set baselines for improvements and to collect experiences to be reused in new projects.

Defined by the Capability Maturity Model, this approach intended to create a framework to achieve a predictable, reliable, and self-improving software development process that produces software of high quality. The strategy consists of step-wise improvements in software organization, defining which processes are key in development. The software process and the software product quality are predictable because they are kept within measurable limits.

Cusumano[8] suggests that there are six phases for software factories:
"
Food Processing Services,"Food processing is the transformation of agricultural products into food, or of one form of food into other forms. Food processing takes many forms, from grinding grain into raw flour, home cooking, and complex industrial methods used in the making of convenience foods.  Some food processing methods play important roles in reducing food waste and improving food preservation, thus reducing the total environmental impact of agriculture and improving food security.

The Nova classification groups food according to different food processing techniques.

Primary food processing is necessary to make most foods edible while secondary food processing turns ingredients into familiar foods, such as bread. Tertiary food processing results in ultra-processed foods and has been widely criticized for promoting overnutrition and obesity, containing too much sugar and salt, too little fiber, and otherwise being unhealthful in respect to dietary needs of humans and farm animals.

Primary food processing turns agricultural products, such as raw wheat kernels or livestock, into something that can eventually be eaten. This category includes ingredients that are produced by ancient processes such as drying, threshing, winnowing and milling grain, shelling nuts, and butchering animals for meat.[1][2]  It also includes deboning and cutting meat, freezing and smoking fish and meat, extracting and filtering oils, canning food, preserving food through food irradiation, and candling eggs, as well as homogenizing and pasteurizing milk.[2][3][4]

Contamination and spoilage problems in primary food processing can lead to significant public health threats, as the resulting foods are used so widely.[2]  However, many forms of processing contribute to improved food safety and longer shelf life before the food spoils.[3]  Commercial food processing uses control systems such as hazard analysis and critical control points (HACCP) and failure mode and effects analysis (FMEA) to reduce the risk of harm.[2]

Secondary food processing is the everyday process of creating food from ingredients that are ready to use. Baking bread, regardless of whether it is made at home, in a small bakery, or in a large factory, is an example of secondary food processing.[2] Fermenting fish and making wine, beer, and other alcoholic products are traditional forms of secondary food processing.[4] Sausages are a common form of secondary processed meat, formed by comminution (grinding) of meat that has already undergone primary processing.[5] Most of the secondary food processing methods known to humankind are commonly described as cooking methods.

Tertiary food processing is the commercial production of what is commonly called processed food.[2]  These are ready-to-eat or heat-and-serve foods, such as frozen meals and re-heated airline meals.

Food processing dates back to the prehistoric ages when crude processing incorporated fermenting, sun drying, preserving with salt, and various types of cooking (such as roasting, smoking, steaming, and oven baking), Such basic food processing involved chemical enzymatic changes to the basic structure of food in its natural form, as well served to build a barrier against surface microbial activity that caused rapid decay. Salt-preservation was especially common for foods that constituted warrior and sailors' diets until the introduction of canning methods. Evidence for the existence of these methods can be found in the writings of the ancient Greek, Chaldean, Egyptian and Roman civilizations as well as archaeological evidence from Europe, North and South America and Asia. These tried and tested processing techniques remained essentially the same until the advent of the Industrial Revolution. Examples of ready-meals also date back to before the preindustrial revolution, and include dishes such as Cornish pasty and Haggis. Both during ancient times and today in modern society these are considered processed foods.

Modern food processing technology developed in the 19th and 20th centuries was developed in a large part to serve military needs. In 1809, Nicolas Appert invented a hermetic bottling technique that would preserve food for French troops which ultimately contributed to the development of tinning, and subsequently canning by Peter Durand in 1810. Although initially expensive and somewhat hazardous due to the lead used in cans, canned goods would later become a staple around the world.[6] Pasteurization, discovered by Louis Pasteur in 1864, improved the quality and safety of preserved foods and introduced the wine, beer, and milk preservation.

In the 20th century, World War II, the space race and the rising consumer society in developed countries contributed to the growth of food processing with such advances as spray drying, evaporation, juice concentrates, freeze drying and the introduction of artificial sweeteners, colouring agents, and such preservatives as sodium benzoate. In the late 20th century, products such as dried instant soups, reconstituted fruits and juices, and self cooking meals such as MRE food ration were developed. By the 20th century, automatic appliances like microwave oven, blender, and rotimatic paved way for convenience cooking.

In western Europe and North America, the second half of the 20th century witnessed a rise in the pursuit of convenience. Food processing companies marketed their products especially towards middle-class working wives and mothers. Frozen foods (often credited to Clarence Birdseye) found their success in sales of juice concentrates and ""TV dinners"".[7] Processors utilised the perceived value of time to appeal to the postwar population, and this same appeal contributes to the success of convenience foods today.

Also in the late 20th century, food manufacturers began changing their product model from a single ""platonic dish"", such as one version of jarred spaghetti sauce, to offering multiple variations, such as a plain version, a spicy version, and a chunky version.[8]

Benefits of food processing include toxin removal, preservation, easing marketing and distribution tasks, and increasing food consistency. In addition, it increases yearly availability of many foods, enables transportation of delicate perishable foods across long distances and makes many kinds of foods safe to eat by de-activating spoilage and pathogenic micro-organisms. Modern supermarkets would not exist without modern food processing techniques, and long voyages would not be possible.

Processed foods are usually less susceptible to early spoilage than fresh foods and are better suited for long-distance transportation from the source to the consumer.[3] When they were first introduced, some processed foods helped to alleviate food shortages and improved the overall nutrition of populations as it made many new foods available to the masses.[9]

Processing can also reduce the incidence of food-borne disease. Fresh materials, such as fresh produce and raw meats, are more likely to harbour pathogenic micro-organisms (e.g. Salmonella) capable of causing serious illnesses.[citation needed]

The extremely varied modern diet is only truly possible on a wide scale because of food processing. Transportation of more exotic foods, as well as the elimination of much hard labor gives the modern eater easy access to a wide variety of food unimaginable to their ancestors.[10]

The act of processing can often improve the taste of food significantly.[11]

Mass production of food is much cheaper overall than individual production of meals from raw ingredients. Therefore, a large profit potential exists for the manufacturers and suppliers of processed food products. Individuals may see a benefit in convenience, but rarely see any direct financial cost benefit in using processed food as compared to home preparation.[citation needed]

Processed food freed people from the large amount of time involved in preparing and cooking ""natural"" unprocessed foods.[12]  The increase in free time allows people much more choice in life style than previously allowed. In many families the adults are working away from home and therefore there is little time for the preparation of food based on fresh ingredients. The food industry offers products that fulfill many different needs: e.g. fully prepared ready meals that can be heated up in the microwave oven within a few minutes.

Modern food processing also improves the quality of life for people with allergies, diabetics, and other people who cannot consume some common food elements.[citation needed] Food processing can also add extra nutrients such as vitamins.

Processing of food can decrease its nutritional density. The amount of nutrients lost depends on the food and processing method. For example, heat destroys vitamin C. Therefore, canned fruits possess less vitamin C than their fresh alternatives. The USDA conducted a study of nutrient retention in 2004, creating a table of foods, levels of preparation, and nutrition.[13]

New research highlighting the importance to human health of a rich microbial environment in the intestine indicates that abundant food processing (not fermentation of foods) endangers that environment.[14]

One of the main sources for sodium in the diet is processed foods. Sodium, mostly in the form of sodium chloride, i.e. salt, is added to prevent spoilage, add flavor and enhance the texture of these foods. Americans consume an average of 3436 milligrams of sodium per day, which is higher than the recommended limit of 2300 milligrams per day for healthy people, and more than twice the limit of 1500 milligrams per day for those at increased risk for heart disease.

While it is not necessary to limit the sugars found naturally in whole, unprocessed foods like fresh fruit, eating too much added sugar found in many processed foods increases the risk of heart disease, obesity, cavities and Type 2 diabetes. The American Heart Association recommends women limit added sugars to no more than 420 kilojoules (100 kilocalories), or 25 grams, and men limit added sugars to no more than 650 kJ (155 kcal), or about 38.75 grams, per day. Currently, Americans consume an average of 1,490 kJ (355 kcal) from added sugars each day.

Processing foods often involves nutrient losses, which can make it harder to meet the body's needs if these nutrients are not added back through fortification or enrichment. For example, using high heat during processing can cause vitamin C losses. Another example is refined grains, which have less fiber, vitamins and minerals than whole grains. Eating refined grains, such as those found in many processed foods, instead of whole grains may increase the risk for high cholesterol, diabetes and obesity, according to a study published in ""The American Journal of Clinical Nutrition"" in December 2007.[citation needed]

Foods that have undergone processing, including some commercial baked goods, desserts, margarine, frozen pizza, microwave popcorn and coffee creamers, sometimes contain trans fats. This is the most unhealthy type of fat, and may increase risk for high cholesterol, heart disease and stroke. The 2010 Dietary Guidelines for Americans recommends keeping trans fat intake as low as possible.

Processed foods may actually take less energy to digest than whole foods, according to a study published in ""Food & Nutrition Research"" in 2010, meaning more of their food energy content is retained within the body. Processed foods also tend to be more allergenic than whole foods, according to a June 2004 ""Current Opinion in Allergy and Clinical Immunology"" article. Although the preservatives and other food additives used in many processed foods are generally recognized as safe, a few may cause problems for some individuals, including sulfites, artificial sweeteners, artificial colors and flavors, sodium nitrate, BHA and BHT, olestra, caffeine and monosodium glutamate — a flavor enhancer.[15]

When designing processes for the food industry the following performance parameters may be taken into account:

Food processing industries and practices include the following:
"
Pharmaceutical Manufacturing,"

Pharmaceutical manufacturing is the process of industrial-scale synthesis of pharmaceutical drugs as part of the pharmaceutical industry.  The process of drug manufacturing can be broken down into a series of unit operations, such as milling, granulation, coating, tablet pressing, and others.

While a laboratory may use dry ice as a cooling agent for reaction selectivity, this process gets complicated on an industrial scale. The cost to cool a typical reactor to this temperature is large, and the viscosity of the reagents typically also increases as the temperature lowers, leading to difficult mixing. This results in added costs to stir harder and replace parts more often, or it results in a non-homogeneous reaction. Finally, lower temperatures can result in crusting of reagents, intermediates, and byproducts to the reaction vessel over time, which will impact the purity of the product.[1]

Different stoichiometric ratios of reagents can result in different ratios of products formed. On the industrial scale, adding a large amount of reagent A to reagent B may take time. During this, the reagent A that is added is exposed to a much higher stoichiometric amount of reagent B until it is all added, and this imbalance can lead to reagent A prematurely reacting, and subsequent products to also react with the huge excess of reagent B.[1]

Whether to add organic solvent into aqueous solvent, or vice versa, becomes important on the industrial scale. Depending on the solvents used, emulsions can form, and the time needed for the layers to separate can be extended if the mixing between solvents is not optimal. When adding organic solvent to aqueous, stoichiometry must be considered again, as the excess of water could hydrolyze organic compounds in only mildly acidic or basic conditions. In an even wider scope, the location of the chemical plant can play a role in the ambient temperature of the reaction vessel. A difference of even a couple of degrees can yield much different levels of extractions between plants located across countries.[1]

In continuous manufacturing, input raw materials and energy are fed into the system at a constant rate, and at the same time, a constant extraction of output products is achieved. The process's performance is heavily dependent on the stability of the material flowrate. For powder-based continuous processes, it is critical to feed powders consistently and accurately into subsequent unit operations of the process line, as feeding is typically the first unit operation.[2] Feeders have been designed to achieve performance reliability, feed rate accuracy, and minimal disturbances. Accurate and consistent delivery of materials by well-designed feeders ensures overall process stability. Loss-in-weight (LIW) feeders are selected for pharmaceutical manufacturing. Loss-in-weight (LIW) feeders control material dispensing by weight at a precise rate and are often selected to minimize the flowrate variability that is caused by change of fill level and material bulk density. Importantly, feeding performance is strongly dependent on powder flow properties.[3][4]

In the pharmaceutical industry, a wide range of excipients may be blended together with the active pharmaceutical ingredient to create the final blend used to manufacture the solid dosage form. The range of materials that may be blended (excipients, API), presents a number of variables which must be addressed to achieve target product quality attributes. These variables may include the particle size distribution (including aggregates or lumps of material), particle shape (spheres, rods, cubes, plates, and irregular), presence of moisture (or other volatile compounds), particle surface properties (roughness, cohesion), and powder flow properties.[5][pages needed][6]

During the drug manufacturing process, milling is often required in order to reduce the average particle size in a drug powder.[7] There are a number of reasons for this, including increasing homogeneity and dosage uniformity, increasing bioavailability, and increasing the solubility of the drug compound.[8] In some cases, repeated powder blending followed by milling is conducted to improve the manufacturability of the blends.

In general, there are two types of granulation: wet granulation and dry granulation. Granulation can be thought of as the opposite of milling; it is the process by which small particles are bound together to form larger particles, called granules. Granulation is used for several reasons. Granulation prevents the ""demixing"" of components in the mixture, by creating a granule which contains all of the components in their required proportions, improves flow characteristics of powders (because small particles do not flow well), and improves compaction properties for tablet formation.[9][10]

Hot melt extrusion is utilized in pharmaceutical solid oral dose processing to enable delivery of drugs with poor solubility and bioavailability. Hot melt extrusion has been shown to molecularly disperse poorly soluble drugs in a polymer carrier increasing dissolution rates and bioavailability. The process involves the application of heat, pressure and agitation to mix materials together and 'extrude' them through a die. Twin-screw high shear extruders blend materials and simultaneously break up particles. The resulting particles can be blended and compressed into tablets or filled into capsules.[11]

The documentation of activities by pharmaceutical manufacturers is a license-to-operate endeavor, supporting both the quality of the product produced and satisfaction of regulators who oversee manufacturing operations and determine whether a manufacturing process may continue or must be terminated and remediated.

A Site Master File is a document in the pharmaceutical industry which provides information about the production and control of manufacturing operations. The document is created by a manufacturer.[12]  The Site Master file contains specific and factual GMP information about the production and control of pharmaceutical manufacturing operations carried out at the named site and any closely integrated operations at adjacent and nearby buildings. If only part of a pharmaceutical operation is carried out on the site, the site master file needs to describe only those operations, e.g., analysis, packaging.[13]
"
Laboratory Services,"A medical laboratory or clinical laboratory is a laboratory where tests are conducted out on clinical specimens to obtain information about the health of a patient to aid in diagnosis, treatment, and prevention of disease.[1] Clinical medical laboratories are an example of applied science, as opposed to research laboratories that focus on basic science, such as found in some academic institutions.

Medical laboratories vary in size and complexity and so offer a variety of testing services.  More comprehensive services can be found in acute-care hospitals and medical centers, where 70% of clinical decisions are based on laboratory testing.[2]  Doctors offices and clinics, as well as skilled nursing and long-term care facilities, may have laboratories that provide more basic testing services. Commercial medical laboratories operate as independent businesses and provide testing that is otherwise not provided in other settings due to low test volume or complexity.[3]

In hospitals and other patient-care settings, laboratory medicine is provided by the Department of Pathology and Medical Laboratory, and generally divided into two sections, each of which will be subdivided into multiple specialty areas.[4] The two sections are:

Layouts of clinical laboratories in health institutions vary greatly from one facility to another. For instance, some health facilities have a single laboratory for the microbiology section, while others have a separate lab for each specialty area.

The following is an example of a typical breakdown of the responsibilities of each area:

The staff of clinical laboratories may include:

The United States has a documented shortage of working laboratory professionals.  For example, as of 2016[update] vacancy rates for Medical Laboratory Scientists ranged from 5% to 9% for various departments.  The decline is primarily due to retirements, and to at-capacity educational programs that cannot expand which limits the number of new graduates.  Professional organizations and some state educational systems are responding by developing ways to promote the lab professions in an effort to combat this shortage. In addition, the vacancy rates for the MLS were tested again in 2018. The percentage range for the various departments has developed a broader range of 4% to as high as 13%.[13] The higher numbers were seen in the Phlebotomy and Immunology.[13] Microbiology was another department that has had a struggle with vacancies.[13] Their average in the 2018 survey was around 10-11% vacancy rate across the United States.[13] Recruitment campaigns, funding for college programs, and better salaries for the laboratory workers are a few ways they are focusing to decrease the vacancy rate.[14]  The National Center For Workforce Analysis has estimated that by 2025 there will be a 24% increase in demand for lab professionals.[15][16] Highlighted by the COVID-19 pandemic, work is being done to address this shortage including bringing pathology and laboratory medicine into the conversation surrounding access to healthcare.[17] COVID-19 brought the laboratory to the attention of the government and the media, thus giving opportunity for the staffing shortages as well as the resource challenges to be heard and dealt with.[18]

In most developed countries, there are two main types of lab processing the majority of medical specimens. Hospital laboratories are attached to a hospital, and perform tests on their patients. Private (or community) laboratories receive samples from general practitioners, insurance companies, clinical research sites and other health clinics for analysis. For extremely specialised tests, samples may go to a research laboratory.  Some tests involve specimens sent between different labs for uncommon tests. For example, in some cases it may be more cost effective if a particular laboratory specializes in a less common tests, receiving specimens (and payment) from other labs, while sending other specimens to other labs for those tests they do not perform.

In many countries there are specialized types of medical laboratories according to the types of investigations carried out. Organisations that provide blood products for transfusion to hospitals, such as the Red Cross, will provide access to their reference laboratory for their customers. Some laboratories specialize in Molecular diagnostic and cytogenetic testing, in order to provide information regarding diagnosis and treatment of genetic or cancer-related disorders.

In a hospital setting, sample processing will usually start with a set of samples arriving with a test request, either on a form or electronically via the laboratory information system (LIS). Inpatient specimens will already be labeled with patient and testing information provided by the LIS.  Entry of test requests onto the LIS system involves typing (or scanning where barcodes are used) in the laboratory number, and entering the patient identification, as well as any tests requested. This allows laboratory analyzers, computers and staff to recognize what tests are pending, and also gives a location (such as a hospital department, doctor or other customer) for results reporting.

Once the specimens are assigned a laboratory number by the LIS, a sticker is typically printed that can be placed on the tubes or specimen containers.  This label has a barcode that can be scanned by automated analyzers and test requests uploaded to the analyzer from the LIS.

Specimens are prepared for analysis in various ways. For example, chemistry samples are usually centrifuged and the serum or plasma is separated and tested. If the specimen needs to go on more than one analyzer, it can be divided into separate tubes.

Many specimens end up in one or more sophisticated automated analysers, that process a fraction of the sample to return one or more test results. Some laboratories use robotic sample handlers (Laboratory automation) to optimize the workflow and reduce the risk of contamination from sample handling by the staff.

The work flow in a hospital laboratory is usually heaviest from 2:00 am to 10:00 am.  Nurses and doctors generally have their patients tested at least once a day with common tests such as complete blood counts and chemistry profiles.  These orders are typically drawn during a morning run by phlebotomists for results to be available in the patient's charts for the attending physicians to consult during their morning rounds.  Another busy time for the lab is after 3:00 pm when private practice physician offices are closing.  Couriers will pick up specimens that have been drawn throughout the day and deliver them to the lab.  Also, couriers will stop at outpatient drawing centers and pick up specimens. These specimens will be processed in the evening and overnight to ensure results will be available the following day.

The large amount of information processed in laboratories is managed by a system of software programs, computers, and terminology standards that exchange data about patients, test requests, and test results known as a Laboratory information system or LIS.  The LIS is often interfaced with the hospital information system, EHR and/or laboratory instruments. Formats for terminologies for test processing and reporting are being standardized  with systems such as Logical Observation Identifiers Names and Codes (LOINC) and Nomenclature for Properties and Units terminology (NPU terminology).

These systems enable hospitals and labs to order the correct test requests for each patient, keep track of individual patient and specimen histories, and help guarantee a better quality of results.  Results are made available to care providers electronically or by printed hard copies for patient charts.

According to various regulations, such as the international ISO 15189 norm, all pathological laboratory results must be verified by a competent professional. In some countries, staffs composed of clinical scientists do the majority of this work inside the laboratory with certain abnormal results referred to the relevant pathologist. Doctor Clinical Laboratory scientists have the responsibility for limited interpretation of testing results in their discipline in many countries. Interpretation of results can be assisted by some software in order to validate normal or non-modified results.

In other testing areas, only professional medical staff (pathologist or clinical Laboratory) is involved with interpretation and consulting.  Medical staff are sometimes also required in order to explain pathology results to physicians. For a simple result given by phone or to explain a technical problem, often a medical technologist or medical lab scientist can provide additional information.

Medical laboratory departments in some countries are exclusively directed by a specialized Doctor laboratory Science. In others, a consultant, medical or non-medical, may be the head the department. In Europe and some other countries, Clinical Scientists with a Masters level education may be qualified to head the department.  Others may have a PhD and can have an exit qualification equivalent to medical staff (e.g., FRCPath in the UK).

In France, only medical staff (Pharm.D. and M.D. specialized in anatomical pathology or clinical Laboratory Science) are authorized to discuss laboratory results.

Credibility of medical laboratories is paramount to the health and safety of the patients relying on the testing services provided by these labs. Credentialing agencies vary by country. The international standard in use today for the accreditation of medical laboratories is ISO 15189 - Medical laboratories - Requirements for quality and competence.

In the United States, billions of dollars is spent on unaccredited lab tests, such as Laboratory developed tests which do not require accreditation or FDA approval; about a billion USD a year is spent on US autoimmune LDTs alone.[19]  Accreditation is performed by the Joint Commission, College of American Pathologists, AAB (American Association of Bioanalysts), and other state and federal agencies.  Legislative guidelines are provided under CLIA 88 (Clinical Laboratory Improvement Amendments) which regulates Medical Laboratory testing and personnel.

The accrediting body in Australia is NATA, where all laboratories must be NATA accredited to receive payment from Medicare.

In France the accrediting body is the Comité français d'accréditation (COFRAC).  In 2010, modification of legislation established ISO 15189 accreditation as an obligation for all clinical laboratories.[20]

In the United Arab Emirates, the Dubai Accreditation Department (DAC) is the accreditation body that is internationally recognised[21] by the International Laboratory Accreditation Cooperation (ILAC) for many facilities and groups, including Medical Laboratories, Testing and Calibration Laboratories, and Inspection Bodies.

In Hong Kong, the accrediting body is Hong Kong Accreditation Service (HKAS).  On 16 February 2004, HKAS launched its medical testing accreditation programme.

In Canada, laboratory accreditation is not mandatory, but is becoming more and more popular.  Accreditation Canada (AC) is the national reference. Different provincial oversight bodies mandate laboratories in EQA participations like LSPQ (Quebec), IQMH (Ontario) for example.

The laboratory industry is a part of the broader healthcare and health technology industry. Companies exist at various levels, including clinical laboratory services, suppliers of instrumentation equipment and consumable materials, and suppliers and developers of diagnostic tests themselves (often by biotechnology companies).[22]

Clinical laboratory services includes large multinational corporations such LabCorp, Quest Diagnostics, and Sonic Healthcare[23] but a significant portion of revenue, estimated at 60% in the United States, is generated by hospital labs.[24] In 2018, the total global revenue for these companies was estimated to reach $146 billion by 2024.[25] Another estimate places the market size at $205 billion, reaching $333 billion by 2023.[26] The American Association for Clinical Chemistry (AACC) represents professionals in the field.

Clinical laboratories are supplied by other multinational companies which focus on materials and equipment, which can be used for both scientific research and medical testing. The largest of these is Thermo Fisher Scientific.[27] In 2016, global life sciences instrumentation sales were around $47 billion, not including consumables, software, and services.[27] In general, laboratory equipment includes lab centrifuges, transfection solutions, water purification systems, extraction techniques, gas generators, concentrators and evaporators, fume hoods, incubators, biological safety cabinets, bioreactors and fermenters, microwave-assisted chemistry, lab washers, and shakers and stirrers.[28]

In the United States, estimated total revenue as of 2016 was $75 billion, about 2% of total healthcare spending.[23] In 2016, an estimated 60% of revenue was done by hospital labs, with 25% done by two independent companies (LabCorp and Quest).[24] Hospital labs may also outsource their lab, known as outreach, to run tests; however, health insurers may pay the hospitals more than they would pay a laboratory company for the same test, but as of 2016, the markups were questioned by insurers.[29] Rural hospitals, in particular, can bill for lab outreach under the Medicare's 70/30 shell rule.[30]

Laboratory developed tests are designed and developed inside a specific laboratory and do not require FDA approval; due to technological innovations, they have become more common[31] and are estimated at a total value of $11 billion in 2016.[32]

Due to the rise of high-deductible health plans, laboratories have sometimes struggled to collect when billing patients; consequently, some laboratories have shifted to become more ""consumer-focused"".[33]
"
Waste Management Services,"Waste Management, Inc., doing business as WM, is a waste management, comprehensive waste, and environmental services company operating in North America. Founded in 1968, the company is headquartered in the Bank of America Tower in Houston, Texas.

The company's network includes 337 transfer stations, 254 active landfill disposal sites, 97 recycling plants, 135 beneficial-use landfill gas projects and six independent power production plants. WM provides environmental services to nearly 21 million residential, industrial, municipal and commercial customers in the United States, Canada, and Puerto Rico. With 26,000 collection and transfer vehicles, WM has the largest trucking fleet in the waste industry. Combined with its largest competitor Republic Services, Inc., the two handle more than half of all garbage collection in the United States.[4]

In 1893, Harm Huizenga, a Dutch immigrant, began hauling garbage at $1.25/wagon in Chicago.[5] In 1968, Harm's grandson Wayne Huizenga, Dean Buntrock, and Larry Beck founded Waste Management, Inc. and began aggressively purchasing many of the smaller garbage collection services across the country. In 1971, Waste Management went public, and by 1972, the company had made 133 acquisitions with $82 million in revenue. It had 60,000 commercial and industrial accounts and 600,000 residential customers in 19 states and the provinces of Ontario and Quebec. In the 1980s, Waste Management acquired Service Corporation of America (SCA) to become the largest waste hauler in the country.

Between the years of 1976 and 1997, the executive officers of Waste Management, Inc. began ""cooking"" the accounting books by refusing to record expenses necessary to write off the costs of unsuccessful and abandoned landfill development projects; establishing inflated environmental reserves (liabilities) in connection with acquisitions so that the excess reserves could be used to avoid recording unrelated operating expenses, improperly capitalizing a variety of expenses; failing to establish sufficient reserves (liabilities) to pay for income taxes and other expenses; avoiding depreciation expenses on their garbage trucks by both assigning unsupported and inflating salvage values and extending their useful lives; assigned arbitrary salvage values to other assets that previously had no salvage value; failed to record expenses for decreases in the value of landfills as they were filled with waste, used netting to eliminate approximately $490 million in current period operating expenses and accumulated prior period accounting misstatements by offsetting them against unrelated one-time gains on the sale or exchange of assets; and used geography entries to move tens of millions of dollars between various line items on the company's income statement.[6] Officers were accused of making ""the financials look the way we want to show them."" The top officers settled with the federal government for $30.8 million in 2005, without admitting guilt.[7]

When a new CEO took charge of the company in 1997, he ordered a review of the company's accounting practices in 1997. In 1998 Waste Management restated its 1992–1997 earnings by $1.7 billion, making it the largest restatement in history.

In 1998 Waste Management merged with USA Waste Services, Inc. USA Waste Services CEO John E. Drury retained the chairmanship and CEO position of the combined company. Waste Management then relocated its headquarters from Chicago to Houston. The merged company retained the Waste Management brand. In late 1999, John Drury stepped down as chairman due to brain surgery. Rodney R. Proto then took the position of chairman and CEO. That year also brought trouble for the newly expanded company in the form of an accounting scandal.

In November 1999, turn-around CE was brought in to help Waste Management recover. The company has since implemented new technologies, safety standards, and operational practices.[8]

On July 14, 2008, Waste Management offered a $34 per share all-cash bid to acquire arch-competitor Republic Services, Inc.[9][10] On August 11, 2008, the bid was raised to $37 per share. On August 15, 2008, Republic Services, Inc. denied Waste Management's bid for a second time. On October 13, 2008, Waste Management withdrew its bid for Republic Services, citing financial market turmoil.[11]

In 2015, Winters Brothers assumed all of WM's operations in Connecticut and New York (excluding New York City, and continues to service these regions under contract with WM.[12]

In February 2022, Waste Management CEO Jim Fish announced that the company would officially shorten its trade name to ""WM"" as part of a rebranding to emphasize its growing focus on sustainability and environmental services, including compressed natural gas and landfill gas utilization.[13]

WM agreed a deal to acquire Stericycle Inc for $7.2 billion, in May 2024. Stericycle is a medical waste and paper shredding company based in Illinois, with the deal anticipated to close in Q4 2024.[14]

As of 2023, WM operated 97 recycling facilities.[15] WM has also participated in recycling-focused initiatives with other companies:

In 2009, Waste Management purchased a 40-percent stake in Shanghai Environment Group Co Ltd, a wholly owned subsidiary of Shanghai Chengtou Holding Co Ltd.[21] SEG sought Waste Management's investment in order to benefit from Waste Management's experience in the waste disposal field, as well as improve their technology for waste disposal.[22]

Shareholders sued Wheelabrator Technologies's (WTI) board of directors for breach of their fiduciary duty, challenging the merger of WTI into Waste Management. In 1995, the case, In re Wheelabrator Technologies, Inc. Shareholders Litigation, came before the Court of Chancery of Delaware on an appeal regarding the Board's motion for summary judgment. The shareholders argued the Board breached their duty of care because there was not sufficient process, they didn't look at alternative transactions, didn't consider information regarding waste's legal liabilities, they didn't appoint a committee of independent directors to negotiate the merger, and they didn't adequately consider the terms of the merger; they breached their duty of loyalty, and; they breached their duty to disclose relevant information regarding the merger. Ultimately, the court dismissed the duty of disclosure claim but allowed the duty of loyalty claim to a degree. In regards to the duty of loyalty claim, the court disagreed with both the shareholders and the Board. It labelled the merger as an interested transaction, not a controlled shareholder transaction, so the business judgment rule applies and the burden to prove waste is on the shareholders.[23][24]

Revelations of irregular accounting led to a major drop in stock price and to the replacement of top executives after a new CEO ordered a review of the company's accounting practices in 1998. Waste Management's shareholders lost more than $6 billion in the market value of their investments when the stock price plummeted by more than 33%.[7] The company had augmented the depreciation time length for their property, plant, and equipment, artificially inflating the company's after-tax profits by US$1.7 billion. On July 8, 1999, a class action lawsuit was filed against WMI and certain officers for issuing false statements. Waste Management paid US$457 million to settle a shareholder class-action suit in 2003. The SEC fined Waste Management's independent auditor, Arthur Andersen, US$7 million for its role.[25][26]

In 2005, Waste Management entered into a software licensing agreement (SLA) with SAP AG. Under the agreement, SAP and its wholly owned subsidiary, Tomorrow Now, were to implement SAP's enterprise resource planning software. The implementation began when an eight-month pilot program was established in Waste Management's New Mexico market area, the market-share area at the time. This initial implementation was to be followed in two months with a company-wide implementation from Waste Management's headquarters in Houston, Texas.[27]

In December 2007, Waste Management ended their ERP implementation effort. Waste Management characterized the ERP implementation as non-functional. An SAP sponsored ""Solution Review"" determined that a customized ERP, based upon an updated SAP ERP, would need to be made in order to accommodate a company-wide implementation.

Waste Management sued SAP for the US$100 million to recover the funds it had spent on the failed ERP implementation.[28] In the lawsuit, Waste Management accused SAP of fraud and deception. SAP countered that Waste Management failed to present knowledgeable workers and accurate business models and failed to migrate data from legacy systems. The suit concluded in 2010 under confidential terms and a one-time payment from SAP to Waste Management disclosed to the SEC.[29]

In 2007, Waste Management locked out Teamsters at its largest hauling operation in Alameda County, CA. The lockout lasted a little less than a month and put 900 members of the Teamsters, ILWU, and Machinists Union on picket lines and raised concerns over sanitary impact on the affected communities.[30] The lockout was stopped when affected communities started legal actions against Waste Management. According to Waste Management officials, the company worked over three months to negotiate an agreement fair to both Waste Management and the union. The union did not want to negotiate over the company's proposals and refused to offer their own proposal unless Waste Management agreed to withdraw all proposals from the table.[31] Oakland's City Council reached a settlement with Waste Management over the dispute in March, 2008. The company rebated more than $3 million to customers and Oakland customers received additional services over the next five years.[32]

In 1990, the board of Waste Management adopted an environmental policy, including a policy of no-net-loss of biodiversity on the company's properties.[33][34] Waste Management also took positions around that time supporting legislation on hazardous waste reduction (1988),[35] waste export control (1989),[36] and protection of endangered species (1992).[37]

Waste Management's operations consist of environmental protection, groundwater protection, environmental engineering, and air and gas management. Waste Management currently operates ten full-scale waste treatment landfill projects in the U.S. and Canada. As a member of the Chicago Climate Exchange (CCX), Waste Management made a commitment during the pilot phase to reduce its greenhouse gas emissions by four percent below the average of its 1998–2001 baseline by 2006.[38] They have also replaced nearly 500 diesel-fueled trucks with vehicles that run on 100 percent natural gas. These new garbage and recycling trucks comprise one of the nation's largest fleets of heavy-duty trucks powered exclusively by natural gas.[39]

In November 2009, at Waste Management's Altamont Landfill, a new plant began producing 13,000 gallons a day of LNG fuel from methane gas from the landfill that had fueled an electric power plant since 1969. Waste Management has said that the plant, announced in April 2008, and built and operated by The Linde Group with state funding, is the world's largest facility to convert landfill gas into vehicle fuel.[40][41][42]

Waste Management works with environmental groups in the U.S. to set aside land to create and manage wetlands and wildlife habitats. The company's landfills currently provide approximately 21,000 acres (85 km2) of protected land for wildlife; 73 landfills are certified[43] by the Wildlife Habitat Council.

In May 2011, Waste Management's Wheelabrator division agreed to pay a record $7.5 million settlement with the Commonwealth of Massachusetts for a host of environmental violations at its plants in North Andover, Saugus, and Millbury, Massachusetts. The settlement was announced on May 2, 2011, by the Massachusetts Department of Environmental Protection and Attorney General Martha Coakley's office.[44]

On November 14, 1997, the company reclassified or adjusted certain items in its financial statements for 1996 and the first nine months of 1997.[45]

On August 3, 1999, the company would have to restate first-quarter results downward, partly because of changes in the value of landfills and other
assets in connection with its acquisition last year of Wheelabrator Technologies Inc.[46]
"
Recycling Services,"

Recycling is the process of converting waste materials into new materials and objects. This concept often includes the recovery of energy from waste materials. The recyclability of a material depends on its ability to reacquire the properties it had in its original state.[1] It is an alternative to ""conventional"" waste disposal that can save material and help lower greenhouse gas emissions. It can also prevent the waste of potentially useful materials and reduce the consumption of fresh raw materials, reducing energy use, air pollution (from incineration) and water pollution (from landfilling).

Recycling is a key component of modern waste reduction and is the third component of the ""Reduce, Reuse, and Recycle"" waste hierarchy.[2][3] It promotes environmental sustainability by removing raw material input and redirecting waste output in the economic system.[4] There are some ISO standards related to recycling, such as ISO 15270:2008 for plastics waste and ISO 14001:2015 for environmental management control of recycling practice.

Recyclable materials include many kinds of glass, paper, cardboard, metal, plastic, tires, textiles, batteries, and electronics. The composting and other reuse of biodegradable waste—such as food and garden waste—is also a form of recycling.[5] Materials for recycling are either delivered to a household recycling center or picked up from curbside bins, then sorted, cleaned, and reprocessed into new materials for manufacturing new products.

In ideal implementations, recycling a material produces a fresh supply of the same material—for example, used office paper would be converted into new office paper, and used polystyrene foam into new polystyrene. Some types of materials, such as metal cans, can be remanufactured repeatedly without losing their purity.[6] With other materials, this is often difficult or too expensive (compared with producing the same product from raw materials or other sources), so ""recycling"" of many products and materials involves their reuse in producing different materials (for example, paperboard). Another form of recycling is the salvage of constituent materials from complex products, due to either their intrinsic value (such as lead from car batteries and gold from printed circuit boards), or their hazardous nature (e.g. removal and reuse of mercury from thermometers and thermostats).

Reusing materials has been a common practice for most of human history with recorded advocates as far back as Plato in the fourth century BC.[7] During periods when resources were scarce, archaeological studies of ancient waste dumps show less household waste (such as ash, broken tools, and pottery), implying that more waste was recycled in place of new material.[8] However, archaeological artefacts made from recyclable material, such as glass or metal, may neither be the original object nor resemble it, with the consequence that a successful ancient recycling economy can become invisible when recycling is synonymous with re-melting rather than reuse.[9]

In pre-industrial times, there is evidence of scrap bronze and other metals being collected in Europe and melted down for continuous reuse.[10] Paper recycling was first recorded in 1031 when Japanese shops sold repulped paper.[11][12] In Britain dust and ash from wood and coal fires was collected by ""dustmen"" and downcycled as a base material for brick making. These forms of recycling were driven by the economic advantage of obtaining recycled materials instead of virgin material, and the need for waste removal in ever-more-densely populated areas.[8] In 1813, Benjamin Law developed the process of turning rags into ""shoddy"" and ""mungo"" wool in Batley, Yorkshire, which combined recycled fibers with virgin wool.[13] The West Yorkshire shoddy industry in towns such as Batley and Dewsbury lasted from the early 19th century to at least 1914.

Industrialization spurred demand for affordable materials. In addition to rags, ferrous scrap metals were coveted as they were cheaper to acquire than virgin ore. Railroads purchased and sold scrap metal in the 19th century, and the growing steel and automobile industries purchased scrap in the early 20th century. Many secondary goods were collected, processed and sold by peddlers who scoured dumps and city streets for discarded machinery, pots, pans, and other sources of metal. By World War I, thousands of such peddlers roamed the streets of American cities, taking advantage of market forces to recycle post-consumer materials into industrial production.[14]

Manufacturers of beverage bottles, including Schweppes,[15] began offering refundable recycling deposits in Great Britain and Ireland around 1800. An official recycling system with refundable deposits for bottles was established in Sweden in 1884, and for aluminum beverage cans in 1982; it led to recycling rates of 84–99%, depending on type (glass bottles can be refilled around 20 times).[16]

New chemical industries created in the late 19th century both invented new materials (e.g. Bakelite in 1907) and promised to transform valueless into valuable materials. Proverbially, you could not make a silk purse of a sow's ear—until the US firm Arthur D. Little published in 1921 ""On the Making of Silk Purses from Sows' Ears"", its research proving that when ""chemistry puts on overalls and gets down to business [...] new values appear. New and better paths are opened to reach the goals desired.""[17]

Recycling—or ""salvage"", as it was then usually known—was a major issue for governments during World War II, where financial constraints and significant material shortages made it necessary to reuse goods and recycle materials.[18] These resource shortages caused by the world wars, and other such world-changing events, greatly encouraged recycling.[19][18] It became necessary for most homes to recycle their waste, allowing people to make the most of what was available. Recycling household materials also meant more resources were left available for war efforts.[18] Massive government campaigns, such as the National Salvage Campaign in Britain and the Salvage for Victory campaign in the United States, occurred in every fighting nation, urging citizens to donate metal, paper, rags, and rubber as a patriotic duty.

A considerable investment in recycling occurred in the 1970s due to rising energy costs.[20] Recycling aluminium uses only 5% of the energy of virgin production. Glass, paper and other metals have less dramatic but significant energy savings when recycled.[21]

Although consumer electronics have been popular since the 1920s, recycling them was almost unheard of until early 1991.[22] The first electronic waste recycling scheme was implemented in Switzerland, beginning with collection of old refrigerators, then expanding to cover all devices.[23] When these programs were created, many countries could not deal with the sheer quantity of e-waste, or its hazardous nature, and began to export the problem to developing countries without enforced environmental legislation. (For example, recycling computer monitors in the United States costs 10 times more than in China.) Demand for electronic waste in Asia began to grow when scrapyards found they could extract valuable substances such as copper, silver, iron, silicon, nickel, and gold during the recycling process.[24] The 2000s saw a boom in both the sales of electronic devices and their growth as a waste stream: In 2002, e-waste grew faster than any other type of waste in the EU.[25] This spurred investment in modern automated facilities to cope with the influx, especially after strict laws were implemented in 2003.[26]

As of 2014, the European Union had about 50% of world share of waste and recycling industries, with over 60,000 companies employing 500,000 people and a turnover of €24 billion.[27] EU countries are mandated to reach recycling rates of at least 50%; leading countries are already at around 65%. The overall EU average was 39% in 2013[28]
and is rising steadily, to 45% in 2015.[29][30]

In 2015, the United Nations General Assembly set 17 Sustainable Development Goals. Goal 12, Responsible Consumption and Production, specifies 11 targets ""to ensure sustainable consumption and production patterns"".[31] The fifth target, Target 12.5, is defined as substantially reducing waste generation by 2030, indicated by the National Recycling Rate.

In 2018, changes in the recycling industry have sparked a global ""crisis"". On 31 December 2017, China announced its ""National Sword"" policy, setting new standards for imports of recyclable material and banning materials deemed too ""dirty"" or ""hazardous"". The new policy caused drastic disruptions in the global recycling market, and reduced the prices of scrap plastic and low-grade paper. Exports of recyclable materials from G7 countries to China dropped dramatically, with many shifting to countries in southeast Asia. This generated significant concern about the recycling industry's practices and environmental sustainability. The abrupt shift caused countries to accept more materials than they could process, and raised fundamental questions about shipping waste from developed countries to countries with few environmental regulations—a practice that predated the crisis.[32]

According to the WHO (2023), “Every year millions of electrical and electronic devices are discarded ... a threat to the environment and to human health if they are not treated, disposed of, and recycled appropriately. Common items ... include computers ... e-waste are recycled using environmentally unsound techniques and are likely stored in homes and warehouses, dumped, exported or recycled under inferior conditions. When e-waste is treated using inferior activities, it can release as many as 1000 different chemical substances ... including harmful neurotoxicants such as lead.”[33] A paper in the journal Sustainable Materials & Technologies remarks upon the difficulty of managing e-waste, particularly from home automation products, which, due to their becoming obsolete at a high rate, are putting increasing strain on recycling systems, which have not adapted to meet the recycling needs posed by this type of product.[34]

Copper slag is obtained when copper and nickel ores are recovered from their source ores using a pyrometallurgical process, and these ores usually contain other elements which include iron, cobalt, silica, and alumina.[35] An estimate of 2.2–3 tons of copper slag is generated per ton of copper produced, resulting in around 24.6 tons of slag per year, which is regarded as waste.[36][37]

Environmental impact of slag include copper paralysis, which leads to death due to gastric hemorrhage, if ingested by humans. It may also cause acute dermatitis upon skin exposure.[38] Toxicity may also be uptaken by crops through soil, consequently spreading animals and food sources and increasing the risk of cardiovascular diseases, cancer, cognitive impairment, chronic anemia, and damage to kidneys, bones, nervous system, brain and skin.[39]

Substituting gravel and grit in quarries has been more cost-effective, due to having its sources with better proximity to consumer markets. Trading between countries and establishment of blast furnaces is helping increase slag utilization, hence reducing wastage and pollution.[40]



Economist Steven Landsburg, author of a paper entitled ""Why I Am Not an Environmentalist"",[41] claimed that paper recycling actually reduces tree populations. He argues that because paper companies have incentives to replenish their forests, large demands for paper lead to large forests while reduced demand for paper leads to fewer ""farmed"" forests.[42]

When foresting companies cut down trees, more are planted in their place; however, such farmed forests are inferior to natural forests in several ways. Farmed forests are not able to fix the soil as quickly as natural forests. This can cause widespread soil erosion and often requiring large amounts of fertilizer to maintain the soil, while containing little tree and wild-life biodiversity compared to virgin forests.[43] Also, the new trees planted are not as big as the trees that were cut down, and the argument that there would be ""more trees"" is not compelling to forestry advocates when they are counting saplings.

In particular, wood from tropical rainforests is rarely harvested for paper because of their heterogeneity.[44] According to the United Nations Framework Convention on Climate Change secretariat, the overwhelming direct cause of deforestation is subsistence farming (48% of deforestation) and commercial agriculture (32%), which is linked to food, not paper production.[45]

Other non-conventional methods of material recycling, like Waste-to-Energy (WTE) systems, have garnered increased attention in the recent past due to the polarizing nature of their emissions. While viewed as a sustainable method of capturing energy from material waste feedstocks by many, others have cited numerous explanations for why the technology has not been scaled globally.[46]

For a recycling program to work, a large, stable supply of recyclable material is crucial. Three legislative options have been used to create such supplies: mandatory recycling collection, container deposit legislation, and refuse bans. Mandatory collection laws set recycling targets for cities, usually in the form that a certain percentage of a material must be diverted from the city's waste stream by a target date. The city is responsible for working to meet this target.[5]

Container deposit legislation mandates refunds for the return of certain containers—typically glass, plastic and metal. When a product in such a container is purchased, a small surcharge is added that the consumer can reclaim when the container is returned to a collection point. These programs have succeeded in creating an average 80% recycling rate.[47] Despite such good results, the shift in collection costs from local government to industry and consumers has created strong opposition in some areas[5]—for example, where manufacturers bear the responsibility for recycling their products. In the European Union, the WEEE Directive requires producers of consumer electronics to reimburse the recyclers' costs.[48]

An alternative way to increase the supply of recyclates is to ban the disposal of certain materials as waste, often including used oil, old batteries, tires, and garden waste. This can create a viable economy for the proper disposal of the products. Care must be taken that enough recycling services exist to meet the supply, or such bans can create increased illegal dumping.[5]

Four forms of legislation have also been used to increase and maintain the demand for recycled materials: minimum recycled content mandates, utilization rates, procurement policies, and recycled product labeling.[5]

Both minimum recycled content mandates and utilization rates increase demand by forcing manufacturers to include recycling in their operations. Content mandates specify that a certain percentage of a new product must consist of recycled material. Utilization rates are a more flexible option: Industries can meet their recycling targets at any point of their operations, or even contract out recycling in exchange for tradable credits. Opponents to these methods cite their large increase in reporting requirements, and claim that they rob the industry of flexibility.[5][49]

Governments have used their own purchasing power to increase recycling demand through ""procurement policies"". These policies are either ""set-asides"", which reserve a certain amount of spending for recycled products; or ""price preference"" programs that provide larger budgets when recycled items are purchased. Additional regulations can target specific cases: in the United States, for example, the Environmental Protection Agency mandates the purchase of oil, paper, tires and building insulation from recycled or re-refined sources whenever possible.[5]

The final government regulation toward increased demand is recycled product labeling. When producers are required to label their packaging with the amount of recycled material it contains (including the packaging), consumers can make more educated choices. Consumers with sufficient buying power can choose more environmentally conscious options, prompting producers to increase the recycled material in their products and increase demand. Standardized recycling labeling can also have a positive effect on the supply of recyclates when it specifies how and where the product can be recycled.[5]

""Recyclate"" is a raw material sent to and processed in a waste recycling plant or materials-recovery facility[50] so it can be used in the production of new materials and products. For example, plastic bottles can be made into plastic pellets and synthetic fabrics.[51]

The quality of recyclates is one of the principal challenges for the success of a long-term vision of a green economy and achieving zero waste. It generally refers to how much of it is composed of target material, versus non-target material and other non-recyclable material.[52] Steel and other metals have intrinsically higher recyclate quality; it is estimated that two-thirds of all new steel comes from recycled steel.[53] Only target material is likely to be recycled, so higher amounts of non-target and non-recyclable materials can reduce the quantity of recycled products.[52] A high proportion of non-target and non-recyclable material can make it more difficult to achieve ""high-quality"" recycling; and if recyclate is of poor quality, it is more likely to end up being down-cycled or, in more extreme cases, sent to other recovery options or landfilled.[52] For example, to facilitate the remanufacturing of clear glass products, there are tight restrictions for colored glass entering the re-melt process. Another example is the downcycling of plastic, where products such as plastic food packaging are often downcycled into lower quality products,  and do not get recycled into the same plastic food packaging.

The quality of recyclate not only supports high-quality recycling, but it can also deliver significant environmental benefits by reducing, reusing, and keeping products out of landfills.[52] High-quality recycling can support economic growth by maximizing the value of waste material.[52] Higher income levels from the sale of quality recyclates can return value significant to local governments, households and businesses.[52] Pursuing high-quality recycling can also promote consumer and business confidence in the waste and resource management sector, and may encourage investment in it.

There are many actions along the recycling supply chain, each of which can affect recyclate quality.[54] Waste producers who place non-target and non-recyclable wastes in recycling collections can affect the quality of final recyclate streams, and require extra efforts to discard those materials at later stages in the recycling process.[54] Different collection systems can induce different levels of contamination. When multiple materials are collected together, extra effort is required to sort them into separate streams and can significantly reduce the quality of the final products.[54] Transportation and the compaction of materials can also make this more difficult. Despite improvements in technology and quality of recyclate, sorting facilities are still not 100% effective in separating materials.[54] When materials are stored outside, where they can become wet, can also cause problems for re-processors. Further sorting steps may be required to satisfactorily reduce the amount of non-target and non-recyclable material.[54]

A number of systems have been implemented to collect recyclates from the general waste stream, occupying different places on the spectrum of trade-off between public convenience and government ease and expense. The three main categories of collection are drop-off centers, buy-back centers and curbside collection.[5] About two-thirds of the cost of recycling is incurred in the collection phase.[55]

Curbside collection encompasses many subtly different systems, which differ mostly on where in the process the recyclates are sorted and cleaned. The main categories are mixed waste collection, commingled recyclables, and source separation.[5] A waste collection vehicle generally picks up the waste.

In mixed waste collection, recyclates are collected mixed with the rest of the waste, and the desired materials are sorted out and cleaned at a central sorting facility. This results in a large amount of recyclable waste (especially paper) being too soiled to reprocess, but has advantages as well: The city need not pay for the separate collection of recyclates, no public education is needed, and any changes to the recyclability of certain materials are implemented where sorting occurs.[5]

In a commingled or single-stream system, recyclables are mixed but kept separate from non-recyclable waste. This greatly reduces the need for post-collection cleaning, but requires public education on what materials are recyclable.[5][10]

Source separation is the other extreme, where each material is cleaned and sorted prior to collection. It requires the least post-collection sorting and produces the purest recyclates. However, it incurs additional operating costs for collecting each material, and requires extensive public education to avoid recyclate contamination.[5] In Oregon, USA, Oregon DEQ surveyed multi-family property managers; about half of them reported problems, including contamination of recyclables due to trespassers such as transients gaining access to collection areas.[56]

Source separation used to be the preferred method due to the high cost of sorting commingled (mixed waste) collection. However, advances in sorting technology have substantially lowered this overhead, and many areas that had developed source separation programs have switched to what is called co-mingled collection.[10]

At buy-back centers, separated, cleaned recyclates are purchased, providing a clear incentive for use and creating a stable supply. The post-processed material can then be sold. If profitable, this conserves the emission of greenhouse gases; if unprofitable, it increases their emission. Buy-back centres generally need government subsidies to be viable. According to a 1993 report by the U.S. National Waste & Recycling Association, it costs an average $50 to process a ton of material that can be resold for $30.[5]

Drop-off centers require the waste producer to carry recyclates to a central location—either an installed or mobile collection station or the reprocessing plant itself. They are the easiest type of collection to establish but suffer from low and unpredictable throughput.

For some waste materials such as plastic, recent technical devices called recyclebots[57] enable a form of distributed recycling called DRAM (distributed recycling additive manufacturing). Preliminary life-cycle analysis (LCA) indicates that such distributed recycling of HDPE to make filament for 3D printers in rural regions consumes less energy than using virgin resin, or using conventional recycling processes with their associated transportation.[58][59]

Another form of distributed recycling mixes waste plastic with sand to make bricks in Africa.[60] Several studies have looked at the properties of recycled waste plastic and sand bricks.[61][62] The composite pavers can be sold at 100% profit while employing workers at 1.5× the minimum wage in the West African region, where distributed recycling has the potential to produce 19 million pavement tiles from 28,000 tons of plastic water sachets annually in Ghana, Nigeria, and Liberia.[63] This has also been done with COVID19 masks.[64]

Once commingled recyclates are collected and delivered to a materials recovery facility, the materials must be sorted. This is done in a series of stages, many of which involve automated processes, enabling a truckload of material to be fully sorted in less than an hour.[10] Some plants can now sort materials automatically; this is known as single-stream recycling. Automatic sorting may be aided by robotics and machine learning.[65][66] In plants, a variety of materials is sorted including paper, different types of plastics, glass, metals, food scraps, and most types of batteries.[67] A 30% increase in recycling rates has been seen in areas with these plants.[68] In the US, there are over 300 materials recovery facilities.[69]

Initially, commingled recyclates are removed from the collection vehicle and placed on a conveyor belt spread out in a single layer. Large pieces of corrugated fiberboard and plastic bags are removed by hand at this stage, as they can cause later machinery to jam.[10]

Next, automated machinery such as disk screens and air classifiers separate the recyclates by weight, splitting lighter paper and plastic from heavier glass and metal. Cardboard is removed from mixed paper, and the most common types of plastic—PET (#1) and HDPE (#2)—are collected, so these materials can be diverted into the proper collection channels. This is usually done by hand; but in some sorting centers, spectroscopic scanners are used to differentiate between types of paper and plastic based on their absorbed wavelengths.[10] Plastics tend to be incompatible with each other due to differences in chemical composition; their polymer molecules repel each other, similar to oil and water.[70]

Strong magnets are used to separate out ferrous metals such as iron, steel and tin cans. Non-ferrous metals are ejected by magnetic eddy currents: A rotating magnetic field induces an electric current around aluminum cans, creating an eddy current inside the cans that is repulsed by a large magnetic field, ejecting the cans from the stream.[10]

Finally, glass is sorted according to its color: brown, amber, green, or clear. It may be sorted either by hand,[10] or by a machine that uses colored filters to detect colors. Glass fragments smaller than 10 millimetres (0.39 in) cannot be sorted automatically, and are mixed together as ""glass fines"".[71]

In 2003, San Francisco's Department of the Environment set a citywide goal of zero waste by 2020.[72] San Francisco's refuse hauler, Recology, operates an effective recyclables sorting facility that has helped the city reach a record-breaking landfill diversion rate of 80% as of 2021.[73] Other American cities, including Los Angeles, have achieved similar rates.

Although many government programs concentrate on recycling at home, 64% of waste in the United Kingdom is generated by industry.[74] The focus of many recycling programs in industry is their cost-effectiveness. The ubiquitous nature of cardboard packaging makes cardboard a common waste product recycled by companies that deal heavily in packaged goods, such as retail stores, warehouses, and goods distributors. Other industries deal in niche and specialized products, depending on the waste materials they handle.

Glass, lumber, wood pulp and paper manufacturers all deal directly in commonly recycled materials; however, independent tire dealers may collect and recycle rubber tires for a profit.

The waste produced from burning coal in a Coal-fired power station is often called fuel ash or fly ash in the United States. It is a very useful material and used in concrete construction. It exhibits Pozzolanic activity.[75]

Levels of metals recycling are generally low. In 2010, the International Resource Panel, hosted by the United Nations Environment Programme (UNEP), published reports on metal stocks[76] and their recycling rates.[76] It reported that the increase in the use of metals during the 20th and into the 21st century has led to a substantial shift in metal stocks from below-ground to use in above-ground applications within society. For example, in the US, in-use copper grew from 73 to 238 kg per capita between 1932–1999.

The report's authors observed that, as metals are inherently recyclable, metal stocks in society can serve as huge above-ground mines (the term ""urban mining"" has thus been coined[77]). However, they found that the recycling rates of many metals are low. They warned that the recycling rates of some rare metals used in applications such as mobile phones, battery packs for hybrid cars and fuel cells, are so low that unless future end-of-life recycling rates are dramatically increased, these critical metals will become unavailable for use in modern technology.

The military recycles some metals. The U.S. Navy's Ship Disposal Program uses ship breaking to reclaim the steel of old vessels. Ships may also be sunk to create artificial reefs. Uranium is a dense metal that has qualities superior to lead and titanium for many military and industrial uses. Uranium left over from processing it into nuclear weapons and fuel for nuclear reactors is called depleted uranium, and is used by all branches of the U.S. military for the development of such things as armor-piercing shells and shielding.

The construction industry may recycle concrete and old road surface pavement, selling these materials for profit.

Some rapidly growing industries, particularly the renewable energy and solar photovoltaic technology industries, are proactively creating recycling policies even before their waste streams have considerable volume, anticipating future demand.[78]


Recycling of plastics is more difficult, as most programs are not able to reach the necessary level of quality. Recycling of PVC often results in downcycling of the material, which means only products of lower quality standard can be made with the recycled material.
E-waste is a growing problem, accounting for 20–50 million metric tons of global waste per year according to the EPA. It is also the fastest growing waste stream in the EU.[25] Many recyclers do not recycle e-waste responsibly. After the cargo barge Khian Sea dumped 14,000 metric tons of toxic ash in Haiti, the Basel Convention was formed to stem the flow of hazardous substances into poorer countries. They created the e-Stewards certification to ensure that recyclers are held to the highest standards for environmental responsibility and to help consumers identify responsible recyclers. It operates alongside other prominent legislation, such as the Waste Electrical and Electronic Equipment Directive of the EU and the United States National Computer Recycling Act, to prevent poisonous chemicals from entering waterways and the atmosphere.

In the recycling process, television sets, monitors, cell phones, and computers are typically tested for reuse and repaired. If broken, they may be disassembled for parts still having high value if labor is cheap enough. Other e-waste is shredded to pieces roughly 10 centimetres (3.9 in) in size and manually checked to separate toxic batteries and capacitors, which contain poisonous metals. The remaining pieces are further shredded to 10 millimetres (0.39 in) particles and passed under a magnet to remove ferrous metals. An eddy current ejects non-ferrous metals, which are sorted by density either by a centrifuge or vibrating plates. Precious metals can be dissolved in acid, sorted, and smelted into ingots. The remaining glass and plastic fractions are separated by density and sold to re-processors. Television sets and monitors must be manually disassembled to remove lead from CRTs and the mercury backlight from LCDs.[79][80][81]

Vehicles, solar panels and wind turbines can also be recycled. They often contain rare-earth elements (REE) and/or other critical raw materials. For electric car production, large amounts of REE's are typically required.[82]

Whereas many critical raw elements and REE's can be recovered, environmental engineer Phillipe Bihouix Archived 6 September 2021 at the Wayback Machine reports that recycling of indium, gallium, germanium, selenium, and tantalum is still very difficult and their recycling rates are very low.[82]

Plastic recycling is the process of recovering scrap or waste plastic and reprocessing the material into useful products, sometimes completely different in form from their original state. For instance, this could mean melting down soft drink bottles and then casting them as plastic chairs and tables.[83] For some types of plastic, the same piece of plastic can only be recycled about 2–3 times before its quality decreases to the point where it can no longer be used.[6]

Some plastics are remelted to form new plastic objects; for example, PET water bottles can be converted into polyester destined for clothing. A disadvantage of this type of recycling is that the molecular weight of the polymer can change further and the levels of unwanted substances in the plastic can increase with each remelt.[84][85]

A commercial-built recycling facility was sent to the International Space Station in late 2019. The facility takes in plastic waste and unneeded plastic parts and physically converts them into spools of feedstock for the space station additive manufacturing facility used for in-space 3D printing.[86]

For some polymers, it is possible to convert them back into monomers, for example, PET can be treated with an alcohol and a catalyst to form a dialkyl terephthalate. The terephthalate diester can be used with ethylene glycol to form a new polyester polymer, thus making it possible to use the pure polymer again. In 2019, Eastman Chemical Company announced initiatives of methanolysis and syngas designed to handle a greater variety of used material.[87]

Another process involves the conversion of assorted polymers into petroleum by a much less precise thermal depolymerization process. Such a process would be able to accept almost any polymer or mix of polymers, including thermoset materials such as vulcanized rubber tires and the biopolymers in feathers and other agricultural waste. Like natural petroleum, the chemicals produced can be used as fuels or as feedstock. A RESEM Technology[88] plant of this type in Carthage, Missouri, US, uses turkey waste as input material. Gasification is a similar process but is not technically recycling since polymers are not likely to become the result.
Plastic Pyrolysis can convert petroleum based waste streams such as plastics into quality fuels, carbons. Given below is the list of suitable plastic raw materials for pyrolysis:

In order to meet recyclers' needs while providing manufacturers a consistent, uniform system, a coding system was developed. The recycling code for plastics was introduced in 1988 by the plastics industry through the Society of the Plastics Industry.[89] Because municipal recycling programs traditionally have targeted packaging—primarily bottles and containers—the resin coding system offered a means of identifying the resin content of bottles and containers commonly found in the residential waste stream.[90]

In the United States, plastic products are printed with numbers 1–7 depending on the type of resin. Type 1 (polyethylene terephthalate) is commonly found in soft drink and water bottles. Type 2 (high-density polyethylene) is found in most hard plastics such as milk jugs, laundry detergent bottles, and some dishware. Type 3 (polyvinyl chloride) includes items such as shampoo bottles, shower curtains, hula hoops, credit cards, wire jacketing, medical equipment, siding, and piping. Type 4 (low-density polyethylene) is found in shopping bags, squeezable bottles, tote bags, clothing, furniture, and carpet. Type 5 is polypropylene and makes up syrup bottles, straws, Tupperware, and some automotive parts. Type 6 is polystyrene and makes up meat trays, egg cartons, clamshell containers, and compact disc cases. Type 7 includes all other plastics such as bulletproof materials, 3- and 5-gallon water bottles, cell phone and tablet frames, safety goggles and sunglasses.[91] Having a recycling code or the chasing arrows logo on a material is not an automatic indicator that a material is recyclable but rather an explanation of what the material is. Types 1 and 2 are the most commonly recycled.

In addition to environmental impact, there is debate over whether recycling is economically efficient. According to a Natural Resources Defense Council study, waste collection and landfill disposal creates less than one job per 1,000 tons of waste material managed; in contrast, the collection, processing, and manufacturing of recycled materials creates 6–13 or more jobs per 1,000 tons.[95]  According to the U.S. Recycling Economic Informational Study, there are over 50,000 recycling establishments that have created over a million jobs in the US.[96] The National Waste & Recycling Association (NWRA) reported in May 2015 that recycling and waste made a $6.7 billion economic impact in Ohio, U.S., and employed 14,000 people.[97] Economists[who?] would classify this extra labor used as a cost rather than a benefit since these workers could have been employed elsewhere; the cost effectiveness of creating these additional jobs remains unclear.[citation needed]

Sometimes cities have found recycling saves resources compared to other methods of disposal of waste. Two years after New York City declared that implementing recycling programs would be ""a drain on the city"", New York City leaders realized that an efficient recycling system could save the city over $20 million.[98] Municipalities often see fiscal benefits from implementing recycling programs, largely due to the reduced landfill costs.[99] A study conducted by the Technical University of Denmark according to the Economist found that in 83 percent of cases, recycling is the most efficient method to dispose of household waste.[10][21] However, a 2004 assessment by the Danish Environmental Assessment Institute concluded that incineration was the most effective method for disposing of drink containers, even aluminium ones.[100]

Fiscal efficiency is separate from economic efficiency. Economic analysis of recycling does not include what economists call externalities: unpriced costs and benefits that accrue to individuals outside of private transactions[citation needed]. Examples include less air pollution and greenhouse gases from incineration and less waste leaching from landfills.  Without mechanisms such as taxes or subsidies, businesses and consumers following their private benefit would ignore externalities despite the costs imposed on society. If landfills and incinerator pollution is inadequately regulated, these methods of waste disposal appear cheaper than they really are, because part of their cost is the pollution imposed on people nearby. Thus, advocates have pushed for legislation to increase demand for recycled materials.[5]  The United States Environmental Protection Agency (EPA) has concluded in favor of recycling, saying that recycling efforts reduced the country's carbon emissions by a net 49 million metric tonnes in 2005.[10] In the United Kingdom, the Waste and Resources Action Programme stated that Great Britain's recycling efforts reduce CO2 emissions by 10–15 million tonnes a year.[10] The question for economic efficiency is whether this reduction is worth the extra cost of recycling and thus makes the artificial demand creates by legislation worthwhile.

Certain requirements must be met for recycling to be economically feasible and environmentally effective. These include an adequate source of recyclates, a system to extract those recyclates from the waste stream, a nearby factory capable of reprocessing the recyclates, and a potential demand for the recycled products. These last two requirements are often overlooked—without both an industrial market for production using the collected materials and a consumer market for the manufactured goods, recycling is incomplete and in fact only ""collection"".[5]

Free-market economist Julian Simon remarked ""There are three ways society can organize waste disposal: (a) commanding, (b) guiding by tax and subsidy, and (c) leaving it to the individual and the market"". These principles appear to divide economic thinkers today.[101]

Frank Ackerman favours a high level of government intervention to provide recycling services. He believes that recycling's benefit cannot be effectively quantified by traditional laissez-faire economics. Allen Hershkowitz supports intervention, saying that it is a public service equal to education and policing. He argues that manufacturers should shoulder more of the burden of waste disposal.[101]

Paul Calcott and Margaret Walls advocate the second option. A deposit refund scheme and a small refuse charge would encourage recycling but not at the expense of illegal dumping. Thomas C. Kinnaman concludes that a landfill tax would force consumers, companies and councils to recycle more.[101]

Most free-market thinkers detest subsidy and intervention, arguing that they waste resources. The general argument is that if cities charge the full cost of garbage collection, private companies can profitably recycle any materials for which the benefit of recycling exceeds the cost (e.g. aluminum[102]) and do not recycle other materials for which the benefit is less than the cost (e.g. glass[103]). Cities, on the other hand, often recycle even when they  not only do not receive enough for the paper or plastic to pay for its collection, but must actually pay private recycling companies to take it off of their hands.[102] Terry Anderson and Donald Leal think that all recycling programmes should be privately operated, and therefore would only operate if the money saved by recycling exceeds its costs. Daniel K. Benjamin argues that it wastes people's resources and lowers the wealth of a population.[101] He notes that recycling can cost a city more than twice as much as landfills, that in the United States landfills are so heavily regulated that their pollution effects are negligible, and that the recycling process also generates pollution and uses energy, which may or may not be less than from virgin production.[104]

Certain countries trade in unprocessed recyclates. Some have complained that the ultimate fate of recyclates sold to another country is unknown and they may end up in landfills instead of being reprocessed. According to one report, in America, 50–80 percent of computers destined for recycling are actually not recycled.[105][106] There are reports of illegal-waste imports to China being dismantled and recycled solely for monetary gain, without consideration for workers' health or environmental damage. Although the Chinese government has banned these practices, it has not been able to eradicate them.[107] In 2008, the prices of recyclable waste plummeted before rebounding in 2009. Cardboard averaged about £53/tonne from 2004 to 2008, dropped to £19/tonne, and then went up to £59/tonne in May 2009. PET plastic averaged about £156/tonne, dropped to £75/tonne and then moved up to £195/tonne in May 2009.[108]

Certain regions have difficulty using or exporting as much of a material as they recycle. This problem is most prevalent with glass: both Britain and the U.S. import large quantities of wine bottled in green glass. Though much of this glass is sent to be recycled, outside the American Midwest there is not enough wine production to use all of the reprocessed material. The extra must be downcycled into building materials or re-inserted into the regular waste stream.[5][10]

Similarly, the northwestern United States has difficulty finding markets for recycled newspaper, given the large number of pulp mills in the region as well as the proximity to Asian markets. In other areas of the U.S., however, demand for used newsprint has seen wide fluctuation.[5]

In some U.S. states, a program called RecycleBank pays people to recycle, receiving money from local municipalities for the reduction in landfill space that must be purchased. It uses a single stream process in which all material is automatically sorted.[109]

Much of the difficulty inherent in recycling comes from the fact that most products are not designed with recycling in mind. In the USA around 6 to 7 percent of plastic is recycled.[110] The concept of sustainable design aims to solve this problem, and was laid out in the 2002 book Cradle to Cradle: Remaking the Way We Make Things by architect William McDonough and chemist Michael Braungart.[111] They suggest that every product (and all packaging it requires) should have a complete ""closed-loop"" cycle mapped out for each component—a way in which every component either returns to the natural ecosystem through biodegradation or is recycled indefinitely.[10][112]

Complete recycling is impossible from a practical standpoint. In summary, substitution and recycling strategies only delay the depletion of non-renewable stocks and therefore may buy time in the transition to true or strong sustainability, which ultimately is only guaranteed in an economy based on renewable resources.[113]: 21 
While recycling diverts waste from entering directly into landfill sites, current recycling misses the dispersive components. Critics believe that complete recycling is impracticable as highly dispersed wastes become so diluted that the energy needed for their recovery becomes increasingly excessive.

As with environmental economics, care must be taken to ensure a complete view of the costs and benefits involved. For example, paperboard packaging for food products is more easily recycled than most plastic, but is heavier to ship and may result in more waste from spoilage.[114] Economic expenses can incentivize fraud.[115]

Critics dispute the net economic and environmental benefits of recycling over its costs, and suggest that proponents of recycling often make matters worse and suffer from confirmation bias. Specifically, critics argue that the costs and energy used in collection and transportation detract from (and outweigh) the costs and energy saved in the production process; also that the jobs produced by the recycling industry can be a poor trade for the jobs lost in logging, mining, and other industries associated with production; and that materials such as paper pulp can only be recycled a few times before material degradation prevents further recycling.[116]

The amount of energy saved through recycling depends upon the material being recycled and the type of energy accounting that is used. Correct accounting for this saved energy can be accomplished with life-cycle analysis using real energy values, and in addition, exergy, which is a measure of how much useful energy can be used. In general, it takes far less energy to produce a unit mass of recycled materials than it does to make the same mass of virgin materials.[117][118][119]

Some scholars use emergy (spelled with an m) analysis, for example, budgets for the amount of energy of one kind (exergy) that is required to make or transform things into another kind of product or service. Emergy calculations take into account economics that can alter pure physics-based results. Using emergy life-cycle analysis researchers have concluded that materials with large refining costs have the greatest potential for high recycle benefits. Moreover, the highest emergy efficiency accrues from systems geared toward material recycling, where materials are engineered to recycle back into their original form and purpose, followed by adaptive reuse systems where the materials are recycled into a different kind of product, and then by-product reuse systems where parts of the products are used to make an entirely different product.[120]

The Energy Information Administration (EIA) states on its website that ""a paper mill uses 40 percent less energy to make paper from recycled paper than it does to make paper from fresh lumber.""[121] Some critics argue that it takes more energy to produce recycled products than it does to dispose of them in traditional landfill methods, since the curbside collection of recyclables often requires a second waste truck. However, recycling proponents point out that a second timber or logging truck is eliminated when paper is collected for recycling, so the net energy consumption is the same. An emergy life-cycle analysis on recycling revealed that fly ash, aluminum, recycled concrete aggregate, recycled plastic, and steel yield higher efficiency ratios, whereas the recycling of lumber generates the lowest recycle benefit ratio. Hence, the specific nature of the recycling process, the methods used to analyse the process, and the products involved affect the energy savings budgets.[120]

It is difficult to determine the amount of energy consumed or produced in waste disposal processes in broader ecological terms, where causal relations dissipate into complex networks of material and energy flow. 

[C]ities do not follow all the strategies of ecosystem development. Biogeochemical paths become fairly straight relative to wild ecosystems, with reduced recycling, resulting in large flows of waste and low total energy efficiencies. By contrast, in wild ecosystems, one population's wastes are another population's resources, and succession results in efficient exploitation of available resources. However, even modernized cities may still be in the earliest stages of a succession that may take centuries or millennia to complete.[122]: 720  
How much energy is used in recycling also depends on the type of material being recycled and the process used to do so. Aluminium is generally agreed to use far less energy when recycled rather than being produced from scratch. The EPA states that ""recycling aluminum cans, for example, saves 95 percent of the energy required to make the same amount of aluminum from its virgin source, bauxite.""[123][124] In 2009, more than half of all aluminium cans produced came from recycled aluminium.[125] Similarly, it has been estimated that new steel produced with recycled cans reduces greenhouse gas emissions by 75%.[126]

Every year, millions of tons of materials are being exploited from the earth's crust, and processed into consumer and capital goods. After decades to centuries, most of these materials are ""lost"". With the exception of some pieces of art or religious relics, they are no longer engaged in the consumption process. Where are they? Recycling is only an intermediate solution for such materials, although it does prolong the residence time in the anthroposphere. For thermodynamic reasons, however, recycling cannot prevent the final need for an ultimate sink.[127]: 1 
Economist Steven Landsburg has suggested that the sole benefit of reducing landfill space is trumped by the energy needed and resulting pollution from the recycling process.[128] Others, however, have calculated through life-cycle assessment that producing recycled paper uses less energy and water than harvesting, pulping, processing, and transporting virgin trees.[129] When less recycled paper is used, additional energy is needed to create and maintain farmed forests until these forests are as self-sustainable as virgin forests.

Other studies have shown that recycling in itself is inefficient to perform the ""decoupling"" of economic development from the depletion of non-renewable raw materials that is necessary for sustainable development.[130] The international transportation or recycle material flows through ""... different trade networks of the three countries result in different flows, decay rates, and potential recycling returns"".[131]: 1  As global consumption of a natural resources grows, their depletion is inevitable. The best recycling can do is to delay; complete closure of material loops to achieve 100 percent recycling of nonrenewables is impossible as micro-trace materials dissipate into the environment causing severe damage to the planet's ecosystems.[132][133][134] Historically, this was identified as the metabolic rift by Karl Marx, who identified the unequal exchange rate between energy and nutrients flowing from rural areas to feed urban cities that create effluent wastes degrading the planet's ecological capital, such as loss in soil nutrient production.[135][136] Energy conservation also leads to what is known as Jevon's paradox, where improvements in energy efficiency lowers the cost of production and leads to a rebound effect where rates of consumption and economic growth increases.[134][137]

Journalist John Tierney notes that it is generally more expensive for municipalities to recycle waste from households than to send it to a landfill and that ""recycling may be the most wasteful activity in modern America.""[138]

The amount of money actually saved through recycling depends on the efficiency of the recycling program used to do it. The Institute for Local Self-Reliance argues that the cost of recycling depends on various factors, such as landfill fees and the amount of disposal that the community recycles. It states that communities begin to save money when they treat recycling as a replacement for their traditional waste system rather than an add-on to it and by ""redesigning their collection schedules and/or trucks"".[139]

In some cases, the cost of recyclable materials also exceeds the cost of raw materials. Virgin plastic resin costs 40 percent less than recycled resin.[121] Additionally, a United States Environmental Protection Agency (EPA) study that tracked the price of clear glass from 15 July to 2 August 1991, found that the average cost per ton ranged from $40 to $60[140] while a USGS report shows that the cost per ton of raw silica sand from years 1993 to 1997 fell between $17.33 and $18.10.[141]

Comparing the market cost of recyclable material with the cost of new raw materials ignores economic externalities—the costs that are currently not counted by the market. Creating a new piece of plastic, for instance, may cause more pollution and be less sustainable than recycling a similar piece of plastic, but these factors are not counted in market cost. A life cycle assessment can be used to determine the levels of externalities and decide whether the recycling may be worthwhile despite unfavorable market costs. Alternatively, legal means (such as a carbon tax) can be used to bring externalities into the market, so that the market cost of the material becomes close to the true cost.

The recycling of waste electrical and electronic equipment can create a significant amount of pollution. This problem is specifically occurrent in India and China. Informal recycling in an underground economy of these countries has generated an environmental and health disaster. High levels of lead (Pb), polybrominated diphenylethers (PBDEs), polychlorinated dioxins and furans, as well as polybrominated dioxins and furans (PCDD/Fs and PBDD/Fs), concentrated in the air, bottom ash, dust, soil, water, and sediments in areas surrounding recycling sites.[142] These materials can make work sites harmful to the workers themselves and the surrounding environment.

In some countries, recycling is performed by the entrepreneurial poor such as the karung guni, zabbaleen, the rag-and-bone man, waste picker, and junk man. With the creation of large recycling organizations that may be profitable, either by law or economies of scale,[143][144] the poor are more likely to be driven out of the recycling and the remanufacturing job market. To compensate for this loss of income, a society may need to create additional forms of societal programs to help support the poor.[145] Like the parable of the broken window, there is a net loss to the poor and possibly the whole of a society to make recycling artificially profitable, e.g. through the law. However, in Brazil and Argentina, waste pickers/informal recyclers work alongside the authorities, in fully or semi-funded cooperatives, allowing informal recycling to be legitimized as a paid public sector job.[146]

Because the social support of a country is likely to be less than the loss of income to the poor undertaking recycling, there is a greater chance for the poor to come in conflict with the large recycling organizations.[147][148] This means fewer people can decide if certain waste is more economically reusable in its current form rather than being reprocessed. Contrasted to the recycling poor, the efficiency of their recycling may actually be higher for some materials because individuals have greater control over what is considered ""waste"".[145]

One labor-intensive underused waste is electronic and computer waste. Because this waste may still be functional and wanted mostly by those on lower incomes, who may sell or use it at a greater efficiency than large recyclers.

Some recycling advocates believe that laissez-faire individual-based recycling does not cover all of society's recycling needs. Thus, it does not negate the need for an organized recycling program.[145] Local government can consider the activities of the recycling poor as contributing to the ruining of property.

Changes that have been demonstrated to increase recycling rates include:

In a study done by social psychologist Shawn Burn,[149] it was found that personal contact with individuals within a neighborhood is the most effective way to increase recycling within a community. In her study, she had 10 block leaders talk to their neighbors and persuade them to recycle. A comparison group was sent fliers promoting recycling. It was found that the neighbors that were personally contacted by their block leaders recycled much more than the group without personal contact. As a result of this study, Shawn Burn believes that personal contact within a small group of people is an important factor in encouraging recycling. Another study done by Stuart Oskamp[150] examines the effect of neighbors and friends on recycling. It was found in his studies that people who had friends and neighbors that recycled were much more likely to also recycle than those who did not have friends and neighbors that recycled.

Many schools have created recycling awareness clubs in order to give young students an insight on recycling. These schools believe that the clubs actually encourage students to not only recycle at school but at home as well.

Recycling of metals varies extremely by type. Titanium and lead have an extremely high recycling rates of over 90%. Copper and cobalt have high rates of recycling around 75%. Only about half of aluminum is recycled. Most of the remaining metals have recycling rates of below 35%, while 34 types of metals have recycling rates of under 1%.[151]

""Between 1960 and 2000, the world production of plastic resins increased 25 times its original amount, while recovery of the material remained below 5 percent.""[152]: 131  Many studies have addressed recycling behaviour and strategies to encourage community involvement in recycling programs. It has been argued[153] that recycling behavior is not natural because it requires a focus and appreciation for long-term planning, whereas humans have evolved to be sensitive to short-term survival goals; and that to overcome this innate predisposition, the best solution would be to use social pressure to compel participation in recycling programs. However, recent studies have concluded that social pressure does not work in this context.[154] One reason for this is that social pressure functions well in small group sizes of 50 to 150 individuals (common to nomadic hunter–gatherer peoples) but not in communities numbering in the millions, as we see today. Another reason is that individual recycling does not take place in the public view.

Following the increasing popularity of recycling collection being sent to the same landfills as trash, some people kept on putting recyclables on the recyclables bin.[155]

Art objects are more and more often made from recycled material.

By extending the lifespan of goods, parts, and materials, a circular economy seeks to minimize waste and maximize resource utilization.[156] Advanced sorting techniques like optical and robotic sorting may separate and recover valuable materials from waste streams, lowering the requirement for virgin resources and accelerating the shift to a circular economy.

Community engagement, such as education and awareness campaigns, may support the acceptance of recycling and reuse programs and encourage the usage of sustainable practices. One can lessen our influence on the environment, save natural resources, and generate economic possibilities by adopting a circular economy using cutting-edge sorting technology and community engagement. According to Melati et al.,[157] to successfully transition to a circular economy, legislative and regulatory frameworks must encourage sustainable practices while addressing possible obstacles and difficulties in putting these ideas into action.
"
Environmental Consulting,"Environmental consulting is often a form of compliance consulting, in which the consultant ensures that the client maintains an appropriate measure of compliance with environmental regulations. Sustainable consulting is a specialized field that offers guidance and solutions for businesses seeking to operate in an environmentally responsible and sustainable way. The goal of sustainable consulting is to help organizations reduce their environmental impact while maintaining profitability and social responsibility. There are many types of environmental consultants, but the two main groups are those who enter the field from the industry side, and those who enter the field from the environmentalist side.

Environmental consultants work in a very wide variety of fields. Whether it be providing construction services such as asbestos hazard assessments or lead hazard assessments or conducting due diligence reports for customers to rid them of possible sanctions. Consultancies may generalize across a wide range of disciplines or specialize in certain areas of environmental consultancy such as waste management.

Environmental consultants usually have an undergraduate degree and sometimes even master's degree in Environmental Engineering, Environmental Science, Environmental Studies, Geology, or some other science discipline. They should have deep knowledge on environmental regulations, which they can advise particular clients in the private industry or public government institutions to help them steer clear of possible fines, legal action or misguided transactions.

Environmental consulting spans a wide spectrum of industry. The most basic industry that environmental consulting remains prominent in is the commercial estate market. Many commercial lenders rely on both small and large environmental firms. Many commercial lenders will not lend monies to borrowers if the property or personal capital does not exceed the worth of the land. If an environmental problem is discovered property owners that deem themselves a responsible party will most likely reserve monies in escrow in order to resolve the environmental impact.

With increasing numbers of construction, agriculture, and scientific companies employing environmental consultancies, the industry can expect growth in the vicinity of 9.7 percent in 2008, amidst mounting public concern over environmental degradation and climate change. And while some companies are genuinely motivated by concern for the environment, for others, hiring consultants to appear to be ""going green"" has proven to be a useful marketing tool. Growing government funding into renewable energy and technologies producing low emissions is also helping growth, as organizations investing in research and development in these areas are often major employers of environmental consultants.[1]

There are numerous areas in which environmental consultants might work:
"
Property Management Services,"Property management is the operation, control, maintenance, and oversight of real estate and physical property. This can include residential, commercial, and land real estate. Management indicates the need for real estate to be cared for and monitored, with accountability for and attention to its useful life and condition. This is much akin to the role of management in any business.

Property management is the administration of personal property, equipment, tooling, and physical capital assets acquired and used to build, repair, and maintain end-item deliverables. Property management involves the processes, systems, and workforce required to manage the life cycle of all acquired property as defined above, including acquisition, control, accountability, responsibility, maintenance, utilization, and disposition.

An owner of a single-family home, condominium, or multi-family building may engage the services of a professional property management company. The company will then advertise the rental property, handle tenant inquiries, screen applicants, select suitable candidates, draw up a lease agreement, conduct a move-in inspection, move the tenant(s) into the property and collect rental income. The company will then coordinate any maintenance issues, supply the owner(s) with financial statements and any relevant information regarding the property, etc.

This profession has many facets, including managing the accounts and finances of real estate properties and participating in or initiating litigation with tenants, contractors, and insurance agencies. Litigation is sometimes considered a separate function set aside for trained attorneys. Although a person will be responsible for this in their job description, there may be an attorney working under a property manager. Special attention is given to landlord/tenant law; most commonly, evictions, non-payment, harassment, reduction of pre-arranged services, and public nuisance are legal subjects that gain the most attention from property managers. Therefore, it is a necessity that a property manager is current with applicable municipal, county, state, and Federal Fair Housing laws and practices.

Every state of Australia except South Australia requires a license to manage property. This is to ensure that a property manager is as well prepared for the job as possible. (There may be exceptions, like managing an extremely small property for a relative.) In South Australia, a property management business must be run by a registered land agent.[1]

In Canada, the laws governing property management and landlord/tenant relations are, generally speaking, a Provincial responsibility.[citation needed] Each Province and Territory makes its own laws on these matters. In most cases, any person or company can offer property management services, and there are licensing requirements.[citation needed] Other than specific laws in each Province and Territory governing these matters, they are governed by English Common Law, except in the province of Quebec, where the Civil Code is used in place of English Common Law.[citation needed] In some cities, the Provincial Legislation is supplemented by City by-laws.

The licensing of property managers is regulated by the provincial government and licensing by the BCFSA is a regulatory agency established by the provincial government. Its mandate is to protect the public interest by enforcing the licensing and licensee conduct requirements of the Real Estate Services Act. The BCFSA is responsible for licensing individuals and brokerages engaged in real estate sales, rental and strata property management. The BCFSA  also enforces entry qualifications, investigates complaints against licensees and imposes disciplinary sanctions under the Act.

The BCFSA is responsible for ensuring that the interests of consumers who use the services of real estate licensees are adequately protected against wrongful actions by the licensees. A wrongful action may be deliberate or may be the consequence of inadequate exercise of reasonable judgment by a licensee in carrying out their duties and responsibilities.

The BCFSA is responsible for determining what is appropriate education in real estate matters for individuals seeking to be licensed as real estate practitioners and arranging for licensing courses and examinations as part of the qualification requirement for licensing. Under the authority of the BCFSA, licensing courses are delivered by the UBC Sauder School of Business, Real Estate Division.

In Ontario, no licensing is required to operate; however, ACMO—the Association of Condo Managers of Ontario—is a self-governing body for certification and designation of its members who run buildings with more than 600 units. (RECO), the Real Estate Council of Ontario, regulates licensed realtors in Ontario. The provincial government is revising its condominium act. After public consultation, it hopes to put forth legislation during the 2015–2016 session requiring Condo Management firms and staff or condo employees and boards to be accredited.

Both require property managers to hold a real estate license.

In Germany, property management is divided into the areas of home owner's association management (WEG-Verwaltung), rental management (Mietverwaltung), and special property management (Sondereigentumsverwaltung) with different clients and tasks. Since 2018, a license in accordance with the Trade Regulation Act (§ 34 C GewO) is mandatory for property managers. This requires sufficient insurance as well as sound financial circumstances and reliability. There are no requirements regarding professional trainings or degrees. However, there is a training obligation of twenty hours within a period of three years. Receiving a license as a property manager in Germany is accompanied by membership of the relevant chamber of industry and commerce.[2]

In Hong Kong, property management companies (PMCs)[3] and property management practitioners (PMPs)[4] are regulated under the Property Management Services Ordinance (PMSO) (Chapter 626 of the Laws of Hong Kong),[5] which was enacted in 2016. Only some sections under the PMSO have commenced operation and they are the ones concerning the establishment of the Property Management Services Authority (PMSA) as the regulator for the industry. Apart from establishing the PMSA, the PMSO provides a legal framework for the regulatory regime, and the details of the regime, including the licensing criteria for PMCs and PMPs, are being formulated by the PMSA (public consultation underway[6]) and will be set out in subsidiary legislation. Other sections of the PMSO will commence operation after the subsidiary legislation is passed by the Legislative Council of Hong Kong and commences operation.

Certain classes of persons are exempt from the licensing requirement.[7] Those not exempt are required to obtain a license, and failure to do so is a criminal offence subject to a maximum penalty of a fine of HK$500,000 and imprisonment for two years.[8]  Those who are licensed are subject to disciplinary actions (including verbal warning, written reprimand, penalty up to HK$300,000, imposition of a condition on licence, suspension and revocation of licences[9]) by the PMSA if they commit a ""disciplinary offence"" as defined under section 4 of the PMSO.[10] The PMSA may issue codes of conduct containing practical guidance to licensees, including the matters that the PMSA considers to be relevant to determining whether a licensee has committed a disciplinary offence.[11]

Under the PMSO, property management services (PMSs) are to be prescribed under seven specified categories[12][13] as follows:

Only those PMCs providing PMSs falling within more than one category of PMSs are required to be licensed,[14] and individuals who assume a managerial or supervisory role in these PMCs are also required to be licensed.[15] In other words, PMCs providing PMSs falling within only one category of PMSs are not required to be licensed, and individuals working in the front line without assuming a managerial or supervisory role are not required to be licensed either. All types of properties (i.e. whether residential, commercial or otherwise) are covered by the PMSO, but ""property"" is given a technical meaning and refers to those which have a deed of mutual covenant (DMC) (a document containing terms that are binding on all flat owners of a multi-unit or multi-story building[16]) since only PMSs provided to buildings with multi-ownership are intended to be regulated.[17] In other words, PMCs and PMPs providing PMSs to properties without a DMC are not to be regulated under the PMSO.

In India, there is no statutory regulation of property management companies, real estate agents or developers. In 2013, a Real Estate Regulation and Development Bill was passed by the Union Cabinet but has yet to take effect. The bill seeks to set up 3 regulatory bodies in the country. The Real Estate Regulation and Development Bill was passed by the Union Cabinet in early 2016 and this is expected to bring about a sea change in the management of real estate in India.[citation needed]

In the Republic of Ireland, there is no legal obligation to form a property management company. However, management companies are generally formed to manage multi-unit developments and must then follow the general rules of company law in terms of ownership and administration.

Since July 2012, it has become mandatory for all property service providers, including property management companies, to be registered and fully licensed by the Property Services Regulatory Authority of Ireland.

The National Consumer Agency (NCA) has campaigned in this area, and in September 2008 it launched a website explaining consumer rights. The NCA does not have a legislative or regulatory function in the area, unless a consumer complaint is in relation to a breach of consumer law.

In Kenya, the Estate Agents Registration Board (EARB)[18] is the regulatory body for estate agency practice, and it derives its mandate from the Estate Agents Act, 1984, Cap 533,[19] which was operationalized[clarification needed] in 1987. Under that Act, the Board is responsible for registering estate agents and ensuring that the competence and conduct of practicing estate agents are good enough to ensure the protection of the public. The Board also keeps a list of registered members on its website that is accessible to members of the public, in accordance with Section 9 of the Estate Agents Act.[20] The Board recently[when?] drafted a proposal with a set of amendments to the Estate Agents Act.[21]

Associations that real estate agents and property developers can join include:

Commercial Property Management leasing agents in New Zealand are required to have a real estate agents licence and operate an audited trust bank account. Commercial leases are covered by the Property Law Act 1952.

Residential property management in New Zealand is an unlicensed and unregulated industry. Property managers in New Zealand do not require any registration or minimum knowledge or skill. The New Zealand Government reviewed whether all forms of property management need any legislation.[24] Following completion of the review, the Associate Minister of Justice, Hon Nathan Guy, announced on 2 July 2009 that no new occupational regulation would be imposed on property managers[25] in part due to there already being existing laws which could be used to protect consumers.

New Zealand licensed Real Estate Agents may offer Residential Property Management service with qualified Real Estate Agents as Property managers or property manager's working under the Licensed Real estate agency. Member Agents are supposed to adhere to the Real Estate Institute of New Zealand property management code of practice,[26] which, according to the REAA, outlines industry best practices for dealing with the public. Critics state the Real Estate Agents Authority complaint committee as having less scope or jurisdiction for adverse judgement against negligent Property Management licences as they would otherwise to those in ""real estate agency work"",[27] Unsatisfactory property management conduct cases can receive findings of ""no further action"" as opposed to ""unsatisfactory conduct""[28] due to ""conduct unrelated to estate agency work"".[29] Best practice guidelines[26] imply Licensed Real Estate agencies conducting property management business should collect rent through an audited trust account, which brings some certainty to the security of the Landlord and Tenants rental Monies, though REAA cases, implies that this is not always so.[29]

The Residential Tenancies Act 1986 sets out the rights and responsibilities of residential landlords and tenants, including the requirement to have a written tenancy agreement and the need to lodge tenancy bonds (if one is required) with the Ministry of Business, Innovation and Employment. The Tenancy Tribunal[30] and its adjudicators/mediators hear and make judgement on disputes (between landlord and tenants) in relation to any breaches of The Residential Tenancies Act 1986 and The Unit Titles Act 2010.

On July 1, 2019, the Healthy Homes Standards became law. The healthy homes standards introduce specific and minimum standards for heating, insulation, ventilation, moisture ingress and drainage, and draft stopping in rental properties. All private rentals must comply within 90 days of any new or renewed tenancy after 1 July 2021, with all private rentals complying by 1 July 2024[31]

The Unit Titles Act 2010 sets out the law for the ownership and management of unit title developments, where multiple owners each hold a unit title. The Act covers the set-up of such developments, body corporate governance, the rights and obligations of the body corporate and unit owners, disclosure between buyers and sellers, dispute resolution, etc. The Unit Titles Regulations 2011 provide operational guidelines. The body corporate is responsible for financial and administrative functions relating to the common property and the development. All unit owners are members of the body corporate. A body corporate can delegate some of its powers and duties to a body corporate committee and a professional body corporate manager may be contracted to provide services.[32]

No license is required to manage properties in Panama, as long as the company is focused on managing properties. Nevertheless, a real estate company that plans to buy and sell properties requires a license.

No specific regulatory or licensing body exists at this time (November 2012). However, under financial business law, any business offering Property Management as a chargeable, fee-earning act of commerce may only do so if such services are listed in their Company Acts of Constitutions, i.e., legally pre-declared list of business activities. Under Romanian law, no business can derive income from any such service that is not declared in this way and should be demonstrable upon request by the client of legal entities.

In the United Kingdom, there is no statutory regulation concerning property management companies. Companies that manage rented residential property are often members of the Association of Residential Letting Agents. Companies or individual landlords who accept tenancy deposits for ""assured shorthold tenancies"" (the usual form of residential tenancy) are required by statute to be members of a Tenancy Deposit Scheme.

Most states, such as New York[33] and Colorado,[34] require property management companies to be licensed real estate brokers if they are collecting rent, listing properties for rent, or helping negotiate leases and doing inspections as a third-party. A property manager may be a licensed real estate salesperson but generally they must be working under a licensed real estate broker. Most states have a public license check system online for anyone holding a real estate salesperson or real estate broker's license.[35] A few states, such as Idaho, Maine, and Vermont, do not require property managers to have real estate licenses. Other states, such as Montana, Oregon, and South Carolina, allow property managers to work under a property management license rather than a broker's license. Some states, like Pennsylvania, allow property managers to work without a real estate license if they do not negotiate leases, hold tenants' money, or enter into leases on the property owner's behalf.

Owners who manage their own property are not required to have a real estate license in many states; however, they must at least have a business license to rent out their own home. Owners who do not live near the rental property may be required, by local government, to hire the services of a property management company.[citation needed] Some states with high tourism numbers, such as Hawaii,[36] have strict property management rules.

In California, third-party apartment property managers must be licensed with the California Bureau of Real Estate as a real estate broker. A broker's license is required for any person or company that, for compensation, leases or rents or offers to lease or rent, or places for rent, or solicits listing of places for rent, or solicits for prospective tenants, or negotiates the sale, purchase, or exchange of leases on real property, or on a business opportunity, or collects rents from real property, or improvements thereon, or from business opportunities.[37] California Code of Regulations, Title 25, Section 42, requires property owners of apartment buildings with 16 or more units to have on-site resident managers living on their properties. There is no such requirement for apartment buildings with less than 16 units.[38]

The designation Real Estate Broker is often confused by those unfamiliar with terms of the industry such as Realtor, real estate agent, or real estate salesperson, and definitions vary from US state to state.

[Are these all in the US, or what?]

Building Owners and Managers Association (BOMA International) offers industry-standard designations that certify the training to property managers:[39]

Institute of Real Estate Management (IREM)

Manufactured Housing Institute (MHI)

National Apartment Association (NAA) has the following designations:

National Association of Residential Property Managers (NARPM) offers designations to certify ethical and professional standards of conduct for property managers:[40]

State-specific designations include the following:

The Community Associations Institute also has designations in the United States for residential property managers who manage planned communities such as Condominiums, homeowner associations, and Cooperatives. National designations include:

The National Association of Home Builders has a specialized designation for the affordable housing industry through the Low Income Housing Tax Credit (LIHTC) program:

In the UK:

In Kenya:

Property management software continues to grow in popularity and importance. As it decreases in price, smaller companies and amateur property managers can function using some of the same best practices and efficiency as larger companies. Online asset management software (OPMS, or online property management software) has been a significant cause of the price declines. In addition to the core property management software options, a quickly growing number of closely related software products are being introduced to the industry.

A property management system, also known as a PMS, is a comprehensive software application used to cover objectives like coordinating the operational functions of the front office, bookings, communication with guests, planning, reporting, etc. This kind of software is used to run large-scale hotels and vacation properties.

This is the most common model and is used by property management companies in the residential space that manage multi-home units and single-family homes. The property owner in this case signs a property management agreement with the company, giving the latter the right to let it out to new tenants and collect rent. The owners don't usually even know who the tenants are. The property management company usually keeps 10-15% of the rent amount and shares the rest with the property owner.

This is the most common revenue model used by companies when monitoring empty homes or empty land sites. The work here involves monitoring the property and ensuring that it is safe and secure, and reporting back to the owner. As there is no income from these properties, a fixed monthly fee is usually charged to the owner.

This model is also used in the residential space, but mostly for small units in high-demand locations. Here, the company signs a rental agreement with the owner and pays them a fixed rent. As per the agreement, the company is given the right to sublet the property for a higher rent. The company's income is the difference between the two rents. As is evident, in this case, the company minimizes the rent paid to the owner, which is usually lower than market rates.

This model applies to the service apartment space and other commercial establishments, such as retail or business centers that generate revenue. In this case, the property manager signs an agreement with the property owner, with the right to convert the property into a revenue-generating business such as a business center, service apartment, etc. Instead of paying rent to the owner, the management company shares a percentage of revenue.[citation needed] There are also hybrid structures here, where a combination of a fixed rent and a share of revenue is shared with the property owner.[citation needed]
"
Real Estate Services,"

Real estate is property consisting of land and the buildings on it, along with its natural resources such as growing crops (e.g. timber), minerals or water, and wild animals; immovable property of this nature; an interest vested in this (also) an item of real property, (more generally) buildings or housing in general.[1][2] In terms of law, real relates to land property and is different from personal property, while estate means the ""interest"" a person has in that land property.[3]

Real estate is different from personal property, which is not permanently attached to the land (or comes with the land), such as vehicles, boats, jewelry, furniture, tools, and the rolling stock of a farm and farm animals.

In the United States, the transfer, owning, or acquisition of real estate can be through business corporations, individuals, nonprofit corporations, fiduciaries, or any legal entity as seen within the law of each U.S. state.[3]

The natural right of a person to own property as a concept can be seen as having roots in Roman law as well as Greek philosophy.[4] The profession of appraisal can be seen as beginning in England during the 1500s, as agricultural needs required land clearing and land preparation. Textbooks on the subject of surveying began to be written and the term ""surveying"" was used in England, while the term ""appraising"" was more used in North America.[5] Natural law which can be seen as ""universal law"" was discussed among writers of the 15th and 16th century as it pertained to ""property theory"" and the inter-state relations dealing with foreign investments and the protection of citizens private property abroad. Natural law can be seen as having an influence in Emerich de Vattel's 1758 treatise The Law of Nations which conceptualized the idea of private property.[6]

One of the largest initial real estate deals in history known as the ""Louisiana Purchase"" happened in 1803 when the Louisiana Purchase Treaty was signed. This treaty paved the way for western expansion and made the U.S. the owners of the ""Louisiana Territory"" as the land was bought from France for fifteen million dollars, making each acre roughly 4 cents.[7] The oldest real estate brokerage firm was established in 1855 in Chicago, Illinois, and was initially known as ""L. D. Olmsted & Co."" but is now known as ""Baird & Warner"".[8] In 1908, the National Association of Realtors was founded in Chicago and in 1916, the name was changed to the National Association of Real Estate Boards and this was also when the term ""realtor"" was coined to identify real estate professionals.[9]

The stock market crash of 1929 and the Great Depression in the U.S. caused a major drop in real estate worth and prices and ultimately resulted in depreciation of 50% for the four years after 1929.[10] Housing financing in the U.S. was greatly affected by the Banking Act of 1933 and the National Housing Act in 1934 because it allowed for mortgage insurance for home buyers and this system was implemented by the Federal Deposit Insurance as well as the Federal Housing Administration.[11] In 1938, an amendment was made to the National Housing Act and Fannie Mae, a government agency, was established to serve as a secondary market for mortgages and to give lenders more money in order for new homes to be funded.[12]

Title VIII of the Civil Rights Act in the U.S., which is also known as the Fair Housing Act, was put into place in 1968 and dealt with the incorporation of African Americans into neighborhoods as the issues of discrimination were analyzed with the renting, buying, and financing of homes.[13] Internet real estate as a concept began with the first appearance of real estate platforms on the World Wide Web (www) and occurred in 1999.

Residential real estate may contain either a single family or multifamily structure that is available for occupation or for non-business purposes.[14]

Residences can be classified by and how they are connected to neighbouring residences and land. Different types of housing tenure can be used for the same physical type. For example, connected residences might be owned by a single entity and leased out, or owned separately with an agreement covering the relationship between units and common areas and concerns.[15]

According to the Congressional Research Service, in 2021, 65% of homes in the U.S. are owned by the occupier.[16]

Other categories

The size of havelis and chawls is measured in Gaz (square yards), Quila, Marla, Beegha, and acre.

See List of house types for a complete listing of housing types and layouts, real estate trends for shifts in the market, and house or home for more general information.

Real estate can be valued or devalued based on the amount of environmental degradation that has occurred. Environmental degradation can cause extreme health and safety risks. There is a growing demand for the use of site assessments (ESAs) when valuing a property for both private and commercial real estate.[17]

Environmental surveying is made possible by environmental surveyors who examine the environmental factors present within the development of real estate as well as the impacts that development and real estate has on the environment.

Green development is a concept that has grown since the 1970s with the environmental movement and the World Commission on Environment and Development. Green development examines social and environmental impacts with real estate and building. There are 3 areas of focus, being the environmental responsiveness, resource efficiency, and the sensitivity of cultural and societal aspects. Examples of Green development are green infrastructure, LEED, conservation development, and sustainability developments.

Real estate in itself has been measured as a contributing factor to the rise in green house gases. According to the International Energy Agency, real estate in 2019 was responsible for 39 percent of total emissions worldwide and 11 percent of those emissions were due to the manufacturing of materials used in buildings.[18]

Real estate development involves planning and coordinating of housebuilding, real estate construction or renovation projects.[19] Real estate development can be less cyclical than real estate investing.[20]

In markets where land and building prices are rising, real estate is often purchased as an investment, whether or not the owner intends to use the property. Often investment properties are rented out, but ""flipping"" involves quickly reselling a property, sometimes taking advantage of arbitrage or quickly rising value, and sometimes after repairs are made that substantially raise the value of the property. Luxury real estate is sometimes used as a way to store value, especially by wealthy foreigners, without any particular attempt to rent it out. Some luxury units in London and New York City have been used as a way for corrupt foreign government officials and business people from countries without strong rule of law to launder money or to protect it from seizure.[21] Investment in real estate can be categorized by financial risk into core, value-added, and opportunistic.[22]
"
Insurance Services,"

Insurance is a means of protection from financial loss in which, in exchange for a fee, a party agrees to compensate another party in the event of a certain loss, damage, or injury. It is a form of risk management, primarily used to protect against the risk of a contingent or uncertain loss.

An entity which provides insurance is known as an insurer, insurance company, insurance carrier, or underwriter. A person or entity who buys insurance is known as a policyholder, while a person or entity covered under the policy is called an insured. The insurance transaction involves the policyholder assuming a guaranteed, known, and relatively small loss in the form of a payment to the insurer (a premium) in exchange for the insurer's promise to compensate the insured in the event of a covered loss. The loss may or may not be financial, but it must be reducible to financial terms. Furthermore, it usually involves something in which the insured has an insurable interest established by ownership, possession, or pre-existing relationship.

The insured receives a contract, called the insurance policy, which details the conditions and circumstances under which the insurer will compensate the insured, or their designated beneficiary or assignee. The amount of money charged by the insurer to the policyholder for the coverage set forth in the insurance policy is called the premium. If the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster. A mandatory out-of-pocket expense required by an insurance policy before an insurer will pay a claim is called a deductible (or if required by a health insurance policy, a copayment). The insurer may hedge its own risk by taking out reinsurance, whereby another insurance company agrees to carry some of the risks, especially if the primary insurer deems the risk too large for it to carry.

Methods for transferring or distributing risk were practiced by Chinese and Indian traders as long ago as the 3rd and 2nd millennia BC, respectively.[1][2] Chinese merchants travelling treacherous river rapids would redistribute their wares across many vessels to limit the loss due to any single vessel capsizing.

Codex Hammurabi Law 238 (c. 1755–1750 BC) stipulated that a sea captain, ship-manager, or ship charterer that saved a ship from total loss was only required to pay one-half the value of the ship to the ship-owner.[3][4][5] In the Digesta seu Pandectae (533), the second volume of the codification of laws ordered by Justinian I (527–565), a legal opinion written by the Roman jurist Paulus in 235 AD was included about the Lex Rhodia (""Rhodian law""). It articulates the general average principle of marine insurance established on the island of Rhodes in approximately 1000 to 800 BC, plausibly by the Phoenicians during the proposed Dorian invasion and emergence of the purported Sea Peoples during the Greek Dark Ages (c. 1100–c. 750).[6][7][8]

The law of general average is the fundamental principle that underlies all insurance.[7] In 1816, an archeological excavation in Minya, Egypt produced a Nerva–Antonine dynasty-era tablet from the ruins of the Temple of Antinous in Antinoöpolis, Aegyptus. The tablet prescribed the rules and membership dues of a burial society collegium established in Lanuvium, Italia in approximately 133 AD during the reign of Hadrian (117–138) of the Roman Empire.[7] In 1851 AD, future U.S. Supreme Court Associate Justice Joseph P. Bradley (1870–1892 AD), once employed as an actuary for the Mutual Benefit Life Insurance Company, submitted an article to the Journal of the Institute of Actuaries. His article detailed an historical account of a Severan dynasty-era life table compiled by the Roman jurist Ulpian in approximately 220 AD that was also included in the Digesta.[9]

Concepts of insurance has been also found in 3rd century BC Hindu scriptures such as Dharmasastra, Arthashastra and Manusmriti.[10] The ancient Greeks had marine loans. Money was advanced on a ship or cargo, to be repaid with large interest if the voyage prospers. However, the money would not be repaid at all if the ship were lost, thus making the rate of interest high enough to pay for not only for the use of the capital but also for the risk of losing it (fully described by Demosthenes). Loans of this character have ever since been common in maritime lands under the name of bottomry and respondentia bonds.[11]

The direct insurance of sea-risks for a premium paid independently of loans began in Belgium about 1300 AD.[11]

Separate insurance contracts (i.e., insurance policies not bundled with loans or other kinds of contracts) were invented in Genoa in the 14th century, as were insurance pools backed by pledges of landed estates. The first known insurance contract dates from Genoa in 1347. In the next century, maritime insurance developed widely, and premiums were varied with risks.[12] These new insurance contracts allowed insurance to be separated from investment, a separation of roles that first proved useful in marine insurance.

The earliest known policy of life insurance was made in the Royal Exchange, London, on 18 June 1583, for £383, 6s. 8d. for twelve months on the life of William Gibbons.[11]

Insurance became far more sophisticated in Enlightenment-era Europe, where specialized varieties developed.

Property insurance as we know it today can be traced to the Great Fire of London, which in 1666 devoured more than 13,000 houses. The devastating effects of the fire converted the development of insurance ""from a matter of convenience into one of urgency, a change of opinion reflected in Sir Christopher Wren's inclusion of a site for ""the Insurance Office"" in his new plan for London in 1667.""[13] A number of attempted fire insurance schemes came to nothing, but in 1681, economist Nicholas Barbon and eleven associates established the first fire insurance company, the ""Insurance Office for Houses"", at the back of the Royal Exchange to insure brick and frame homes. Initially, 5,000 homes were insured by his Insurance Office.[14]

At the same time, the first insurance schemes for the underwriting of business ventures became available. By the end of the seventeenth century, London's growth as a centre for trade was increasing due to the demand for marine insurance. In the late 1680s, Edward Lloyd opened a coffee house, which became the meeting place for parties in the shipping industry wishing to insure cargoes and ships, including those willing to underwrite such ventures. These informal beginnings led to the establishment of the insurance market Lloyd's of London and several related shipping and insurance businesses.[15]

Life insurance policies were taken out in the early 18th century. The first company to offer life insurance was the Amicable Society for a Perpetual Assurance Office, founded in London in 1706 by William Talbot and Sir Thomas Allen.[16][17] Upon the same principle, Edward Rowe Mores established the Society for Equitable Assurances on Lives and Survivorship in 1762.

It was the world's first mutual insurer and it pioneered age based premiums based on mortality rate laying ""the framework for scientific insurance practice and development"" and ""the basis of modern life assurance upon which all life assurance schemes were subsequently based.""[18]

In the late 19th century ""accident insurance"" began to become available.[19] The first company to offer accident insurance was the Railway Passengers Assurance Company, formed in 1848 in England to insure against the rising number of fatalities on the nascent railway system. 

The first international insurance rule was the York Antwerp Rules (YAR) for the distribution of costs between ship and cargo in the event of general average. In 1873 the ""Association for the Reform and Codification of the Law of Nations"", the forerunner of the International Law Association (ILA), was founded in Brussels. It published the first YAR in 1890, before switching to the present title of the ""International Law Association"" in 1895.[20][21]

By the late 19th century governments began to initiate national insurance programs against sickness and old age. Germany built on a tradition of welfare programs in Prussia and Saxony that began as early as in the 1840s. In the 1880s Chancellor Otto von Bismarck introduced old age pensions, accident insurance and medical care that formed the basis for Germany's welfare state.[22][23] In Britain more extensive legislation was introduced by the Liberal government in the National Insurance Act 1911. This gave the British working classes the first contributory system of insurance against illness and unemployment.[24] This system was greatly expanded after the Second World War under the influence of the Beveridge Report, to form the first modern welfare state.[22][25]

In 2008, the International Network of Insurance Associations (INIA), then an informal network, became active and it has been succeeded by the Global Federation of Insurance Associations (GFIA), which was formally founded in 2012 to aim to increase insurance industry effectiveness in providing input to international regulatory bodies and to contribute more effectively to the international dialogue on issues of common interest. It consists of its 40 member associations and 1 observer association in 67 countries, which companies account for around 89% of total insurance premiums worldwide.[26]

Insurance involves pooling funds from many insured entities (known as exposures) to pay for the losses that only some insureds may incur. The insured entities are therefore protected from risk for a fee, with the fee being dependent upon the frequency and severity of the event occurring. In order to be an insurable risk, the risk insured against must meet certain characteristics. Insurance as a financial intermediary is a commercial enterprise and a major part of the financial services industry, but individual entities can also self-insure through saving money for possible future losses.[27]

Risk which can be insured by private companies typically share seven common characteristics:[28]

When a company insures an individual entity, there are basic legal requirements and regulations. Several commonly cited legal principles of insurance include:[29]

To ""indemnify"" means to make whole again, or to be reinstated to the position that one was in, to the extent possible, prior to the happening of a specified event or peril. Accordingly, life insurance is generally not considered to be indemnity insurance, but rather ""contingent"" insurance (i.e., a claim arises on the occurrence of a specified event). There are generally three types of insurance contracts that seek to indemnify an insured:

From an insured's standpoint, the result is usually the same: the insurer pays the loss and claims expenses.

If the Insured has a ""reimbursement"" policy, the insured can be required to pay for a loss and then be ""reimbursed"" by the insurance carrier for the loss and out of pocket costs including, with the permission of the insurer, claim expenses.[30][note 1]

Under a ""pay on behalf"" policy, the insurance carrier would defend and pay a claim on behalf of the insured who would not be out of pocket for anything. Most modern liability insurance is written on the basis of ""pay on behalf"" language, which enables the insurance carrier to manage and control the claim.

Under an ""indemnification"" policy, the insurance carrier can generally either ""reimburse"" or ""pay on behalf of"", whichever is more beneficial to it and the insured in the claim handling process.

An entity seeking to transfer risk (an individual, corporation, or association of any type, etc.) becomes the ""insured"" party once risk is assumed by an ""insurer"", the insuring party, by means of a contract, called an insurance policy. Generally, an insurance contract includes, at a minimum, the following elements: identification of participating parties (the insurer, the insured, the beneficiaries), the premium, the period of coverage, the particular loss event covered, the amount of coverage (i.e., the amount to be paid to the insured or beneficiary in the event of a loss), and exclusions (events not covered). An insured is thus said to be ""indemnified"" against the loss covered in the policy.

When insured parties experience a loss for a specified peril, the coverage entitles the policyholder to make a claim against the insurer for the covered amount of loss as specified by the policy. The fee paid by the insured to the insurer for assuming the risk is called the premium. Insurance premiums from many insureds are used to fund accounts reserved for later payment of claims – in theory for a relatively few claimants – and for overhead costs. So long as an insurer maintains adequate funds set aside for anticipated losses (called reserves), the remaining margin is an insurer's profit.

Policies typically include a number of exclusions, for example:

Insurers may prohibit certain activities which are considered dangerous and therefore excluded from coverage. One system for classifying activities according to whether they are authorised by insurers refers to ""green light"" approved activities and events, ""yellow light"" activities and events which require insurer consultation and/or waivers of liability, and ""red light"" activities and events which are prohibited and outside the scope of insurance cover.[33]

Insurance can have various effects on society through the way that it changes who bears the cost of losses and damage. On one hand it can increase fraud; on the other it can help societies and individuals prepare for catastrophes and mitigate the effects of catastrophes on both households and societies.

Insurance can influence the probability of losses through moral hazard, insurance fraud, and preventive steps by the insurance company. Insurance scholars have typically used moral hazard to refer to the increased loss due to unintentional carelessness and insurance fraud to refer to increased risk due to intentional carelessness or indifference.[34] Insurers attempt to address carelessness through inspections, policy provisions requiring certain types of maintenance, and possible discounts for loss mitigation efforts. While in theory insurers could encourage investment in loss reduction, some commentators have argued that in practice insurers had historically not aggressively pursued loss control measures—particularly to prevent disaster losses such as hurricanes—because of concerns over rate reductions and legal battles. However, since about 1996 insurers have begun to take a more active role in loss mitigation, such as through building codes.[35]

According to the study books of The Chartered Insurance Institute, there are variant methods of insurance as follows:

Insurers may use the subscription business model, collecting premium payments periodically in return for on-going and/or compounding benefits offered to policyholders.

Insurers' business model aims to collect more in premium and investment income than is paid out in losses, and to also offer a competitive price which consumers will accept. Profit can be reduced to a simple equation:

Insurers make money in two ways:

The most complicated aspect of insuring is the actuarial science of ratemaking (price-setting) of policies, which uses statistics and probability to approximate the rate of future claims based on a given risk. After producing rates, the insurer will use discretion to reject or accept risks through the underwriting process.

At the most basic level, initial rate-making involves looking at the frequency and severity of insured perils and the expected average payout resulting from these perils. Thereafter an insurance company will collect historical loss-data, bring the loss data to present value, and compare these prior losses to the premium collected in order to assess rate adequacy.[36] Loss ratios and expense loads are also used. Rating for different risk characteristics involves—at the most basic level—comparing the losses with ""loss relativities""—a policy with twice as many losses would, therefore, be charged twice as much. More complex multivariate analyses are sometimes used when multiple characteristics are involved and a univariate analysis could produce confounded results. Other statistical methods may be used in assessing the probability of future losses.

Upon termination of a given policy, the amount of premium collected minus the amount paid out in claims is the insurer's underwriting profit on that policy. Underwriting performance is measured by something called the ""combined ratio"", which is the ratio of expenses/losses to premiums.[37] A combined ratio of less than 100% indicates an underwriting profit, while anything over 100 indicates an underwriting loss. A company with a combined ratio over 100% may nevertheless remain profitable due to investment earnings.

Insurance companies earn investment profits on ""float"". Float, or available reserve, is the amount of money on hand at any given moment that an insurer has collected in insurance premiums but has not paid out in claims. Insurers start investing insurance premiums as soon as they are collected and continue to earn interest or other income on them until claims are paid out. The Association of British Insurers (grouping together 400 insurance companies and 94% of UK insurance services) has almost 20% of the investments in the London Stock Exchange.[38] In 2007, U.S. industry profits from float totaled $58 billion. In a 2009 letter to investors, Warren Buffett wrote, ""we were paid $2.8 billion to hold our float in 2008"".[39]

In the United States, the underwriting loss of property and casualty insurance companies was $142.3 billion in the five years ending 2003. But overall profit for the same period was $68.4 billion, as the result of float. Some insurance-industry insiders, most notably Hank Greenberg, do not believe that it is possible to sustain a profit from float forever without an underwriting profit as well, but this opinion is not universally held. Reliance on float for profit has led some industry experts to call insurance companies ""investment companies that raise the money for their investments by selling insurance"".[40]

Naturally, the float method is difficult to carry out in an economically depressed period. Bear markets do cause insurers to shift away from investments and to toughen up their underwriting standards, so a poor economy generally means high insurance-premiums. This tendency to swing between profitable and unprofitable periods over time is commonly known as the underwriting, or insurance, cycle.[41]

Claims and loss handling is the materialized utility of insurance; it is the actual ""product"" paid for. Claims may be filed by insureds directly with the insurer or through brokers or agents. The insurer may require that the claim be filed on its own proprietary forms, or may accept claims on a standard industry form, such as those produced by ACORD.

Insurance company claims departments employ a large number of claims adjusters, supported by a staff of records management and data entry clerks. Incoming claims are classified based on severity and are assigned to adjusters, whose settlement authority varies with their knowledge and experience. An adjuster undertakes an investigation of each claim, usually in close cooperation with the insured, determines if coverage is available under the terms of the insurance contract (and if so, the reasonable monetary value of the claim), and authorizes payment.

Policyholders may hire their own public adjusters to negotiate settlements with the insurance company on their behalf. For policies that are complicated, where claims may be complex, the insured may take out a separate insurance policy add-on, called loss recovery insurance, which covers the cost of a public adjuster in the case of a claim.

Adjusting liability insurance claims is particularly difficult because they involve a third party, the plaintiff, who is under no contractual obligation to cooperate with the insurer and may in fact regard the insurer as a deep pocket. The adjuster must obtain legal counsel for the insured—either inside (""house"") counsel or outside (""panel"") counsel, monitor litigation that may take years to complete, and appear in person or over the telephone with settlement authority at a mandatory settlement conference when requested by a judge.

If a claims adjuster suspects underinsurance, the condition of average may come into play to limit the insurance company's exposure.

In managing the claims-handling function, insurers seek to balance the elements of customer satisfaction, administrative handling expenses, and claims overpayment leakages. In addition to this balancing act, fraudulent insurance practices are a major business risk that insurers must manage and overcome. Disputes between insurers and insureds over the validity of claims or claims-handling practices occasionally escalate into litigation (see insurance bad faith).

Insurers will often use insurance agents to initially market or underwrite their customers. Agents can be captive, meaning they write only for one company, or independent, meaning that they can issue policies from several companies. The existence and success of companies using insurance agents is likely due to the availability of improved and personalised services. Companies also use Broking firms, Banks and other corporate entities (like Self Help Groups, Microfinance Institutions, NGOs, etc.) to market their products.[42]

Any risk that can be quantified can potentially be insured. Specific kinds of risk that may give rise to claims are known as perils. An insurance policy will set out in detail which perils are covered by the policy and which are not. Below are non-exhaustive lists of the many different types of insurance that exist. A single policy may cover risks in one or more of the categories set out below. For example, vehicle insurance would typically cover both the property risk (theft or damage to the vehicle) and the liability risk (legal claims arising from an accident). A home insurance policy in the United States typically includes coverage for damage to the home and the owner's belongings, certain legal claims against the owner, and even a small amount of coverage for medical expenses of guests who are injured on the owner's property.

Business insurance can take a number of different forms, such as the various kinds of professional liability insurance, also called professional indemnity (PI), which are discussed below under that name; and the business owner's policy (BOP), which packages into one policy many of the kinds of coverage that a business owner needs, in a way analogous to how homeowners' insurance packages the coverages that a homeowner needs.[43]

Vehicle insurance protects the policyholder against financial loss in the event of an incident involving a vehicle they own, such as in a traffic collision.

Coverage typically includes:

GAP (Guaranteed Asset Protection) insurance covers the excess amount on an auto loan in an instance where the policyholder's insurance company does not cover the entire loan. Depending on the company's specific policies it might or might not cover the deductible as well. This coverage is marketed for those who put low down payments, have high interest rates on their loans, and those with 60-month or longer terms. Gap insurance is typically offered by a finance company when the vehicle owner purchases their vehicle, but many auto insurance companies offer this coverage to consumers as well.

Health insurance policies cover the cost of medical treatments. Dental insurance, like medical insurance, protects policyholders for dental costs. In most developed countries, all citizens receive some health coverage from their governments, paid through taxation. In most countries, health insurance is often part of an employer's benefits.

Casualty insurance insures against accidents, not necessarily tied to any specific property. It is a broad spectrum of insurance that a number of other types of insurance could be classified, such as auto, workers compensation, and some liability insurances.

Life insurance provides a monetary benefit to a decedent's family or other designated beneficiary, and may specifically provide for income to an insured person's family, burial, funeral and other final expenses. Life insurance policies often allow the option of having the proceeds paid to the beneficiary either in a lump sum cash payment or an annuity. In most states, a person cannot purchase a policy on another person without their knowledge.

Annuities provide a stream of payments and are generally classified as insurance because they are issued by insurance companies, are regulated as insurance, and require the same kinds of actuarial and investment management expertise that life insurance requires. Annuities and pensions that pay a benefit for life are sometimes regarded as insurance against the possibility that a retiree will outlive his or her financial resources. In that sense, they are the complement of life insurance and, from an underwriting perspective, are the mirror image of life insurance.

Certain life insurance contracts accumulate cash values, which may be taken by the insured if the policy is surrendered or which may be borrowed against. Some policies, such as annuities and endowment policies, are financial instruments to accumulate or liquidate wealth when it is needed.

In many countries, such as the United States and the UK, the tax law provides that the interest on this cash value is not taxable under certain circumstances. This leads to widespread use of life insurance as a tax-efficient method of saving as well as protection in the event of early death.

In the United States, the tax on interest income on life insurance policies and annuities is generally deferred. However, in some cases the benefit derived from tax deferral may be offset by a low return. This depends upon the insuring company, the type of policy and other variables (mortality, market return, etc.). Moreover, other income tax saving vehicles (e.g., IRAs, 401(k) plans, Roth IRAs) may be better alternatives for value accumulation.

Burial insurance is an old type of life insurance which is paid out upon death to cover final expenses, such as the cost of a funeral. The Greeks and Romans introduced burial insurance c. 600 CE when they organized guilds called ""benevolent societies"" which cared for the surviving families and paid funeral expenses of members upon death. Guilds in the Middle Ages served a similar purpose, as did friendly societies during Victorian times.

Property insurance provides protection against risks to property, such as fire, theft or weather damage. This may include specialized forms of insurance such as fire insurance, flood insurance, earthquake insurance, home insurance, inland marine insurance or boiler insurance.
The term property insurance may, like casualty insurance, be used as a broad category of various subtypes of insurance, some of which are listed below:

Liability insurance is a broad superset that covers legal claims against the insured. Many types of insurance include an aspect of liability coverage. For example, a homeowner's insurance policy will normally include liability coverage which protects the insured in the event of a claim brought by someone who slips and falls on the property; automobile insurance also includes an aspect of liability insurance that indemnifies against the harm that a crashing car can cause to others' lives, health, or property. The protection offered by a liability insurance policy is twofold: a legal defense in the event of a lawsuit commenced against the policyholder and indemnification (payment on behalf of the insured) with respect to a settlement or court verdict. Liability policies typically cover only the negligence of the insured, and will not apply to results of wilful or intentional acts by the insured.

Often a commercial insured's liability insurance program consists of several layers. The first layer of insurance generally consists of primary insurance, which provides first dollar indemnity for judgments and settlements up to the limits of liability of the primary policy. Generally, primary insurance is subject to a deductible and obligates the insurer to defend the insured against lawsuits, which is normally accomplished by assigning counsel to defend the insured. In many instances, a commercial insured may elect to self-insure. Above the primary insurance or self-insured retention, the insured may have one or more layers of excess insurance to provide coverage additional limits of indemnity protection. There are a variety of types of excess insurance, including ""stand-alone"" excess policies (policies that contain their own terms, conditions, and exclusions), ""follow form"" excess insurance (policies that follow the terms of the underlying policy except as specifically provided), and ""umbrella"" insurance policies (excess insurance that in some circumstances could provide coverage that is broader than the underlying insurance).[50]

Credit insurance repays some or all of a loan when the borrower is insolvent.

Cyber-insurance is a business lines insurance product intended to provide coverage to corporations from Internet-based risks, and more generally from risks relating to information technology infrastructure, information privacy, information governance liability, and activities related thereto.

Some communities prefer to create virtual insurance among themselves by other means than contractual risk transfer, which assigns explicit numerical values to risk. A number of religious groups, including the Amish and some Muslim groups, depend on support provided by their communities when disasters strike. The risk presented by any given person is assumed collectively by the community who all bear the cost of rebuilding lost property and supporting people whose needs are suddenly greater after a loss of some kind. In supportive communities where others can be trusted to follow community leaders, this tacit form of insurance can work. In this manner the community can even out the extreme differences in insurability that exist among its members. Some further justification is also provided by invoking the moral hazard of explicit insurance contracts.

In the United Kingdom, The Crown (which, for practical purposes, meant the civil service) did not insure property such as government buildings. If a government building was damaged, the cost of repair would be met from public funds because, in the long run, this was cheaper than paying insurance premiums. Since many UK government buildings have been sold to property companies and rented back, this arrangement is now less common.

In the United States, the most prevalent form of self-insurance is governmental risk management pools. They are self-funded cooperatives, operating as carriers of coverage for the majority of governmental entities today, such as county governments, municipalities, and school districts. Rather than these entities independently self-insure and risk bankruptcy from a large judgment or catastrophic loss, such governmental entities form a risk pool. Such pools begin their operations by capitalization through member deposits or bond issuance. Coverage (such as general liability, auto liability, professional liability, workers compensation, and property) is offered by the pool to its members, similar to coverage offered by insurance companies. However, self-insured pools offer members lower rates (due to not needing insurance brokers), increased benefits (such as loss prevention services) and subject matter expertise. Of approximately 91,000 distinct governmental entities operating in the United States, 75,000 are members of self-insured pools in various lines of coverage, forming approximately 500 pools. Although a relatively small corner of the insurance market, the annual contributions (self-insured premiums) to such pools have been estimated up to 17 billion dollars annually.[57]

Insurance companies may provide any combination of insurance types, but are often classified into three groups:[58]

General insurance companies can be further divided into these sub categories.

In most countries, life and non-life insurers are subject to different regulatory regimes and different tax and accounting rules. The main reason for the distinction between the two types of company is that life, annuity, and pension business is long-term in nature – coverage for life assurance or a pension can cover risks over many decades. By contrast, non-life insurance cover usually covers a shorter period, such as one year.

Insurance companies are commonly classified as either mutual or proprietary companies.[59] Mutual companies are owned by the policyholders, while shareholders (who may or may not own policies) own proprietary insurance companies.

Demutualization of mutual insurers to form stock companies, as well as the formation of a hybrid known as a mutual holding company, became common in some countries, such as the United States, in the late 20th century. However, not all states permit mutual holding companies.

Reinsurance companies are insurance companies that provide policies to other insurance companies, allowing them to reduce their risks and protect themselves from substantial losses.[60] The reinsurance market is dominated by a few large companies with huge reserves. A reinsurer may also be a direct writer of insurance risks as well.

Captive insurance companies can be defined as limited-purpose insurance companies established with the specific objective of financing risks emanating from their parent group or groups. This definition can sometimes be extended to include some of the risks of the parent company's customers. In short, it is an in-house self-insurance vehicle. Captives may take the form of a ""pure"" entity, which is a 100% subsidiary of the self-insured parent company; of a ""mutual"" captive, which insures the collective risks of members of an industry; and of an ""association"" captive, which self-insures individual risks of the members of a professional, commercial or industrial association. Captives represent commercial, economic and tax advantages to their sponsors because of the reductions in costs they help create and for the ease of insurance risk management and the flexibility for cash flows they generate. Additionally, they may provide coverage of risks which is neither available nor offered in the traditional insurance market at reasonable prices.

The types of risk that a captive can underwrite for their parents include property damage, public and product liability, professional indemnity, employee benefits, employers' liability, motor and medical aid expenses. The captive's exposure to such risks may be limited by the use of reinsurance.

Captives are becoming an increasingly important component of the risk management and risk financing strategy of their parent. This can be understood against the following background:

Other possible forms for an insurance company include reciprocals, in which policyholders reciprocate in sharing risks, and Lloyd's organizations.[61]

Admitted insurance companies are those in the United States that have been admitted or licensed by the state licensing agency. The insurance they provide is called admitted insurance. Non-admitted companies have not been approved by the state licensing agency, but are allowed to provide insurance under special circumstances when they meet an insurance need that admitted companies cannot or will not meet.[62]

There are also companies known as ""insurance consultants"". Like a mortgage broker, these companies are paid a fee by the customer to shop around for the best insurance policy among many companies. Similar to an insurance consultant, an ""insurance broker"" also shops around for the best insurance policy among many companies. However, with insurance brokers, the fee is usually paid in the form of commission from the insurer that is selected rather than directly from the client.

Neither insurance consultants nor insurance brokers are insurance companies and no risks are transferred to them in insurance transactions. Third party administrators are companies that perform underwriting and sometimes claims handling services for insurance companies. These companies often have special expertise that the insurance companies do not have.

The financial stability and strength of an insurance company is a consideration when buying an insurance contract. An insurance premium paid currently provides coverage for losses that might arise many years in the future. For that reason, a more financially stable insurance carrier reduces the risk of the insurance company becoming insolvent, leaving their policyholders with no coverage (or coverage only from a government-backed insurance pool or other arrangements with less attractive payouts for losses). A number of independent rating agencies provide information and rate the financial viability of insurance companies.

Insurance companies are rated by various agencies such as AM Best. The ratings include the company's financial strength, which measures its ability to pay claims. It also rates financial instruments issued by the insurance company, such as bonds, notes, and securitization products.

Advanced economies account for the bulk of the global insurance industry. According to Swiss Re, the global insurance market wrote $7.186 trillion in direct premiums in 2023.[63]  (""Direct premiums"" means premiums written directly by insurers before accounting for ceding of risk to reinsurers.) As usual, the United States was the country with the largest insurance market with $3.226 trillion (44.9%) of direct premiums written, with the People's Republic of China coming in second at only $723 billion (10.1%), the United Kingdom coming in third at $374 billion (5.2%), and Japan coming in fourth at $362 billion (5.0%).[63] However, the European Union's single market is the actual second largest market, with 16 percent market share.[63]

In the United States, insurance is regulated by the states under the McCarran–Ferguson Act, with ""periodic proposals for federal intervention"", and a nonprofit coalition of state insurance agencies called the National Association of Insurance Commissioners works to harmonize the country's different laws and regulations.[64] The National Conference of Insurance Legislators (NCOIL) also works to harmonize the different state laws.[65] 1988 California Proposition 103 is claimed to reduce home insurance rates,[66] while it is blamed by some for reduced availability of home insurance in wildfire-distressed neighborhoods.[67]

In the European Union, the Third Non-Life Directive and the Third Life Directive, both passed in 1992 and effective 1994, created a single insurance market in Europe and allowed insurance companies to offer insurance anywhere in the EU (subject to permission from authority in the head office) and allowed insurance consumers to purchase insurance from any insurer in the EU.[68] As far as insurance in the United Kingdom, the Financial Services Authority took over insurance regulation from the General Insurance Standards Council in 2005;[69] laws passed include the Insurance Companies Act 1973 and another in 1982,[70] and reforms to warranty and other aspects under discussion as of 2012[update].[71]

The insurance industry in China was nationalized in 1949 and thereafter offered by only a single state-owned company, the People's Insurance Company of China, which was eventually suspended as demand declined in a communist environment. In 1978, market reforms led to an increase in the market and by 1995 a comprehensive Insurance Law of the People's Republic of China[72] was passed, followed in 1998 by the formation of China Insurance Regulatory Commission (CIRC), which has broad regulatory authority over the insurance market of China.[73]

In India IRDA is insurance regulatory authority. As per the section 4 of IRDA Act 1999, Insurance Regulatory and Development Authority (IRDA), which was constituted by an act of parliament. National Insurance Academy, Pune is apex insurance capacity builder institute promoted with support from Ministry of Finance and by LIC, Life & General Insurance companies.

In 2017, within the framework of the joint project of the Bank of Russia and Yandex, a special check mark (a green circle with a tick and 'Реестр ЦБ РФ' (Unified state register of insurance entities) text box) appeared in the search for Yandex system, informing the consumer that the company's financial services are offered on the marked website, which has the status of an insurance company, a broker or a mutual insurance association.[74]

Insurance is just a risk transfer mechanism wherein the financial burden which may arise due to some fortuitous event is transferred to a bigger entity (i.e., an insurance company) by way of paying premiums. This only reduces the financial burden and not the actual chances of happening of an event. Insurance is a risk for both the insurance company and the insured. The insurance company understands the risk involved and will perform a risk assessment when writing the policy.  

As a result, the premiums may go up if they determine that the policyholder will file a claim. However, premiums might reduce if the policyholder commits to a risk management program as recommended by the insurer.[75] It is therefore important that insurers view risk management as a joint initiative between policyholder and insurer since a robust risk management plan minimizes the possibility of a large claim for the insurer while stabilizing or reducing premiums for the policyholder. University of Tennessee research published in 2014 found that all company staff in the businesses they surveyed recognised the importance of insurance but largely they were too distant within their organization from the provision or cost of insurance to be able to relate to company insurance needs.[76]

If a person is financially stable and plans for life's unexpected events, they may be able to go without insurance. However, they must have enough to cover a total and complete loss of employment and of their possessions. Some states will accept a surety bond, a government bond, or even making a cash deposit with the state.[citation needed]

An insurance company may inadvertently find that its insureds may not be as risk-averse as they might otherwise be (since, by definition, the insured has transferred the risk to the insurer), a concept known as moral hazard. This 'insulates' many from the true costs of living with risk, negating measures that can mitigate or adapt to risk and leading some to describe insurance schemes as potentially maladaptive.[77]

Insurance policies can be complex and some policyholders may not understand all the fees and coverages included in a policy. As a result, people may buy policies on unfavorable terms. In response to these issues, many countries have enacted detailed statutory and regulatory regimes governing every aspect of the insurance business, including minimum standards for policies and the ways in which they may be advertised and sold.

For example, most insurance policies in the English language today have been carefully drafted in plain English; the industry learned the hard way that many courts will not enforce policies against insureds when the judges themselves cannot understand what the policies are saying. Typically, courts construe ambiguities in insurance policies against the insurance company and in favor of coverage under the policy.

Many institutional insurance purchasers buy insurance through an insurance broker. While on the surface it appears the broker represents the buyer (not the insurance company), and typically counsels the buyer on appropriate coverage and policy limitations, in the vast majority of cases a broker's compensation comes in the form of a commission as a percentage of the insurance premium, creating a conflict of interest in that the broker's financial interest is tilted toward encouraging an insured to purchase more insurance than might be necessary at a higher price. A broker generally holds contracts with many insurers, thereby allowing the broker to ""shop"" the market for the best rates and coverage possible.

Insurance may also be purchased through an agent. A tied agent, working exclusively with one insurer, represents the insurance company from whom the policyholder buys (while a free agent sells policies of various insurance companies). Just as there is a potential conflict of interest with a broker, an agent has a different type of conflict. Because agents work directly for the insurance company, if there is a claim the agent may advise the client to the benefit of the insurance company. Agents generally cannot offer as broad a range of selection compared to an insurance broker.

An independent insurance consultant advises insureds on a fee-for-service retainer, similar to an attorney, and thus offers completely independent advice, free of the financial conflict of interest of brokers or agents. However, such a consultant must still work through brokers or agents in order to secure coverage for their clients.

In the United States, economists and consumer advocates generally consider insurance to be worthwhile for low-probability, catastrophic losses, but not for high-probability, small losses. Because of this, consumers are advised to select high deductibles and to not insure losses which would not cause a disruption in their life. However, consumers have shown a tendency to prefer low deductibles and to prefer to insure relatively high-probability, small losses over low-probability, perhaps due to not understanding or ignoring the low-probability risk. This is associated with reduced purchasing of insurance against low-probability losses, and may result in increased inefficiencies from moral hazard.[78]

Redlining is the practice of denying insurance coverage in specific geographic areas, supposedly because of a high likelihood of loss, while the alleged motivation is unlawful discrimination. Racial profiling or redlining has a long history in the property insurance industry in the United States. From a review of industry underwriting and marketing materials, court documents, and research by government agencies, industry and community groups, and academics, it is clear that race has long affected and continues to affect the policies and practices of the insurance industry.[79]

In July 2007, the US Federal Trade Commission (FTC) released a report presenting the results of a study concerning credit-based insurance scores in automobile insurance. The study found that these scores are effective predictors of risk. It also showed that African-Americans and Hispanics are substantially overrepresented in the lowest credit scores, and substantially underrepresented in the highest, while Caucasians and Asians are more evenly spread across the scores. The credit scores were also found to predict risk within each of the ethnic groups, leading the FTC to conclude that the scoring models are not solely proxies for redlining. The FTC indicated little data was available to evaluate benefit of insurance scores to consumers.[80] The report was disputed by representatives of the Consumer Federation of America, the National Fair Housing Alliance, the National Consumer Law Center, and the Center for Economic Justice, for relying on data provided by the insurance industry.[81]

All states have provisions in their rate regulation laws or in their fair trade practice acts that prohibit unfair discrimination, often called redlining, in setting rates and making insurance available.[82]

In determining premiums and premium rate structures, insurers consider quantifiable factors, including location, credit scores, gender, occupation, marital status, and education level. However, the use of such factors is often considered to be unfair or unlawfully discriminatory, and the reaction against this practice has in some instances led to political disputes about the ways in which insurers determine premiums and regulatory intervention to limit the factors used.

An insurance underwriter's job is to evaluate a given risk as to the likelihood that a loss will occur. Any factor that causes a greater likelihood of loss should theoretically be charged a higher rate. This basic principle of insurance must be followed if insurance companies are to remain solvent.[citation needed] Thus, ""discrimination"" against (i.e., negative differential treatment of) potential insureds in the risk evaluation and premium-setting process is a necessary by-product of the fundamentals of insurance underwriting.[citation needed] For instance, insurers charge older people significantly higher premiums than they charge younger people for term life insurance. Older people are thus treated differently from younger people (i.e., a distinction is made, discrimination occurs). The rationale for the differential treatment goes to the heart of the risk a life insurer takes: older people are likely to die sooner than young people, so the risk of loss (the insured's death) is greater in any given period of time and therefore the risk premium must be higher to cover the greater risk.[citation needed] However, treating insureds differently when there is no actuarially sound reason for doing so is unlawful discrimination.

New assurance products can now[when?] be protected from copying with a business method patent in the United States.

A recent example of a new insurance product that is patented is Usage Based auto insurance. Early versions were independently invented and patented by a major US auto insurance company, Progressive Auto Insurance (U.S. patent 5,797,134) and a Spanish independent inventor, Salvador Minguijon Perez.[83]

Many independent inventors are in favor of patenting new insurance products since it gives them protection from big companies when they bring their new insurance products to market. Independent inventors account for 70% of the new U.S. patent applications in this area.[citation needed]

Patenting new insurance products can be risky, as it is practically impossible for insurance companies to determine if their product will infringe on a pre-existing patent.[84] For example, in 2004, The Hartford insurance company had to pay $80 million to an independent inventor, Bancorp Services, in order to settle a patent infringement and theft of trade secret lawsuit for a type of corporate owned life insurance product invented and patented by Bancorp.[85]

There are currently about 150 new patent applications on insurance inventions filed per year in the United States. The rate at which patents have been issued has steadily risen from 15 in 2002 to 44 in 2006.[86]

The first US insurance patent was granted in 2005, which concerned coverage of data transferred over the internet.[87] Another example of an application posted was posted in 2009.[88][dead link‍] This patent application describes a method for increasing the ease of changing insurance companies.[89]

Insurance on demand (also IoD) is an insurance service that provides clients with coverage for a specific occasion or event when needed; i.e. only episodic rather than on a 24/7 basis as is typically provided by traditional policies. For example, air travelers can purchase a policy for one single plane flight, rather than a longer-lasting travel insurance plan.[citation needed]

Certain insurance products and practices have been described as rent-seeking by critics.[citation needed] That is, some insurance products or practices are useful primarily because of legal benefits, such as reducing taxes, as opposed to providing protection against risks of adverse events.[citation needed]

Muslim scholars have varying opinions about life insurance. Life insurance policies that earn interest (or guaranteed bonus/NAV) are generally considered to be a form of riba (usury) and some consider even policies that do not earn interest to be a form of gharar (speculation). Some argue that gharar is not present due to the actuarial science behind the underwriting.[citation needed] Jewish rabbinical scholars also have expressed reservations regarding insurance as an avoidance of God's will but most find it acceptable in moderation.[90]

Some Christians believe insurance represents a lack of faith.[91][92] There is a long history of resistance to commercial insurance in Anabaptist communities (Mennonites, Amish, Hutterites, Brethren in Christ), but many participate in community-based self-insurance programs that spread risk within their communities.[93][94][95]

Country-specific articles:
"
Financial Services,"Financial services are economic services tied to finance provided by financial institutions. Financial services encompass a broad range of service sector activities, especially as concerns financial management and consumer finance. 

The finance industry in its most common sense concerns commercial banks that provide market liquidity, risk instruments, and brokerage for large public companies and multinational corporations at a macroeconomic scale that impacts domestic politics and foreign relations. The extragovernmental power and scale of the finance industry remains an ongoing controversy in many industrialized Western economies, as seen in the American Occupy Wall Street civil protest movement of 2011.

Styles of financial institution include credit union, bank, savings and loan association, trust company, building society, brokerage firm, payment processor, many types of broker, and some government-sponsored enterprise.[1]

Financial services include accountancy, investment banking, investment management, and personal asset management. 

Financial products include insurance, credit cards, mortgage loans, and pension funds. 

The term ""financial services"" became more prevalent in the United States partly as a result of the Gramm–Leach–Bliley Act of the late 1990s, which enabled different types of companies operating in the U.S. financial services industry at that time to merge.[3]

Companies usually have two distinct approaches to this new type of business. One approach would be a bank that simply buys an insurance company or an investment bank, keeps the original brands of the acquired firm, and adds the acquisition to its holding company simply to diversify its earnings. Outside the U.S. (e.g. Japan), non-financial services companies are permitted within the holding company. In this scenario, each company still looks independent and has its own customers, etc. In the other style, a bank would simply create its own insurance division or brokerage division and attempt to sell those products to its own existing customers, with incentives for combining all things with one company.[citation needed]

The financial sector is traditionally among those to receive government support in times of widespread economic crisis. Such bailouts, however, enjoy less public support than those for other industries.[4]

A commercial bank is what is commonly referred to as simply a bank. The term ""commercial"" is used to distinguish it from an investment bank, a type of financial services entity which instead of lending money directly to a business, helps businesses raise money from other firms in the form of bonds (debt) or share capital (equity).

The primary operations of commercial banks include:

The United States is the largest commercial banking services location.

New York City and London are the largest centers of investment banking services. NYC is dominated by U.S. domestic business, while in London international business and commerce make up a significant portion of investment banking activity.[5]

FX or Foreign exchange services are provided by many banks and specialists foreign exchange brokers around the world. Foreign exchange services include:

London handled 36.7% of global currency transactions in 2009[update] – an average daily turnover of US$1.85 trillion – with more US dollars traded in London than New York, and more Euros traded than in every other city in Europe combined.[6][7][8][9][10]

New York City is the largest center of investment services, followed by London.[12]

The United States, followed by Japan and the United Kingdom are the largest insurance markets in the world.[14]


A financial export is a financial service provided by a domestic firm (regardless of ownership) to a foreign firm or individual. While financial services such as banking, insurance, and investment management are often seen as domestic services, an increasing proportion of financial services are now being handled abroad, in other financial centres, for a variety of reasons. Some smaller financial centres, such as Bermuda, Luxembourg, and the Cayman Islands, lack sufficient size for a domestic financial services sector and have developed a role providing services to non-residents as offshore financial centres. The increasing competitiveness of financial services has meant that some countries, such as Japan, which were once self-sufficient, have increasingly imported financial services.[citation needed]

The leading financial exporter, in terms of exports less imports, is the United Kingdom, which had $95 billion of financial exports in 2014.[15] The UK's position is helped by both unique institutions (such as Lloyd's of London for insurance, the Baltic Exchange for shipping etc.)[16] and an environment that attracts foreign firms;[17] many international corporations have global or regional headquarters in the London and are listed on the London Stock Exchange, and many banks and other financial institutions operate there or in Edinburgh.[18][19]
"
Legal Services,"The Legal Services Corporation (LSC) is a publicly funded, 501(c)(3) non-profit corporation established by the United States Congress. It seeks to ensure equal access to justice under the law for all Americans by providing funding for civil legal aid to those who otherwise would be unable to afford it. The LSC was created in 1974 with bipartisan congressional sponsorship and the support of the Nixon administration, and LSC is funded through the congressional appropriations process.

LSC has a board of eleven directors, appointed by the president of the United States and confirmed by the United States Senate, that set LSC policy. By law the board is bipartisan; no more than six members can come from the same party.[1] LSC has a president and other officers who implement those policies and oversee the corporation's operations.[2]

LSC is the largest single funder of civil legal aid in the country, distributing more than 90 percent of its total funding to 132 independent nonprofit legal aid programs.[3]

For Fiscal Year 2023, Congress appropriated $560 million to LSC to fund civil legal aid.[4]

LSC is one of the organizational descendants of the former Office of Economic Opportunity (OEO).[5] The Economic Opportunity Act of 1964, a key part of President Lyndon B. Johnson's Great Society vision, established the OEO. Building on the work of a 1964 essay, ""The War on Poverty: A Civilian Perspective"" by Edgar Cahn and Jean Camper Cahn, in 1965 OEO budgeted $1 million per year[5] to create and fund 269 local legal services programs around the country,[6] such as California Rural Legal Assistance,[6] which made a name for themselves suing local officials and sometimes stirring up resentment against their federal funding.[6] Jean Cahn was the first director of the National
Legal Services Program in OEO.[7]

By the early 1970s the Nixon administration began dismantling the OEO; funding for legal services for the poor began to wither, and supporters looked for an alternative arrangement.[6] In 1971 a bipartisan congressional group, including Senators Ted Kennedy, William A. Steiger, and Walter Mondale, proposed a national, independent Legal Services Corporation;[8] at the same time, administration officials such as Attorney General John N. Mitchell and chief domestic advisor John Ehrlichman were proposing their own somewhat similar solution.[8]

The idea behind the LSC was to create a new corporate entity that would be funded by Congress but run independently, with eleven board members to be appointed by the president, subject to Senate confirmation.[6]

LSC was created by the Legal Services Corporation Act of 1974 (Pub. L. 93–355).[9]  The LSC Act contains certain rules and restrictions regarding what LSC grantees can do.[9]  The initial budget was set at $90 million.[5]

Naming and confirmation of the first LSC board was delayed by inaction and opposition,[6] but by July 1975, President Gerald R. Ford had named and the Senate had approved the first board, with Cornell University Law School Dean Roger Conant Cramton as its first chair.[6] South Dakota legal services lawyer and prosecutor Bill Janklow was another member of the initial board.[10] Thomas Ehrlich, at the time the dean of Stanford Law School, became the LSC's first president.

Debate existed from the start among the board members as to whether LSC's role should be the same as the OEO's of using lawsuits and other means to attack broad underlying difficulties of the poor or whether the focus should be more narrowly defined to addressing small, specific situations.[5][6] The LSC Act said that the organization was to pursue ""equal access to justice,"" but Cramton wrote that while the law was intended to proscribe the blatantly-political objects of the 1960s OEO's work, it was worded ambiguously.[5]

In December 1977, President Jimmy Carter nominated Hillary Rodham to the board of directors of the LSC,[11] for a term to expire in July 1980.[11] Rodham, an attorney with Rose Law Firm in Little Rock, Arkansas and the wife of Arkansas Attorney General Bill Clinton, had a background in children's law and policy and had worked in providing legal services for the poor while at Yale Law School. She had also done 1976 campaign coordination work for Carter in Indiana.[12][13] This was a recess appointment, so Rodham took her place on the board without immediate Senate confirmation. Rodham was nominated again in January 1978 as a regular appointment.[14] In mid-1978, the Carter administration chose the thirty-year-old Rodham to become chair of the board, the first woman to become so.[5]  The position entailed her traveling monthly from Arkansas to Washington, D.C., for two-day meetings.[5]

During Rodham's Senate confirmation hearings, she subscribed to the philosophy that LSC should seek to reform laws and regulations that it viewed as ""unresponsive to the needs of the poor.""[15]  Rodham was successful in getting increases in Congressional funding for LSC, stressing its usual role in providing low-income people with attorneys to assist them in commonplace legal issues and framed its funding as being neither a liberal nor a conservative cause.[16] By her third year on the LSC board, Rodham had gotten the LSC budget tripled.[17] Opposition to LSC during this time came from both Republican Congressman James Sensenbrenner, who favored a ""judicare"" approach of compensating private lawyers for work done for the poor,[17] and Conservative Caucus head Howard Phillips, who objected to LSC representing gays.[17]

LSC funding was at its highest-ever mark, in inflation adjusted dollars, in fiscal 1980,[13][18] with a budget of $303 million.[19]  Some 6,200 poverty lawyers filed suits using its funds on behalf of 1.5 million eligible poor clients;[20] the lawyers won almost 80 percent of their cases, which mostly involved divorces, evictions, repossessions, and interrupted payments from federal agencies.[20] For fiscal 1981 it was budgeted at $321 million.[21]

In June 1980, Carter renominated Rodham for another term on the board, to expire in July 1983.[22]  Sometime between about April 1980[23] and September 1980,[24] F. William McCalpin replaced her as chair of the board.  He would remain chair through late 1981.[25][26][27]

LSC was strongly opposed by some political groups. As Governor of California in the 1960s, Ronald Reagan had advocated elimination of all federal subsidies for free legal services to the poor in civil cases,[20] and had tried to block a grant to California Rural Legal Assistance in 1970.[20] Indeed, Time magazine would state, ""Of all the social programs growing out of the Great Society, there is none that Ronald Reagan dislikes more than the Legal Services Corporation.""[21] The CRLA's executive director would characterize Reagan's attitude towards the organization as akin to that of Darth Vader.[21]

When President Reagan took office in January 1981, he attempted to eliminate the LSC by zero funding it.[20] Supporters of LSC rallied to defend it; American Bar Association president W. Reece Smith, Jr. led 200 lawyers to Washington to press its case.[18]  In response to Reagan's clear intentions against the LSC, the Coalition for Legal Services was formed to lobby outside, but on behalf of the LSC, which showed support via grant recipients.[28]

The U.S. House Judiciary Committee blocked Reagan's zero-funding action in May 1981,[29] but did cut financing to $260 million for both of the next two years as well as place additional restrictions on LSC lawyers.[29] By the following month, the now Republican-controlled U.S. Senate Labor and Human Resources Committee had cut proposed financing to $100 million,[30] as part of what The New York Times deemed an ""increasingly bitter ideological struggle"".[30] Moreover, Reagan administration officials accused LSC of having ""concealed and understated"" its lobbying activity and support for politically motivated legislation.[30]

In November 1981, the Reagan administration, although still hoping to eliminate LSC, decided to replace all eleven LSC board members with nominations of their own.[31] In return the LSC began to set up ""mirror corporations"" to circumvent congressional restrictions and reuse funds for political advocacy.[28] The proposed new chairman was Ronald Zumbrun, president of the ideologically opposite Pacific Legal Foundation,[31] which had previously defended the state of California against several legal aid lawsuits.[31] For fiscal 1982, LSC's budget was reduced by 25 percent to $241 million,[20] with new rules prohibiting most class action suits and lobbying.[20]  Zumbrun's nomination was sufficiently controversial that in January 1982, the Reagan administration dropped it, and instead made a recess appointment of William J. Olson to be chair.[32] Olson had headed the Reagan transition team dealing with LSC and had personally recommended its abolition, so LSC advocates were not mollified.[32]

At the same time, the Reagan administration had named six other board members as recess appointments.[32] In February 1982, the Carter-appointed members of the previously existing board filed suit to against the recess appointments, claiming they were unlawful and that they should be enjoined from holding meetings.[33]  Rodham hired fellow Rose Law Firm associate Vince Foster to represent her in the case[33] and to seek a restraining order against Reagan.[13] The Reagan nominees may  have been prohibited from meeting with the Legal Service Corporation before confirmation.[13]

Rodham also prodded Senate Democrats to vote against Reagan's nominees.[13]  The nominees did undergo heavy criticism in Congress, with one labeled a bigot and Olson lambasted for his transition position.[33]  In March 1982, yet another new chair was named, Indiana University law professor William F. Harvey,[34] although Olson would remain on the board.[35]  Harvey and Rodham had a conference call in which Rodham reiterated her desire for the lawsuit.[33]  That action, McCalpin v. Dana,[36] was decided in favor of the defendants by summary judgment in October 1982.[36]

By December 1982, the Senate was willing to confirm six of Reagan's more moderate nominees, but not Harvey, Olson, and another;[37] the Reagan administration instead pulled the names of all of them.[35]  This board then closed its last meeting in a public debacle,[37] with Olson lambasting LSC as full of ""abuses and rampant illegality"" and a ""waste of the taxpayers' money through the funding of the left,""[37] while being harangued by a hostile audience.[37]  And too, the Reagan appointees to the board were being criticized for collecting substantially higher fees than previous board members.[27][37]

In September 1983 the General Accounting Office found that in early 1981, LSC officials and its local affiliates had used federal funds in assembling opposition to Reagan's efforts to eliminate LSC, and that this use had been in violation of the LSC Act's restrictions against such political activity.[38]  Such actions against the LSC Act were not crimes, and the GAO report did not claim any crimes had taken place.[38] The investigation had been initiated by the LSC in 1983 ordering a series of ""raids"" on their own offices to attempt to discover evidence of questionable actions taken by the LSC in 1981,[21] prompting Time magazine to declare LSC ""an organization at war with itself.""[21]

More recess appointments were made by Reagan in late 1983, in 1984, and in early 1985, with again none of them being confirmed by the Senate.[36] Indeed, LSC's board would go a total of three and a half years populated by recess appointments.[36]  Finally in June 1985 the Senate confirmed the latest batch of Reagan nominations.[36] The Carter board lawsuit, since renamed and appealed as McCalpin v. Durant to the United States Court of Appeals, District of Columbia Circuit, was then decided later in June 1985 as moot.[36]

Overt White House hostility towards LSC ended with the George H. W. Bush administration, with calls for level funding rather than decreases.[18] Under board chair George Wittgraff, LSC began to ease relations with private lawyers and with state grantees.[18]  In fiscal 1992, LSC saw a funding increase back to $350 million.[18]

Hillary Rodham's husband, the aforementioned Bill Clinton, took office as U.S. president in January 1993. The first two years of the Clinton administration saw more growth for LSC, as former chair McCalpin returned to the board and the former chair Hillary was now First Lady of the United States.[18]  Funding rose to a high mark in absolute terms of $400 million for fiscal years 1994 and 1995.[18]

Things turned upon the advent of the Republican Revolution.[18] In fiscal 1996, once the Republican party had taken over Congress the year prior, LSC had its funding cut again, from $400 million to $278 million.[39]  A new set of much more extensive restrictions were added to LSC grantees. The organization's supporters expressed disappointment that the Clinton administration did not make LSC a critical priority in its budget battles with the Republican Congress, especially given Hillary Clinton's former role in it.[39]

As part of a comprehensive ""welfare reform"" of federal welfare laws beginning in 1996, most significantly the Personal Responsibility and Work Opportunity Act, Congress imposed restrictions on the types of work that LSC grantee legal services organizations could engage in. For example, LSC-funded organizations could no longer serve as counsel in class action lawsuits[39] challenging the way public benefits are administered. Additionally, LSC grantees faced tightened restrictions on representing immigrants, specifically those illegally in the country.[39] However, in 2001, the restriction on welfare advocacy was ruled unconstitutional in Legal Services Corp. v. Velazquez.

However, non-LSC funded organizations are not subject to these restrictions leading the legal services community to adopt a two-track approach: LSC restricted counsel taking on individual clients but not engaging in class actions, and non-restricted counsel (using private donor funding) both taking on individuals as well as engaging in otherwise restricted litigation. Poverty lawyers in both tracks still work together where they can, being careful not to run afoul of LSC restrictions.

In 2004, veteran Legal Aid Society attorney Helaine M. Barnett was named President of the LSC.[40]

According to LSC's 2009 report ""Documenting the Justice Gap in America: The Current Unmet Civil Legal Needs of Low-Income Americans,"" all legal aid offices nationwide, LSC-funded or not, were together able to meet only about 20 percent of the estimated legal needs of low-income people in the United States.[41]

For 2007, LSC had a budget of some $350 million.[42]

In 2009 during the Obama administration, the LSC was on the path to getting a $50 million increase in its $390 million budget.[43]

However, the LSC came under criticism from Senator Charles Grassley, who said, ""There's just a lot of money being wasted,"" citing several General Accounting Office and Inspector General reports.[43] At the same time, Congress rolled back the restriction that LSC-funded attorneys could not take attorneys' fees-generating cases; LSC finalized the regulation in 2010 after President Obama signed an appropriations bill into law.[44]

By fiscal 2011, the annual budget amount for the LSC was $420 million.[45]  In early 2011, House now-majority Republican proposed a $75 million reduction in that current-year amount, while Obama's suggestion budget proposed a $30 million increase for the subsequent year.[45]

On December 16, 2014, President Obama signed into law the Consolidated and Further Continuing Appropriations Act for FY 2015 that includes $375 million for LSC.[3]

From its inception in 2017, President Trump's administration repeatedly called for the elimination of funding for LSC.[46] LSC has strong bipartisan support on behalf of robust funding for LSC. External stakeholders, including members of the legal and business communities, state attorneys general, and law school deans across the country sent letters to the House and Senate appropriations committees advocating for robust funding for LSC. They included:

In addition, 209 members of the House of Representatives signed a bipartisan letter in support of funding for LSC, the largest number in history, and 46 bipartisan Senators signed a similar letter in support of funding for LSC. Ultimately, Trump signed into law increases in LSC funding during his tenure. Throughout Trump's presidency, Congress increased LSC's funding such that it eventually reached $490 million in Fiscal Year 2020, aided partly by a $50 million emergency supplement from the CARES Act.[47] In the Consolidated Appropriations Act, 2021, Congress appropriated $465 million in Fiscal Year 2021 for LSC; this amount was an increase of $25 million over LSC's baseline appropriation (without the CARES Act supplement) of $440 million in the prior fiscal year.[48]

In 2021, President Joe Biden proposed a $600 million budget appropriation for LSC.[49]

In March 2022, Congress passed an FY2022 appropriations bill in which they allocated $489 million to LSC.[50]

Later in 2022, LSC asked for $1.26 billion for Fiscal Year 2023 and Biden proposed $700 million.[51] Ultimately, for FY2023, Congress passed an FY2023 approprations bill in which they allocated $560 million to LSC, the highest ever real dollar amount appropriated to LSC.[52]

Due to the up-and-down nature of LSC's political history, there are many restrictions on lobbying, advocacy, and general impact work which apply to LSC-funded organizations.  Here they are broken into categories of expressly forbidden, forbidden with LSC funds, and expressly permitted.

Recipients may never:

Recipients may, with non-LSC funding:

In many of their regulations, LSC only states activities that their funding cannot be used to support. In 45 CFR 1610.2(c)–(h), however, several different types of non-LSC funding are defined:

With these definitions in mind, 45 CFR 1610.4 goes on to specify what each type of funding can be used for:

In addition, the category of general non-LSC funds may be used to:

Recipients may, with any funding:

The board of directors is composed of 11 members, who are appointed by the president of the United States with the consent of the United States Senate. Of these, majority must be members of the bar of the highest court of any state, and none can be a full-time employee of the federal government. No more than six of the members may be affiliated with the same political party. They are appointed to terms of three years, but they may continue to serve on the board until a successor is confirmed. A member may not serve more than three terms continuously.[56]

The board elects a chairman from among its members annually.[56]

The board appoints the president of the LSC, who must be a member of the bar of the highest court of a state. They serve as a non-voting ex officio member of the board.[57] The current president of the LSC is Ronald S. Flagg, who was appointed effective February 20, 2020.[58]

The current board members as of December 22, 2024[update]:[59]

The chairs of the LSC board throughout its history have included:

By law, LSC's headquarters is located in Washington, D.C.  In the 1970s and 1980s, LSC also had regional offices. LSC currently has one office in Washington, D.C., that administers all of LSC's work. LSC itself does not provide legal representation to the poor.

Alabama

Alaska

American Samoa

Arizona

Arkansas

California

Colorado

Connecticut

Delaware

District of Columbia

Florida

Georgia

Guam

Hawaii

Idaho

Illinois

Indiana

Iowa

Kansas

Kentucky

Louisiana

Maine

Maryland

Massachusetts

Michigan

Micronesia

Minnesota

Mississippi



Missouri

Montana

Nebraska

Nevada

New Hampshire

New Jersey

New Mexico

New York

North Carolina

North Dakota

Ohio

Oklahoma

Oregon

Pennsylvania

Puerto Rico

Rhode Island

South Carolina

South Dakota

Tennessee

Texas

Utah

Vermont

Virgin Islands

Virginia

Washington

West Virginia

Wisconsin

Wyoming


"
Consulting Services,"A consulting firm or simply consultancy is a professional service firm that provides expertise and specialised labour for a fee, through the use of consultants. Consulting firms may have one employee or thousands; they may consult in a broad range of domains, for example, management, engineering, and so on.

Management consultants, in particular, typically work with company executives and provide them with generalists and industry-specific specialists, known as subject-matter experts, usually trained in management or in business schools. The deliverable of a management consultant is usually recommendations for achieving a company objective, leading to a company project.

Many consulting firms complement the recommendations with implementation support, either by the consultants or by technicians and other experts.

Consulting services are part of the professional services and account for several hundred billion dollars in annual revenues. Between 2010 and 2015, the 10 largest consulting firms alone made 170 billion dollars growth revenue and the average annual growth rate is around 4%.

According to The Economist, the industry’s most important firms are the ""Great eight"" consulting firms which consist of Bain, BCG, McKinsey, Deloitte, EY, KPMG, PwC and Accenture.[1]

The segmentation of advisory services varies widely across organizations and countries. Categorization is unclear, in part because of the upheavals that have occurred in this industry in recent years.[2]

One approach is to separate services into five broad service delivery families, considering the managers they are targeting:

A consulting firm's model of business may be compared to staffing, wherein the objective is to lower labour costs for clients for an intended result, or relative to an intended result or output, in order to charge for a profit margin for the consulting firm. Clients are looking to procure or purchase external help and assistance. Consulting firms sustain their revenues from a labour economic point of view as a method for distributing labour, where certain positions, roles or fields of expertise within the labour market find it more suitable for contract work, as contrasted to in-house employment, for a few conceivable reasons:

Aside from the economic arguments stated above, consulting also acts as a corporate services model:

consequently acting as a source of profit for clients, consulting firms and society as proffered.[11][12][13] The consulting business model can be seen as a result of the knowledge economy, and as a subset of the knowledge industry.[14][15] Today it is not rare for consulting firms to offer what may be considered turnkey solutions to clients.[16][17] Knowledge transfer is also a prevalent sales argument for consulting services.[18]

It is common practice for consulting firms to be involved in the sale of outsourcing services as well. Similarly, outsourcing firms may offer consulting services as a way to help integrate their services with the client. Many consulting firms offer several service packages as part of their business portfolio. While consulting services and outsourcing services are compatible, issues arise if the clients are not aware of the differences between the two. From an ethical standpoint, it is important that clients are aware of what type or types of services they are procuring, as consulting services are meant to be a complementary service to the client firm, whereas outsourcing effectively aims to replace parts of the client firm that are imperative to their operational ability.[19][20]

There are different types of consulting firms serving different sectors. They mainly fall under the following fields:

Some consulting firms also serve niche sectors, such as:

In 2013, there was a randomized trial in Mexico where 432 small and medium enterprises were allowed access to management consulting services for one year.[22] As a result of this trial, there were many positive impacts. Such positive impacts include: increase in entrepreneurial spirit, increases in employment and higher wages for employees. Even after 5 years after the trial, positive impacts are still active.[23] These results were achieved by advertising a consulting program to 432 enterprises and recorded data on the positive effects.

The impact of consulting firms on local businesses in emerging economies do not always have positive effects.[24] One reason for this is that firms in emerging economies suffer from the inferiority of their technologies and innovation capabilities, thus, although they have access to consulting firms, they cannot make the most of the advice given. Advice given by consulting firms to clients may not be used efficiently as clients firms in emerging markets tend to suffer due to a lack of infrastructure, organisation, and education. Another reason firms in emerging economies struggle to effectively use consulting services is that innovation is very costly and risky.

As noted above, consulting firms in emerging economies do also have positive impacts. Positive impacts include: increases in employment, increase in entrepreneurial spirit and higher wages for employees.

One study shows that there is a significant difference between efficiency between consulting firms in America (developed economy) and consulting firms in Asia Pacific regions (emerging economy).[25] Efficiency scores of consulting firms in America were significantly higher than consulting firms in Asia Pacific regions. This is because firms in developed economies have better infrastructure, organisation and education, thus advice given by consulting firms is used efficiently.

Many consulting firms provide services across a range of industries. Notable firms include KPMG, Boston Consulting Group, Deloitte, PwC, and Ernst & Young.
"
Marketing Services,"Services marketing is a specialized branch of marketing which emerged as a separate field of study in the early 1980s, following the recognition that the unique characteristics of services required different strategies compared with the marketing of physical goods.

Services marketing typically refers to both business to consumer (B2C) and business-to-business (B2B) services, and includes the marketing of services such as telecommunications services, financial services, all types of hospitality, tourism leisure and entertainment services, car rental services, health care services, professional services and trade services. Service marketers often use an expanded marketing mix which consists of the seven Ps: product, price, place, promotion, people, physical evidence and process. A contemporary approach, known as service-dominant logic, argues that the demarcation between products and services that persisted throughout the 20th century was artificial and has obscured the fact that everyone sells service. The S-D logic approach is changing the way that marketers understand value-creation and is changing concepts of the consumer's role in service delivery processes.

The American Marketing Association defines service marketing as an organizational function and a set of processes for identifying or creating, communicating, and delivering value to customers and for managing customer relationship in a way that benefit the organization and stake-holders. Services are (usually) intangible economic activities offered by one party to another. Often time-based, services performed bring about desired results to recipients, objects, or other assets for which purchasers have responsibility. In exchange for money, time, and effort, service customers expect value from access to goods, labor, professional skills, facilities , networks, and systems; but they do not normally take ownership of any of the physical elements involved.[1]

Scholars have long debated the nature of services. Some of the earliest attempts to define services focused on what makes them different from goods. Late-eighteenth and early-nineteenth century definitions highlighted the nature of ownership and wealth creation. Classical economists contended that goods were objects of value over which ownership rights could be established and exchanged. Ownership implied possession of a tangible object that had been acquired through purchase, barter or gift from the producer or previous owner and was legally identifiable as the property of the current owner. In contrast, when services were purchased, no title to goods changed hands.

Adam Smith's seminal work, The Wealth of Nations (1776), distinguished between the outputs of what he termed ""productive"" and ""unproductive"" labor. The former, he stated, produced goods that could be stored after production and subsequently exchanged for money or other items of value.[citation needed] But unproductive labor, however ""honourable, ...useful, or... necessary"" created services that perished at the time of production and therefore didn't contribute to wealth.

French economist Jean-Baptiste Say argued that production and consumption were inseparable in services, coining the term ""immaterial products"" to describe them.[3] In the 1920s, Alfred Marshall was still using the idea that services ""are immaterial products.""[4]

In the mid nineteenth century John Stuart Mill wrote that services are ""utilities not fixed or embodied in any object, but consisting of a mere service rendered ...without leaving a permanent acquisition.""[5]

When services marketing emerged as a separate sub-branch within the marketing discipline in the early 1980s, it was largely a protest against the dominance of prevailing product-centric view.[6][7] In 1960, the US economy changed forever. In that year, for the first time in a major trading nation, more people were employed in the service sector than in manufacturing industries.[8] Other developed nations soon followed by shifting to a service based economy.[9] Scholars soon began to recognise that services were important in their own right, rather than as some residual category left over after goods were taken into account. This recognition triggered a change in the way services were defined. By the mid twentieth century, scholars began defining services in terms of their own unique characteristics, rather than by comparison with products.[10] The following set of definitions shows how scholars were grappling with the distinctive aspects of service products and developing new definitions of service.[11][12]

A recently proposed alternative view is that services involve a form of rental through which customers can obtain benefits.[19] Customers are willing to pay for aspirational experiences and solutions that add value to their lifestyle. The term, rent, can be used as a general term to describe payment made for use of something or access to skills and expertise, facilities or networks (usually for a defined period of time), instead of buying it outright (which is not even possible in many instances).[19][20]

There are five broad categories within the non-ownership framework

Throughout the 1980s and 1990s, the so-called unique characteristics of services dominated much of the literature.

The four most commonly cited characteristics of services are:[21]

The unique characteristics of services give rise to problems and challenges that are rarely paralleled in product marketing. Services are complex, multi-dimensional and multi-layered. Not only are there multiple benefits, but there are also a multiplicity of interactions between customers and organizations as well as between customers and other customers.

There are many ways to classify services. One classification considers who or what is being processed and identifies four classes of services: people processing (e.g. beauty services, child care, medical services); mental stimulus processing (e.g. education services, counselling services, life-coaching), possession processing (e.g. pet care, appliance repair, piano tuning) and information processing (e.g. financial services, data warehousing services).[22][23] Another method used to classify services uses the degree of customer interaction in the service process and classifies services as high contact (e.g. hospitality, dental care, hairdressing) or low contact  (e.g. telecommunications, utility services).[24]

Both economists and marketers make extensive use of the Search → Experience → Credence (SEC) classification of goods and services. The classification scheme is based on the ease or difficulty of consumer evaluation activities and identifies three broad classes of goods.[25][26]

While some services may possess a number of search attributes (tangible dimensions), most services are high in experience or credence properties.  Empirical studies have shown that consumers' perceived risk increases along the search-experience-credence continuum.[28] The implication is that services tend to be high involvement decisions – where the consumer invests more heavily in information search activities during the purchase decision.

Perceived risk is associated with all purchasing decisions, both products and services alike. In terms of risk perception, marketers and economists argue that perceived purchase risk is higher for experience goods and credence goods with implications for consumer evaluation processes.[29] Given that perceived risk drives the search for information in the pre-purchase stages of the consumer's decision process, consumers of services are more likely to engage in information acquisition activities as a means of ameliorating that risk. Any activity that a consumer undertakes in an effort to reduce perceived risk is known as a risk reduction activity.

Risk perception has been defined as ""a perception or feeling ""based on consumer's judgments of the likelihood of negative outcomes (uncertainty) and the degree of importance of these outcomes to the individual [consequences]"".[30] Thus, pre-purchase risk is a function of two dimensions, namely:

For example, consider the case of a prospective air traveler. Most of us know that the probability of being involved in an airline disaster is low (low uncertainty).[31] It is conventional wisdom that travelers are safer in the air than on the roads. Statistically, you are much more likely to be involved in a vehicular accident than an aircraft disaster. While the likelihood of personal harm arising from air travel is indeed very low, the consequences of an airline disaster however are very serious indeed (high consequence).  Whereas, car travelers who have been involved in a traffic accident often walk away with minor injuries, the same cannot be said for airline travelers.  It is the severity of the consequence rather than the uncertainty that plays into airline passengers' fears. Consumers are constantly weighing up uncertainty and consequences to reach subjective evaluations of the overall risk attached to various purchase decisions.

Risk perception drives the information search process. Heightened risk perception may become a barrier to the natural progression of the purchase decision process and prevent customers from making a final brand choice. Consumers who are risk-averse tend to spend more time and effort engaged in information acquisition in the pre-purchase stage and look for specific types of information that will alleviate their perceptions of risk. Typical risk relievers might include such things as a reliance on personal sources of recommendation including word-of-mouth referrals; reliance on known and trusted brands, reading manufacturers' specifications, limited scale trial, reliance on warranties or guarantees etc.[32]

Risk relievers that are especially relevant in service settings include:[32][33]

Service operations are often characterized by far more uncertainty with respect to the flow of demand. Service firms are often said to be capacity constrained.[34]  This refers to the finite carrying capacity for most service operators and the lack of inventory which serves as a buffer against unexpected or peak demand.

There are two components to capacity (i.e., supply) in service operations:

The factors contributing to uneven demand are more complex and difficult to predict. The components of demand may be seen as comprising long term demand patterns (trends), short term seasonal fluctuations and irregular effects.[35]

When demand is low, capacity may be under-utilized while excess demand places enormous pressures on the service system. Service managers need to develop strategies for managing demand and supply under different conditions. Strategies for managing capacity involve careful consideration of both demand-side considerations and supply-side considerations.[36]

On the capacity side:[34]

On the demand side:[37]

When demand exceeds capacity, then reductions in service quality are a probable outcome. Over-crowding and lengthy waiting lines potentially erode the customer experience and place stresses on employees and the service system. Employees may compensate by minimizing the time spent with each customer in an effort to serve more people, but such responses have the potential to introduce human error into service delivery. When capacity far exceeds supply, then capacity becomes idle. Spare capacity is unproductive and an inefficient use of resources. A short-term solution to spare capacity is to seek reductions in operating costs. For instance, management might ask staff to take leave, reduce number of check in counters open, limit number lifts operating and close off entire floors of a building to reduce operating costs during off peak periods as a means of achieving cost savings. In addition, routine maintenance tasks or planned refurbishment activities, which involve downtime, should be carried out during off peak periods to minimize disruption to patrons.

When demand exceeds capacity, customers may have to wait for services. Lovelock identifies a range of different types of waiting lines or queuing systems:[38]

The argument that services require different marketing strategies is based on the insight that services are fundamentally different to goods and that services marketing requires different models to understand the marketing of services to customers.[39]  The ""marketing mix"" (also known as the four Ps) is a foundation concept in marketing and has defined the so-called managerial approach since the 1960s.  The marketing mix or marketing program is understood to refer to the ""set of marketing tools that the firm uses to pursue its marketing objectives in the target market"".[40] The traditional marketing mix refers to four broad levels of marketing decision, namely: product, price, promotion, and place.[41][42]

The prospect of expanding and modifying the marketing mix for services first took hold at the inaugural AMA Conference dedicated to Services Marketing in 1981, and built on earlier theoretical works pointing to many important limitations of the 4 Ps concept.[43] Taken collectively, the papers presented at that conference indicate that service marketers were thinking about a revision to the general marketing mix based on an understanding that services were fundamentally different to products, and therefore required different tools and strategies. At the Services Marketing Conference in 1981, Booms and Bitner proposed a model of seven Ps, comprising the original four Ps plus process, people and physical evidence, as being more applicable for services marketing.[44] Since then there have been a number of different proposals for a service marketing mix (with various numbers of Ps – 6 Ps, 7 Ps, 8 Ps, 9 Ps and occasionally more). The model of 7 Ps has gained widespread acceptance, to the extent that some theorists have argued for the 7 Ps framework proposed by Booms and Bitner to be applied to products as a replacement for the four Ps.[45]

The extended marketing mix for services is more than the simple addition of three extra Ps. Rather it also modifies the traditional mix of product, price, place and promotion for superior application to services.

Service products are conceptualized as consisting of a bundle of tangible and intangible elements:[46]

The distinction between supplementary and facilitating services varies, depending on the nature of the service. For instance, the provision of coffee and tea would be considered a supporting service in a bank, but would be a facilitating service in a bed and breakfast facility. Whether an element is classified as facilitating or supporting depends on the context.

Service marketers need to consider a range of other issues in price setting and management of prices:

Many service firms operate in industries where price is restricted by professional codes of conduct or by government influences which may have implications for pricing. It is possible to identify three broad scenarios:[47]

In situations where the service is subject to some type of public regulation, government departments may establish ceiling prices which effectively limit the amount that can be charged.

The concept of a social price may be more important for service marketers. A social price refers to ""non financial aspects of price"". Fine identifies four types of social price: Time, Effort, Lifestyle and Psyche.[48] In effect, this means that consumers may be more acutely aware of the opportunity costs associated with the consumption of a service. In practice, this may mean that consumers of services experience a heightened sense of temporal risk.

The most widely used pricing tactics in services marketing are:[49]

In making place decisions, there are several related questions which must be asked. What is the purpose of the distribution program? Who are the customers? Who should the intermediaries be?

Contemporary service marketing texts tend to be organized around a framework of seven Ps or eight Ps. The 7 Ps comprises the original 4 Ps plus process, people, physical environment.[55] The eight Ps framework; comprises the 7 Ps plus performance which refers to the standards of service performance or service quality.[56]

Given the intangible nature of services, consumers often rely on the physical evidence to evaluate service quality. Therefore, service marketers must manage the physical evidence – which includes any element of the service environment which impacts on one or more of the customers five senses – the sense of smell, taste, hearing, sight and touch.[57] Theorists identify two types of physical evidence, namely;[58]

A number of different theoretical traditions can be used to inform the study of service environments including stimulus-organism-response (SOR) models; environmental psychology; semiotics and Servicescapes.

The SOR model (stimulus→organism→response model) describes the way that organisms, which includes both customers and employees, respond to environmental stimuli. In a service setting the environmental stimuli might include lighting, ambient temperature, background music, layout and interior-design. In essence, the model proposes that people's responses exhibit both emotional and behavioural responses to stimuli in the external environment.

Environmental psychologists investigate the impact of spatial environments on behaviour. Emotional responses to environmental stimuli fall into three dimensions; pleasure, arousal and dominance. The individual's  emotional state is thought to mediate the behavioural response, namely approach or avoidance behaviour towards the environment. Architects and designers can use insights from environmental psychology to design environments that promote desired emotional or behavioural outcomes.[59]

Three emotional responses are suggested in the model. These responses should be understood as a continuum, rather than a discrete emotion, and customers can be visualized as falling anywhere along the continuum.[60]

The individual's emotional response mediate the individual's behavioural response of Approach→ Avoidance. Approach refers to the act of physically moving towards something while avoidance interferes with people's ability to interact. In a service environment, approach behaviours might be characterized by a desire to explore an unfamiliar environment, remain in the service environment, interact with the environment and with other persons in the environment and a willingness to perform tasks within that environment. Avoid behaviours are characterized by a desire to leave the establishment, ignore the service environment, and feelings disappointment with the service experience. Environments in which people feel they lack control are unattractive. Customers often understand the concept of approach intuitively when they comment that a particular place ""looks inviting"". The desired level of emotional arousal depends on the situation. For example, at a gym arousal might be more important than pleasure (No Pain; No gain). In a leisure setting, pleasure might be more important. If the environment pleases, then the customer will be induced to stay longer and explore all that the service has to offer. Too much arousal can be counter-productive. For instance, a romantic couple might feel out of place in a busy, noisy and cluttered restaurant. Obviously, some level of arousal is necessary as a motivation to buy. The longer a customer stays in an environment, the greater the opportunities to cross-sell a range of service offerings.

Mehrabian and Russell identified two types of environment based on the degree of information processing and stimulation:[61]

Activities or tasks that are low load require a more stimulating environment for optimum performance. If the task to be performed in relatively simple, routine or boring then users benefit from a slightly more stimulating environment. On the other hand, tasks that are complex or difficult may benefit from a low load environment. In a service environment, a high load environment encourages patrons to enter and explore the various service offerings and spaces.

The servicescapes model was developed by Mary Jo Bitner and published in 1992. It is an applied model, specifically developed to inform the analysis of service environments, and was influenced by both stimulus-response theory and environmental psychology.

As the diagram of the servicescapes model illustrates, the service environment consists of physical environment dimensions which act as stimuli. Environmental simulis are normally considered as three broad categories including:[62][63]

Each element in the physical environment serves specific roles -and some may perform multiple roles. Signage may provide information, but may also serve to assist customers navigate their way through a complex service environment. For instance, furnishings may serve a functional role in that they provide seating, but the construction materials, such as fabric, tapestry and velvet may serve a symbolic role. Plush fabrics and generous drapery may suggest an elegant, up-market venue, while plastic chairs may signify an inexpensive, family-friendly venue.  When evaluating the servicescape, the combined effect of all the elements must also be taken into consideration.

When consumers enter a servicescape, they scan the ambient conditions, layout, furnishings and artefacts and aggregate them to derive an overall impression of the environment. In other words, the holistic environment represents the cumulative effect of multiple stimuli, most of which are processed within a split second. These types of global judgments represent the summation of processing multiple stimuli to form a single, overall impression in the consumer's mind.[64]

Through careful design of the physical environment and ambient conditions, managers are able to communicate the service firm's values and positioning. Ideally, the physical environment will be designed to achieve desired behavioural outcomes. Clever use of space can be used to encourage patrons to stay longer since longer stays result in more opportunities to sell services. At other times, the ambient conditions can be manipulated to encourage avoidance behaviour. For example, at the end of a busy night of trading, a bar manager might turn the air conditioning up, turn up the lights, turn off the background music and start stacking chairs on top of tables. These actions send a signal to patrons that it is closing time.

Customers and employees represent the two groups that regularly inhabit the servicescape. Their perceptions of the environment are likely to differ, because each comes to the space with different purposes and motivations. For example, a waiter in a restaurant is likely to be pleased to see a crowded dining room because more customers means more tips. Customers, on the other hand, might be less pleased with a crowded space because the noise and queues have the potential to diminish the service experience.

In the servicescape model, a moderator is anything that changes the standard stimulus-response emotional states of pleasure-displeasure, arousal-non-arousal or dominance-submissiveness while the mediator explains the response behaviour, typically in terms of internal responses (cognitive, emotional and physiological responses).[65] The consumer's response to an environment depends, at least in part, on situational factors such as the purpose or reason for being in the environment.[66] For example, a waiter in a restaurant is likely to be pleased to see a crowded dining room because more customers means more tips. Customers, on the other hand, might be less pleased with a crowded space because the noise and queues have the potential to diminish the service experience.

The model shows that there are different types of response – individual response (approach and avoid)  and  interaction responses (e.g. social interactions).

In the context of servicescapes, approach has a special meaning.  It refers to how customers utilize the space, during and after the service encounter. Approach behaviours demonstrated during the encounter include:[67]

Different types of approach behaviours demonstrated at the conclusion of the encounter or after the encounter may include: affiliation – a willingness to become a regular user, form intention to revisit; commitment – the formation of intention to become brand advocate, to provide referrals, write favourable online reviews or give positive word-of-mouth recommendations.[68]

Bitner's pioneering work on servicescapes identified two broad types of service environment:[69]

According to the model's developer, the servicescape acts like a ""product's package"" – by communicating a total image to customers and providing information about how to use the service. It can also serve as a point of difference by signalling which segments of the market are served, positioning the organization and conveying competitive distinctiveness.[70]

When customers enter a service firm they participate in a process. During that process, customers become quasi-employees; that is they are partial producers and they have the opportunity to see the organization from the employee's perspective. To use a manufacturing analogy, customers are able to examine 'unfinished goods' – that is faulty and defective goods, glitches in the production system are in full view, with obvious implications for customer enjoyment and satisfaction. In addition, customers interactions with both employees and other customers becomes part of the total service experience with obvious implications for service quality and productivity.  Both customers and staff must be educated to effectively use the process. Controlling the service delivery process is more than a simple management issue. The customer's presence in the system means that the service process must be treated as a marketing issue.

Blueprinting is a technique designed to document the visible customer experience.[71] In its simplest form, the service blueprint is an applied process chart which shows the service delivery process from the customer's perspective. The original service blueprint is a highly visual, graphical map that delineates the key contact points in the service process and the nature of the contact – whether with physical evidence, personnel or procedures. It can be seen as a two dimensional map in which the horizontal axis represents time and the vertical axis represents the basic steps in the process.  A line of visibility is included to separate actions visible to the customer from actions out of sight.  Employee latitude, which refers to the amount of discretion given to employees to vary the service process, is shown on the map a call-out sign attached to the step a shown in the figure. Process complexity is shown simply by the number of steps in the process.

It was originally intended to be used as a tool to assist with service design and structural positioning.[72]  However, since its inception it has been used extensively as a diagnostic tool, used to detect operational inefficiencies and potential trouble spots including fail points and bottlenecks.[73][74][75] In the event that any deficiencies are identified by the blueprinting process, management can develop operational standards for critical steps in the process.[76][77]

When interpreting service blueprints, there are two basic considerations, complexity and divergence. Complexity refers to the number and intricacy of the steps required to perform the service. Divergence refers to the degree of latitude, freedom, judgment, discretion, variability or situational adaptation permitted within any step of the process. 

Manipulations of the blueprint diagram might include increasing complexity, by adding more steps, or increasing divergence by allowing employees greater latitude in varying each step. In general, service processes that include high levels of employee discretion to vary steps to meet the needs of individual customers are moving towards customization. On the other hand, reducing divergence, by standardizing each step, often adds to complexity, but can result in a production-line approach to service process design. By manipulating complexity and divergence, it is possible to envisage four different positioning strategies:[78]

Subway sandwich bars provide an excellent example of how a business can integrate both process design and the servicescape into the customer's in-store experience. Like many fast food restaurants, Subway utilizes a race to corral customers and move them in a one-way direction. Prominently displayed 'In' and 'Out' signage reinforces the direction of the desired traffic flow. Customers can peruse an overhead backlit menu while they are waiting in line which speeds up the order-taking process and reduces opportunities for bottlenecks. Other signage provides customers with information about the Subway process e.g. select bread, select filling, select sauce, pay and exit. The arrangement of food behind the glass counter not only displays the choice of sandwich fillings, but supports the process since customers must select their preferences in a specific sequence, as they inch their way towards the cash register. In Australia, the distinctive moves used by Subway customers as they shuffle along the race, selecting their sandwich breads and fillings has become affectionately known as the ‘Subway shuffle'.[79][80] Every aspect of Subway's store design and layout reinforces the core objectives of customization, volume-operations (i.e. rapid turnover) and operational efficiency.

The people dimension refers to the human actors who are participants in the service encounter, namely employees and customers.[81] For many service marketers, the human interaction constitutes the heart of the service experience.[82] Service personnel are important because they are the face of the company and represent the company's values to customers. Customers are important because they are the reason for being in business and are the source of revenue. Service firms must manage interactions between customers and interactions between employees and customers.[83] Scholars have developed the concept of service-profit-chain to understand how customers and firms interact with each other in service settings.[84] Strategically, service personnel are a source of differentiation.[85]

Personnel are said to have a boundary-spanning role because they link the organization with its external environment by interacting with customers and feed information back to the organization[86][87] As boundary spanners, front line staff are likely to encounter the various stresses associated with that role.  Studies have shown that emotional labour can lead to undesirable consequences for employees including job-related stress, burnout, job dissatisfaction and withdrawal. If left untreated, these types of stressors can be very damaging to morale.

Managing the behaviour of customers and employees in the service encounter is difficult. Consistent behaviour cannot be prescribed. It can, however, be nurtured in subtle and indirect ways.[88]  Recruitment and training can help to ensure that the service firm employs the right people.

For some marketing theorists, services are analogous to theatre. This analogy is also known as a dramaturgical perspective. In such an analogy, service personnel are the actors, customers are the audience; uniforms are costumes; the work setting is the stage (front-stage for areas where interaction occurs and back-stage for areas off limits to customers); discrete steps in the service process are scenes and finally the words and actions that occur represent the performance.[89][90]

A dramaturgical perspective may be appropriate in specific service contexts:

Managerial insights generated by a dramaturgical perspective include:[93]

When asked to perform emotional labour, employees can adopt one of two approaches:[94]

Some evidence suggests that employees who are able to fully immerse themselves in the role and engage in deep acting are more resilient to role-related stress. In addition, deep acting is often seen as  more authentic, while surface acting can be seen as contrived and inauthentic.[95] Service work, by its very nature, is stressful for employees. Managers need to develop techniques to assist employees manage role related stress.

There is widespread consensus amongst researchers and practitioners that service quality is an elusive and abstract concept that is difficult to define and measure.[96] It is believed to be a multi-dimensional construct, but there is little consensus as to what constitutes the specific dimensions. Indeed, some researchers argue that the dimensions of service quality may vary from industry to industry and that no universal set of dimensions exists for all contexts.[97]

Within the services marketing literature, there are several different theoretical traditions that inform the understanding of service quality including the Nordic school, the Gaps model (also known as the American model and the performance only approach.

The Nordic school was one of the earliest attempts to define and measure service quality. In this school of thought, service quality is conceptualized as consisting of two broad dimensions, namely:[98]

The technical dimension can usually be measured – but the functional dimension is difficult to measure due to subjective interpretations which vary from customer to customer.[99]

The model of service quality or the gaps model as it is popularly known, was developed by team of researchers, Parasuraman, Zeithaml and Berry, in the mid to late 1980s.[100] and has become the dominant approach for identifying service quality problems and diagnosing their probable causes.[101] This approach conceptualizes service quality  as a gap between consumer's expectations of a forthcoming service encounter and their actual perceptions of that encounter.[102] Accordingly, service quality can be represented by the equation:[103]

SQ = P − E

The model which provides the overall conceptual framework helps analysts to identify the service quality gap (Gap 5 in the model) and to understand the probable causes of service quality related problems (Gaps 1-4 in the model). The diagnostic value of the model accounts at least, in part, for the instrument's continuing currency in service quality research.[104][105][106]

The model's developers also devised a research instrument, called SERVQUAL, to measure the size and direction of service quality problems (i.e. gap 5).[107] The questionnaire is multi-dimensional instrument, designed to  having capture five dimensions of service quality; namely reliability, assurance, tangibles, empathy and responsiveness, which are believed to represent the consumer's understanding of service quality. The questionnaire consists of matched pairs of items; 22 expectation items and 22 perceptions items, organized into the five dimensions which align with the consumer's mental map of service quality dimensions. Both the expectations component and the perceptions component of the questionnaire consist a total of 22 items, comprising 4 items to capture tangibles, 5 items to capture reliability, 4 items for responsiveness, 5 items for assurance and 5 items to capture empathy.[108] The questionnaire, which is designed to be administered in a face-to-face interview and requires a moderate to large size sample for statistical reliability, is lengthy and can take more than one hour to administer to reach respondent. In practice, researchers customarily add extra items to the 44 SERVQUAL items to capture information about the respondent's demographic profile, prior experience with the brand or category and behavioural intentions (intention to revisit/ repurchase, loyalty intentions and propensity to give word-of-mouth referrals). Thus, the final questionnaire may have up to 60 items, which contributes to substantial time and cost in terms of administration, coding and data analysis.

Expectations Item: When excellent telephone companies promise to do something by a certain time, they do so.

Perceptions Item: XYZ company provides it services at the promised time. 

Expectations Item: The behaviour of employees in excellent banks will instill confidence in customers.

Perceptions Item:  The behaviour of employees in the XYZ bank instils confidence in you.

Expectations Item: The physical facilities at excellent telephone will be visually pleasing.

Perceptions Item:  XYZ's physical facilities are visually pleasing.

Expectations Item: Employees in excellent banks will understand the specific needs of their customers.

Perceptions Item:  XYZ employees understand my needs.

Expectations Item: Employees in excellent banks will tell customers exactly when services will be performed.

Perceptions Item:  Employees in the XYZ bank always tell me when they plan to deliver a service.

Cronin and Taylor developed a scale based on perceived performance only (i.e. excluded expectations) as a simpler alternative to SERVQUAL.[110] The scale is known as SERVPERF and is considerably shorter than SERVQUAL, and therefore easier and cheaper to administer. Results from the use of SERVPERF correlate well with SERVQUAL.[111] This approach utilises a different conceptualisation of service quality, which can be represented by the equation:

SQ = P

Although SERVPERF has a number of advantages in terms of administration, it has attracted criticism. The performance only instrument lacks the diagnostic value of the SERVQUAL since it includes only one variable (P) compared to SERVQUAL's richer data with two variables (P and E).[112][113] To illustrate, consider one source of quality related problems which occurs when customers have unrealistically high expectations. SERVQUAL has no problem detecting such problems, however, SERVPERF can never detect this problem because it does not capture expectations. When choosing an appropriate instrument for investigations into service quality, service marketers must weigh up the expediency of SERVPERF against the diagnostic power of SERVQUAL.

Service-dominant logic (SDL) is a new way of thinking about marketing, especially the goods versus services division and especially a fresh way of thinking about customer value and the value-creation process. Vargo and Lusch did not intend for service-dominant logic to be published as a workable theory that offers solutions to everyday marketing problems and issues. Instead, it offers a framework for thinking about goods and services. Their work did not put forward hypotheses that could be tested empirically, Instead they offer ""foundational propositions"".  The original article offered eight such propositions[114] and subsequently added two more propositions to arrive at a total of ten:[115]

Some of the implications that have been identified in the literature include:

SDL offers the promise of a unified marketing theory: To date, marketing research and practice have failed to integrate the traditional goods/services dichotomy. Some efforts have been made to get product accepted as a joint term for goods and services and to use offering, package or solution as all inclusive, concepts for what consumers the buys, but this has not been successful.[116] Service-dominant logic, however, promises a truly unified framework. For many academics, this is the most exciting implication. It is highly likely that the 4 Ps, as the central marketing framework, is about to come to a close.

Compete Through Innovative Co-production and Co-creation: Some theorists point out that, thanks largely to the Internet, consumers have been actively engaging themselves in explicit dialogue with manufacturers and service providers.[117] The challenge is for service firms to find innovative ways to achieve co-production and co-creation. Customer co-creation has become the foundation concept for social sharing web sites such as YouTube, Myspace and Twitter.  Many companies have moved from testing products in the contrived and artificial conditions of a laboratory to product testing in customer environments. At Microsoft, for example, consumers acted as product researchers by testing Windows 2000 in their native environments. A different approach is to use embedded intelligence to provide enhanced personalized experiences.

Research Priorities: SDL has forced the discipline to review its research priorities.  Researchers and scholars are beginning to identify a range of subjects that require more detailed exploration. Some theorists have argued that marketing practitioners must find new ways of understanding customers' value creation and of developing marketing strategies with an aim to engage suppliers with their customers' consumption processes in order to enhance customer satisfaction.[118] Other research priorities include: the personalized customer experience,[119] resource integration,[120] improved use of IT to map processes and activities in order to increase productivity and standardize service.
"
Human Resources Services,"

Human resources (HR) is the set of people who make up the workforce of an organization, business sector, industry, or economy.[1][2] A narrower concept is human capital, the knowledge and skills which the individuals command.[3] Similar terms include manpower, labor, labor-power, or personnel.

The Human Resources department (HR department, sometimes just called ""Human Resource"")[4] of an organization performs human resource management, overseeing various aspects of employment, such as compliance with labor law and employment standards, interviewing and selection, performance management, administration of employee benefits, organizing of employee files with the required documents for future reference, and some aspects of recruitment (also known as talent acquisition), talent management, staff wellbeing, and employee offboarding.[5] They serve as the link between an organization's management and its employees.

The duties include planning, recruitment and selection process, posting job ads, evaluating the performance of employees, organizing resumes and job applications, scheduling interviews and assisting in the process and ensuring background checks. Another job is payroll and benefits administration which deals with ensuring vacation and sick time are accounted for, reviewing payroll, and participating in benefits tasks, like claim resolutions, reconciling benefits statements, and approving invoices for payment.[6] Human Resources also coordinates employee relations activities and programs including,  but not limited to, employee counseling.[7] The last job is regular maintenance, this job makes sure that the current HR files and databases are up to date, maintaining employee benefits and employment status and performing payroll/benefit-related reconciliations.[6]

A human resources manager can have various functions in a company, including to: [8]

Human resource management used to be referred to as ""personnel administration"".[9][10] In the 1920s, personnel administration focused mostly on the aspects of hiring, evaluating, and compensating employees.[11][12] However, they did not focus on any employment relationships at an organizational performance level or on the systematic relationships in any parties. This led to a lacked unifying paradigm in the field during this period.[13]

According to an HR Magazine article, the first personnel management department started at the National Cash Register Co. in 1900. The owner, John Henry Patterson, organized a personnel department to deal with grievances, discharges and safety, and information for supervisors on new laws and practices after several strikes and employee lockouts. This action was followed by other companies; for example, Ford had high turnover ratios of 380 percent in 1913, but just one year later, the line workers of the company had doubled their daily salaries from $2.50 to $5, even though $2.50 was a fair wage at that time.[14] This example clearly shows the importance of effective management which leads to a greater outcome of employee satisfaction as well as encouraging employees to work together in order to achieve better business objectives.

During the 1970s, American businesses began experiencing challenges due to the substantial increase in competitive pressures. Companies experienced globalization, deregulation, and rapid technological change which caused the major companies to enhance their strategic planning – a process of predicting future changes in a particular environment and focus on ways to promote organizational effectiveness. This resulted in developing more jobs and opportunities for people to show their skills which were directed to effectively applying employees toward the fulfillment of individual, group, and organizational goals. Many years later the major/minor of human resource management was created at universities and colleges also known as business administration. It consists of all the activities that companies used to ensure the more effective use of employees.[15]

Now, human resources focus on the people side of management.[15] There are two real definitions of HRM (Human Resource Management); one is that it is the process of managing people in organizations in a structured and thorough manner.[15] This means that it covers the hiring, firing, pay and perks, and performance management.[15] This first definition is the modern and traditional version more like what a personnel manager would have done back in the 1920s.[15] The second definition is that HRM circles the ideas of management of people in organizations from a macromanagement perspective like customers and competitors in a marketplace.[15] This involves the focus on making the ""employment relationship"" fulfilling for both management and employees.[15]

Some research showed that employees can perform at a much higher rate of productivity when their supervisors and managers paid more attention to them.[14] The Father of Human relations, Elton Mayo, was the first person to reinforce the importance of employee communications, cooperation, and involvement.[14] His studies concluded that sometimes the human factors are more important than physical factors, such as quality of lighting and physical workplace conditions. As a result, individuals often place value more on how they feel.[14] For example, a rewarding system in Human resource management, applied effectively, can further encourage employees to achieve their best performance.

Pioneering economist John R. Commons mentioned ""human resource"" in his 1893 book The Distribution of Wealth but did not elaborate.[16] The expression was used during the 1910s to 1930s to promote the idea that human beings are of worth (as in human dignity); by the early 1950s, it meant people as a means to an end (for employers).[17] Among scholars the first use of the phrase in that sense was in a 1958 report by economist E. Wight Bakke.[18]

In regard to how individuals respond to the changes in a labor market, the following must be understood:

New terminology includes people operations, employee experience, employee success, people@, and partner resources.[20]

One major concern about considering people as assets or resources is that they will be commoditized, objectified, and abused. Critics of the term human resources would argue that human beings are not ""commodities"" or ""resources"", but are creative and social beings in a productive enterprise. The 2000 revision of ISO 9001, in contrast, requires identifying the processes, their sequence, and interaction, and to define and communicate responsibilities and authorities.[citation needed] In general, heavily unionized nations such as France and Germany have adopted and encouraged such approaches. Also, in 2001, the International Labour Organization decided to revisit and revise its 1975 Recommendation 150 on Human Resources Development, resulting in its ""Labour is not a commodity"" principle. One view of these trends is that a strong social consensus on political economy and a good social welfare system facilitate labor mobility and tend to make the entire economy more productive, as labor can develop skills and experience in various ways, and move from one enterprise to another with little controversy or difficulty in adapting.

Another important controversy regards labor mobility and the broader philosophical issue with the usage of the phrase ""human resources"".[21] Governments of developing nations often regard developed nations that encourage immigration or ""guest workers"" as appropriating human capital that is more rightfully part of the developing nation and required to further its economic growth. Over time, the United Nations have come to more generally support[22]  the developing nations' point of view, and have requested significant offsetting ""foreign aid"" contributions so that a developing nation losing human capital does not lose the capacity to continue to train new people in trades, professions, and the arts.[22] Some businesses and companies are choosing to rename this department using other terms, such as ""people operations"" or ""culture department,"" in order to erase this stigma.[23]

Human resource companies play an important part in developing and making a company or organization at the beginning or making a success at the end, due to the labor provided by employees.[24] Human resources are intended to show how to have better employment relations in the workforce.[25] Also, to bring out the best work ethic of the employees and therefore making a move to a better working environment.[26] Moreover, green human resource development is suggested as a paradigm shift from traditional approaches of human resource companies to bring awareness of ways that expertise can be applied to green practices. By integrating the expertise, knowledge, and competencies of human resource development practitioners with industry practitioners, most industries have the potential to be transformed into a sector with ecofriendly and pro-environmental culture.[27]

Human resources also deals with essential motivators in the workplace such as payroll, benefits, team morale and workplace harassment.[5]

Administration and operations used to be the two role areas of HR. The strategic planning component came into play as a result of companies recognizing the need to consider HR needs in goals and strategies. HR directors commonly sit on company executive teams because of the HR planning function. Numbers and types of employees and the evolution of compensation systems are among elements in the planning role.[28] Various factors affecting Human Resource planning include organizational structure, growth, business location, demographic changes, environmental uncertainties, expansion.[29]
"
Management Consulting,"Management consulting is the practice of providing consulting services to organizations to improve their performance or in any way to assist in achieving organizational objectives. Organizations may draw upon the services of management consultants for a number of reasons, including gaining external (and presumably objective) advice and accessing consultants' specialized expertise regarding concerns that call for additional oversight.[1]

As a result of their exposure to and relationships with numerous organizations, consulting firms are typically aware of industry ""best practices"".[2] However, the specific nature of situations under consideration may limit the ability or appropriateness of transferring such practices from one organization to another. Management consulting is an additional service to internal management functions and, for various legal and practical reasons, may not be seen as a replacement for internal management. Unlike interim management, management consultants do not become part of the organization to which they provide services.[3][4][5]

Consultancies provide services such as: organizational change management assistance, development of coaching skills, process analysis, technology implementation, strategy development, or operational improvement services.  Management consultants often bring their own proprietary methodologies or frameworks to guide the identification of problems and to serve as the basis for recommendations with a view to more effective or efficient ways of performing work tasks.[3]

The economic function of management consulting firms is in general to help and facilitate the development, rationalization and optimization of the various markets pertaining to the geographic areas and jurisdictions in which they operate.[6][7] However, the exact nature of the value of such a service model may vary greatly across markets and its description is therefore contingent.[a]

Management consulting grew with the rise of management, as a unique field of study.[1]  One of the first management consulting firms was Arthur D. Little Inc., founded in 1886 as a partnership, and later incorporated in 1909.[9] Although Arthur D. Little later became a general management consultancy, it originally specialized in technical research.[10]

As Arthur D. Little focused on technical research for the first few years, the first management consultancy was that of Frederick Winslow Taylor, who in 1893 opened an independent consulting practice in Philadelphia. His business card read ""Consulting Engineer – Systematizing Shop Management and Manufacturing Costs a Specialty"". By inventing Scientific Management, also known as Taylor's method, Frederick Winslow Taylor invented the first method of organizing work, spawning the careers of many more management consultants. For example, one of Taylor's early collaborators, Morris Llewellyn Cooke, opened his own management consultancy in 1905. Taylor's method was used worldwide until industry switched to a method invented by W. Edwards Deming.[citation needed]

The initial period of growth in the consulting industry was triggered by the Glass–Steagall Banking Act in the 1930s, and was driven by demand for advice on finance, strategy and organization.[11] From the 1950s onwards, consultancies expanded their activities considerably in the United States, and also opened offices in Europe and later in Asia and South America.

The management consulting firms Stern Stewart,[12] Marakon Associates,[13][14] and Alcar pioneered value-based management (VBM), or ""managing for value"", in the 1980s based on the academic work of Joel Stern, Bill Alberts, and Professor Alfred Rappaport.[15] Other consulting firms including McKinsey and BCG developed VBM approaches.[13] Value-based management became prominent during the late 1980s and 1990s.[15]

The industry experienced significant growth in the 1980s and 1990s, gaining considerable importance in relation to national gross domestic product.

A period of significant growth in the early 1980s was driven by demand for strategy and organization consultancies. The wave of growth in the 1990s was driven by both strategy and information technology advice. In the second half of the 1980s, the big accounting firms entered the IT consulting segment. The then Big Eight, now Big Four, accounting firms (PricewaterhouseCoopers, KPMG, Ernst & Young and Deloitte Touche Tohmatsu) had always offered advice in addition to their traditional services, but after the late 1980s these activities became increasingly important in relation to the maturing market of accounting and auditing. By the mid-1990s these firms had outgrown those service providers focusing on corporate strategy and organization. While three of the Big Four legally divided the different service lines after the Enron scandal and the ensuing breakdown of Arthur Andersen, they are now back in the consulting business. In 2000, Andersen Consulting broke off from Arthur Andersen and announced their new name Accenture.[16] The name change was effective starting January 1, 2001, and Accenture is currently the largest consulting firm in the world in employee headcount.[17] They are publicly traded on the NYSE with ticker ACN.[18]

The industry stagnated in 2001 before recovering after 2003 and then enjoying a period of sustained double-digit annual revenue growth until the financial crisis of 2007–2008. As financial services and government were two of the largest spenders on consulting services, the financial crash and the resulting public sector austerity drives hit consulting revenues hard. In some markets such as the UK there was a recession in the consulting industry, something which had never happened before or since. There has been a gradual recovery in the consulting industry's growth rate in the intervening years, with a current trend towards a clearer segmentation of management consulting firms. In recent years, management consulting firms actively recruit top graduates from Ivy League universities, Rhodes Scholars,[19] and students from top MBA programs.[10]

In more recent times, traditional management consulting firms have had to face increasing challenges from disruptive online marketplaces that are aiming to cater to the increasing number of freelance management consulting professionals.[20]

The functions of consulting services are commonly broken down into eight task categories.[21] Consultants can function as bridges for information and knowledge, and external consultants can provide these bridging services more economically than client firms themselves.[22] Consultants can be engaged proactively, without significant external enforcement, and reactively, with external pressure.[23] Proactive consultant engagement is engaged mainly with aim to find hidden weak spots and improve performance, while the reactive consultant engagement is mostly aimed at solving problems identified by external stakeholders.[24][25]

Marvin Bower, McKinsey's long-term director, has mentioned the benefits of a consultant's externality, that they have varied experience outside the client company.[26]

Management consulting could be classified into two categories:[citation needed]

Management consulting often involves a mix of both of these categories. In the modern economic environment, management consulting firms are typically classified under the umbrella term of corporate service providers.[citation needed]

Consultants have specialized skills on tasks that would involve high internal coordination costs for clients, such as organization-wide changes or the implementation of information technology. In addition, because of economies of scale, consultants' focus on and experience in gathering information across markets and industries enables a higher cost-efficiency than if clients were to perform research themselves.[27][28]

Three consulting firms are widely regarded as the Big Three or MBB:[29]

The Big Four audit firms (Deloitte, KPMG, PwC, Ernst & Young) have been working in the strategy consulting market since 2010.[30] In 2013, Deloitte acquired Monitor Group—now Monitor Deloitte—while PwC acquired PRTM in 2011 and Booz & Company in 2013—now Strategy&. From 2010 to 2013, several Big Four firms have tried to acquire Roland Berger.[31] EY followed the trend, with acquisitions of The Parthenon Group in 2014, and both the BeNeLux and French businesses of OC&C in 2016 and 2017, with all now under the EY-Parthenon brand.[32]

In 2013, an article in Harvard Business Review discussed the prevalent trends within the consulting industry to evolve. The authors noted that with knowledge being democratized and information becoming more and more accessible to anyone, the role of management consultants is rapidly changing. Moreover, with more online platforms that connect business executives to relevant consultants, the role of the traditional 'firm' is being questioned.[33]

Large management consulting firms and professional networks have adopted a structure of industry-specific branches, with one branch per industry or market segment served.[citation needed] As such, the firms utilize their ability to serve as knowledge brokers within each market segment and industry addressed.[citation needed]

Some for-profit consulting firms, including McKinsey and BCG, offer consulting services to nonprofits at subsidized rates as a form of corporate social responsibility.[citation needed] Other for-profit firms have spun off nonprofit consulting organizations, e.g. Bain creating Bridgespan.[34]

Many firms outside of the Big Three offer management consulting services to nonprofits, philanthropies, and mission-driven organizations. Some, but not all, are nonprofits themselves.[35][36]

As with all client-contractor work, liability depends heavily on the subject of contract terms. While the management consulting service provider for obvious reasons has a business reputation to protect, legally there is little protection for the client.[citation needed] This is due to the scope of the contract being the only thing subject to potential insurance claims as well as lawsuits.

As with other client-contractor relationships, settling for liabilities that exist outside the scope of the contract deliverables has been proven to be of considerable difficulty,[37] also in management consulting.[38] For this reason, it is important that clients procuring management consulting services think twice about what type of help they need, so that the scope, length and content of contract reflects such need.[39]

Management consultants are sometimes criticized for the overuse of buzzwords, reliance on and propagation of management fads, and a failure to develop plans that are executable by the client.  As stated above, management consulting is an unregulated profession; anyone or any company can style themselves as management consultants. A number of critical books about management consulting argue that the mismatch between management consulting advice and the ability of executives to actually create the change suggested results in substantial damages to existing businesses.[40] In his book, Flawed Advice and the Management Trap, Chris Argyris believes that much of the advice given today has real merit. However, a close examination shows that most advice given today contains gaps and inconsistencies that may prevent positive outcomes in the future.[41]

Ichak Adizes and coauthors also criticize the timing of consultant services. Client organizations, which are usually lacking the knowledge they want to obtain from the consultant, cannot correctly estimate the right timing for an engagement of consultants. Consultants are usually engaged too late when problems become visible to the top of the client's organizational pyramid. A proactive checkup, like a regular medical checkup, is recommended.[42] On the other side, this opens additional danger for abuse from disreputable practitioners.

ISO published the international standard ISO 20700 Guidelines for Management Consultancy Services on June 1, 2017, replacing EN 16114.

This document represents the first international standard for the management consultancy industry.[43]

An international qualification for a management consulting practitioner is Certified Management Consultant (CMC) available in the United States through the Institute of Management Consultants USA. Additional trainings and courses exist, often as part of an MBA training;[44]
see Master of Business Administration § Content.
"
Business Development Services,"Business development entails tasks and processes to develop and implement growth opportunities within and between organizations.[1] It is a subset of the fields of business, commerce and organizational theory. Business development is the creation of long-term value for an organization from customers, markets, and relationships.[2] Business development can be taken to mean any activity by either a small or large organization, non-profit or for-profit enterprise which serves the purpose of 'developing' the business in some way. In addition, business development activities can be done internally or externally by a business development consultant. External business development can be facilitated through planning systems, which are put in place by governments to help small businesses. In addition, reputation building has also proven to help facilitate business development.

In the limited scholarly work available on the subject, business development is conceptualized as or related to discrete projects, specific modes of growth, and organizational units, activities, and practices. Sorensen [3] integrates these different perspectives with insights from chairmen and managing directors, senior business developers, and venture capitalists from successful high-tech firms worldwide, which is adopted in the Palgrave Encyclopedia of Strategic Management:

″Business development is defined as the tasks and processes concerning the analytical preparation of potential growth opportunities, and the support and monitoring of the implementation of growth opportunities, but does not include decisions on strategy and implementation of growth opportunities.″ [4]

In practice, the term business development and the role of the business developer have evolved to encompass a wide range of applications. Today, the responsibilities associated with business development vary across industries and countries, often including tasks performed by IT programmers, specialized engineers, advanced marketers, key account managers, and professionals involved in sales and relationship management with current and prospective customers. As a result, it has become challenging to clearly define the unique characteristics of the business development function and to determine whether these activities directly contribute to profitability.

Recent systematic research on the subject has outlined that the contours of an emerging business development function with a unique role in the innovation management process. The business development function seems to be more utterly matured in high-tech, and especially the pharma and biotech industries.[5][6][7]

The business developer is concerned with the analytical preparation of potential growth opportunities for the senior management or board of directors as well as the subsequent support and monitoring of its implementation. Both in the development phase and the implementation phase, the business developer collaborates and integrates the knowledge and feedback from the organization's specialist functions, for example, research and development, production, marketing, and sales to assure that the organization is capable of implementing the growth opportunity successfully.[3] The business developers' tools to address the business development tasks are the business model answering ""how do we make money"" and its analytical backup and roadmap for implementation, the business plan.

Business development professionals frequently have had earlier experience in sales, financial services, investment banking or management consulting, and delivery; although some find their route to this area by climbing the corporate ladder in functions such as operations management. Skill sets and experience for business-development specialists usually consist of a mixture of the following (depending on the business requirements):

The ""pipeline"" refers to the flow of potential clients that a company has started developing. Business development staff assign to each potential client in the pipeline a percent chance of success, with projected sales volumes attached. Planners can use the weighted average of all the potential clients in the pipeline to project staffing to manage the new activity when finalized. Enterprises usually support pipelines with some kind of customer relationship management tool or database, either web-based solution or an in-house system. Sometimes business development specialists manage and analyze the data to produce sales management information. Such management of information could include:

For larger and well-established companies, especially in technology-related industries, the term ""business development"" often refers to setting up and managing strategic relationships and  alliances with other, third-party companies. In these instances, the companies may leverage each other's expertise, technologies, or other intellectual property to expand their capacities for identifying, researching, analyzing and bringing to market new businesses and new products. Business development focuses on the implementation of the strategic business plan through equity financing, acquisition/divestiture of technologies, products, and companies, plus the establishment of strategic partnerships where appropriate.

Business development is to be thought of as a marketing tactic. The objectives include branding, expansion in markets, new user acquisition, and awareness. However, the main function of business development is to utilize partners in selling to the right customers. Creating opportunities for value to be ongoing in the long term is important. To be successful in business development the partnership must be built on strong relationships.[8]

Business development is affected by external factors. Planning systems are systems set in place in order to regulate businesses. In many cases, ruling agencies deem the necessary for business survival.[9] There is a section of business that is dedicated to facilitating ethical business development in developing countries. In the early 2000s, business ethics was dedicated to helping the businesses in need that are in these countries. However, owing to the strong backlash from critics, they have changed their focus into helping businesses that are going to help the most people develop. These policies have improved the quality of life of the people. However, this facilitation changes the norms and, in turn, harms some groups. In order to enforce the new policies in an ethical manner business ethicists have created a cost-benefit analysis, placing an emphasis on basic necessities. These concerns have become so great that business ethicists have created a new department called development ethics. Now, instead of simply helping developing businesses, international business developers have begun ensuring that the companies keep basic human rights in mind. This especially applies to countries where the laws are not so strict and allow for abuse to take place. These development policies now have to follow the criteria that Penz created, consisting of: security, empowerment, rights, equity, integrity, and cultural freedom.[10] The idea of providing people with human rights in order to facilitate business development can be seen through the rapid development of China in the last few decades. The policies that were implemented in the last couple decades coincide with these developments. In the 1980s, government policies facilitated the rise in literacy rate and education. The following decade, healthcare coverage increased significantly. This development was not originally seen as monetary capital, but instead, it was seen as human capital. With more workers able to bring skill and maximum effort to their workplace, companies were able to develop extremely rapidly.[11]

With companies becoming more and more conscious of ethical practices, closely watching themselves to avoid scrutiny, a company's reputation has become a great concern. Ethical business practices are closely tied with reputation which makes it essential to follow ethical guidelines if a company is looking to build their reputation. In fact, businesses that develop quickly and successfully have tendencies to show honesty, impartiality, and service to all of their stakeholders. In order for a company to be considered ""ethical"", it must cater to the needs of the customer, keeping their best interest in mind. This will influence customers to make repeated purchases and lead to more profit. In order for a company to build a strong reputation with their suppliers, it is crucial for them to focus on impartial business interactions and developing long relationships. These relationships can lead to mutually-beneficial business deals for both the company and its supplier. With the employees, they must take their interests into consideration and facilitate teamwork as opposed to rigorous competition. This ensures that the company will keep their most loyal and dedicated employees for as long as possible. Funding for further development can rise when a company is able to develop strong relationship with each stakeholder individually, and ethically. This is based on the concept of reciprocation, which states how in order for social change to take place between groups of people, trust must be built between them through mutually beneficial actions. This can be supported through the results of a questionnaire study that was conducted on technology industries in GTSM and TSE.[12] In addition, in order for a company to practice business ethics, and ensure strong business development, it is essential to maintain a positive relationship with the environment. With concerns about the recent decline of the environment increasing, stakeholders have become more involved in efforts to preserve resources and a negative impact on the environment brings about risks of damaging stakeholder relationships.[13]
"
Project Management Services,"

Project management is the process of supervising the work of a team to achieve all project goals within the given constraints.[1] This information is usually described in project documentation, created at the beginning of the development process. The primary constraints are scope, time and budget.[2] The secondary challenge is to optimize the allocation of necessary inputs and apply them to meet predefined objectives.

The objective of project management is to produce a complete project which complies with the client's objectives. In many cases, the objective of project management is also to shape or reform the client's brief to feasibly address the client's objectives. Once the client's objectives are established, they should influence all decisions made by other people involved in the project– for example, project managers, designers, contractors and subcontractors. Ill-defined or too tightly prescribed project management objectives are detrimental to the decisionmaking process.

A project is a temporary and unique endeavor designed to produce a product, service or result with a defined beginning and end (usually time-constrained, often constrained by funding or staffing) undertaken to meet unique goals and objectives, typically to bring about beneficial change or added value.[3][4] The temporary nature of projects stands in contrast with business as usual (or operations),[5] which are repetitive, permanent or semi-permanent functional activities to produce products or services. In practice, the management of such distinct production approaches requires the development of distinct technical skills and management strategies.[6]

Until 2001, civil engineering projects were generally managed by creative architects, engineers, and master builders themselves, for example, Vitruvius (first century BC), Christopher Wren (1632–1723), Thomas Telford (1757–1834), and Isambard Kingdom Brunel (1806–1859).[7] In the 1950s, organizations started to apply project-management tools and techniques more systematically to complex engineering projects.[8]

As a discipline, project management developed from several fields of application including civil construction, engineering, and heavy defense activity.[9] Two forefathers of project management are Henry Gantt, called the father of planning and control techniques,[10] who is famous for his use of the Gantt chart as a project management tool (alternatively Harmonogram first proposed by Karol Adamiecki);[11] and Henri Fayol for his creation of the five management functions that form the foundation of the body of knowledge associated with project and program management.[12] Both Gantt and Fayol were students of Frederick Winslow Taylor's theories of scientific management. His work is the forerunner to modern project management tools including work breakdown structure (WBS) and resource allocation.

The 1950s marked the beginning of the modern project management era, where core engineering fields came together to work as one. Project management became recognized as a distinct discipline arising from the management discipline with the engineering model.[13] In the United States, prior to the 1950s, projects were managed on an ad-hoc basis, using mostly Gantt charts and informal techniques and tools. At that time, two mathematical project-scheduling models were developed. The critical path method (CPM) was developed as a joint venture between DuPont Corporation and Remington Rand Corporation for managing plant maintenance projects. The program evaluation and review technique (PERT), was developed by the U.S. Navy Special Projects Office in conjunction with the Lockheed Corporation and Booz Allen Hamilton as part of the Polaris missile submarine program.[14]

PERT and CPM are very similar in their approach but still present some differences. CPM is used for projects that assume deterministic activity times; the times at which each activity will be carried out are known. PERT, on the other hand, allows for stochastic activity times; the times at which each activity will be carried out are uncertain or varied. Because of this core difference, CPM and PERT are used in different contexts. These mathematical techniques quickly spread into many private enterprises.

At the same time, as project-scheduling models were being developed, technology for project cost estimating, cost management and engineering economics was evolving, with pioneering work by Hans Lang and others. In 1956, the American Association of Cost Engineers (now AACE International; the Association for the Advancement of Cost Engineering) was formed by early practitioners of project management and the associated specialties of planning and scheduling, cost estimating, and project control. AACE continued its pioneering work and in 2006, released the first integrated process for portfolio, program, and project management (total cost management framework).

In 1969, the Project Management Institute (PMI) was formed in the USA.[15] PMI publishes the original version of A Guide to the Project Management Body of Knowledge (PMBOK Guide) in 1996 with William Duncan as its primary author, which describes project management practices that are common to ""most projects, most of the time.""[16]

Project management methods can be applied to any project. It is often tailored to a specific type of project based on project size, nature, industry or sector. For example, the construction industry, which focuses on the delivery of things like buildings, roads, and bridges, has developed its own specialized form of project management that it refers to as construction project management and in which project managers can become trained and certified.[17] The information technology industry has also evolved to develop its own form of project management that is referred to as IT project management and which specializes in the delivery of technical assets and services that are required to pass through various lifecycle phases such as planning, design, development, testing, and deployment. Biotechnology project management focuses on the intricacies of biotechnology research and development.[18] Localization project management includes application of many standard project management practices to translation works even though many consider this type of management to be a very different discipline. For example, project managers have a key role in improving the translation even when they do not speak the language of the translation, because they know the study objectives well to make informed decisions.[19] Similarly, research study management can also apply a project manage approach.[20] There is public project management that covers all public works by the government, which can be carried out by the government agencies or contracted out to contractors. Another classification of project management is based on the hard (physical) or soft (non-physical) type.

Common among all the project management types is that they focus on three important goals: time, quality, and cost. Successful projects are completed on schedule, within budget, and according to previously agreed quality standards i.e. meeting the Iron Triangle or Triple Constraint in order for projects to be considered a success or failure.[21]

For each type of project management, project managers develop and utilize repeatable templates that are specific to the industry they're dealing with.  This allows project plans to become very thorough and highly repeatable, with the specific intent to increase quality, lower delivery costs, and lower time to deliver project results.

A 2017 study suggested that the success of any project depends on how well four key aspects are aligned with the contextual dynamics affecting the project, these are referred to as the four P's:[22]

There are a number of approaches to organizing and completing project activities, including phased, lean, iterative, and incremental. There are also several extensions to project planning, for example, based on outcomes (product-based) or activities (process-based).

Regardless of the methodology employed, careful consideration must be given to the overall project objectives, timeline, and cost, as well as the roles and responsibilities of all participants and stakeholders.[23]

Benefits realization management (BRM) enhances normal project management techniques through a focus on outcomes (benefits) of a project rather than products or outputs and then measuring the degree to which that is happening to keep a project on track. This can help to reduce the risk of a completed project being a failure by delivering agreed upon requirements (outputs) i.e. project success but failing to deliver the benefits (outcomes) of those requirements i.e. product success. Note that good requirements management will ensure these benefits are captured as requirements of the project and their achievement monitored throughout the project.

In addition, BRM practices aim to ensure the strategic alignment between project outcomes and business strategies. The effectiveness of these practices is supported by recent research evidencing BRM practices influencing project success from a strategic perspective across different countries and industries. These wider effects are called the strategic impact.[24]

An example of delivering a project to requirements might be agreeing to deliver a computer system that will process staff data and manage payroll, holiday, and staff personnel records in shorter times with reduced errors. Under BRM, the agreement might be to achieve a specified reduction in staff hours and errors required to process and maintain staff data after the system installation when compared without the system.

Critical path method (CPM) is an algorithm for determining the schedule for project activities. It is the traditional process used for predictive-based project planning. The CPM method evaluates the sequence of activities, the work effort required, the inter-dependencies, and the resulting float time per line sequence to determine the required project duration. Thus, by definition, the critical path is the pathway of tasks on the network diagram that has no extra time available (or very little extra time).""[25]

Critical chain project management (CCPM) is an application of the theory of constraints (TOC) to planning and managing projects and is designed to deal with the uncertainties inherent in managing projects, while taking into consideration the limited availability of resources (physical, human skills, as well as management & support capacity) needed to execute projects.

The goal is to increase the flow of projects in an organization (throughput). Applying the first three of the five focusing steps of TOC, the system constraint for all projects, as well as the resources, are identified. To exploit the constraint, tasks on the critical chain are given priority over all other activities.

Earned value management (EVM) extends project management with techniques to improve project monitoring.[26] It illustrates project progress towards completion in terms of work and value (cost). Earned Schedule is an extension to the theory and practice of EVM.

In critical studies of project management, it has been noted that phased approaches are not well suited for projects which are large-scale and multi-company,[27] with undefined, ambiguous, or fast-changing requirements,[28] or those with high degrees of risk, dependency, and fast-changing technologies. The cone of uncertainty explains some of this as the planning made on the initial phase of the project suffers from a high degree of uncertainty. This becomes especially true as software development is often the realization of a new or novel product.

These complexities are better handled with a more exploratory or iterative and incremental approach.[29] Several models of iterative and incremental project management have evolved, including agile project management, dynamic systems development method, extreme project management, and Innovation Engineering®.[30]

Lean project management uses the principles from lean manufacturing to focus on delivering value with less waste and reduced time.


There are five phases to a project lifecycle; known as process groups.  Each process group represents a series of inter-related processes to manage the work through a series of distinct steps to be completed. This type of project approach is often referred to as ""traditional""[31] or ""waterfall"".[32] The five process groups are:
Some industries may use variations of these project stages and rename them to better suit the organization. For example, when working on a brick-and-mortar design and construction, projects will typically progress through stages like pre-planning, conceptual design, schematic design, design development, construction drawings (or contract documents), and construction administration.

While the phased approach works well for small, well-defined projects, it often results in challenge or failure on larger projects, or those that are more complex or have more ambiguities, issues, and risks[33] - see the parodying 'six phases of a big project'.

The incorporation of process-based management has been driven by the use of maturity models such as the OPM3 and the CMMI (capability maturity model integration; see Image:Capability Maturity Model.jpg

Project production management is the application of operations management to the delivery of capital projects. The Project production management framework is based on a project as a production system view, in which a project transforms inputs (raw materials, information, labor, plant & machinery) into outputs (goods and services).[34]

Product-based planning is a structured approach to project management, based on identifying all of the products (project deliverables) that contribute to achieving the project objectives. As such, it defines a successful project as output-oriented rather than activity- or task-oriented.[35] The most common implementation of this approach is PRINCE2.[36]

Traditionally (depending on what project management methodology is being used), project management includes a number of elements: four to five project management process groups, and a control system. Regardless of the methodology or terminology used, the same basic project management processes or stages of development will be used. Major process groups generally include:[38]

In project environments with a significant exploratory element (e.g., research and development), these stages may be supplemented with decision points (go/no go decisions) at which the project's continuation is debated and decided. An example is the Phase–gate model.

Project management relies on a wide variety of meetings to coordinate actions. For instance, there is the kick-off meeting, which broadly involves stakeholders at the project's initiation. Project meetings or project committees enable the project team to define and monitor action plans. Steering committees are used to transition between phases and resolve issues. Project portfolio and program reviews are conducted in organizations running parallel projects. Lessons learned meetings are held to consolidate learnings. All these meetings employ techniques found in meeting science, particularly to define the objective, participant list, and facilitation methods.

The initiating processes determine the nature and scope of the project.[39] If this stage is not performed well, it is unlikely that the project will be successful in meeting the business' needs. The key project controls needed here are an understanding of the business environment and making sure that all necessary controls are incorporated into the project. Any deficiencies should be reported and a recommendation should be made to fix them.

The initiating stage should include a plan that encompasses the following areas. These areas can be recorded in a series of documents called Project Initiation documents.
Project Initiation documents are a series of planned documents used to create an order for the duration of the project. 
These tend to include: 

After the initiation stage, the project is planned to an appropriate level of detail (see an example of a flowchart).[37] The main purpose is to plan time, cost, and resources adequately to estimate the work needed and to effectively manage risk during project execution. As with the Initiation process group, a failure to adequately plan greatly reduces the project's chances of successfully accomplishing its goals.

Project planning generally consists of[40]

Additional processes, such as planning for communications and for scope management, identifying roles and responsibilities, determining what to purchase for the project, and holding a kick-off meeting are also generally advisable.

For new product development projects, conceptual design of the operation of the final product may be performed concurrent with the project planning activities and may help to inform the planning team when identifying deliverables and planning activities.

While executing we must know what are the planned terms that need to be executed.
The execution/implementation phase ensures that the project management plan's deliverables are executed accordingly. This phase involves proper allocation, coordination, and management of human resources and any other resources such as materials and budgets. The output of this phase is the project deliverables.

Documenting everything within a project is key to being successful. To maintain budget, scope, effectiveness and pace a project must have physical documents pertaining to each specific task. With correct documentation, it is easy to see whether or not a project's requirement has been met. To go along with that, documentation provides information regarding what has already been completed for that project. Documentation throughout a project provides a paper trail for anyone who needs to go back and reference the work in the past. 
In most cases, documentation is the most successful way to monitor and control the specific phases of a project.  With the correct documentation, a project's success can be tracked and observed as the project goes on. If performed correctly, documentation can be the backbone of a project's success

Monitoring and controlling consist of those processes performed to observe project execution so that potential problems can be identified in a timely manner and corrective action can be taken, when necessary, to control the execution of the project. The key benefit is that project performance is observed and measured regularly to identify variances from the project management plan.

Monitoring and controlling include:[41]

Two main mechanisms support monitoring and controlling in projects. On the one hand, contracts offer a set of rules and incentives often supported by potential penalties and sanctions.[42] On the other hand, scholars in business and management have paid attention to the role of integrators (also called project barons) to achieve a project's objectives.[43][44] In turn, recent research in project management has questioned the type of interplay between contracts and integrators. Some have argued that these two monitoring mechanisms operate as substitutes[45] as one type of organization would decrease the advantages of using the other one.

In multi-phase projects, the monitoring and control process also provides feedback between project phases, to implement corrective or preventive actions to bring the project into compliance with the project management plan.

Project maintenance is an ongoing process, and it includes:[38]

In this stage, auditors should pay attention to how effectively and quickly user problems are resolved.

Over the course of any construction project, the work scope may change. Change is a normal and expected part of the construction process. Changes can be the result of necessary design modifications, differing site conditions, material availability, contractor-requested changes, value engineering, and impacts from third parties, to name a few. Beyond executing the change in the field, the change normally needs to be documented to show what was actually constructed. This is referred to as change management. Hence, the owner usually requires a final record to show all changes or, more specifically, any change that modifies the tangible portions of the finished work. The record is made on the contract documents – usually, but not necessarily limited to, the design drawings. The end product of this effort is what the industry terms as-built drawings, or more simply, ""as built."" The requirement for providing them is a norm in construction contracts. Construction document management is a highly important task undertaken with the aid of an online or desktop software system or maintained through physical documentation. The increasing legality pertaining to the construction industry's maintenance of correct documentation has caused an increase in the need for document management systems.

When changes are introduced to the project, the viability of the project has to be re-assessed. It is important not to lose sight of the initial goals and targets of the projects. When the changes accumulate, the forecasted result may not justify the original proposed investment in the project. Successful project management identifies these components, and tracks and monitors progress, so as to stay within time and budget frames already outlined at the commencement of the project. Exact methods were suggested to identify the most informative monitoring points along the project life-cycle regarding its progress and expected duration.[46]

Closing includes the formal acceptance of the project and the ending thereof. Administrative activities include the archiving of the files and documenting lessons learned.

This phase consists of:[38]

Also included in this phase is the post implementation review. This is a vital phase of the project for the project team to learn from experiences and apply to future projects. Normally a post implementation review consists of looking at things that went well and analyzing things that went badly on the project to come up with lessons learned.

Project control (also known as Cost Engineering)
should be established as an independent function in project management. It implements verification and controlling functions during the processing of a project to reinforce the defined performance and formal goals.[47] The tasks of project control are also: 

Fulfillment and implementation of these tasks can be achieved by applying specific methods and instruments of project control. The following methods of project control can be applied:

Project control is that element of a project that keeps it on track, on time, and within budget.[41] Project control begins early in the project with planning and ends late in the project with post-implementation review, having a thorough involvement of each step in the process. Projects may be audited or reviewed while the project is in progress. Formal audits are generally risk or compliance-based and management will direct the objectives of the audit. An examination may include a comparison of approved project management processes with how the project is actually being managed.[51] Each project should be assessed for the appropriate level of control needed: too much control is too time-consuming, too little control is very risky. If project control is not implemented correctly, the cost to the business should be clarified in terms of errors and fixes.

Control systems are needed for cost, risk, quality, communication, time, change, procurement, and human resources. In addition, auditors should consider how important the projects are to the financial statements, how reliant the stakeholders are on controls, and how many controls exist.  Auditors should review the development process and procedures for how they are implemented. The process of development and the quality of the final product may also be assessed if needed or requested. A business may want the auditing firm to be involved throughout the process to catch problems earlier on so that they can be fixed more easily. An auditor can serve as a controls consultant as part of the development team or as an independent auditor as part of an audit.

Businesses sometimes use formal systems development processes. This help assure systems are developed successfully. A formal process is more effective in creating strong controls, and auditors should review this process to confirm that it is well designed and is followed in practice. A good formal systems development plan outlines:

There are five important characteristics of a project:

(i) It should always have specific start and end dates.

(ii) They are performed and completed by a group of people.

(iii) The output is the delivery of a unique product or service.

(iv) They are temporary in nature.

(v) It is progressively elaborated.

Examples are: designing a new car or writing a book.

Complexity and its nature play an important role in the area of project management. Despite having a number of debates on this subject matter, studies suggest a lack of definition and reasonable understanding of complexity in relation to the management of complex projects.[52][53]

Project complexity is the property of a project which makes it difficult to understand, foresee, and keep under control its overall behavior, even when given reasonably complete information about the project system.[54]

The identification of complex projects is specifically important to multi-project engineering environments.[55]

As it is considered that project complexity and project performance are closely related, it is important to define and measure the complexity of the project for project management to be effective.[56]

Complexity can be:

Based on the Cynefin framework,[58] complex projects can be classified as:

By applying the discovery in measuring work complexity described in Requisite Organization and Stratified Systems Theory, Elliott Jaques classifies projects and project work (stages, tasks) into seven basic levels of project complexity based on such criteria as time-span of discretion and complexity of a project's output:[62][63]

Benefits from measuring Project Complexity are to improve project people feasibility by matching the level of a project's complexity with an effective targeted completion time, with the respective capability level of the project manager and of the project members.[65]

Similarly with the Law of requisite variety and The law of requisite complexity, project complexity is sometimes required in order for the project to reach its objectives, and sometimes it has beneficial outcomes. Based on the effects of complexity, Stefan Morcov proposed its classification as Positive, Appropriate, or Negative.[66][61]

A project manager is a professional in the field of project management. Project managers are in charge of the people in a project. People are the key to any successful project. Without the correct people in the right place and at the right time a project cannot be successful. Project managers can have the responsibility of the planning, execution, controlling, and closing of any project typically relating to the construction industry, engineering, architecture, computing, and telecommunications. Many other fields of production engineering, design engineering, and heavy industrial have project managers.

A project manager needs to understand the order of execution of a project to schedule the project correctly as well as the time necessary to accomplish each individual task within the project. A project manager is the person accountable for accomplishing the stated project objectives on behalf of the client. Project Managers tend to have multiple years' experience in their field. A project manager is required to know the project in and out while supervising the workers along with the project. Typically in most construction, engineering, architecture, and industrial projects, a project manager has another manager working alongside of them who is typically responsible for the execution of task on a daily basis. This position in some cases is known as a superintendent. A superintendent and project manager work hand in hand in completing daily project tasks. Key project management responsibilities include creating clear and attainable project objectives, building the project requirements, and managing the triple constraint (now including more constraints and calling it competing constraints) for projects, which is cost, time, quality and scope for the first three but about three additional ones in current project management. A typical project is composed of a team of workers who work under the project manager to complete the assignment within the time and budget targets. A project manager normally reports directly to someone of higher stature on the completion and success of the project.

A project manager is often a client representative and has to determine and implement the exact needs of the client, based on knowledge of the firm they are representing. The ability to adapt to the various internal procedures of the contracting party, and to form close links with the nominated representatives, is essential in ensuring that the key issues of cost, time, quality and above all, client satisfaction, can be realized.

A complete project manager, a term first coined by Robert J. Graham in his simulation, has been expanded upon by Randall L. Englund and Alfonso Bucero. They describe a complete project manager as a person who embraces multiple disciplines, such as leadership, influence, negotiations, politics, change and conflict management, and humor. These are all ""soft"" people skills that enable project leaders to be more effective and achieve optimized, consistent results.

There is a tendency to confuse the project success with project management success. They are two different things. ""Project success"" has 2 perspectives: 

Project management success criteria are different from project success criteria. The project management is said to be successful if the given project is completed within the agreed upon time, met the agreed upon scope and within the agreed upon budget. Subsequent to the triple constraints, multiple constraints have been considered to ensure project success. However, the triple or multiple constraints indicate only the efficiency measures of the project, which are indeed the project management success criteria during the project lifecycle.

The priori criteria leave out the more important after-completion results of the project which comprise four levels i.e. the output (product) success, outcome (benefits) success and impact (strategic) success during the product lifecycle. These posterior success criteria indicate the effectiveness measures of the project product, service or result, after the project completion and handover. This overarching multilevel success framework of projects, programs and portfolios has been developed by Paul Bannerman in 2008.[70] In other words, a project is said to be successful, when it succeeds in achieving the expected business case which needs to be clearly identified and defined during the project inception and selection before starting the development phase. This multilevel success framework conforms to the theory of project as a transformation depicted as the input-process / activity-output-outcome-impact in order to generate whatever value intended. Emanuel Camilleri in 2011 classifies all the critical success and failure factors into groups and matches each of them with the multilevel success criteria in order to deliver business value.[71]

An example of a performance indicator used in relation to project management is the ""backlog of commissioned projects"" or ""project backlog"".[72]

The United States Department of Defense states that ""Cost, Schedule, Performance, and Risk"" are the four elements through which Department of Defense acquisition professionals make trade-offs and track program status.[73] There are also international standards. Risk management applies proactive identification (see tools) of future problems and understanding of their consequences allowing predictive decisions about projects. ERM system plays a role in overall risk management.[74]

The work breakdown structure (WBS) is a tree structure that shows a subdivision of the activities required to achieve an objective – for example a portfolio, program, project, and contract. The WBS may be hardware-, product-, service-, or process-oriented (see an example in a NASA reporting structure (2001)).[75]  Beside WBS for project scope management, there are organizational breakdown structure (chart), cost breakdown structure and risk breakdown structure.

A WBS can be developed by starting with the end objective and successively subdividing it into manageable components in terms of size, duration, and responsibility (e.g., systems, subsystems, components, tasks, sub-tasks, and work packages), which include all steps necessary to achieve the objective.[33]

The work breakdown structure provides a common framework for the natural development of the overall planning and control of a contract and is the basis for dividing work into definable increments from which the statement of work can be developed and technical, schedule, cost, and labor hour reporting can be established.[75]
The work breakdown structure can be displayed in two forms, as a table with subdivision of tasks or as an organizational chart whose lowest nodes are referred to as ""work packages"".

It is an essential element in assessing the quality of a plan, and an initial element used during the planning of the project. For example, a WBS is used when the project is scheduled, so that the use of work packages can be recorded and tracked.

Similarly to work breakdown structure (WBS), other decomposition techniques and tools are: organization breakdown structure (OBS), product breakdown structure (PBS), cost breakdown structure (CBS), risk breakdown structure (RBS), and resource breakdown structure (ResBS).[76][61]

There are several project management standards, including:

Some projects, either identical or different, can be managed as program management.  
Programs are collections of projects that support a common objective and set of goals. While individual projects have clearly defined and specific scope and timeline, a program's objectives and duration are defined with a lower level of granularity.

Besides programs and portfolios, additional structures that combine their different characteristics are: project networks, mega-projects, or mega-programs.

A project network is a temporary project formed of several different distinct evolving phases, crossing organizational lines. 
Mega-projects and mega-programs are defined as exceptional in terms of size, cost, public and political attention, and competencies required.[61]

An increasing number of organizations are using what is referred to as project portfolio management (PPM) as a means of selecting the right projects and then using project management techniques[81] as the means for delivering the outcomes in the form of benefits to the performing public, private or not-for-profit organization.

Portfolios are collections of similar projects. Portfolio management supports efficiencies of scale, increasing success rates, and reducing project risks, by applying similar standardized techniques to all projects in the portfolio, by a group of project management professionals sharing common tools and knowledge. 
Organizations often create project management offices as an organizational structure to support project portfolio management in a structured way.[61] 
Thus, PPM is usually performed by a dedicated team of managers organized within an enterprise project management office (PMO), usually based within the organization, and headed by a PMO director or chief project officer. In cases where strategic initiatives of an organization form the bulk of the PPM, the head of the PPM is sometimes titled as the chief initiative officer.

Project management software is software used to help plan, organize, and manage resource pools, develop resource estimates and implement plans. Depending on the sophistication of the software, functionality may include estimation and planning, scheduling, cost control and budget management, resource allocation, collaboration software, communication, decision-making, workflow, risk, quality, documentation, and/or administration systems.[82][83]

Virtual program management (VPM) is management of a project done by a virtual team, though it rarely may refer to a project implementing a virtual environment[84]  It is noted that managing a virtual project is fundamentally different from managing traditional projects,[85] combining concerns of remote work and global collaboration (culture, time zones, language).[86]
"
Technology Consulting,"Information technology consultants help enterprises and organizations meet their business goals through information technology (IT).

There are basically two types of IT consulting companies:

This category has the following 2 subcategories, out of 2 total.

The following 4 pages are in this category, out of  4 total. This list may not reflect recent changes.
"
Software Development Services,"

Software development is the process of designing and implementing a software solution to satisfy a user. The process is more encompassing than programming, writing code, in that it includes conceiving the goal, evaluating feasibility, analyzing requirements, design, testing and release. The process is part of software engineering which also includes organizational management, project management, configuration management and other aspects.[1]

Software development involves many skills and job specializations including programming, testing, documentation, graphic design, user support, marketing, and fundraising. 

Software development involves many tools including: compiler, integrated development environment (IDE), version control, computer-aided software engineering, and word processor.

The details of the process used for a development effort varies. The process may be confined to a formal, documented standard, or it can be customized and emergent for the development effort. The process may be sequential, in which each major phase (i.e. design, implement and test) is completed before the next begins, but an iterative approach – where small aspects are separately designed, implemented and tested – can reduce risk and cost and increase quality.

Each of the available methodologies are best suited to specific kinds of projects, based on various technical, organizational, project, and team considerations.[3]

Another focus in many programming methodologies is the idea of trying to catch issues such as security vulnerabilities and bugs as early as possible (shift-left testing) to reduce the cost of tracking and fixing them.[13]

In 2009, it was estimated that 32 percent of software projects were delivered on time and budget, and with the full functionality. An additional 44 percent were delivered, but missing at least one of these features. The remaining 24 percent were cancelled prior to release.[14]

Software development life cycle refers to the systematic process of developing applications.[15]

The sources of ideas for software products are plentiful. These ideas can come from market research including the demographics of potential new customers, existing customers, sales prospects who rejected the product, other internal software development staff, or a creative third party. Ideas for software products are usually first evaluated by marketing personnel for economic feasibility, fit with existing channels of distribution, possible effects on existing product lines, required features, and fit with the company's marketing objectives. In the marketing evaluation phase, the cost and time assumptions become evaluated.[16] The feasibility analysis estimates the project's return on investment, its development cost and timeframe. Based on this analysis, the company can make a business decision to invest in further development.[17] After deciding to develop the software, the company is focused on delivering the product at or below the estimated cost and time, and with a high standard of quality (i.e., lack of bugs) and the desired functionality. Nevertheless, most software projects run late and sometimes compromises are made in features or quality to meet a deadline.[18]

Software analysis begins with a requirements analysis to capture the business needs of the software.[19] Challenges for the identification of needs are that current or potential users may have different and incompatible needs, may not understand their own needs, and change their needs during the process of software development.[20] Ultimately, the result of analysis is a detailed specification for the product that developers can work from. Software analysts often decompose the project into smaller objects, components that can be reused for increased cost-effectiveness, efficiency, and reliability.[19] Decomposing the project may enable a multi-threaded implementation that runs significantly faster on multiprocessor computers.[21]

During the analysis and design phases of software development, structured analysis is often used to break down the customer's requirements into pieces that can be implemented by software programmers.[22] The underlying logic of the program may be represented in data-flow diagrams, data dictionaries, pseudocode, state transition diagrams, and/or entity relationship diagrams.[23] If the project incorporates a piece of legacy software that has not been modeled, this software may be modeled to help ensure it is correctly incorporated with the newer software.[24]

Design involves choices about the implementation of the software, such as which programming languages and database software to use, or how the hardware and network communications will be organized. Design may be iterative with users consulted about their needs in a process of trial and error. Design often involves people expert in aspect such as database design, screen architecture, and the performance of servers and other hardware.[19] Designers often attempt to find patterns in the software's functionality to spin off distinct modules that can be reused with object-oriented programming. An example of this is the model–view–controller, an interface between a graphical user interface and the backend.[25]

The central feature of software development is creating and understanding the software that implements the desired functionality.[26] There are various strategies for writing the code. Cohesive software has various components that are independent from each other.[19] Coupling is the interrelation of different software components, which is viewed as undesirable because it increases the difficulty of maintenance.[27] Often, software programmers do not follow industry best practices, resulting in code that is inefficient, difficult to understand, or lacking documentation on its functionality.[28] These standards are especially likely to break down in the presence of deadlines.[29] As a result, testing, debugging, and revising the code becomes much more difficult. Code refactoring, for example adding more comments to the code, is a solution to improve the understandability of code.[30]

Testing is the process of ensuring that the code executes correctly and without errors. Debugging is performed by each software developer on their own code to confirm that the code does what it is intended to. In particular, it is crucial that the software executes on all inputs, even if the result is incorrect.[31] Code reviews by other developers are often used to scrutinize new code added to the project, and according to some estimates dramatically reduce the number of bugs persisting after testing is complete.[32] Once the code has been submitted, quality assurance—a separate department of non-programmers for most large companies—test the accuracy of the entire software product. Acceptance tests derived from the original software requirements are a popular tool for this.[31] Quality testing also often includes stress and load checking (whether the software is robust to heavy levels of input or usage), integration testing (to ensure that the software is adequately integrated with other software), and compatibility testing (measuring the software's performance across different operating systems or browsers).[31] When tests are written before the code, this is called test-driven development.[33]

Production is the phase in which software is deployed to the end user.[34] During production, the developer may create technical support resources for users[35][34] or a process for fixing bugs and errors that were not caught earlier. There might also be a return to earlier development phases if user needs changed or were misunderstood.[34]

Software development is performed by software developers, usually working on a team. Efficient communications between team members is essential to success. This is more easily achieved if the team is small, used to working together, and located near each other.[36] Communications also help identify problems at an earlier state of development and avoid duplicated effort. Many development projects avoid the risk of losing essential knowledge held by only one employee by ensuring that multiple workers are familiar with each component.[37] Software development involves professionals from various fields, not just software programmers but also individuals specialized in testing, documentation writing, graphic design, user support, marketing, and fundraising. Although workers for proprietary software are paid, most contributors to open-source software are volunteers.[38] Alternately, they may be paid by companies whose business model does not involve selling the software, but something else—such as services and modifications to open source software.[39]

Computer-aided software engineering (CASE) is tools for the partial automation of software development.[40] CASE enables designers to sketch out the logic of a program, whether one to be written, or an already existing one to help integrate it with new code or reverse engineer it (for example, to change the programming language).[41]

Documentation comes in two forms that are usually kept separate—that intended for software developers, and that made available to the end user to help them use the software.[42][43] Most developer documentation is in the form of code comments for each file, class, and method that cover the application programming interface (API)—how the piece of software can be accessed by another—and often implementation details.[44] This documentation is helpful for new developers to understand the project when they begin working on it.[45] In agile development, the documentation is often written at the same time as the code.[46] User documentation is more frequently written by technical writers.[47]

Accurate estimation is crucial at the feasibility stage and in delivering the product on time and within budget. The process of generating estimations is often delegated by the project manager.[48] Because the effort estimation is directly related to the size of the complete application, it is strongly influenced by addition of features in the requirements—the more requirements, the higher the development cost. Aspects not related to functionality, such as the experience of the software developers and code reusability, are also essential to consider in estimation.[49] As of 2019[update], most of the tools for estimating the amount of time and resources for software development were designed for conventional applications and are not applicable to web applications or mobile applications.[50]

An integrated development environment (IDE) supports software development with enhanced features compared to a simple text editor.[51] IDEs often include automated compiling, syntax highlighting of errors,[52] debugging assistance,[53] integration with version control, and semi-automation of tests.[51]

Version control is a popular way of managing changes made to the software. Whenever a new version is checked in, the software saves a backup of all modified files. If multiple programmers are working on the software simultaneously, it manages the merging of their code changes. The software highlights cases where there is a conflict between two sets of changes and allows programmers to fix the conflict.[54]

A view model is a framework that provides the viewpoints on the system and its environment, to be used in the software development process. It is a graphical representation of the underlying semantics of a view.

The purpose of viewpoints and views is to enable human engineers to comprehend very complex systems and to organize the elements of the problem around domains of expertise. In the engineering of physically intensive systems, viewpoints often correspond to capabilities and responsibilities within the engineering organization.[55]

Fitness functions are automated and objective tests to ensure that the new developments don't deviate from the established constraints, checks and compliance controls.[56]

Intellectual property can be an issue when developers integrate open-source code or libraries into a proprietary product, because most open-source licenses used for software require that modifications be released under the same license. As an alternative, developers may choose a proprietary alternative or write their own software module.[57]
"
Data Analysis Services,"Microsoft SQL Server Analysis Services (SSAS[1]) is an online analytical processing (OLAP) and data mining tool in Microsoft SQL Server. SSAS is used as a tool by organizations to analyze and make sense of information possibly spread out across multiple databases, or in disparate tables or files. Microsoft has included a number of services in SQL Server related to business intelligence and data warehousing. These services include Integration Services, Reporting Services and Analysis Services. Analysis Services includes a group of OLAP and data mining capabilities and comes in two flavors multidimensional and tabular, where the difference between the two is how the data is presented.[citation needed] In a tabular model, the information is arranged in two-dimensional tables which can thus be more readable for a human. A multidimensional model can contain information with many degrees of freedom, and must be unfolded to increase readability by a human.[citation needed]

In 1996, Microsoft began its foray into the OLAP Server business by acquiring the OLAP software technology from Canada-based Panorama Software.[2]
Just over two years later, in 1998, Microsoft released OLAP Services as part of SQL Server 7. OLAP Services supported MOLAP, ROLAP, and HOLAP architectures, and it used OLE DB for OLAP as the client access API and MDX as a query language. It could work in client-server mode or offline mode with local cube files.[3]

In 2000, Microsoft released Analysis Services 2000. It was renamed from ""OLAP Services"" due to the inclusion of data mining services. Analysis Services 2000 was considered an evolutionary release, since it was built on the same architecture as OLAP Services and was therefore backward compatible with it. Major improvements included more flexibility in dimension design through support of parent child dimensions, changing dimensions, and virtual dimensions. Another feature was a greatly enhanced calculation engine with support for unary operators, custom rollups, and cell calculations. Other features were dimension security, distinct count, connectivity over HTTP, session cubes, grouping levels, and many others.[4]

In 2005, Microsoft released the next generation of OLAP and data mining technology as Analysis Services 2005. It maintained backward compatibility on the API level: although applications written with OLE DB for OLAP and MDX continued to work, the architecture of the product was completely different. The major change came to the model in the form of UDM - Unified Dimensional Model.[5][clarification needed]

The key events in the history of Microsoft Analysis Services cover a period starting in 1996.

Microsoft Analysis Services takes a neutral position in the MOLAP vs. ROLAP arguments among OLAP products.
It allows all the flavors of MOLAP, ROLAP and HOLAP to be used within the same model.



Microsoft Analysis Services supports different sets of APIs and object models for different operations and in different programming environments.

Microsoft Analysis Services supports the following query languages

DDL in Analysis Services is XML based and supports commands such as <Create>, <Alter>, <Delete>, and <Process>.
For data mining models import and export, it also supports PMML.
"
Market Research Services,"Market research is an organized effort to gather information about target markets and customers. It involves understanding who they are and what they need.[1] It is an important component of business strategy[2] and a major factor in maintaining competitiveness. Market research helps to identify and analyze the needs of the market, the market size and the competition. Its techniques encompass both qualitative techniques such as focus groups, in-depth interviews, and ethnography, as well as quantitative techniques such as customer surveys, and analysis of secondary data.

It includes social and opinion research, and is the systematic gathering and interpretation of information about individuals or organizations using statistical and analytical methods and techniques of the applied social sciences to gain insight or support decision making.[3]

Market research, marketing research, and marketing are a sequence of business activities;[4][5] sometimes these are handled informally.[6]

The field of marketing research is much older than that of market research.[7] Although both involve consumers, Marketing research is concerned specifically about marketing processes, such as advertising effectiveness and salesforce effectiveness, while market research is concerned specifically with markets and distribution.[8] Two explanations given for confusing Market research with Marketing research are the similarity of the terms and also that Market Research is a subset of Marketing Research.[9][10][11] Further confusion exists because of major companies with expertise and practices in both areas.[12]

Although market research started to be conceptualized and put into formal practice during the 1930s as an offshoot of the advertising boom of the Golden Age of radio in the United States, this was based on 1920s work by Daniel Starch. Starch ""developed a theory that advertising had to be seen, read, believed, remembered, and most importantly, acted upon, in order to be considered effective.""[13] Advertisers realized the significance of demographics by the patterns in which they sponsored different radio programs.[citation needed]

The Gallup Organization helped invent the public opinion poll; today, ""Market research is a way of paying for it.""[14]

Market research is a way of getting an overview of consumers' wants, needs and beliefs. It can also involve discovering how they act. The research can be used to determine how a product could be marketed.  Peter Drucker believed[15] market research to be the quintessence of marketing.
Market research is a way that producers and the marketplace study the consumer and gather information about the consumers' needs.
There are two major types of market research: primary research, which is sub-divided into quantitative and qualitative research, and secondary research.

Factors that can be investigated through market research include:

Another factor that can be measured is marketing effectiveness. This includes:

""Rigorous sampling methodologies combined with high-quality data collection"" is what the magazine Advertising Age considers the backbone of market research.[18] Data collection can be done by observing customer behavior through in-situ studies or by processing e.g. log files, by interviewing customers, potential customers, stakeholders, or a sample of the general population. The data can be quantitative in nature (counting sales, clicks, eye-tracking) or qualitative (surveys, questionnaires, interviews, feedback). Aggregating, visualizing, and turning data into actionable insights is one of the major challenges of market research and today, text analytics affords market researches methods to process large amounts of qualitative information and turn it into quantitative data, which is easier to visualize and use for formalized decision making.[19]

Data collection can use larger audience samples than the few hundred or thousand typically used in market research.[20] Also required is the (at least passive)[21] cooperation of those being surveyed;[22] trust[23] is also helpful.[24] Translation is an essential comprehension tool for global consumers and is not a simple act of replacing words in one language with words in another.[25]

Some data collection is incentivized: a simple form is when those on the road contribute to traffic reporting of which they are consumers. More complex is the relationship of consumer-to-business (C2B), which sometimes introduces reliability problems.[26] Other data collection is to know more about the market,[27] which is the purpose of market research.[28]

The international growth of available research both from and via the Internet[13] has influenced a vast number of consumers and those from whom they make purchases.[29] Although emerging global markets, such as China, Indonesia and Russia are still smaller than the US in B2B e-commerce, their internet-fueled growth factor is stimulated by product-enhancing websites, graphics, and content designed to attract corporate and consumer/B2C shoppers. Estimates for 2010 show between US$400 billion and $600 billion in revenue was generated by this medium.

A report titled ""Global B2C E-Commerce and Online Payment Market 2014"" indicated a decrease in overall growth rates in North America and Western Europe, even as absolute growth numbers rose.

The UK Market Research Society (MRS) listed the top social media platforms primarily used by millennials are LinkedIn, Facebook, YouTube and Instagram.

Regarding details for worldwide corporate market research, ""most of them are never written about because they are the consumer research done by the country's manufacturers.""[30] Also less written about is tailored translation approaches based on the expertise or resources available in the local country.[25] To mitigate implicit and unconscious bias in market research design, researchers have suggested conducting bias testing via interviewer-moderated technology-aided, unmoderated methods.[31]

Market research data has loss prevention aspects; that less than 60 percent of all proposed modifications and new products are deemed failures.[30] When information about the market is difficult to acquire, and the cost of ""going ahead with the decision"" to offer the product or service is affordable, the research cost may be more profitably used ""to ensure that the new line got the advertising send-off it needed to have the best chances of succeeding.""[32]

As measured in revenue, US based Amazon is the worldwide E-Commerce leader.[33]

The film industry is an example where the importance of testing film content and marketing material involves:

Market research is an industry that overlaps with and is often referred to as the ""insights"" industry.[35] However, the distinctive methods and techniques of market research not always correspond to the digital-first approach of insights vendors. The emergence of insights focusing on data analytics rather than fieldwork is competing with market research for managerial attention and funding. Current research with market research practitioners shows two pressing concerns for the industry: online data commoditization and the increasing distance between market researchers and top management within client organizations. Both concerns boil down to the risk they perceived of market research becoming a legacy activity of the marketing department rather than the cornerstone of business strategy.[35]

Market research aims to produce so-called ""actionable knowledge"" that firms find useful in their operations:[36]

Small organizations and non-profits can derive needed information by observing the environment of their location. Small scale surveys and focus groups are low cost ways to gather information from potential and existing customers and donors. While secondary data (statistics, demographics, etc.) is available to the public in libraries or on the internet, primary sources, done well, can be quite valuable: talking for an hour each, to twelve people, two apiece from six potential clients, can ""get inside their minds.. get a feel for their needs, wants and pain. You can't get that from a questionnaire.""[37]

 This article incorporates public domain material from websites or documents of the Small Business Administration.
"
Strategic Planning Services,"Strategic planning is the activity undertaken by an organization through which it seeks to define its future direction and makes decisions such as resource allocation aimed at achieving its intended goals. ""Strategy"" has many definitions, but it generally involves setting major goals, determining actions to achieve these goals, setting a timeline, and mobilizing resources to execute the actions. A strategy describes how the ends (goals) will be achieved by the means (resources) in a given span of time. Often, Strategic planning is long term and organizational action steps are established from two to five years in the future.[1] Strategy can be planned (""intended"") or can be observed as a pattern of activity (""emergent"") as the organization adapts to its environment or competes in the market.

The senior leadership of an organization is generally tasked with determining strategy. It is executed by strategic planners or strategists, who involve many parties and research sources in their analysis of the organization and its relationship to the environment in which it competes.[2]

Strategy includes processes of formulation and implementation; strategic planning helps coordinate both. However, strategic planning is analytical in nature (i.e., it involves ""finding the dots""); strategy formation itself involves synthesis (i.e., ""connecting the dots"") via strategic thinking. As such, strategic planning occurs around the strategy formation activity.[2]

Strategic planning became prominent in corporations during the 1960s and remains an important aspect of strategic management. 

McKinsey & Company developed a capability maturity model in the 1970s to describe the sophistication of planning processes, with strategic management ranked the highest. The four stages include:

Categories 3 and 4 are strategic planning, while the first two categories are non-strategic or essentially financial planning. Each stage builds on the previous stages; that is, a stage 4 organization completes activities in all four categories.[3]

In 1993, President Bill Clinton signed into law the Government Performance and Results Act, which required US federal agencies to develop strategic plans for how they would deliver high quality products and services to the American people.[4]

In the business sector, McKinsey research undertaken and published in 2006 found that, although many companies had a formal strategic-planning process, the process was not being used for their ""most important decisions"".[5]

For Michael C. Sekora, Project Socrates founder in the Reagan White House, during the Cold War the economically challenged Soviet Union was able to keep on western military capabilities by using technology-based planning while the U.S. was slowed by finance-based planning, until the Reagan administration launched the Socrates Project, which should be revived to keep up with China as an emerging superpower.[6]

Strategic planning is a process and thus has inputs, activities, outputs and outcomes. This process, like all processes, has constraints. It may be formal or informal and is typically iterative, with feedback loops throughout the process. Some elements of the process may be continuous and others may be executed as discrete projects with a definitive start and end during a period. Strategic planning provides inputs for strategic thinking: these are best seen as distinct but complementary activities.[7] Strategic thinking guides the actual strategy formation. Typical strategic planning efforts include the evaluation of the organization's mission and strategic issues to strengthen current practices and determine the need for new programming.[8] The end result is the organization's strategy, including a diagnosis of the environment and competitive situation, a guiding policy on what the organization intends to accomplish, and key initiatives or action plans for achieving the guiding policy.[9]

Michael Porter wrote in 1980 that formulation of competitive strategy includes consideration of four key elements:

The first two elements relate to factors internal to the company (i.e., the internal environment), while the latter two relate to factors external to the company (i.e., the external environment).[10] These elements are considered throughout the strategic planning process.

Data is gathered from various sources, such as interviews with key executives, review of publicly available documents on the competition or market, primary research (e.g., visiting or observing competitor places of business or comparing prices), industry studies, reports of the organization's performance, etc. This may be part of a competitive intelligence program. Inputs are gathered to help establish a baseline, support an understanding of the competitive environment and its opportunities and risks. Other inputs include an understanding of the values of key stakeholders, such as the board, shareholders, and senior management. These values may be captured in an organization's vision and mission statements.

The essence of formulating competitive strategy is relating a company to its environment.

Strategic planning activities include meetings and other communication among the organization's leaders and personnel to develop a common understanding regarding the competitive environment and what the organization's response to that environment should be. A variety of strategic planning tools may be completed as part of strategic planning activities.

The organization's leaders may have a series of questions they want to be answered in formulating the strategy and gathering inputs.[2][11]

The output of strategic planning includes documentation and communication describing the organization's strategy and how it should be implemented, sometimes referred to as the strategic plan.[12] The strategy may include a diagnosis of the competitive situation, a guiding policy for achieving the organization's goals, and specific action plans to be implemented.[9] A strategic plan may cover multiple years and be updated periodically.

The organization may use a variety of methods of measuring and monitoring progress towards the strategic objectives and measures established, such as a balanced scorecard or strategy map. Organizations may also plan their financial statements (i.e., balance sheets, income statements, and cash flows) for several years when developing their strategic plan, as part of the goal-setting activity. The term operational budget is often used to describe the expected financial performance of an organization for the upcoming year. Capital budgets very often form the backbone of a strategic plan, especially as it increasingly relates to Information and Communications Technology (ICT).

While the planning process produces outputs, strategy implementation or execution of the strategic plan produces outcomes. These outcomes will invariably differ from the strategic goals. How close they are to the strategic goals and vision will determine the success or failure of the strategic plan. Unintended outcomes might also be an issue. They need to be attended to and understood for strategy development and execution to be a true learning process.

A variety of analytical tools and techniques are used in strategic planning.[2] These were developed by companies and management consulting firms to help provide a framework for strategic planning. Such tools include:

Strategic planning can be used in project management with a focus on the development of a standard repeatable methodology adding to the likelihood of achieving project objectives. This requires a lot of thinking process and interaction among stakeholders. Strategic planning in Project Management provides an organization the framework and consistency of action. In addition, it ensures communication of overall goals and understanding roles of teams or individual to achieve them. The commitment of top management must be evident throughout the process to reduce resistance to change, ensure acceptance, and avoid common pitfalls. Strategic planning does not guarantee success but will help improve likelihood of success of an organization.[14]

strategic planning is also desirable within educational institutions. We are already in a transitional period in which old practices are no longer permanent but require revision to meet the needs of academia, which is frustrating in the educational sector. To meet the changing needs of this new society, educational institutions must  reorganize.[15] Finding ways to maintain achievements while improving effectiveness can be difficult for educational institutions. Keeping up with society's rapid changes. Some strategic planners are hesitant to address societal outcomes, so they often ignore them and assume they will happen on their own. Instead of defining the vision for how we want our children to live, they direct their attention to courses, content, and resources with the mistaken belief that societally useful outcomes will follow. When this occurs, the true strategic plan is never developed or implemented.[16]

Simply extending financial statement projections into the future without consideration of the competitive environment is a form of financial planning or budgeting, not strategic planning. In business, the term ""financial plan"" is often used to describe the expected financial performance of an organization for future periods. The term ""budget"" is used for a financial plan for the upcoming year. A ""forecast"" is typically a combination of actual performance year-to-date plus expected performance for the remainder of the year, so is generally compared against plan or budget and prior performance. The financial plans accompanying a strategic plan may include three–five years of projected performance.

Strategic planning has been criticized for attempting to systematize strategic thinking and strategy formation, which Henry Mintzberg argues are inherently creative activities involving synthesis or ""connecting the dots"" which cannot be systematized. Mintzberg argues that strategic planning can help coordinate planning efforts and measure progress on strategic goals, but that it occurs ""around"" the strategy formation process rather than within it. It functions remote from the ""front lines"" or contact with the competitive environment (i.e., in business, facing the customer where the effect of competition is most clearly evident) may not be effective at supporting strategy efforts.[2]

While much criticism surrounds strategic planning, evidence suggests that it does work. In a 2019 meta-analysis including data from almost 9,000 public and private organizations, strategic planning is found to have a positive impact on organizational performance. Strategic planning is particularly potent in enhancing an organization's capacity to achieve its goals (i.e., effectiveness). However, the study argues that just having a plan is not enough. For strategic planning to work, it needs to include some formality (i.e., including an analysis of the internal and external environment and the stipulation of strategies, goals and plans based on these analyses), comprehensiveness (i.e., producing many strategic options before selecting the course to follow) and careful stakeholder management (i.e., thinking carefully about whom to involve during the different steps of the strategic planning process, how, when and why).[17]

Henry Mintzberg in the article ""The Fall and Rise of Strategic Planning"" (1994),[18] argued that the lesson that should be accepted is that managers will never be able to take charge of strategic planning through a formalized process. Therefore, he underscored the role of plans as tools to communicate and control. It ensures that there is coordination wherein everyone in the organization is moving in the same direction. The plans are the prime media communicating the management's strategic intentions, thereby promoting a common direction instead of individual discretion. It is also the tool to secure the support of the organization's external sphere, such as financiers, suppliers or government agencies, who are helping achieve the organization's plans and goals.[18]

Cornut et al (2012) [19] studied the particular features of the strategic plan genre of communication by examining a corpus of strategic plans from public and non-profit organizations. They defined strategic plans as the ""key material manifestation"" of organizations' strategies and argued that, even though strategic plans are specific to an organization, there is a generic quality that draws on shared institutional understanding on the substance, form and communicative purposes of the strategic plan. Hence, they posit that strategic plan is a genre of organizational communication (Bhatia, 2004; Yates and Orlikowski, 1992 as cited in Cornut et al., 2012).[19] In this sense, genre is defined as the ""conventionalized discursive actions in which participating individuals or institutions have shared perceptions of communicative purposes as well as those of constraints operating their construction, interpretation and conditions of use""  (Bhatia, 2004: 87; see also Frow, 2005; Swales, 1990 as cited in Cornut et al., 2012).[19]

The authors compared the corpus of strategic plans with nine other corpora. This included annual reports from the public sector and nongovernment organizations, research articles, project plans, executive speeches, State of the Union addresses, horoscopes, religious sermons, business magazine articles and annual reports for-profit corporations included in the Standard & Poor's 500 largest companies (S&P 500).

The authors used textual analysis, including content analysis and corpus linguistics. Content analysis was used to identify themes and concepts, such as values and cognition; while corpus linguistics was used to identify naturally occurring texts and patterns (Biber et al., 1998 as cited in Cornut et al., 2012).[19]

The strategic plans showed significantly less self-reference than all other corpora, with the exemption of project plans and S&P 500 annual reports. The results indicated that strategic plans have more moderate verbs of deontic value. This was interpreted as an indication that ""commands and commitments are not overtly hedged, but neither are they particularly strong"".

Guidance on the sections of a strategic plan abound but there are few studies about the nature of language used for these documents. Cornut et al.'s (2012) [19] study showed that writers of strategic plans have a shared understanding of what is the appropriate language. Thus, the authors argued, a true strategist is one who is able to instantiate the genre strategic plan through appropriate application of language.[19]

Spee et. al. (2011)[20] explored the strategic planning as communicative process based on Ricoeur's concepts of decontextualization and recontextualization, they conceptualize strategic planning activities as being constituted through the iterative and recursive relationship of talk and text, this elaborate the construction of a strategic plan as a communicative process. This study looks at the way that texts within the planning process, such as PowerPoint presentations, planning documents and targets that are part of a strategic plan, are constructed in preparation, through a series of communicative interface. Throughout the process, strategy documents were essential in detaining the developing strategy as they were constantly revised up until an ultimate plan was accepted.

A book edited by Mandeville-Gamble (2015) sees the roles of managers as important in terms of communicating the strategic vision of the organization.[21] Many of the authors in the book by Mandeville-Gamble agree that a strategic plan is merely an unrealized vision unless it is widely shared and sparks the willingness to change within individuals in the organization. Similarly, Goodman in 2017[22] emphasized that the advent of the internet and social media has become one of the most important vehicle to which corporate strategic plan can be distributed to an organizations internal and external stakeholders. This distribution of knowledge allows for staff of organization to access and share the institutional thinking this able to reformulate it in their own words.

Strategic planning through control mechanisms (mostly by the way of a communication program) is set in the hopes of coming to desired outcomes that reflect company or organizational goals. As further supplement to this idea, controls can also be realized in both measurable and intangible controls, specifically output controls, behavioural controls, and clan controls. By way of simple definition, output controls work toward to tangible and quantifiable results; behavioural controls are geared toward behaviours of people in an organization; and clan controls are dependent and are executed while keeping in mind norms, traditions, and organizational culture. All these three are implemented in order to keep systems and strategies running and focused toward desired results (n.d.).

Strategic planning is both the impetus for and result of critical thinking, optimization, and motivation for the growth and development of organizations. The core disciplines, which are inherent in systems thinking, personal and organizational mastery, mental models, building a shared vision, and team learning. In a time of machine learning and data analytics, these core disciplines remain to be relevant in so far as having human resource and human interest become the driving force behind organizations.

Moreover, it cannot be denied that communication plays a role in the realization of learning organizations and strategic planning. In a study by Barker and Camarata (1998),[23] the authors noted that there are theories that could explain the invaluable role of communication, and these are from Rational Choice Theory to Social Exchange Theory where costs, rewards, and outcomes are valued in maintaining communication and thus relationships to serve the ends of an organization and its members. Thus, while many organizations and companies try their best to become learning organizations and exercise strategic planning, without communication, relationships fail and the core disciplines are never truly met (Barker & Camarata, 1998).[23]
"
Training Services,"Training is teaching, or developing in oneself or others, any skills and knowledge  or fitness that relate to specific useful competencies. Training has specific goals of improving one's capability, capacity, productivity and performance. It forms the core of apprenticeships and provides the backbone of content at institutes of technology (also known as technical colleges or polytechnics). In addition to the basic training required for a trade, occupation or profession, training may continue beyond initial competence to maintain, upgrade and update skills throughout working life. People within some professions and occupations may refer to this sort of training as professional development. Training also refers to the development of physical fitness related to a specific competence, such as sport, martial arts, military applications and some other occupations. Training methods of all types can be improved by setting specific, time-based, and difficult goals. This allows for the progressive mastery of a topic with a measured outcome.[1]

Physical training concentrates on mechanistic goals: training programs in this area develop specific motor skills, agility, strength or physical fitness, often with an intention of peaking at a particular time.

In military use, training means gaining the physical ability to perform and survive in combat, and learn the many skills needed in a time of war. These include how to use a variety of weapons, outdoor survival skills, and how to survive being captured by the enemy, among many others.  See military education and training.

For psychological or physiological reasons, people who believe it may be beneficial to them can choose to practice relaxation training, or autogenic training, in an attempt to increase their ability to relax or deal with stress.[2]
While some studies have indicated relaxation training is useful for some medical conditions, autogenic training has limited results or has been the result of few studies.

Some occupations are inherently hazardous, and require a minimum level of competence before the practitioners can perform the work at an acceptable level of safety to themselves or others in the vicinity. Occupational diving, rescue, firefighting and operation of certain types of machinery and vehicles may require assessment and certification of a minimum acceptable competence before the person is allowed to practice as a licensed instructor.

Some commentators use a similar term for workplace learning to improve performance: ""training and development"". There are also additional services available online for those who wish to receive training above and beyond what is offered by their employers. Some examples of these services include career counseling, skill assessment, and supportive services.[3] One can generally categorize such training as on-the-job or off-the-job.

The on-the-job training method takes place in a normal working situation, using the actual tools, equipment, documents or materials that trainees will use when fully trained. On-the-job training has a general reputation as most effective for vocational work.[4] It involves employees training at the place of work while they are doing the actual job. Usually, a professional trainer (or sometimes an experienced and skilled employee) serves as the instructor using hands-on practical experience which may be supported by formal classroom presentations. Sometimes training can occur by using web-based technology or video conferencing tools. On-the-job training is applicable on all departments within an organization.

Simulation based training is another method which uses technology to assist in trainee development. This is particularly common in the training of skills requiring a very high degree of practice, and in those which include a significant responsibility for life and property. An advantage is that simulation training allows the trainer to find, study, and remedy skill deficiencies in their trainees in a controlled, virtual environment. This also allows the trainees an opportunity to experience and study events that would otherwise be rare on the job, e.g., in-flight emergencies, system failure, etc., wherein the trainer can run 'scenarios' and study how the trainee reacts, thus assisting in improving his/her skills if the event was to occur in the real world. Examples of skills that commonly include simulator training during stages of development include piloting aircraft, spacecraft, locomotives, and ships, operating air traffic control airspace/sectors, power plant operations training, advanced military/defense system training, and advanced emergency response training like fire training or first-aid training.

Off-the-job training method takes place away from normal work situations — implying that the employee does not count as a directly productive worker while such training takes place. Off-the-job training method also involves employee training at a site away from the actual work environment. It often utilizes lectures, seminars, case studies, role playing, and simulation, having the advantage of allowing people to get away from work and concentrate more thoroughly on the training itself. This type of training has proven more effective in inculcating concepts and ideas[citation needed]. Many personnel selection companies offer a service which would help to improve employee competencies and change the attitude towards the job.[citation needed] The internal personnel training topics can vary from effective problem-solving skills to leadership training.

A more recent development in job training is the On-the-Job Training Plan or OJT Plan. According to the United States Department of the Interior, a proper OJT plan should include: An overview of the subjects to be covered, the number of hours the training is expected to take, an estimated completion date, and a method by which the training will be evaluated.[5]

In religious and spiritual use, the word ""training"" may refer to the purification of the mind, heart, understanding and actions to obtain a variety of spiritual goals such as (for example) closeness to God or freedom from suffering.[citation needed]  Note for example the institutionalised spiritual training of Threefold Training in Buddhism, meditation in Hinduism or discipleship in Christianity.[citation needed] These aspects of training can be short-term or can last a lifetime, depending on the context of the training and which religious group it is a part of.[citation needed]

Compare religious ritual.

Learning processes developed for artificial intelligence are typically also known as training. Evolutionary algorithms, including genetic programming and other methods of machine learning, use a system of feedback based on ""fitness functions"" to allow computer programs to determine how well an entity performs a task. The methods construct a series of programs, known as a “population” of programs, and then automatically test them for ""fitness"", observing how well they perform the intended task. The system automatically generates new programs based on members of the population that perform the best. These new members replace programs that perform the worst. The procedure repeats until the achievement of optimum performance.[6]
In robotics, such a system can continue to run in real-time after initial training, allowing robots to adapt to new situations and to changes in themselves, for example, due to wear or damage. Researchers have also developed robots that can appear to mimic simple human behavior as a starting point for training.[7]
"
Public Relations Services,"

Public relations (PR) is the practice of managing and disseminating information from an individual or an organization (such as a business, government agency, or a nonprofit organization) to the public in order to influence their perception. Public relations and publicity differ in that PR is controlled internally, whereas publicity is not controlled and contributed by external parties.[1] Public relations may include an organization or individual gaining exposure to their audiences using topics of public interest and news items that do not require direct payment.[2] The exposure is mostly media-based, and this differentiates it from advertising as a form of marketing communications. Public relations often aims to create or obtain coverage for clients for free, also known as earned media, rather than paying for marketing or advertising also known as paid media. However, advertising, especially of the type that focuses on distributing information or core PR messages, is also a part of broader PR activities.[3]

An example of public relations would be generating an article featuring a PR firm's client, rather than paying for the client to be advertised next to the article. The aim of public relations is to inform the public, prospective customers, investors, partners, employees, and other stakeholders, and persuade them to maintain a positive or favorable view about the organization, its leadership, products, or political decisions. Public relations professionals typically work for PR and marketing firms, businesses and companies, government, and public officials as public information officers and nongovernmental organizations, and nonprofit organizations. Jobs central to public relations include internal positions such as public relations coordinator, public relations specialist, and public relations manager, and outside agency positions such as account coordinator, account executive, account supervisor, and media relations manager.[4] In the UK, the equivalent job titles are Account Executive, Account Manager, Account Director and Director.

Public relations specialists establish and maintain relationships with an organization's target audiences, the media, relevant trade media, and other opinion leaders. Common responsibilities include designing communications campaigns, writing press releases and other content for news, working with the press, arranging interviews for company spokespeople, writing speeches for company leaders, acting as an organization's spokesperson, preparing clients for press conferences, media interviews and speeches, writing website and social media content, managing company reputation, crisis management, managing internal communications, and marketing activities like brand awareness and event management.[5] Success in the field of public relations requires a deep understanding of the interests and concerns of each of the company's many stakeholders. The public relations professional must know how to effectively address those concerns using the most powerful tool of the public relations trade, which is publicity.[6]

Ivy Lee, the man who turned around the Rockefeller name and image, and his friend, Edward Louis Bernays, established the first definition of public relations in the early 20th century as: ""a management function, which tabulates public attitudes, defines the policies, procedures and interests of an organization... followed by executing a program of action to earn public understanding and acceptance.""[7] However, when Lee was later asked about his role in a hearing with the United Transit Commission, he said ""I have never been able to find a satisfactory phrase to describe what I do.""[8] In 1948, historian Eric Goldman noted that the definition of public relations in Webster's Dictionary would be ""disputed by both practitioners and critics in the field.""[8]

According to Bernays, the public relations counsel is the agent working with both modern media of communications and group formations of society in order to provide ideas to the public's consciousness. Furthermore, he is also concerned with ideologies and courses of actions as well as material goods and services and public utilities and industrial associations and large trade groups for which it secures popular support.[9]


In August 1978, the World Assembly of Public Relations Associations defined the field as 
""the art and social science of analyzing trends, predicting their consequences, counselling organizational leaders and implementing planned programs of action, which will serve both the organization and the public interest.""[10]

The Public Relations Society of America,[11] a professional trade association, defined public relations in 1982 as: 
""Public relations helps an organization and its publics adapt mutually to each other.""[12]
In 2011 and 2012, the PRSA solicited crowd-supplied definitions for the term and allowed the public to vote on one of three finalists. The winning definition stated that: 

""Public relations is a strategic communication process that builds mutually beneficial relationships between organizations and their publics.""[13]
The UK-based Chartered Institute of Public Relations focuses its definition on reputation:

""Public Relations is about reputation – the result of what you do, what you say and what others say about you. Public Relations is the discipline which looks after reputation, with the aim of earning understanding and support and influencing opinion and behaviour. It is the planned and sustained effort to establish and maintain goodwill and mutual understanding between an organisation and its publics.""[14]
Public relations can also be defined as the practice of managing communication between an organization and its publics.[15]

Quentin Langley argues the use of the word ""publics"" in the plural is ""central to the understanding"" of public relations, writing ""all organisations have a series of publics, or stakeholders, on whom their success depends"".[16] He follows Roger Hayward (1991)[17] in dividing the publics into ""customers (past, present, and future), staff (past, present, and future), investors (past, present, and future), politicians and regulators, neighbours, and business partners (suppliers, distributors, etc.)"". Langley also contests the marketing perspective of seeing public relations as part of marketing, which he claims is too focused on just one of Hayward's six publics: customers.[16]

Public relations has historical roots pre-dating the 20th century. Most textbooks regard the establishment of the ""Publicity Bureau"" in Boston in 1900 as marking the founding of a public relations profession.[18]  Academics have found early forms of public influence and communications management in ancient civilizations. Aristotle's Rhetoric, for example, explains core foundations for persuasion.[19] Evidence shows that it continued to evolve during the settling of the New World and during the movement to abolish slavery in England.[20][21] Basil Clarke is considered the founder of public relations in the United Kingdom for his establishment of ""Editorial Services"" in 1924.[22]

The United States, the United Kingdom, Germany, and others used the concept of propaganda, which later[when?] evolved into public relations, to rally domestic support and to demonize enemies during the World Wars (compare journalism). World War I (1914–1918), which affected not only military but whole populations, is considered to be ""modern propaganda's launching pad"".[23] This led to more sophisticated commercial publicity efforts as public-relations talent entered the private sector.[citation needed] Most[quantify] historians believe modern-day public relations was first established in the US by Ivy Lee (1877–1934) in 1903 when he started working as the image maker for and corporate advisor for Rockefeller. Edward Bernays (1891–1995), who handled the publicity of theatrical associations in 1913,[which?] then spread internationally.[citation needed] Meanwhile, in the nascent Soviet Russia of the 1920s, artists and poets (such as Mayakovsky[24]) engaged in public-relations campaigns for various state agencies and causes (note for example Likbez).

Many American companies with PR departments spread the practice to Europe when they set up European subsidiaries in the wake of the Marshall plan of 1948–1952.[25]

In the second half of the 20th century, public relations entered an era of professional development. Trade associations, PR news-magazines, international PR agencies, and academic principles for the profession were established. In the early 2000s, press-release services began offering social-media press releases. The Cluetrain Manifesto predicted the effect of social media in 1999.[26] As of 2024, social media has been widely used by businesses for advertising and direct engagement with customers, and is considered a necessary tool for influence.[27]

Public relations professionals present the face of an organization or individual, usually to articulate its objectives and official views on issues of relevance, primarily to the media. Public relations contributes to the way an organization is perceived by influencing the media and maintaining relationships with stakeholders. According to Jacquie L'Etang from Queen Margaret University, public relations professionals can be viewed as ""discourse workers specializing in communication and the presentation of argument and employing rhetorical strategies to achieve managerial aims.""[28]

Specific public relations disciplines include:

Building and managing relationships with those who influence an organization or individual's audiences have a central role in public relations.[29] After a public relations practitioner has been working in the field, they develop relationships with the media and other influencers that become an asset, especially for those in media relations. Media directories are also available that offer extensive lists of broadcast, print and online media that list the names of editors, deadlines and the type of contributions they may accept. Perhaps foremost among these is Cision Media Contacts Database, formerly known as Bacon's Media Directories. Media can be searched and organized in a variety of ways including by type (e.g. magazines, newspapers, radio, TV, websites/blogs), industry, and publication frequency (e.g. daily, weekly, monthly, online).

Within each PR discipline, typical activities include publicity events, speaking opportunities, press releases, newsletters, blogs, social media, press kits, and outbound communication to members of the press. Video and audio news releases (VNRs and ANRs) are often produced and distributed to TV outlets for potential use in regular program content.

A fundamental PR technique  is to identify target audience(s) and tailor messages relevant to each audience.[30] Audience targeting requires public relations professionals to have a deep understanding of the needs and desires of each audience segment they want to reach. Sometimes the interests of differing audiences and stakeholders common to a public relations effort necessitate the creation of several distinct but complementary messages. These messages however should be relevant to each other, thus creating consistency in the overall message and theme. Audience targeting tactics are important for public relations practitioners because they face all kinds of problems: low visibility, lack of public understanding, opposition from critics, and insufficient support from funding sources.[31]

On the other hand, stakeholder theory identifies people who have a stake in a given institution or issue.[32] All audiences are stakeholders (or presumptive stakeholders), but not all stakeholders are members of a target audience. For example, if a charity commissions a public relations agency to create an advertising campaign to raise money to find a cure for a disease, the charity and the people with the disease are stakeholders, but the audience is anyone who is likely to donate money. Public relations experts possess deep skills in media relations, market positioning, and branding. They are powerful agents that help clients deliver clear, unambiguous information to a target audience that matters to them.[33]

A public is any group whose members have a common interest or common values in a particular subject, such as a political party. Those members would then be considered stakeholders, which are people who have a stake or an interest in an organization or issue that potentially involves the organization or group they are interested in. The Publics in Public Relations are:

Early literature authored by James Grunig (1978) suggested that publics develop in stages determined by their levels of problem recognition, constraint recognition and involvement in addressing the issue. The theory posited that publics develop in the following stages:

Messaging is the process of creating a consistent story around: a product, person, company, or service. Messaging aims to prevent readers from receiving contradictory or confusing information that could instill doubt in their purchasing choices, or other decisions that affect the company. Brands aim to have the same problem statement, industry viewpoint, or brand perception shared across sources and media.

Digital marketing is the use of Internet tools and technologies such as search engines, Web 2.0 social bookmarking, new media relations, blogging, and social media marketing. Interactive PR allows companies and organizations to disseminate information without relying solely on mainstream publications and to communicate directly with the public, customers and prospects.

PR practitioners have always relied on the media such as TV, radio, and magazines, to promote their ideas and messages tailored specifically to a target audience. Social media marketing is not only a new way to achieve that goal, but also a continuation of a strategy that existed for decades. Lister et al. said that ""Digital media can be seen as a continuation and extension of a principal or technique that was already in place"".[36]

Social media platforms enable users to connect with audiences to build brands, increase sales, and drive website traffic. This involves publishing content on social media profiles, engaging with followers, analyzing results, and running social media advertisements. The goal is to produce content that users will share with their social network to help a company increase brand exposure and broaden customer reach. Some of the major social media platforms currently include Facebook, Instagram, Twitter, LinkedIn, Pinterest, YouTube, and Snapchat.[37]

As digital technology has evolved, the methods for measuring the effectiveness of online public relations have improved. The Public Relations Society of America, which has been developing PR strategies since 1947, has identified five steps for measuring online public relations effectiveness.

Publicists[39] can work in a host of different types of business verticals such as entertainment, technology, music, travel, television, food, consumer electronics and more. Many publicists build their career in a specific business space to leverage relationships and contacts. There are different kinds of press strategies for such as B2B (business to business) or B2C (business to consumer). Business to business publicity highlights service providers who provide services and products to other businesses. Business to Consumer publicizes products and services for regular consumers, such as toys, travel, food, entertainment, personal electronics and music.

Litigation public relations is the management of the communication process during the course of any legal dispute or adjudicatory processing so as to affect the outcome or its effect on the client's overall reputation.[40]

Public relations plays a crucial role in crisis management by helping organizations prepare for, navigate, and recover from unexpected events that threaten their reputation, operations, or stakeholders.[41] A crisis can range from natural disasters and product recalls to scandals and cybersecurity breaches. Effective crisis communication is essential to mitigate negative impacts and maintain public trust.

Public relations professionals both serve the public's interest and private interests of businesses, associations, non-profit organizations, and governments. This dual obligation gave rise to heated debates among scholars of the discipline and practitioners over its fundamental values. This conflict represents the main ethical predicament of public relations.[44] In 2000, the Public Relations Society of America (PRSA) responded to the controversy by acknowledging in its new code of ethics ""advocacy"" – for the first time – as a core value of the discipline.[44]

The field of public relations is generally highly un-regulated, but many professionals voluntarily adhere to the code of conduct of one or more professional bodies to avoid exposure for ethical violations.[45] The Chartered Institute of Public Relations, the Public Relations Society of America, and The Institute of Public Relations are a few organizations that publish an ethical code. Still, Edelman's 2003 semi-annual trust survey found that only 20 percent of survey respondents from the public believed paid communicators within a company were credible.[46] Individuals in public relations are growing increasingly concerned with their company's marketing practices, questioning whether they agree with the company's social responsibility. They seek more influence over marketing and more of a counseling and policy-making role. On the other hand, individuals in marketing are increasingly interested in incorporating publicity as a tool within the realm marketing.[47]

According to Scott Cutlip, the social justification for public relations is the right for an organization to have a fair hearing of their point of view in the public forum, but to obtain such a hearing for their ideas requires a skilled advocate.[48]

Marketing and communications strategist, Ira Gostin, believes there is a code of conduct when conducting business and using public relations. Public relations specialists have the ability to influence society. Fact-checking and presenting accurate information is necessary to maintain credibility with employers and clients.[49]

The Public Relations Society of America has established a set of fundamental guidelines that people within the public relations professions should practice and use in their business atmosphere. These values are:

Other than the ethics put in place in the United States of America there are also International ethics set to ensure proper and, legal worldwide communication. Regarding these ethics, there are broad codes used specifically for international forms of public relations, and then there are more specific forms from different countries. For example, some countries have certain associations to create ethics and standards to communication across their country.

The International Association of Business Communication (founded in 1971),[51] or also known as IABC, has its own set of ethics in order to enforce a set of guidelines that ensure communication internationality is legal, ethical, and is in good taste. Some principles that members of the board of IABC follow include.

The IABC members use the following list of ethics in order to work to improve values of communication throughout the world:[51]

Spin has been interpreted historically to mean overt deceit that is meant to manipulate the public, but since the 1950s has shifted to describing a ""polishing of the truth.""[52] Today, spin refers to providing a certain interpretation of information meant to sway public opinion.[53] Companies may use spin to create the appearance of the company or other events are going in a slightly different direction than they actually are.[52] Within the field of public relations, spin is seen as a derogatory term, interpreted by professionals as meaning blatant deceit and manipulation.[54][55] Skilled practitioners of spin are sometimes called ""spin doctors.""

In Stuart Ewen's PR! A Social History of Spin, he argues that public relations can be a real menace to democracy as it renders the public discourse powerless. Corporations are able to hire public relations professionals and transmit their messages through the media channels and exercise a huge amount of influence upon the individual who is defenseless against such a powerful force. He claims that public relations is a weapon for capitalist deception and the best way to resist is to become media literate and use critical thinking when interpreting the various mediated messages.[56]

According to Jim Hoggan, ""public relations is not by definition 'spin'. Public relations is the art of building good relationships. You do that most effectively by earning trust and goodwill among those who are important to you and your business... Spin is to public relations what manipulation is to interpersonal communications. It's a diversion whose primary effect is ultimately to undermine the central goal of building trust and nurturing a good relationship.""[57]

The techniques of spin include selectively presenting facts and quotes that support ideal positions (cherry picking), the so-called ""non-denial denial"", phrasing that in a way presumes unproven truths, euphemisms for drawing attention away from items considered distasteful, and ambiguity in public statements. Another spin technique involves careful choice of timing in the release of certain news so it can take advantage of prominent events in the news.

Negative public relations, also called dark public relations (DPR), 'black hat PR' and in some earlier writing ""Black PR"", is a process of destroying the target's reputation and/or corporate identity. The objective in DPR is to discredit someone else, who may pose a threat to the client's business or be a political rival. DPR may rely on IT security, industrial espionage, social engineering and competitive intelligence. Common techniques include using dirty secrets from the target, producing misleading facts to fool a competitor.[58][59][60][61] In politics, a decision to use negative PR is also known as negative campaigning.

The T.A.R.E.S. is a five-point test that evaluates ethical persuasion and provides boundaries in persuasive practices.

In Propaganda (1928), Bernays argued that the manipulation of public opinion was a necessary part of democracy.[63] In public relations, lobby groups are created to influence government policy, corporate policy or public opinion, typically in a way that benefits the sponsoring organization.

In fact, Bernays stresses that we are in fact dominated in almost every aspect of our lives, by a relatively small number of persons who have mastered the 'mental processes and social patterns of the masses,' which include our behavior, political and economic spheres or our morals.[64] In theory, each individual chooses his own opinion on behavior and public issues. However, in practice, it is impossible for one to study all variables and approaches of a particular question and come to a conclusion without any external influence. This is the reason why the society has agreed upon an 'invisible government' to interpret on our behalf information and narrow the choice field to a more practical scale.[65]

When a lobby group hides its true purpose and support base, it is known as a front group.[66] Front groups are a form of astroturfing, because they intend to sway the public or the government without disclosing their financial connection to corporate or political interests. They create a fake grass-roots movement by giving the appearance of a trusted organization that serves the public, when they actually serve their sponsors.

Politicians also employ public relations professionals to help project their views, policies and even personalities to their best advantages.[67][68]

Some PR firms perform reputation laundering services.  In these situations, a client will hire a PR firm to conceal unethical, corrupt, or criminal behavior. The PR firm will supply services that improve the client's reputation and obscure the client's history, such as: arranging publication of positive press, coordinating donations to charities, arranging sponsorships and advertising (such as of sports teams), arranging attendance at major social events, and recommending prominent associations that the client can join.[69][70][71]  Other mechanisms employed by PR firms on behalf of the purportedly corrupt or criminal customers include fake social media accounts, blogs by fake personalities, or partisan op-eds.[72]

Notable PR firms that have engaged in reputation laundering include British PR firm Bell Pottinger, which employed reputation laundering in support of clients such as Alexander Lukashenko, Bahrain, and the Pinochet Foundation.[73]  PR firms Havas, Publicis, and Qorvis were hired by Saudi Arabia to perform reputation laundering after 9/11 and the assassination of Jamal Khashoggi.[71] Most beneficiaries of reputational laundering are politicians or politically affiliated individuals and organizations, but this type of PR can also be employed by businesses and non-politicians. 

The United Kingdom government published reports  stating that Russian oligarchs had been ""extending patronage and building influence across a wide sphere of the British establishment"" and had employed public relations firms that were ""willing beneficiaries, contributing to a ‘reputation laundering' process"".[74][75][76]
"
Event Planning Services,"Event management is the application of project management to the creation and development of small and/or large-scale personal or corporate events such as festivals, conferences, ceremonies, weddings, formal parties, concerts, or conventions. It involves studying the brand, identifying its target audience, devising the event concept, and coordinating the technical aspects before actually launching the event.[1]

The events industry now includes events of all sizes from the Olympics down to business breakfast meetings. Many industries, celebrities, charitable organizations, and interest groups hold events in order to market their label, build business relationships, raise money, or celebrate achievement.

The process of planning and coordinating the event is usually referred to as  event planning and which can include budgeting, scheduling, site selection, acquiring necessary permits, coordinating transportation and parking, arranging for speakers or entertainers, arranging decor, event security, catering, coordinating with third-party vendors, and emergency plans. Each event is different in its nature so process of planning and execution of each event differs on basis of the type of event.

The event manager is the person who plans and executes the event, taking responsibility for the creative, technical, and logistical elements.  This includes overall event design, brand building, marketing and communication strategy, audio-visual production, script writing, logistics, budgeting, negotiation, and client service.

Due to the complexities involved, the extensive body of knowledge required, and the rapidly changing environment, event management is frequently cited as one of the most stressful career paths, in line next to surgeons.[2]

Event management might be a tool for strategic marketing and communication, used by companies of every size. Companies can benefit from promotional events as a way to communicate with current and potential customers.  For instance, these advertising-focused events can occur as press conferences, promotional events, or product launches.

Event managers may also use traditional news media in order to target their audience, hoping to generate media coverage which will reach thousands or millions of people. They can also invite their audience to their events and reach them at the actual event.[3]

An event venue may be an onsite or offsite location. The event manager is responsible for operations at a rented event or entertainment venue as they are coordinating directly with the property owner. An event manager will monitor all aspects of the event on-site. Some of the tasks listed in the introduction may pass to the venue, but usually at a cost.

Events present substantial liability risk to organizers and venues. Consequently, most venues require the organizers to obtain blanket or event-specific general liability insurance of an amount not less than $1,000,000 per occurrence and $2,000,000 aggregate, which is the industry standard.[4][5]

Corporate event managers book event venues to host corporate meetings, conferences, networking events, trade shows, product launches, team-building retreats or training sessions in a more tailored environment.

Sustainable event management (also known as event greening) is the process used to produce an event with particular concern for environmental, economic, and social issues.[6] Sustainability in event management incorporates socially and environmentally responsible decision making into the planning, organization and implementation of, and participation in, an event. It involves including sustainable development principles and practices in all levels of event organization, and aims to ensure that an event is hosted responsibly. It represents the total package of interventions at an event, and needs to be done in an integrated manner. Event greening should start at the inception of the project, and should involve all the key role players, such as clients, organizers, venues, sub-contractors, and suppliers.[7] A recent study shows that the trend of moving events from in-person to virtual and hybrid modes can reduce the carbon footprint by 94% (virtual) and by 67% (hybrid mode with over 50% in-person participation rate due to trade-offs between the per capita carbon footprint and in-person participation level).[8]

Event management software companies provide event planning with software tools to handle many common activities such as delegate registration, hotel booking, travel booking, or allocation of exhibition floor space.

A recent trend in event technology is the use of mobile apps for events. This technology is advancing and allowing event professionals to simplify and manage intricate and simple events more effectively.[9] Mobile apps have a range of uses. They can be used to hold relatively static information such as the agenda, speaker biographies, and general FAQs. They can also encourage audience participation and engagement through interactive tools such as live voting/polling, submitting questions to speakers during Q&A, or building live interactive ""word clouds"". Mobile event apps can also be used by event organizers as a means of communication. The mobile apps help to make a better overall outcome of events and also help to remove a lot of a tedious work from event organizers.[10] Organizers can communicate with participants through the use of alerts, notifications, and push messages. They can also be used to collect feedback from the participants through the use of surveys in app. Some mobile event apps can help participants to engage with each other, with sponsors, and with the organizers with built-in networking functionality.

There are an increasing number of universities which offer training in event management in the form of both certificates and undergraduate or graduate degrees.

The University of Central Florida's Rosen College of Hospitality Management offered the first ever Bachelor of Science degree in Event Management beginning in 2006.[11][12] The program leverages core training in both hospitality, covering lodging operations, tourism, guest services, accounting, and marketing as well as event management, including sales, promotion, technology, design, risk management, and catering with electives available for specific interests, such as cruises, clubbing, wine, or trade shows.[13] Other degree programs that do not offer a full degree usually offer concentrations, such as New York University, which offers a Bachelor of Science degree in Hotel and Tourism Management with a concentration in event management.[14] The University of Florida offers a similar program as well.[15]

Because of the limited number of undergraduate degree programs available, it is not uncommon for event managers to earn their degrees in business administration, marketing, or public relations. To supplement their candidacy, persons interested in event management typically earn one or more certifications which offer specialization into particular fields. Certifications available include:
"
Catering Services,"Catering is the business of providing food services at a remote site or a site such as a hotel, hospital, pub, aircraft, cruise ship, park, festival, filming location or film studio.

The earliest account of major services being catered in the United States was an event for William Howe of Philadelphia in 1778. The event served local foods that were a hit with the attendees, who eventually popularized catering as a career. The official industry began to be recognized around the 1820’s, with the caterers being disproportionately African-American.[1] The catering business began to form around 1820, centered in Philadelphia.[1][2]

The industry began to professionalize under the reigns of Robert Bogle who is recognized as ""the originator of catering.""[2] Catering was originally done by servants of wealthy elites. Butlers and house slaves, which were often black, were in a good position to become caterers. Essentially, caterers in the 1860s were ""public butlers"" as they organized and executed the food aspect of a social gathering. A public butler was a butler working for several households. Bogle took on the role of public butler and took advantage of the food service market in the hospitality field.[3]

Caterers like Bogle were involved with events likely to be catered today, such as weddings and funerals.[3] Bogle also is credited with creating the Guild of Caterers and helping train other black caterers.[3] This is important because catering provided not only jobs to black people but also opportunities to connect with elite members of Philadelphia society. Over time, the clientele of caterers became the middle class, who could not afford lavish gatherings and increasing competition from white caterers led to a decline in black catering businesses.[3]

By the 1840s many restaurant owners began to combine catering services with their shops. Second-generation caterers grew the industry on the East Coast, becoming more widespread. [2] Common usage of the word ""caterer"" came about in the 1880s at which point local directories began to use these term to describe the industry.[1] White businessmen took over the industry by the 1900’s, with the Black Catering population disappearing.[1]

In the 1930s, the Soviet Union, creating more simple menus, began developing state public catering establishments as part of its collectivization policies.[4] A rationing system was implemented during World War II, and people became used to public catering. After the Second World War, many businessmen embraced catering as an alternative way of staying in business after the war.[5] By the 1960s, the home-made food was overtaken by eating in public catering establishments.[4]

By the 2000s, personal chef services started gaining popularity, with more women entering the workforce.[citation needed] People between 15 and 24 years of age spent as little as 11–17 minutes daily on food preparation and clean-up activities in 2006-2016, according to figures revealed by the American Time Use Survey conducted by the US Bureau of Labor Statistics.[6] There are many types of catering, including Event catering, Wedding Catering and Corporate Catering.

An event caterer serves food at indoor and outdoor events, including corporate and workplace events and parties at home and venues.  

A mobile caterer serves food directly from a vehicle, cart or truck which is designed for the purpose.[7] Mobile catering is common at outdoor events such as concerts, workplaces, and downtown business districts. Mobile catering services require less maintenance costs when compared with other catering services. Mobile caterers may also be known as food trucks in some areas. Mobile catering is popular throughout New York City, though sometimes can be unprofitable.[8] Ice cream vans are a familiar example of a catering truck in Canada, the United States and the United Kingdom.[9]

Seat-back catering was a service offered by some charter airlines in the United Kingdom (e.g., Court Line, which introduced the idea in the early 1970s, and Dan-Air[10]) that involved embedding two meals in a single seat-back tray. ""One helping was intended for each leg of a charter flight, but Alan Murray, of Viking Aviation, had earlier revealed that 'with the ingenious use of a nail file or coin, one could open the inbound meal and have seconds'. The intention of participating airlines was to ""save money, reduce congestion in the cabin and give punters the chance to decide when to eat their meal"".[11] By requiring less galley space on board, the planes could offer more passenger seats.[12]

According to TravelUpdate's columnist, ""The Flight Detective"", ""Salads and sandwiches were the usual staples,"" and ""a small pellet of dry ice was put into the compartment for the return meal to try to keep it fresh.""[12] However, in addition to the fact that passengers on one leg were able to consume the food intended for other passengers on the following leg, there was a ""food hygiene"" problem,[11] and the concept was discontinued by 1975.[12]

A canapé caterer serves canapés at events. They have become a popular type of food at events, Christmas parties and weddings. A canapé is a type of hors d'oeuvre, a small, prepared, and often decorative food, consisting of a small piece of bread or pastry. They should be easier to pick up and not be bigger than one or two bites. The bite-sized food is usually served before the starter or main course or alone with drinks at a drinks party.

A wedding caterer provides food for a wedding reception and party, traditionally called a wedding breakfast.[13] A wedding caterer can be hired independently or can be part of a package designed by the venue.[14] Catering service providers are often skilled and experienced in preparing and serving high-quality cuisine.[15][16] They offer a diverse and rich selection of food, creating a great experience for their customers. There are many different types of wedding caterers, each with their approach to food.

Merchant ships – especially ferries, cruise liners, and large cargo ships – often carry Catering Officers. In fact, the term ""catering"" was in use in the world of the merchant marine long before it became established as a land-bound business.[citation needed]
"
Travel Services,"

A travel agency is a private retailer or public service that provides travel and tourism-related services to the general public on behalf of accommodation or travel suppliers to offer different kinds of travelling packages for each destination.

Travel agencies can provide outdoor recreation, arranging logistics for luggage and medical items delivery for travellers upon request, public transport timetables, car rentals, and bureau de change services. Travel agencies can also serve as general sales agents for airlines that do not have offices in a specific region. A travel agency's main function is to act as an agent, selling travel products and services on behalf of a supplier. They are also called Travel Advisors. They do not keep inventory in-hand unless they have pre-booked hotel rooms or cabins on a cruise ship for a group travel event such as a wedding, honeymoon, or other group event.

Travel agencies often receive commissions and other benefits and incentives from providers or may charge a fee to the end users.[1] Hotel owners and tour operators typically pay a higher commission rate to travel agencies, whereas airlines typically pay no commission.[2] The customer is normally not made aware of how much the travel agent is earning in commissions and other benefits.[3] A 2016 survey of 1,193 travel agents in the United States found that on average 78% of their revenue was from commissions and 22% was generated from fees.[4]

Travel agencies are recognized by vendors through their accreditation numbers. In the United States, the main accreditation numbers are issued by Airline Reporting Corporation, Cruise Lines International Association, International Air Transport Association.

If more than one travel agency is booking under the same accreditation number, the agency of record is called a host agency.[5] This is a popular model in the United States, with surveys show anywhere from 43-85% of leisure agencies now booking under a host agency. [6][7][8]

Travel agencies use the services of the major computer reservations systems, also known as global distribution systems (GDS), including: Amadeus CRS, Galileo GDS, Sabre, and Worldspan, which is a subsidiary of Travelport, which allow for comparison and sorting of hotel and flight rates with multiple companies.[9] Bookings made via travel agents, including online travel agents, may or may not be confirmed instantly. Unlike online travel agencies, metasearch engines and scraper sites, such as Skyscanner, Kayak.com, Rome2rio, and TripAdvisor, travel agencies may or may not have their own booking engine, and instead provide results for search queries and then divert traffic to service providers or online travel agencies for booking.[10][11][12][13] Travel agents may also work with airline consolidators.[14][15]

Some companies use technology to promote sustainable tourism and bring carbon-neutrality.[16]

A traditional travel agent may work for a travel agency or work freelance.[17][18][19] Many traditional agents prefer the term ""travel advisor"" as opposed to ""travel agent"" to emphasize their advice, expertise, and connections that are of great value.[20] While most point-to-point travel is now booked online, traditional agents specialize in niche markets such as corporate travel, luxury travel, cruises, complicated and important trips, and specialty trips.[21] Other niche markets for traditional travel agencies include travelers with disabilities, travelers over the age of 60, women traveling alone, LGBT tourism,[22] or a particular group interested in a similar activity, such as a sport.[23][24]

Helloworld Travel is an example of a franchised travel agency, giving agents access to internal systems for product and bookings.[25]

A online travel agency (OTA) uses a platform business model to generate revenue. The Expedia Group is the largest OTA globally. Booking Holdings is the second largest OTA.[26]

In many countries, all travel agencies are required to be licensed by the International Air Transport Association (IATA).[27] Many are also bonded and represented by IATA, and, for those that issue air tickets, the Air Travel Organisers' Licensing (ATOL) in the United Kingdom, and the Airlines Reporting Corporation in the United States also serve those purposes.[28] ABTA – The Travel Association the Association of Canadian Travel Agencies (ACTA) The American Society of Travel Advisors (ASTA), represent travel agencies in the United Kingdom, Canada, and the United States respectively.[29][30]

In 1758, Cox & Kings became the first travel agency in modern history.[31][32]

In 1840, the Abreu Agency was established in Porto by Bernardo Abreu, becoming the world's first agency to open its services to the public.

In 1841, Thomas Cook, a Baptist preacher who believed that alcohol was to blame for social problems, reached an agreement with the Midland Railway to organize the transportation of 500 members of his temperance movement from the Leicester Campbell Street railway station to a rally in Loughborough in exchange for a commission.[33][34] He formed Thomas Cook & Son, which later became The Thomas Cook Group. It filed bankruptcy and underwent liquidation in 2019.[35]

In 1871, Dean and Dawson was founded in the United Kingdom and in the 1950s, it was acquired by Thomas Cook.[36]

In 1870, the Polytechnic Touring Association was founded in the United Kingdom.

In 1887, Walter T. Brownell established Brownell Travel, the first travel agency in the United States, and led 10 travelers on a European tour setting sail from New York on the SS Devonia.[37]

In 1895, Baldwins Travel was founded by Alfred K Baldwin, originally a printer, bookbinder and publisher in Tunbridge Wells. Baldwins begins selling railway tickets and helping friends to travel to Europe and beyond. News spreads and the former printers slowly build a strong side-line in travel at the back of the Baldwins Stationery shop at 27 Grosvenor Road.[38]

In 1905, Nippon Travel Agency became the first travel agency in Japan.[39]

Originally, travel agencies largely catered to middle and upper-class customers but they became more commonplace with the development of commercial aviation.

In 1923, after being treated badly by a British travel agency, K. P. Chen formed what became the China Travel Service, the first travel agency in China.[40]

The industry suffered during World War II. However, the Post–World War II economic expansion in mass-market package tours resulted in the proliferation of travel agencies catering to the working class.[41]

In 1929, Intourist was formed as the official state travel agency of the Soviet Union, with the goal of convincing outsiders to visit the country.[42]

In 1931, the US trade organization ASTA (originally the American Steamship and Tourist Agents Association, now the American Society of Travel Advisors) was created.[43]

During the Cold War, travel agents were used by people from Western countries to travel behind the Iron Curtain.[44]

In 1951, the precursor to Helloworld Travel became one of the first travel agencies in Australia.

In 1955, Henderson Travel Service in Atlanta, Georgia became the first African-American-owned travel company and the first to take large groups of black American tourists to Africa.[45][46]

In the early 1980s, American Airlines' Sabre unit created a direct-to-consumer booking tool[clarification needed] for flights, hotels and cars called eAAsySabre.[47]

In 1989, with the liberalization of travel for South Koreans, Mode Tour became the first travel agency in the country.[48]

In 1991, Hotel Reservations Network, the precursor of Hotels.com, was founded. At first, hotels did not pay much in commissions.[47]

With the advent of the internet, travel agencies migrated online and underwent disintermediation by the reduction in costs caused by removing layers from the package holiday distribution network.[49]

In 1994, Travelweb.com launched as the first online directory of hotels.[50]

In 1995, Internet Travel Network sold the first airline ticket via the World Wide Web.[50]

In October 1996, Expedia.com, funded with hundreds of millions of dollars by Microsoft launched as the first large online travel agency.[47]

At the same time, Cheapflights started as a listing service for flight deals from consolidators.[47]

In 1998, Lastminute.com was founded in the United Kingdom.[50]

In 1999, Expedia went public on the Nasdaq stock exchange.  From 1999 to 2006, the number of travel agents in the United States plunged from 124,000 to 88,000 as many Americans switched to making their own travel arrangements online.[51]

Also in 1999, European airlines began eliminating or reducing commissions,[52] while Singapore Airlines did so in parts of Asia. In 2002, several airlines in the United States did the same, which led to an unsuccessful lawsuit alleging collusion among the airlines, that was decided on appeal in 2009.[53][54]

In 2007, the launch of the iPhone and related mobile apps increased travel bookings made online.[50]

In 2008, the launch of Airbnb created an online marketplace for spare bedrooms and apartments.[50]

In 2011, the launch of HotelTonight highlighted instantaneous same-day hotel room booking.[50]

In 2021, travel agency Baldwins Travel Group, which was founded in 1895 was bought by business group[55] Inc & Co.

According to the United States Bureau of Labor Statistics, in 2022, there were 66,300 people who were employed as travel agents for their full-time jobs. That number is projected to increase by 3% over the next 10 years. In 2022, the BLS lists the median travel agent salary as $46,400 per year.[56]

Host Agency Reviews lists employee salaries by compensation structure, listing the 2022 income for travel agents that earn salary + commissions (25% of travel advisor employees) at $88,909, those that earn salary/hourly only at $50,792 (44% of employee travel agents), and commission only travel employees at $21,932 (31%).[57]

However, job prospects should be best for travel agents who specialize in specific destinations or particular types of travelers. 

Several reports show that the number of people using travel agents to book travel has been increasing.[58][59][60][61]
"
Digital Marketing Services,"

Digital marketing is the component of marketing that uses the Internet and online-based digital technologies such as desktop computers, mobile phones, and other digital media and platforms to promote products and services.[2][3]

It has significantly transformed the way brands and businesses utilize technology for marketing since the 1990s and 2000s. As digital platforms became increasingly incorporated into marketing plans and everyday life,[4] and as people increasingly used digital devices instead of visiting physical shops,[5][6] digital marketing campaigns have become prevalent, employing combinations of methods. Some of these methods include: search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing, e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e-books, and optical disks and games. Digital marketing extends to non-Internet channels that provide digital media, such as television, mobile phones (SMS and MMS), callbacks, and on-hold mobile ringtones.[7]

The extension to non-Internet channels differentiates digital marketing from online marketing.[8]

Digital marketing effectively began in 1990 when the Archie search engine was created as an index for FTP sites. In the 1980s, the storage capacity of computers was already large enough to store huge volumes of customer information. Companies started choosing online techniques, such as database marketing, rather than limited list brokers.[9] Databases allowed companies to track customers' information more effectively, transforming the relationship between buyer and seller.

In the 1990s, the term digital marketing was coined.[citation needed] The first clickable banner ad, the ""You Will"" campaign by AT&T, went live in 1994, and over the first four months, 44% of all people who saw it clicked on the ad.[10][11] Early digital marketing efforts focused on simple HTML websites and the burgeoning practice of email marketing, which allowed for direct communication with consumers.[12]

In the 2000s, with increasing numbers of Internet users and the birth of the iPhone, customers began searching for products and making decisions about their needs online first, instead of consulting a salesperson, which created a new problem for the marketing department of a company.[13] In addition, a survey in 2000 in the United Kingdom found that most retailers still needed to register their own domain address.[14] These problems encouraged marketers to find new ways to integrate digital technology into market development. At the same time, PPC advertising,[expand acronym] introduced by Google AdWords in 2000, allowed businesses to target specific keywords, making digital marketing more measurable and cost-effective.[15]

The mid-2000s saw the emergence of social media platforms like Facebook (2004), YouTube (2005), and Twitter (2006). These platforms revolutionized digital marketing by facilitating direct and interactive engagement with consumers. In 2007, marketing automation was developed as a response to the ever-evolving marketing climate. Marketing automation is the process by which software is used to automate conventional marketing processes.[16] Marketing automation helps companies segment customers, launch multichannel marketing campaigns, and provide personalized information for customers.,[16] based on their specific activities. In this way, users' activity (or lack thereof) triggers a personal message that is customized to the user in their preferred platform. However, despite the benefits of marketing automation many companies are struggling to adapt it to their everyday uses correctly.[17][page needed]

Digital marketing became more sophisticated in the 2000s and the 2010s, 
when[18][19] the proliferation of devices capable of accessing digital media led to sudden growth.[20] Statistics produced in 2012 and 2013 showed that digital marketing was still growing.[21][22]
With the development of social media in the 2000s, such as LinkedIn, Facebook, YouTube, and Twitter, consumers became highly dependent on digital electronics in their daily lives. Therefore, they expected a seamless user experience across different channels for searching product information. The change in customer behavior improved the diversification of marketing technology.[23]

Digital media growth was estimated at 4.5 trillion online ads served annually with digital media spending at 48% growth in 2010.[24] An increasing portion of advertising stems from businesses employing Online Behavioural Advertising (OBA) to tailor advertising for internet users, but OBA raises concerns about consumer privacy and data protection.[20]

Nonlinear marketing, a type of interactive marketing, is a long-term marketing approach that builds on businesses collecting information about an Internet user's online activities and trying to be visible in multiple areas.[25]

Unlike traditional marketing techniques, which involve direct, one-way messaging to consumers (via print, television, and radio advertising), nonlinear digital marketing strategies are centered on reaching prospective customers across multiple online channels.[26]

Combined with higher consumer knowledge and the demand for more sophisticated consumer offerings, this change has forced many businesses to rethink their outreach strategy and adopt or incorporate omnichannel, nonlinear marketing techniques to maintain sufficient brand exposure, engagement, and reach.[27]

Nonlinear marketing strategies involve efforts to adapt the advertising to different platforms[28] and to tailor the advertising to different individual buyers rather than a large coherent audience.[29]

Tactics may include:

Some studies indicate that consumer responses to traditional marketing approaches are becoming less predictable for businesses.[30]  According to a 2018 study, nearly 90% of online consumers in the United States researched products and brands online before visiting the store or making a purchase.[31] The Global Web Index estimated that in 2018, a little more than 50% of consumers researched products on social media.[32] Businesses often rely on individuals portraying their products in a positive light on social media, and may adapt their marketing strategy to target people with large social media followings in order to generate such comments.[33] In this manner, businesses can use consumers to advertise their products or services, decreasing the cost for the company.[34]

One of the key objectives of modern digital marketing is to raise brand awareness, the extent to which customers and the public are familiar with and recognize a particular brand.

Enhancing brand awareness is important in digital marketing, and marketing in general, because of its impact on brand perception and consumer decision-making. According to the 2015 essay, ""Impact of Brand on Consumer Behavior"":

""Brand awareness, as one of the fundamental dimensions of brand equity, is often considered to be a prerequisite of consumers’ buying decision, as it represents the main factor for including a brand in the consideration set. Brand awareness can also influence consumers’ perceived risk assessment and their confidence in the purchase decision, due to familiarity with the brand and its characteristics.""[35]

Recent trends show that businesses and digital marketers are prioritizing brand awareness, focusing more on their digital marketing efforts on cultivating brand recognition and recall than in previous years. This is evidenced by a 2019 Content Marketing Institute study, which found that 81% of digital marketers have worked on enhancing brand recognition over the past year.[36]

Another Content Marketing Institute survey revealed that 89% of B2B marketers now believe improving brand awareness to be more important than efforts directed at increasing sales.[37]

Increasing brand awareness is a focus of digital marketing strategy for a number of reasons:

Digital marketing strategies may include the use of one or more online channels and techniques (omnichannel) to increase brand awareness among consumers.

Building brand awareness may involve such methods/tools as:

Search engine optimization techniques may be used to improve the visibility of business websites and brand-related content for common industry-related search queries.

The importance of SEO to increase brand awareness is said to correlate with the growing influence of search results and search features like featured snippets, knowledge panels, and local SEO on customer behavior.[45]

SEM, also known as PPC advertising, involves the purchase of ad space in prominent, visible positions atop search results pages and websites. Search ads have been shown to have a positive impact on brand recognition, awareness and conversions.[46]

33% of searchers who click on paid ads do so because they directly respond to their particular search query.[47]

Social media marketing has the characteristics of being in the marketing state and interacting with consumers all the time, emphasizing content and interaction skills. The marketing process needs to be monitored, analyzed, summarized and managed in real-time, and the marketing target needs to be adjusted according to the real-time feedback from the market and consumers.[48] 70% of marketers list increasing brand awareness as their number one goal for marketing on social media platforms. Facebook, Instagram, Twitter, and YouTube are listed as the top platforms currently used by social media marketing teams.[citation needed] As of 2021, LinkedIn has been added as one of the most-used social media platforms by business leaders for its professional networking capabilities.[49]

56% of marketers believe personalization content – brand-centered blogs, articles, social updates, videos, landing pages – improves brand recall and engagement.[50]

One of the major changes that occurred in traditional marketing was the ""emergence of digital marketing"", this led to the reinvention of marketing strategies in order to adapt to this major change in traditional marketing.

As digital marketing is dependent on technology which is ever-evolving and fast-changing, the same features should be expected from digital marketing developments and strategies. This portion is an attempt to qualify or segregate the notable highlights existing and being used as of press time.[when?]

To summarize, Pull digital marketing is characterized by consumers actively seeking marketing content while Push digital marketing occurs when marketers send messages without that content being actively sought by the recipients.

An important consideration today while deciding on a strategy is that the digital tools have democratized the promotional landscape.

Six principles for building online brand content:[56]

The new digital era has enabled brands to selectively target their customers that may potentially be interested in their brand or based on previous browsing interests. Businesses can now use social media to select the age range, location, gender, and interests of whom they would like their targeted post to be seen. Furthermore, based on a customer's recent search history they can be ‘followed’ on the internet so they see advertisements from similar brands, products, and services,[57] This allows businesses to target the specific customers that they know and feel will most benefit from their product or service, something that had limited capabilities up until the digital era.

Digital marketing activity is still growing across the world according to the headline global marketing index. A study published in September 2018, found that global outlays on digital marketing tactics are approaching $100 billion.[59] Digital media continues to rapidly grow. While the marketing budgets are expanding, traditional media is declining.[60] Digital media helps brands reach consumers to engage with their product or service in a personalized way. Five areas, which are outlined as current industry practices that are often ineffective are prioritizing clicks, balancing search and display, understanding mobiles, targeting, viewability, brand safety and invalid traffic, and cross-platform measurement.[61] Why these practices are ineffective and some ways around making these aspects effective are discussed surrounding the following points.

Prioritizing clicks refers to display click ads, although advantageous by being ‘simple, fast and inexpensive’ rates for display ads in 2016 is only 0.10 percent in the United States. This means one in a thousand click ads is relevant therefore having little effect. This displays that marketing companies should not just use click ads to evaluate the effectiveness of display advertisements.[61]

Balancing search and display for digital display ads is important. marketers tend to look at the last search and attribute all of the effectiveness of this. This, in turn, disregards other marketing efforts, which establish brand value within the consumer's mind. ComScore determined through drawing on data online, produced by over one hundred multichannel retailers that digital display marketing poses strengths when compared with or positioned alongside, paid search.[61] This is why it is advised that when someone clicks on a display ad the company opens a landing page, not its home page. A landing page typically has something to draw the customer in to search beyond this page. Commonly marketers see increased sales among people exposed to a search ad. But the fact of how many people you can reach with a display campaign compared to a search campaign should be considered.  Multichannel retailers have an increased reach if the display is considered in synergy with search campaigns. Overall, both search and display aspects are valued as display campaigns build awareness for the brand so that more people are likely to click on these digital ads when running a search campaign.[61]

Understanding mobile devices is a significant aspect of digital marketing because smartphones and tablets are now responsible for 64% of the time US consumers are online.[61] Apps provide a big opportunity as well as challenge for the marketers because firstly the app needs to be downloaded and secondly the person needs to actually use it. This may be difficult as ‘half the time spent on smartphone apps occurs on the individuals single most used app, and almost 85% of their time on the top four rated apps’.[61] Mobile advertising can assist in achieving a variety of commercial objectives and it is effective due to taking over the entire screen, and voice or status is likely to be considered highly. However, the message must not be seen or thought of as intrusive.[61]  Disadvantages of digital media used on mobile devices also include limited creative capabilities, and reach.  Although there are many positive aspects including the user's entitlement to select product information, digital media creating a flexible message platform and there is potential for direct selling.[62]

The number of marketing channels continues to expand, as measurement practices are growing in complexity. A cross-platform view must be used to unify audience measurement and media planning. Market researchers need to understand how the Omni-channel affects consumer's behavior, although when advertisements are on a consumer's device this does not get measured. Significant aspects to cross-platform measurement involve deduplication and understanding that you have reached an incremental level with another platform, rather than delivering more impressions against people that have previously been reached.[61] An example is ‘ESPN and comScore partnered on Project Blueprint discovering the sports broadcaster achieved a 21% increase in unduplicated daily reach thanks to digital advertising’.[61] Television and radio industries are the electronic media, which competes with digital and other technological advertising. Yet television advertising is not directly competing with online digital advertising due to being able to cross platform with digital technology. Radio also gains power through cross platforms, in online streaming content. Television and radio continue to persuade and affect the audience, across multiple platforms.[63]

Targeting, viewability, brand safety, and invalid traffic all are aspects used by marketers to help advocate digital advertising. Cookies are a form of digital advertising, which are tracking tools within desktop devices, causing difficulty, with shortcomings including deletion by web browsers, the inability to sort between multiple users of a device, inaccurate estimates for unique visitors, overstating reach, understanding frequency, problems with ad servers, which cannot distinguish between when cookies have been deleted and when consumers have not previously been exposed to an ad. Due to the inaccuracies influenced by cookies, demographics in the target market are low and vary.[61] Another element, which is affected by digital marketing, is ‘viewability’ or whether the ad was actually seen by the consumer. Many ads are not seen by a consumer and may never reach the right demographic segment. Brand safety is another issue of whether or not the ad was produced in the context of being unethical or having offensive content. Recognizing fraud when an ad is exposed is another challenge marketers face. This relates to invalid traffic as premium sites are more effective at detecting fraudulent traffic, although non-premium sites are more so the problem.[61]

Digital Marketing Channels are systems based on the Internet that can create, accelerate, and transmit product value from producer to a consumer terminal, through digital networks.[64][65] Digital marketing is facilitated by multiple Digital Marketing channels, as an advertiser one's core objective is to find channels which result in maximum two-way communication and a better overall ROI for the brand. There are multiple digital marketing channels available namely:[66]

It is important for a firm to reach out to consumers and create a two-way communication model, as digital marketing allows consumers to give back feedback to the firm on a community-based site or straight directly to the firm via email.[80] Firms should seek this long-term communication relationship by using multiple forms of channels and using promotional strategies related to their target consumer as well as word-of-mouth marketing.[80]

Possible benefits of digital marketing include:

The ICC Code has integrated rules that apply to marketing communications using digital interactive media throughout the guidelines. There is also an entirely updated section dealing with issues specific to digital interactive media techniques and platforms. Code self-regulation on the use of digital interactive media includes:

Digital marketing planning is a term used in marketing management.  It describes the first stage of forming a digital marketing strategy for the wider digital marketing system. The difference between digital and traditional marketing planning is that it uses digitally based communication tools and technology such as Social, Web, Mobile, Scannable Surface.[85][86] Nevertheless, both are aligned with the vision, the mission of the company and the overarching business strategy.[87]

Dr. Dave Chaffey, an author on marketing topics, has suggested that successful digital marketing strategies have do digital marketing planning (DMP), which is a three-stage approach: Opportunity, Strategy, and Action. This generic strategic approach often has phases of situation review, goal setting, strategy formulation, resource allocation and monitoring.[87]

To create an effective DMP, a business first needs to review the marketplace and set ""SMART"" (Specific, Measurable, Actionable, Relevant, and Time-Bound) objectives.[88] They can set SMART objectives by reviewing the current benchmarks and key performance indicators (KPIs) of the company and competitors. It is pertinent that the analytics used for the KPIs be customized to the type, objectives, mission, and vision of the company.[89][90]

Companies can scan for marketing and sales opportunities by reviewing their own outreach as well as influencer outreach. This means they have competitive advantage because they are able to analyse their co-marketers influence and brand associations.[91]

To seize the opportunity, the firm should summarize its current customers' personas and purchase journey from this they are able to deduce their digital marketing capability. This means they need to form a clear picture of where they are currently and how many resources, they can allocate for their digital marketing strategy, i.e., labor, time, etc. By summarizing the purchase journey, they can also recognize gaps and growth for future marketing opportunities that will either meet objectives or propose new objectives and increase profit.

To create a planned digital strategy, the company must review their digital proposition (what you are offering to consumers) and communicate it using digital customer targeting techniques. So, they must define online value proposition (OVP), this means the company must express clearly what they are offering customers online e.g., brand positioning.

The company should also (re)select target market segments and personas and define digital targeting approaches.

After doing this effectively, it is important to review the marketing mix for online options. The marketing mix comprises the 4Ps – Product, Price, Promotion, and Place.[92][93] Some academics have added three additional elements to the traditional 4Ps of marketing Process, Place, and Physical appearance making it 7Ps of marketing.[94]

The third and final stage requires the firm to set a budget and management systems. These must be measurable touchpoints, such as the audience reached across all digital platforms. Furthermore, marketers must ensure the budget and management systems are integrating the paid, owned, and earned media of the company.[95] The Action and final stage of planning also requires the company to set in place measurable content creation e.g. oral, visual or written online media.[96]

After confirming the digital marketing plan, a scheduled format of digital communications (e.g., Gantt chart) should be encoded throughout the internal operations of the company. This ensures that all platforms used fall in line and complement each other for the succeeding stages of digital marketing strategy.

One way marketers can reach out to consumers and understand their thought process is through what is called an empathy map. An empathy map is a four-step process. The first step is through asking questions that the consumer would be thinking in their demographic. The second step is to describe the feelings that the consumer may be having. The third step is to think about what the consumer would say in their situation. The final step is to imagine what the consumer will try to do based on the other three steps. This map is so marketing teams can put themselves in their target demographics shoes.[97] Web Analytics are also a very important way to understand consumers. They show the habits that people have online for each website.[98] One particular form of these analytics is predictive analytics which helps marketers figure out what route consumers are on. This uses the information gathered from other analytics and then creates different predictions of what people will do so that companies can strategize on what to do next, according to the people's trends.[99]

The ""sharing economy"" refers to an economic pattern that aims to obtain a resource that is not fully used.[102] Nowadays, the sharing economy has had an unimagined effect on many traditional elements including labor, industry, and distribution system.[102] This effect is not negligible that some industries are obviously under threat.[102][103] The sharing economy is influencing the traditional marketing channels by changing the nature of some specific concept including ownership, assets, and recruitment.[103]

Digital marketing channels and traditional marketing channels are similar in function that the value of the product or service is passed from the original producer to the end user by a kind of supply chain.[104] Digital Marketing channels, however, consist of internet systems that create, promote, and deliver products or services from producer to consumer through digital networks.[105] Increasing changes to marketing channels has been a significant contributor to the expansion and growth of the sharing economy.[105] Such changes to marketing channels has prompted unprecedented and historic growth.[105] In addition to this typical approach, the built-in control, efficiency and low cost of digital marketing channels is an essential features in the application of sharing economy.[104]

Digital marketing channels within the sharing economy are typically divided into three domains including, e-mail, social media, and search engine marketing or SEM.[105]

Other emerging digital marketing channels, particularly branded mobile apps, have excelled in the sharing economy.[105] Branded mobile apps are created specifically to initiate engagement between customers and the company. This engagement is typically facilitated through entertainment, information, or market transaction.[105]
"
E-Commerce Services,"

E-commerce (electronic commerce) refers to commercial activities including the electronic buying or selling products and services which are conducted on online platforms or over the Internet.[1] E-commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. E-commerce is the largest sector of the electronics industry and is in turn driven by the technological advances of the semiconductor industry.

The term was coined and first employed by Robert Jacobson, Principal Consultant to the California State Assembly's Utilities & Commerce Committee, in the title and text of California's Electronic Commerce Act, carried by the late Committee Chairwoman Gwen Moore (D-L.A.) and enacted in 1984.

E-commerce typically uses the web for at least a part of a transaction's life cycle although it may also use other technologies such as e-mail. Typical e-commerce transactions include the purchase of products (such as books from Amazon) or services (such as music downloads in the form of digital distribution such as the iTunes Store).[2] There are three areas of e-commerce: online retailing, electronic markets, and online auctions. E-commerce is supported by electronic business.[3] The existence value of e-commerce is to allow consumers to shop online and pay online through the Internet, saving the time and space of customers and enterprises, greatly improving transaction efficiency, especially for busy office workers, and also saving a lot of valuable time.[4]

E-commerce businesses may also employ some or all of the following:

There are five essential categories of E-commerce:[7]

Contemporary electronic commerce can be classified into two categories. The first category is business based on types of goods sold (involves everything from ordering ""digital"" content for immediate online consumption, to ordering conventional goods and services, to ""meta"" services to facilitate other types of electronic commerce). The second category is based on the nature of the participant (B2B, B2C, C2B and C2C).[8]

On the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.

Aside from traditional e-commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce[9] have also been used.

In the United States, California's Electronic Commerce Act (1984), enacted by the Legislature, the more recent California Privacy Rights Act (2020), enacted through a popular election proposition and to control specifically how electronic commerce may be conducted in California. In the US in its entirety, electronic commerce activities are regulated more broadly by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive.[10] Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information.[11] As a result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.

The Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.[12]

Conflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996).[13]

Internationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.

There is also Asia Pacific Economic Cooperation. APEC was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.

In Australia, trade is covered under Australian Treasury Guidelines for electronic commerce and the Australian Competition & Consumer Commission[14] regulates and offers advice on how to deal with businesses online,[15] and offers specific advice on what happens if things go wrong.[16]

The European Union undertook an extensive enquiry into e-commerce in 2015–16 which observed significant growth in the development of e-commerce, along with some developments which raised concerns, such as increased use of selective distribution systems, which allow manufacturers to control routes to market, and ""increased use of contractual restrictions to better control product distribution"". The European Commission felt that some emerging practices might be justified if they could improve the quality of product distribution, but ""others may unduly prevent consumers from benefiting from greater product choice and lower prices in e-commerce and therefore warrant Commission action"" in order to promote compliance with EU competition rules.[17]

In the United Kingdom, the Financial Services Authority (FSA)[18] was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority.[19] The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.[20]

In India, the Information Technology Act 2000 governs the basic applicability of e-commerce.

In China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce.[21] On the same day, the Administrative Measures on Internet Information Services were released, the first administrative regulations to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China.[22] On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted an Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China's e-commerce legislation. It was a milestone in the course of improving China's electronic commerce legislation, and also marks the entering of China's rapid development stage for electronic commerce legislation.[23]

E-commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.[24][25]

Cross-border e-Commerce is also an essential field for e-Commerce businesses.  It has responded to the trend of globalization. It shows that numerous firms have opened up new businesses, expanded new markets, and overcome trade barriers; more and more enterprises have started exploring the cross-border cooperation field. In addition, compared with traditional cross-border trade, the information on cross-border e-commerce is more concealed. In the era of globalization, cross-border e-commerce for inter-firm companies means the activities, interactions, or social relations of two or more e-commerce enterprises. However, the success of cross-border e-commerce promotes the development of small and medium-sized firms, and it has finally become a new transaction mode. It has helped the companies solve financial problems and realize the reasonable allocation of resources field. SMEs ( small and medium enterprises) can also precisely match the demand and supply in the market, having the industrial chain majorization and creating more revenues for companies.[26]

In 2012, e-commerce sales topped $1 trillion for the first time in history.[27]

Mobile devices are playing an increasing role in the mix of e-commerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.[28]

For traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested an enormous volume of investment in mobile applications. The DeLone and McLean Model stated that three perspectives contribute to a successful e-business: information system quality, service quality and users' satisfaction.[29] There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in the company.[30]

Modern 3D graphics technologies, such as Facebook 3D Posts, are considered by some social media marketers and advertisers as a preferable way to promote consumer goods than static photos, and some brands like Sony are already paving the way for augmented reality commerce. Wayfair now lets you inspect a 3D version of its furniture in a home setting before buying.[31]

Among emerging economies, China's e-commerce presence continues to expand every year. With 668 million Internet users as of 2014, China's online shopping sales reached $253 billion in the first half of 2015, accounting for 10% of total Chinese consumer retail sales in that period.[32] The Chinese retailers have been able to help consumers feel more comfortable shopping online.[33] e-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade.[34] In 2013, Alibaba had an e-commerce market share of 80% in China.[35] In 2014, Alibaba still dominated the B2B marketplace in China with a market share of 44.82%, followed by several other companies including Made-in-China.com at 3.21%, and GlobalSources.com at 2.98%, with the total transaction value of China's B2B market exceeding 4.5 billion yuan.[36]

China is also the largest e-commerce market in the world by value of sales, with an estimated US$899 billion in 2016.[37] It accounted for 42.4% of worldwide retail e-commerce in that year, the most of any country.[38]: 110  Research shows that Chinese consumer motivations are different enough from Western audiences to require unique e-commerce app designs instead of simply porting Western apps into the Chinese market.[39]

The expansion of e-commerce in China has resulted in the development of Taobao villages, clusters of e-commerce businesses operating in rural areas.[38]: 112  Because Taobao villages have increased the incomes or rural people and entrepreneurship in rural China, Taobao villages have become a component of rural revitalization strategies.[40]: 278 

In 2015, the State Council promoted the Internet Plus initiative, a five-year plan to integrate traditional manufacturing and service industries with big data, cloud computing, and Internet of things technology.[41]: 44  The State Council provided support for Internet Plus through policy support in area including cross-border e-commerce and rural e-commerce.[41]: 44 

In 2019, the city of Hangzhou established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to e-commerce and internet-related intellectual property claims.[42]: 124 

In 2010, the United Kingdom had the highest per capita e-commerce spending in the world.[43] As of 2013, the Czech Republic was the European country where e-commerce delivers the biggest contribution to the enterprises' total revenue. Almost a quarter (24%) of the country's total turnover is generated via the online channel.[44]

The rate of growth of the number of internet users in the Arab countries has been rapid – 13.1% in 2015. A significant portion of the e-commerce market in the Middle East comprises people in the 30–34 year age group. Egypt has the largest number of internet users in the region, followed by Saudi Arabia and Morocco; these constitute 3/4th of the region's share. Yet, internet penetration is low: 35% in Egypt and 65% in Saudi Arabia.[45]

The Gulf Cooperation Council countries have a rapidly growing market and are characterized by a population that becomes wealthier (Yuldashev). As such, retailers have launched Arabic-language websites as a means to target this population. Secondly, there are predictions of increased mobile purchases and an expanding internet audience (Yuldashev). The growth and development of the two aspects make the GCC countries become larger players in the electronic commerce market with time progress. Specifically, research shows that the e-commerce market is expected to grow to over $20 billion by 2020 among these GCC countries (Yuldashev). The e-commerce market has also gained much popularity among western countries, and in particular Europe and the U.S. These countries have been highly characterized by consumer-packaged goods (CPG) (Geisler, 34). However, trends show that there are future signs of a reverse. Similar to the GCC countries, there has been increased purchase of goods and services in online channels rather than offline channels. Activist investors are trying hard to consolidate and slash their overall cost and the governments in western countries continue to impose more regulation on CPG manufacturers (Geisler, 36). In these senses, CPG investors are being forced to adapt to e-commerce as it is effective as well as a means for them to thrive.

The future trends in the GCC countries will be similar to that of the western countries. Despite the forces that push business to adapt e-commerce as a means to sell goods and products, the manner in which customers make purchases is similar in countries from these two regions. For instance, there has been an increased usage of smartphones which comes in conjunction with an increase in the overall internet audience from the regions. Yuldashev writes that consumers are scaling up to more modern technology that allows for mobile marketing.
However, the percentage of smartphone and internet users who make online purchases is expected to vary in the first few years. It will be independent on the willingness of the people to adopt this new trend (The Statistics Portal). For example, UAE has the greatest smartphone penetration of 73.8 per cent and has 91.9 per cent of its population has access to the internet. On the other hand, smartphone penetration in Europe has been reported to be at 64.7 per cent (The Statistics Portal). Regardless, the disparity in percentage between these regions is expected to level out in future because e-commerce technology is expected to grow to allow for more users.

The e-commerce business within these two regions will result in competition. Government bodies at the country level will enhance their measures and strategies to ensure sustainability and consumer protection (Krings, et al.). These increased measures will raise the environmental and social standards in the countries, factors that will determine the success of the e-commerce market in these countries. For example, an adoption of tough sanctions will make it difficult for companies to enter the e-commerce market while lenient sanctions will allow ease of companies. As such, the future trends between GCC countries and the Western countries will be independent of these sanctions (Krings, et al.). These countries need to make rational conclusions in coming up with effective sanctions.

India has an Internet user base of about 460 million as of December 2017.[46] Despite being the third largest user base in the world, the penetration of the Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around six million new entrants every month.[citation needed] In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities.[47][citation needed] The India retail market is expected to rise from 2.5% in 2016 to 5% in 2020.[48]

In 2013, Brazil's e-commerce was growing quickly with retail e-commerce sales expected to grow at a double-digit pace through 2014. By 2016, eMarketer expected retail e-commerce sales in Brazil to reach $17.3 billion.[49]

Logistics in e-commerce mainly concerns fulfillment. Online markets and retailers have to find the best possible way to fill orders and deliver products. Small companies usually control their own logistic operation because they do not have the ability to hire an outside company. Most large companies hire a fulfillment service that takes care of a company's logistic needs.[50] The optimization of logistics processes that contains long-term investment in an efficient storage infrastructure system and adoption of inventory management strategies is crucial to prioritize customer satisfaction throughout the entire process, from order placement to final delivery.[51]

E-commerce markets are growing at noticeable rates. The online market is expected to grow by 56% in 2015–2020. In 2017, retail e-commerce sales worldwide amounted to 2.3 trillion US dollars and e-retail revenues are projected to grow to 4.891 trillion US dollars in 2021.[52] Traditional markets are only expected 2% growth during the same time. Brick and mortar retailers are struggling because of online retailer's ability to offer lower prices and higher efficiency. Many larger retailers are able to maintain a presence offline and online by linking physical and online offerings.[53]

E-commerce allows customers to overcome geographical barriers and allows them to purchase products anytime and from anywhere. Online and traditional markets have different strategies for conducting business. Traditional retailers offer fewer assortment of products because of shelf space where, online retailers often hold no inventory but send customer orders directly to the manufacturer. The pricing strategies are also different for traditional and online retailers. Traditional retailers base their prices on store traffic and the cost to keep inventory. Online retailers base prices on the speed of delivery.

There are two ways for marketers to conduct business through e-commerce: fully online or online along with a brick and mortar store. Online marketers can offer lower prices, greater product selection, and high efficiency rates. Many customers prefer online markets if the products can be delivered quickly at relatively low price. However, online retailers cannot offer the physical experience that traditional retailers can. It can be difficult to judge the quality of a product without the physical experience, which may cause customers to experience product or seller uncertainty. Another issue regarding the online market is concerns about the security of online transactions. Many customers remain loyal to well-known retailers because of this issue.[54]

Security is a primary problem for e-commerce in developed and developing countries. E-commerce security is protecting businesses' websites and customers from unauthorized access, use, alteration, or destruction. The type of threats include: malicious codes, unwanted programs (ad ware, spyware), phishing, hacking, and cyber vandalism. E-commerce websites use different tools to avert security threats. These tools include firewalls, encryption software, digital certificates, and passwords.[citation needed]

For a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.[55]

E-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimized the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.[55]

In addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems, like SAP ERP, Xero, or Megaventory, have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.[55]

E-commerce helps create new job opportunities due to information related services, software app and digital products. It also causes job losses. The areas with the greatest predicted job-loss are retail, postal, and travel agencies. The development of e-commerce will create jobs that require highly skilled workers to manage large amounts of information, customer demands, and production processes. In contrast, people with poor technical skills cannot enjoy the wages welfare. On the other hand, because e-commerce requires sufficient stocks that could be delivered to customers in time, the warehouse becomes an important element. Warehouse needs more staff to manage, supervise and organize, thus the condition of warehouse environment will be concerned by employees.[56]

E-commerce brings convenience for customers as they do not have to leave home and only need to browse websites online, especially for buying products which are not sold in nearby shops. It could help customers buy a wider range of products and save customers' time. Consumers also gain power through online shopping. They are able to research products and compare prices among retailers. Thanks to the practice of user-generated ratings and reviews from companies like Bazaarvoice, Trustpilot, and Yelp, customers can also see what other people think of a product, and decide before buying if they want to spend money on it.[57][58] Also, online shopping often provides sales promotion or discounts code, thus it is more price effective for customers. Moreover, e-commerce provides products' detailed information; even the in-store staff cannot offer such detailed explanation. Customers can also review and track the order history online.

E-commerce technologies cut transaction costs by allowing both manufactures and consumers to skip through the intermediaries. This is achieved through by extending the search area best price deals and by group purchase. The success of e-commerce in urban and regional levels depend on how the local firms and consumers have adopted to e-commerce.[59]

However, e-commerce lacks human interaction for customers, especially who prefer face-to-face connection. Customers are also concerned with the security of online transactions and tend to remain loyal to well-known retailers. In recent years, clothing retailers such as Tommy Hilfiger have started adding Virtual Fit platforms to their e-commerce sites to reduce the risk of customers buying the wrong sized clothes, although these vary greatly in their fit for purpose.[60] When the customer regret the purchase of a product, it involves returning goods and refunding process. This process is inconvenient as customers need to pack and post the goods. If the products are expensive, large or fragile, it refers to safety issues.[53]

In 2018, E-commerce generated 1.3 million short tons (1.2 megatonnes) of container cardboard in North America, an increase from 1.1 million (1.00)) in 2017. Only 35 percent of North American cardboard manufacturing capacity is from recycled content. The recycling rate in Europe is 80 percent and Asia is 93 percent. Amazon, the largest user of boxes, has a strategy to cut back on packing material and has reduced packaging material used by 19 percent by weight since 2016. Amazon is requiring retailers to manufacture their product packaging in a way that does not require additional shipping packaging. Amazon also has an 85-person team researching ways to reduce and improve their packaging and shipping materials.[61]

Accelerated movement of packages around the world includes accelerated movement of living things, with all its attendant risks.[62] Weeds, pests, and diseases all sometimes travel in packages of seeds.[62] Some of these packages are part of brushing manipulation of e-commerce reviews.[62]

E-commerce has been cited as a major force for the failure of major U.S. retailers in a trend frequently referred to as a ""retail apocalypse.""[63] The rise of e-commerce outlets like Amazon has made it harder for traditional retailers to attract customers to their stores and forced companies to change their sales strategies. Many companies have turned to sales promotions and increased digital efforts to lure shoppers while shutting down brick-and-mortar locations.[64] The trend has forced some traditional retailers to shutter its brick and mortar operations.[65]

In March 2020, global retail website traffic hit 14.3 billion visits[66] signifying an unprecedented growth of e-commerce during the lockdown of 2020. Later studies show that online sales increased by 25% and online grocery shopping increased by over 100% during the crisis in the United States.[67] Meanwhile, as many as 29% of surveyed shoppers state that they will never go back to shopping in person again; in the UK, 43% of consumers state that they expect to keep on shopping the same way even after the lockdown is over.[68]

Retail sales of e-commerce shows that COVID-19 has a significant impact on e-commerce and its sales are expected to reach $6.5 trillion by 2023.[69]

Some common applications related to electronic commerce are:

A timeline for the development of e-commerce:
"
Online Marketing Services,"

Digital marketing is the component of marketing that uses the Internet and online-based digital technologies such as desktop computers, mobile phones, and other digital media and platforms to promote products and services.[2][3]

It has significantly transformed the way brands and businesses utilize technology for marketing since the 1990s and 2000s. As digital platforms became increasingly incorporated into marketing plans and everyday life,[4] and as people increasingly used digital devices instead of visiting physical shops,[5][6] digital marketing campaigns have become prevalent, employing combinations of methods. Some of these methods include: search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing, e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e-books, and optical disks and games. Digital marketing extends to non-Internet channels that provide digital media, such as television, mobile phones (SMS and MMS), callbacks, and on-hold mobile ringtones.[7]

The extension to non-Internet channels differentiates digital marketing from online marketing.[8]

Digital marketing effectively began in 1990 when the Archie search engine was created as an index for FTP sites. In the 1980s, the storage capacity of computers was already large enough to store huge volumes of customer information. Companies started choosing online techniques, such as database marketing, rather than limited list brokers.[9] Databases allowed companies to track customers' information more effectively, transforming the relationship between buyer and seller.

In the 1990s, the term digital marketing was coined.[citation needed] The first clickable banner ad, the ""You Will"" campaign by AT&T, went live in 1994, and over the first four months, 44% of all people who saw it clicked on the ad.[10][11] Early digital marketing efforts focused on simple HTML websites and the burgeoning practice of email marketing, which allowed for direct communication with consumers.[12]

In the 2000s, with increasing numbers of Internet users and the birth of the iPhone, customers began searching for products and making decisions about their needs online first, instead of consulting a salesperson, which created a new problem for the marketing department of a company.[13] In addition, a survey in 2000 in the United Kingdom found that most retailers still needed to register their own domain address.[14] These problems encouraged marketers to find new ways to integrate digital technology into market development. At the same time, PPC advertising,[expand acronym] introduced by Google AdWords in 2000, allowed businesses to target specific keywords, making digital marketing more measurable and cost-effective.[15]

The mid-2000s saw the emergence of social media platforms like Facebook (2004), YouTube (2005), and Twitter (2006). These platforms revolutionized digital marketing by facilitating direct and interactive engagement with consumers. In 2007, marketing automation was developed as a response to the ever-evolving marketing climate. Marketing automation is the process by which software is used to automate conventional marketing processes.[16] Marketing automation helps companies segment customers, launch multichannel marketing campaigns, and provide personalized information for customers.,[16] based on their specific activities. In this way, users' activity (or lack thereof) triggers a personal message that is customized to the user in their preferred platform. However, despite the benefits of marketing automation many companies are struggling to adapt it to their everyday uses correctly.[17][page needed]

Digital marketing became more sophisticated in the 2000s and the 2010s, 
when[18][19] the proliferation of devices capable of accessing digital media led to sudden growth.[20] Statistics produced in 2012 and 2013 showed that digital marketing was still growing.[21][22]
With the development of social media in the 2000s, such as LinkedIn, Facebook, YouTube, and Twitter, consumers became highly dependent on digital electronics in their daily lives. Therefore, they expected a seamless user experience across different channels for searching product information. The change in customer behavior improved the diversification of marketing technology.[23]

Digital media growth was estimated at 4.5 trillion online ads served annually with digital media spending at 48% growth in 2010.[24] An increasing portion of advertising stems from businesses employing Online Behavioural Advertising (OBA) to tailor advertising for internet users, but OBA raises concerns about consumer privacy and data protection.[20]

Nonlinear marketing, a type of interactive marketing, is a long-term marketing approach that builds on businesses collecting information about an Internet user's online activities and trying to be visible in multiple areas.[25]

Unlike traditional marketing techniques, which involve direct, one-way messaging to consumers (via print, television, and radio advertising), nonlinear digital marketing strategies are centered on reaching prospective customers across multiple online channels.[26]

Combined with higher consumer knowledge and the demand for more sophisticated consumer offerings, this change has forced many businesses to rethink their outreach strategy and adopt or incorporate omnichannel, nonlinear marketing techniques to maintain sufficient brand exposure, engagement, and reach.[27]

Nonlinear marketing strategies involve efforts to adapt the advertising to different platforms[28] and to tailor the advertising to different individual buyers rather than a large coherent audience.[29]

Tactics may include:

Some studies indicate that consumer responses to traditional marketing approaches are becoming less predictable for businesses.[30]  According to a 2018 study, nearly 90% of online consumers in the United States researched products and brands online before visiting the store or making a purchase.[31] The Global Web Index estimated that in 2018, a little more than 50% of consumers researched products on social media.[32] Businesses often rely on individuals portraying their products in a positive light on social media, and may adapt their marketing strategy to target people with large social media followings in order to generate such comments.[33] In this manner, businesses can use consumers to advertise their products or services, decreasing the cost for the company.[34]

One of the key objectives of modern digital marketing is to raise brand awareness, the extent to which customers and the public are familiar with and recognize a particular brand.

Enhancing brand awareness is important in digital marketing, and marketing in general, because of its impact on brand perception and consumer decision-making. According to the 2015 essay, ""Impact of Brand on Consumer Behavior"":

""Brand awareness, as one of the fundamental dimensions of brand equity, is often considered to be a prerequisite of consumers’ buying decision, as it represents the main factor for including a brand in the consideration set. Brand awareness can also influence consumers’ perceived risk assessment and their confidence in the purchase decision, due to familiarity with the brand and its characteristics.""[35]

Recent trends show that businesses and digital marketers are prioritizing brand awareness, focusing more on their digital marketing efforts on cultivating brand recognition and recall than in previous years. This is evidenced by a 2019 Content Marketing Institute study, which found that 81% of digital marketers have worked on enhancing brand recognition over the past year.[36]

Another Content Marketing Institute survey revealed that 89% of B2B marketers now believe improving brand awareness to be more important than efforts directed at increasing sales.[37]

Increasing brand awareness is a focus of digital marketing strategy for a number of reasons:

Digital marketing strategies may include the use of one or more online channels and techniques (omnichannel) to increase brand awareness among consumers.

Building brand awareness may involve such methods/tools as:

Search engine optimization techniques may be used to improve the visibility of business websites and brand-related content for common industry-related search queries.

The importance of SEO to increase brand awareness is said to correlate with the growing influence of search results and search features like featured snippets, knowledge panels, and local SEO on customer behavior.[45]

SEM, also known as PPC advertising, involves the purchase of ad space in prominent, visible positions atop search results pages and websites. Search ads have been shown to have a positive impact on brand recognition, awareness and conversions.[46]

33% of searchers who click on paid ads do so because they directly respond to their particular search query.[47]

Social media marketing has the characteristics of being in the marketing state and interacting with consumers all the time, emphasizing content and interaction skills. The marketing process needs to be monitored, analyzed, summarized and managed in real-time, and the marketing target needs to be adjusted according to the real-time feedback from the market and consumers.[48] 70% of marketers list increasing brand awareness as their number one goal for marketing on social media platforms. Facebook, Instagram, Twitter, and YouTube are listed as the top platforms currently used by social media marketing teams.[citation needed] As of 2021, LinkedIn has been added as one of the most-used social media platforms by business leaders for its professional networking capabilities.[49]

56% of marketers believe personalization content – brand-centered blogs, articles, social updates, videos, landing pages – improves brand recall and engagement.[50]

One of the major changes that occurred in traditional marketing was the ""emergence of digital marketing"", this led to the reinvention of marketing strategies in order to adapt to this major change in traditional marketing.

As digital marketing is dependent on technology which is ever-evolving and fast-changing, the same features should be expected from digital marketing developments and strategies. This portion is an attempt to qualify or segregate the notable highlights existing and being used as of press time.[when?]

To summarize, Pull digital marketing is characterized by consumers actively seeking marketing content while Push digital marketing occurs when marketers send messages without that content being actively sought by the recipients.

An important consideration today while deciding on a strategy is that the digital tools have democratized the promotional landscape.

Six principles for building online brand content:[56]

The new digital era has enabled brands to selectively target their customers that may potentially be interested in their brand or based on previous browsing interests. Businesses can now use social media to select the age range, location, gender, and interests of whom they would like their targeted post to be seen. Furthermore, based on a customer's recent search history they can be ‘followed’ on the internet so they see advertisements from similar brands, products, and services,[57] This allows businesses to target the specific customers that they know and feel will most benefit from their product or service, something that had limited capabilities up until the digital era.

Digital marketing activity is still growing across the world according to the headline global marketing index. A study published in September 2018, found that global outlays on digital marketing tactics are approaching $100 billion.[59] Digital media continues to rapidly grow. While the marketing budgets are expanding, traditional media is declining.[60] Digital media helps brands reach consumers to engage with their product or service in a personalized way. Five areas, which are outlined as current industry practices that are often ineffective are prioritizing clicks, balancing search and display, understanding mobiles, targeting, viewability, brand safety and invalid traffic, and cross-platform measurement.[61] Why these practices are ineffective and some ways around making these aspects effective are discussed surrounding the following points.

Prioritizing clicks refers to display click ads, although advantageous by being ‘simple, fast and inexpensive’ rates for display ads in 2016 is only 0.10 percent in the United States. This means one in a thousand click ads is relevant therefore having little effect. This displays that marketing companies should not just use click ads to evaluate the effectiveness of display advertisements.[61]

Balancing search and display for digital display ads is important. marketers tend to look at the last search and attribute all of the effectiveness of this. This, in turn, disregards other marketing efforts, which establish brand value within the consumer's mind. ComScore determined through drawing on data online, produced by over one hundred multichannel retailers that digital display marketing poses strengths when compared with or positioned alongside, paid search.[61] This is why it is advised that when someone clicks on a display ad the company opens a landing page, not its home page. A landing page typically has something to draw the customer in to search beyond this page. Commonly marketers see increased sales among people exposed to a search ad. But the fact of how many people you can reach with a display campaign compared to a search campaign should be considered.  Multichannel retailers have an increased reach if the display is considered in synergy with search campaigns. Overall, both search and display aspects are valued as display campaigns build awareness for the brand so that more people are likely to click on these digital ads when running a search campaign.[61]

Understanding mobile devices is a significant aspect of digital marketing because smartphones and tablets are now responsible for 64% of the time US consumers are online.[61] Apps provide a big opportunity as well as challenge for the marketers because firstly the app needs to be downloaded and secondly the person needs to actually use it. This may be difficult as ‘half the time spent on smartphone apps occurs on the individuals single most used app, and almost 85% of their time on the top four rated apps’.[61] Mobile advertising can assist in achieving a variety of commercial objectives and it is effective due to taking over the entire screen, and voice or status is likely to be considered highly. However, the message must not be seen or thought of as intrusive.[61]  Disadvantages of digital media used on mobile devices also include limited creative capabilities, and reach.  Although there are many positive aspects including the user's entitlement to select product information, digital media creating a flexible message platform and there is potential for direct selling.[62]

The number of marketing channels continues to expand, as measurement practices are growing in complexity. A cross-platform view must be used to unify audience measurement and media planning. Market researchers need to understand how the Omni-channel affects consumer's behavior, although when advertisements are on a consumer's device this does not get measured. Significant aspects to cross-platform measurement involve deduplication and understanding that you have reached an incremental level with another platform, rather than delivering more impressions against people that have previously been reached.[61] An example is ‘ESPN and comScore partnered on Project Blueprint discovering the sports broadcaster achieved a 21% increase in unduplicated daily reach thanks to digital advertising’.[61] Television and radio industries are the electronic media, which competes with digital and other technological advertising. Yet television advertising is not directly competing with online digital advertising due to being able to cross platform with digital technology. Radio also gains power through cross platforms, in online streaming content. Television and radio continue to persuade and affect the audience, across multiple platforms.[63]

Targeting, viewability, brand safety, and invalid traffic all are aspects used by marketers to help advocate digital advertising. Cookies are a form of digital advertising, which are tracking tools within desktop devices, causing difficulty, with shortcomings including deletion by web browsers, the inability to sort between multiple users of a device, inaccurate estimates for unique visitors, overstating reach, understanding frequency, problems with ad servers, which cannot distinguish between when cookies have been deleted and when consumers have not previously been exposed to an ad. Due to the inaccuracies influenced by cookies, demographics in the target market are low and vary.[61] Another element, which is affected by digital marketing, is ‘viewability’ or whether the ad was actually seen by the consumer. Many ads are not seen by a consumer and may never reach the right demographic segment. Brand safety is another issue of whether or not the ad was produced in the context of being unethical or having offensive content. Recognizing fraud when an ad is exposed is another challenge marketers face. This relates to invalid traffic as premium sites are more effective at detecting fraudulent traffic, although non-premium sites are more so the problem.[61]

Digital Marketing Channels are systems based on the Internet that can create, accelerate, and transmit product value from producer to a consumer terminal, through digital networks.[64][65] Digital marketing is facilitated by multiple Digital Marketing channels, as an advertiser one's core objective is to find channels which result in maximum two-way communication and a better overall ROI for the brand. There are multiple digital marketing channels available namely:[66]

It is important for a firm to reach out to consumers and create a two-way communication model, as digital marketing allows consumers to give back feedback to the firm on a community-based site or straight directly to the firm via email.[80] Firms should seek this long-term communication relationship by using multiple forms of channels and using promotional strategies related to their target consumer as well as word-of-mouth marketing.[80]

Possible benefits of digital marketing include:

The ICC Code has integrated rules that apply to marketing communications using digital interactive media throughout the guidelines. There is also an entirely updated section dealing with issues specific to digital interactive media techniques and platforms. Code self-regulation on the use of digital interactive media includes:

Digital marketing planning is a term used in marketing management.  It describes the first stage of forming a digital marketing strategy for the wider digital marketing system. The difference between digital and traditional marketing planning is that it uses digitally based communication tools and technology such as Social, Web, Mobile, Scannable Surface.[85][86] Nevertheless, both are aligned with the vision, the mission of the company and the overarching business strategy.[87]

Dr. Dave Chaffey, an author on marketing topics, has suggested that successful digital marketing strategies have do digital marketing planning (DMP), which is a three-stage approach: Opportunity, Strategy, and Action. This generic strategic approach often has phases of situation review, goal setting, strategy formulation, resource allocation and monitoring.[87]

To create an effective DMP, a business first needs to review the marketplace and set ""SMART"" (Specific, Measurable, Actionable, Relevant, and Time-Bound) objectives.[88] They can set SMART objectives by reviewing the current benchmarks and key performance indicators (KPIs) of the company and competitors. It is pertinent that the analytics used for the KPIs be customized to the type, objectives, mission, and vision of the company.[89][90]

Companies can scan for marketing and sales opportunities by reviewing their own outreach as well as influencer outreach. This means they have competitive advantage because they are able to analyse their co-marketers influence and brand associations.[91]

To seize the opportunity, the firm should summarize its current customers' personas and purchase journey from this they are able to deduce their digital marketing capability. This means they need to form a clear picture of where they are currently and how many resources, they can allocate for their digital marketing strategy, i.e., labor, time, etc. By summarizing the purchase journey, they can also recognize gaps and growth for future marketing opportunities that will either meet objectives or propose new objectives and increase profit.

To create a planned digital strategy, the company must review their digital proposition (what you are offering to consumers) and communicate it using digital customer targeting techniques. So, they must define online value proposition (OVP), this means the company must express clearly what they are offering customers online e.g., brand positioning.

The company should also (re)select target market segments and personas and define digital targeting approaches.

After doing this effectively, it is important to review the marketing mix for online options. The marketing mix comprises the 4Ps – Product, Price, Promotion, and Place.[92][93] Some academics have added three additional elements to the traditional 4Ps of marketing Process, Place, and Physical appearance making it 7Ps of marketing.[94]

The third and final stage requires the firm to set a budget and management systems. These must be measurable touchpoints, such as the audience reached across all digital platforms. Furthermore, marketers must ensure the budget and management systems are integrating the paid, owned, and earned media of the company.[95] The Action and final stage of planning also requires the company to set in place measurable content creation e.g. oral, visual or written online media.[96]

After confirming the digital marketing plan, a scheduled format of digital communications (e.g., Gantt chart) should be encoded throughout the internal operations of the company. This ensures that all platforms used fall in line and complement each other for the succeeding stages of digital marketing strategy.

One way marketers can reach out to consumers and understand their thought process is through what is called an empathy map. An empathy map is a four-step process. The first step is through asking questions that the consumer would be thinking in their demographic. The second step is to describe the feelings that the consumer may be having. The third step is to think about what the consumer would say in their situation. The final step is to imagine what the consumer will try to do based on the other three steps. This map is so marketing teams can put themselves in their target demographics shoes.[97] Web Analytics are also a very important way to understand consumers. They show the habits that people have online for each website.[98] One particular form of these analytics is predictive analytics which helps marketers figure out what route consumers are on. This uses the information gathered from other analytics and then creates different predictions of what people will do so that companies can strategize on what to do next, according to the people's trends.[99]

The ""sharing economy"" refers to an economic pattern that aims to obtain a resource that is not fully used.[102] Nowadays, the sharing economy has had an unimagined effect on many traditional elements including labor, industry, and distribution system.[102] This effect is not negligible that some industries are obviously under threat.[102][103] The sharing economy is influencing the traditional marketing channels by changing the nature of some specific concept including ownership, assets, and recruitment.[103]

Digital marketing channels and traditional marketing channels are similar in function that the value of the product or service is passed from the original producer to the end user by a kind of supply chain.[104] Digital Marketing channels, however, consist of internet systems that create, promote, and deliver products or services from producer to consumer through digital networks.[105] Increasing changes to marketing channels has been a significant contributor to the expansion and growth of the sharing economy.[105] Such changes to marketing channels has prompted unprecedented and historic growth.[105] In addition to this typical approach, the built-in control, efficiency and low cost of digital marketing channels is an essential features in the application of sharing economy.[104]

Digital marketing channels within the sharing economy are typically divided into three domains including, e-mail, social media, and search engine marketing or SEM.[105]

Other emerging digital marketing channels, particularly branded mobile apps, have excelled in the sharing economy.[105] Branded mobile apps are created specifically to initiate engagement between customers and the company. This engagement is typically facilitated through entertainment, information, or market transaction.[105]
"
Content Creation Services,"

Content creation or content creative is the act of producing and sharing information or media content for specific audiences, particularly in digital contexts. According to Dictionary.com, content refers to ""something that is to be expressed through some medium, as speech, writing or any of various arts""[1] for self-expression, distribution, marketing and/or publication. Content creation encompasses various activities including maintaining and updating web sites, blogging, article writing, photography, videography, online commentary, social media accounts, and editing and distribution of digital media. In a survey conducted by the Pew Research Center, content creation was defined as ""the material people contribute to the online world"".[2]

News organizations, especially those with a large and global reach like The New York Times, NPR, and CNN, consistently create some of the most shared content on the Web, especially in relation to current events. In the words of a 2011 report from the Oxford School for the Study of Journalism and the Reuters Institute for the Study of Journalism, ""Mainstream media is the lifeblood of topical social media conversations in the UK.""[3] While the rise of digital media has disrupted traditional news outlets, many have adapted and have begun to produce content that is designed to function on the web and be shared on social media. The social media site Twitter is a major distributor and aggregator of breaking news from various sources, and the function and value of Twitter in the distribution of news is a frequent topic of discussion and research in journalism.[4] User-generated content, social media blogging and citizen journalism have changed the nature of news content in recent years.[5] The company Narrative Science is now using artificial intelligence to produce news articles and interpret data.[6]

Academic institutions, such as colleges and universities, create content in the form of books, journal articles, white papers, and some forms of digital scholarship, such as blogs that are group edited by academics, class wikis, or video lectures that support a massive open online course (MOOC). Through an open data initiative, institutions may make raw data supporting their experiments or conclusions available on the Web. Academic content may be gathered and made accessible to other academics or the public through publications, databases, libraries, and digital libraries. Academic content may be closed source or open access (OA). Closed-source content is only available to authorized users or subscribers. For example, an important journal or a scholarly database may be a closed source, available only to students and faculty through the institution's library. Open-access articles are open to the public, with the publication and distribution costs shouldered by the institution publishing the content.

Corporate content includes advertising and public relations content, as well as other types of content produced for profit, including white papers and sponsored research. Advertising can also include auto-generated content, with blocks of content generated by programs or bots for search engine optimization.[7] Companies also create annual reports which are part of their company's workings and a detailed review of their financial year. This gives the stakeholders of the company insight into the company's current and future prospects and direction.[8]

Cultural works, like music, movies, literature, and art, are also major forms of content. Examples include traditionally published books and e-books as well as self-published books, digital art, fanfiction, and fan art. Independent artists, including authors and musicians, have found commercial success by making their work available on the Internet.[9]

Through digitization, sunshine laws, open records laws and data collection, governments may make statistical, legal or regulatory information available on the Internet. National libraries and state archives turn historical documents, public records, and unique relics into online databases and exhibits. This has raised significant privacy issues.[10] In 2012, The Journal News, a New York state paper, sparked an outcry when it published an interactive map of the state's gun owner locations using legally obtained public records.[11] Governments also create online or digital propaganda or misinformation to support domestic and international goals. This can include astroturfing, or using media to create a false impression of mainstream belief or opinion.[12]

Governments can also use open content, such as public records and open data, in service of public health, educational and scientific goals, such as crowdsourcing solutions to complex policy problems.[13] In 2013, the National Aeronautics and Space Administration (NASA) joined the asteroid mining company Planetary Resources to crowdsource the hunt for near-Earth objects.[14] Describing NASA's crowdsourcing work in an interview, technology transfer executive David Locke spoke of the ""untapped cognitive surplus that exists in the world"" which could be used to help develop NASA technology.[15] In addition to making governments more participatory, open records and open data have the potential to make governments more transparent and less corrupt.[16]

The introduction of Web 2.0 made it possible for content consumers to be more involved in the generation and sharing of content. With the advent of digital media, the amount of user generated content, as well as the age and class range of users, has increased. 8% of Internet users are very active in content creation and consumption.[17] Worldwide, about one in four Internet users are significant content creators,[18] and users in emerging markets lead the world in engagement.[19] Research has also found that young adults of a higher socioeconomic background tend to create more content than those from lower socioeconomic backgrounds.[20] 69% of American and European internet users are ""spectators"", who consume—but do not create—online and digital media.[19] The ratio of content creators to the amount of content they generate is sometimes referred to as the 1% rule, a rule of thumb that suggests that only 1% of a forum's users create nearly all of its content. Motivations for creating new content may include the desire to gain new knowledge, the possibility of publicity, or simple altruism.[21] Users may also create new content in order to bring about social reforms. However, researchers caution that in order to be effective, context must be considered, a diverse array of people must be included, and all users must participate throughout the process.[22]

According to a 2011 study, minorities create content in order to connect with their communities online. African-American users have been found to create content as a means of self-expression that was not previously available. Media portrayals of minorities are sometimes inaccurate and stereotypical which affects the general perception of these minorities.[23] African-Americans respond to their portrayals digitally through the use of social media such as Twitter and Tumblr. The creation of Black Twitter has allowed a community to share their problems and ideas.[24]

Younger users now have greater access to content, content creating applications, and the ability to publish to different types of media, such as Facebook, Blogger, Instagram, DeviantArt, or Tumblr.[25] As of 2005, around 21 million teens used the internet and 57%, or 12 million teens, consider themselves content creators.[26] This proportion of media creation and sharing is higher than that of adults. With the advent of the Internet, teens have had more access to tools for sharing and creating content. Increase in accessibility to technology, especially due to lower prices, has led to an increase in accessibility of content creation tools as well for teens.[27] Some teens use this to become content creators through online platforms like YouTube, while others use it to connect to friends through social networking sites.[28]

The rise of anonymous and user-generated content presents both opportunities and challenges to Web users. Blogging, self-publishing and other forms of content creation give more people access to larger audiences. However, this can also perpetuate rumors and lead to misinformation. It can make it more difficult for users to find content that meets their information needs.

The feature of user-generated content and personalized recommendation algorithms of digital media also gives a rise to confirmation bias. Users may tend to seek out information that confirms their existing beliefs and ignore information that contradicts them. This can lead to one-sided, unbalanced content that does not present a complete picture of an issue.

The quality of digital contents varies from traditional academic or published writing. Digital media writing is often more engaging and accessible to a broader audience than academic writing, which is usually intended for a specialized audience. Digital media writers often use a conversational tone, personal anecdotes, and multimedia elements like images and videos to enhance the reader's experience. For example, the veteran populist anti-EU campaigner Farage's tweets in 2017–2018 used a lot of colloquial expressions and catchphrases to resonate the ""common sense"" with audiences.[29]

At the same time, digital media is also necessary for professional (academic) communicators to reach an audience,[30] as well as with connecting to scholars in their areas of expertise.[31]

The quality of digital contents is also influenced by capitalism and market-driven consumerism.[32] Writers may have commercial interests that influence the content they produce. For example, a writer who is paid to promote a particular product or service may write articles that are biased in favor of that product or service, even if it is not the best option for the reader. 

Digital content is difficult to organize and categorize. Websites, forums, and publishers all have different standards for metadata, or information about the content, such as its author and date of creation. The perpetuation of different standards of metadata can create problems of accessibility and discoverability.

Digital writing and content creation has evolved significantly. This has led to various ethical issues, including privacy, individual rights, and representation.[33] A focus on cultural identity has helped increase accessibility, empowerment, and social justice in digital media, but might also prevent users from freely communicating and expressing.[33]

The ownership, origin, and right to share digital content can be difficult to establish. User-generated content presents challenges to traditional content creators (professional writers, artists, filmmakers, musicians, choreographers, etc.) with regard to the expansion of unlicensed and unauthorized derivative works, piracy and plagiarism. Also, the enforcement of copyright laws, such as the Digital Millennium Copyright Act in the U.S., makes it less likely that works will fall into the public domain.

Content creation serves as a useful form of protest on social media platforms. The 2011 Egyptian revolution was one example of content creation being used to network protestors globally for the common cause of protesting the ""authoritarian regimes in the Middle East and North Africa throughout 2011"".[34] The protests took place in multiple cities in Egypt, and quickly evolved from peaceful protest into open conflict. Social media outlets allowed protestors from different regions to network with each other and raise awareness of the widespread corruption in Egypt's government, as well as helping coordinate their response. Youth activists promoting the rebellion were able to formulate a Facebook group, ""Progressive Youth of Tunisia"".[34]

Examples of recent social media protest through online content include the global widespread use of the hashtags #MeToo, used to raise awareness against sexual abuse, and #BlackLivesMatter, which focused on police brutality against black people.
"
Website Development Services,"

Web development is the work involved in developing a website for the Internet (World Wide Web) or an intranet (a private network).[1] Web development can range from developing a simple single static page of plain text to complex web applications, electronic businesses, and social network services. A more comprehensive list of tasks to which Web development commonly refers, may include Web engineering, Web design, Web content development, client liaison, client-side/server-side scripting, Web server and network security configuration, and e-commerce development.

Among Web professionals, ""Web development"" usually refers to the main non-design aspects of building Web sites: writing markup and coding.[2] Web development may use content management systems (CMS) to make content changes easier and available with basic technical skills.

For larger organizations and businesses, Web development teams can consist of hundreds of people (Web developers) and follow standard methods like Agile methodologies while developing Web sites.[1] Smaller organizations may only require a single permanent or contracting developer, or secondary assignment to related job positions such as a graphic designer or information systems technician. Web development may be a collaborative effort between departments rather than the domain of a designated department. There are three kinds of Web developer specialization: front-end developer, back-end developer, and full-stack developer.[3] Front-end developers are responsible for behavior and visuals that run in the user browser, while back-end developers deal with the servers.[4] Since the commercialization of the Web, the industry has boomed and has become one of the most used technologies ever.

Tim Berners-Lee created the World Wide Web in 1989 at CERN.[5]

The primary goal in the development of the Web was to fulfill the automated information-sharing needs of academics affiliated with institutions and various global organizations. Consequently, HTML was developed in 1993.[6]

Web 1.0 is described as the first paradigm wherein users could only view material and provide a small amount of information.[7] Core protocols of web 1.0 were HTTP, HTML and URI.[8]

Web 2.0, a term popularised by Dale Dougherty, then vice president of O'Reilly, during a 2004 conference with Media Live, marks a shift in internet usage, emphasizing interactivity.[9][10]

Web 2.0 introduced increased user engagement and communication. It evolved from the static, read-only nature of Web 1.0 and became an integrated network for engagement and communication. It is often referred to as a user-focused, read-write online network.[7]

In the realm of Web 2.0 environments, users now have access to a platform that encourages sharing activities such as creating music, files, images, and movies.[11] The architecture of Web 2.0 is often considered the ""backbone of the internet,"" using standardized XML (Extensible Markup Language) tags to authorize information flow from independent platforms and online databases.[7]

Web 3.0, considered the third and current version of the web, was introduced in 2014. The concept envisions a complete redesign of the web. Key features include the integration of metadata, precise information delivery, and improved user experiences based on preferences, history, and interests.[citation needed]

Web 3.0 aims to turn the web into a sizable, organized database, providing more functionality than traditional search engines. Users can customize navigation based on their preferences, and the core ideas involve identifying data sources, connecting them for efficiency, and creating user profiles.[7]

This version is sometimes also known as Semantic Web.[12]

The journey of web development technologies began with simple HTML pages in the early days of the internet. Over time, advancements led to the incorporation of CSS for styling and JavaScript for interactivity. This evolution transformed static websites into dynamic and responsive platforms, setting the stage for the complex and feature-rich web applications we have today.

Web development in future will be driven by advances in browser technology, Web internet infrastructure, protocol standards, software engineering methods, and application trends.[8]

The web development life cycle is a method that outlines the stages involved in building websites and web applications. It provides a structured approach, ensuring optimal results throughout the development process.[citation needed]

A typical Web Development process can be divided into 7 steps.

Debra Howcraft and John Carroll proposed a methodology in which web development process can be divided into sequential steps. They mentioned different aspects of analysis.[17]

Phase one involves crafting a web strategy and analyzing how a website can effectively achieve its goals. Keil et al.'s research[18] identifies the primary reasons for software project failures as a lack of top management commitment and misunderstandings of system requirements. To mitigate these risks, Phase One establishes strategic goals and objectives, designing a system to fulfill them. The decision to establish a web presence should ideally align with the organization's corporate information strategy.

The analysis phase can be divided into 3 steps:

During this phase, the previously outlined objectives and available resources undergo analysis to determine their feasibility. This analysis is divided into six tasks, as follows:

Following this analysis, a more refined set of objectives is documented. Objectives that cannot be presently fulfilled are recorded in a Wish List, constituting part of the Objectives Document. This documentation becomes integral to the iterative process during the subsequent cycle of the methodology.[17]

It is crucial for web developers to be engaged in formulating a plan and determining the optimal architecture and selecting the frameworks.[citation needed] Additionally, developers/consultants play a role in elucidating the total cost of ownership associated with supporting a website, which may surpass the initial development expenses.

Key aspects in this step are:

Following the analysis phase, the development process moves on to the design phase, which is guided by the objectives document. Recognizing the incremental growth of websites and the potential lack of good design architecture, the methodology includes iteration to account for changes and additions over the life of the site. The design phase, which is divided into Information Design and Graphic Design, results in a detailed Design Document that details the structure of the website, database data structures, and CGI scripts.*

The following step, design testing, focuses on early, low-cost testing to identify inconsistencies or flaws in the design. This entails comparing the website's design to the goals and objectives outlined in the first three steps. Phases One and Two involve an iterative loop in which objectives in the Objectives Document are revisited to ensure alignment with the design. Any objectives that are removed are added to the Wish List for future consideration.[17]

Key aspects in this step are:

No matter how visually appealing a website is, good communication with clients is critical. The primary purpose of content production is to create a communication channel through the user interface by delivering relevant information about your firm in an engaging and easily understandable manner. This includes:[citation needed]

The stage of content production is critical in establishing the branding and marketing of your website or web application. It serves as a platform for defining the purpose and goals of your online presence through compelling and convincing content.

During this critical stage, the website is built while keeping its fundamental goal in mind, paying close attention to all graphic components to assure the establishment of a completely working site.

The procedure begins with the development of the main page, which is followed by the production of interior pages. The site's navigational structure is being refined in particular.

During this development phase, key functionality such as the Content Management System, interactive contact forms, and shopping carts are activated.

The coding process includes creating all of the site's software and installing it on the appropriate Web servers. This can range from simple things like posting to a Web server to more complex tasks like establishing database connections.

In any web project, the testing phase is incredibly intricate and difficult. Because web apps are frequently designed for a diverse and often unknown user base running in a range of technological environments, their complexity exceeds that of traditional Information Systems (IS). To ensure maximum reach and efficacy, the website must be tested in a variety of contexts and technologies. The website moves to the delivery stage after gaining final approval from the designer. To ensure its preparation for launch, the quality assurance team performs rigorous testing for functionality, compatibility, and performance.

Additional testing is carried out, including integration, stress, scalability, load, resolution, and cross-browser compatibility. When the approval is given, the website is pushed to the server via FTP, completing the development process.

Key aspects in this step are:

The web development process goes beyond deployment to include a variety of post-deployment tasks.

Websites, in example, are frequently under ongoing maintenance, with new items being uploaded on a daily basis. The maintenance costs increases immensely as the site grows in size. The accuracy of content on a website is critical, demanding continuous monitoring to verify that both information and links, particularly external links, are updated. Adjustments are made in response to user feedback, and regular support and maintenance actions are carried out to maintain the website's long-term effectiveness.[17]

Debra Howcraft and John Carroll discussed a few traditional web development methodologies in their research paper:[17]

Developing a fundamental knowledge of client-side and server-side dynamics is crucial.[citation needed]

The goal of front-end development is to create a website's user interface and visual components that users may interact with directly. On the other hand, back-end development works with databases, server-side logic, and application functionality. Building reliable and user-friendly online applications requires a comprehensive approach, which is ensured by collaboration between front-end and back-end engineers.

Front-end development is the process of designing and implementing the user interface (UI) and user experience (UX) of a web application. It involves creating visually appealing and interactive elements that users interact with directly. The primary technologies and concepts associated with front-end development include:

The 3 core technologies for front-end development are:

User experience design focuses on creating interfaces that are intuitive, accessible, and enjoyable for users. It involves understanding user behavior, conducting usability studies, and implementing design principles to enhance the overall satisfaction of users interacting with a website or application. This involves wireframing, prototyping, and implementing design principles to enhance user interaction. Some of the popular tools used for UI Wireframing are -

Another key aspect to keep in mind while designing is Web Accessibility- Web accessibility ensures that digital content is available and usable for people of all abilities. This involves adhering to standards like the Web Content Accessibility Guidelines (WCAG), implementing features like alternative text for images, and designing with considerations for diverse user needs, including those with disabilities.

It is important to ensure that web applications are accessible and visually appealing across various devices and screen sizes. Responsive design uses CSS media queries and flexible layouts to adapt to different viewing environments.

A framework is a high-level solution for the reuse of software pieces, a step forward in simple library-based reuse that allows for sharing common functions and generic logic of a domain application.[19]

Frameworks and libraries are essential tools that expedite the development process. These tools enhance developer productivity and contribute to the maintainability of large-scale applications. Some popular front-end frameworks are:

Managing the state of a web application to ensure data consistency and responsiveness. State management libraries like Redux (for React) or Vuex (for Vue.js) play a crucial role in complex applications.

Back-end development involves building the server-side logic and database components of a web application. It is responsible for processing user requests, managing data, and ensuring the overall functionality of the application. Key aspects of back-end development include:

An essential component of the architecture of a web application is a server or cloud instance. A cloud instance is a virtual server instance that can be accessed via the Internet and is created, delivered, and hosted on a public or private cloud. It functions as a physical server that may seamlessly move between various devices with ease or set up several instances on one server. It is therefore very dynamic, scalable, and economical.

Database management is crucial for storing, retrieving, and managing data in web applications. Various database systems, such as MySQL, PostgreSQL, and MongoDB, play distinct roles in organizing and structuring data. Effective database management ensures the responsiveness and efficiency of data-driven web applications. There are 3 types of databases:

The choice of a database depends on various factors such as the nature of the data, scalability requirements, performance considerations, and the specific use case of the application being developed. Each type of database has its strengths and weaknesses, and selecting the right one involves considering the specific needs of the project.

Application Programming Interfaces are sets of rules and protocols that allow different software applications to communicate with each other. APIs define the methods and data formats that applications can use to request and exchange information.

Programming languages aimed at server execution, as opposed to client browser execution, are known as server-side languages. These programming languages are used in web development to perform operations including data processing, database interaction, and the creation of dynamic content that is delivered to the client's browser. A key element of server-side programming is server-side scripting, which allows the server to react to client requests in real time.

Some popular server-side languages are:

Implementing security measures to protect against common vulnerabilities, including SQL injection, cross-site scripting (XSS), and cross-site request forgery (CSRF). Authentication and authorization mechanisms are crucial for securing data and user access.

Thorough testing and debugging processes are essential for identifying and resolving issues in a web application. Testing may include unit testing, integration testing, and user acceptance testing. Debugging involves pinpointing and fixing errors in the code, ensuring the reliability and stability of the application.

Full-stack development refers to the practice of designing, building, and maintaining the entire software stack of a web application. This includes both the frontend (client-side) and backend (server-side) components, as well as the database and any other necessary infrastructure. A full-stack developer is someone who has expertise in working with both the frontend and backend technologies, allowing them to handle all aspects of web application development.

Efficient web development relies on a set of tools and environments that streamline the coding and collaboration processes:

Security is paramount in web development to protect against cyber threats and ensure the confidentiality and integrity of user data. Best practices include encryption, secure coding practices, regular security audits, and staying informed about the latest security vulnerabilities and patches.

Agile is a set of principles and values for software development that prioritize flexibility, collaboration, and customer satisfaction. The four key values are:
"
SEO Services,"

Search engine optimization (SEO) is the process of improving the quality and quantity of website traffic to a website or a web page from search engines.[1][2] SEO targets unpaid search traffic (usually referred to as ""organic"" results) rather than direct traffic, referral traffic, social media traffic, or paid traffic. 

Unpaid search engine traffic may originate from a variety of kinds of searches, including image search, video search, academic search,[3] news search, and industry-specific vertical search engines.

As an Internet marketing strategy, SEO considers how search engines work, the computer-programmed algorithms that dictate search engine results, what people search for, the actual search queries or keywords typed into search engines, and which search engines are preferred by a target audience. SEO is performed because a website will receive more visitors from a search engine when websites rank higher within a search engine results page (SERP), with the aim of either converting the visitors or building brand awareness.[4]

Webmasters and content providers began optimizing websites for search engines in the mid-1990s, as the first search engines were cataloging the early Web. Initially, webmasters submitted the address of a page, or URL to the various search engines, which would send a web crawler to crawl that page, extract links to other pages from it, and return information found on the page to be indexed.[5]

According to a 2004 article by former industry analyst and current Google employee Danny Sullivan, the phrase ""search engine optimization"" probably came into use in 1997. Sullivan credits SEO practitioner Bruce Clay as one of the first people to popularize the term.[6]

Early versions of search algorithms relied on webmaster-provided information such as the keyword meta tag or index files in engines like ALIWEB. Meta tags provide a guide to each page's content. Using metadata to index pages was found to be less than reliable, however, because the webmaster's choice of keywords in the meta tag could potentially be an inaccurate representation of the site's actual content. Flawed data in meta tags, such as those that were inaccurate or incomplete, created the potential for pages to be mischaracterized in irrelevant searches.[7][dubious – discuss] Web content providers also manipulated attributes within the HTML source of a page in an attempt to rank well in search engines.[8] By 1997, search engine designers recognized that webmasters were making efforts to rank in search engines and that some webmasters were manipulating their rankings in search results by stuffing pages with excessive or irrelevant keywords. Early search engines, such as Altavista and Infoseek, adjusted their algorithms to prevent webmasters from manipulating rankings.[9]

By heavily relying on factors such as keyword density, which were exclusively within a webmaster's control, early search engines suffered from abuse and ranking manipulation. To provide better results to their users, search engines had to adapt to ensure their results pages showed the most relevant search results, rather than unrelated pages stuffed with numerous keywords by unscrupulous webmasters. This meant moving away from heavy reliance on term density to a more holistic process for scoring semantic signals.[10]

Search engines responded by developing more complex ranking algorithms, taking into account additional factors that were more difficult for webmasters to manipulate.[citation needed]

Some search engines have also reached out to the SEO industry and are frequent sponsors and guests at SEO conferences, webchats, and seminars. Major search engines provide information and guidelines to help with website optimization.[11][12] Google has a Sitemaps program to help webmasters learn if Google is having any problems indexing their website and also provides data on Google traffic to the website.[13] Bing Webmaster Tools provides a way for webmasters to submit a sitemap and web feeds, allows users to determine the ""crawl rate"", and track the web pages index status.

In 2015, it was reported that Google was developing and promoting mobile search as a key feature within future products. In response, many brands began to take a different approach to their Internet marketing strategies.[14]

In 1998, two graduate students at Stanford University, Larry Page and Sergey Brin, developed ""Backrub"", a search engine that relied on a mathematical algorithm to rate the prominence of web pages. The number calculated by the algorithm, PageRank, is a function of the quantity and strength of inbound links.[15] PageRank estimates the likelihood that a given page will be reached by a web user who randomly surfs the web and follows links from one page to another. In effect, this means that some links are stronger than others, as a higher PageRank page is more likely to be reached by the random web surfer.

Page and Brin founded Google in 1998.[16] Google attracted a loyal following among the growing number of Internet users, who liked its simple design.[17] Off-page factors (such as PageRank and hyperlink analysis) were considered as well as on-page factors (such as keyword frequency, meta tags, headings, links and site structure) to enable Google to avoid the kind of manipulation seen in search engines that only considered on-page factors for their rankings. Although PageRank was more difficult to game, webmasters had already developed link-building tools and schemes to influence the Inktomi search engine, and these methods proved similarly applicable to gaming PageRank. Many sites focus on exchanging, buying, and selling links, often on a massive scale. Some of these schemes involved the creation of thousands of sites for the sole purpose of link spamming.[18]

By 2004, search engines had incorporated a wide range of undisclosed factors in their ranking algorithms to reduce the impact of link manipulation.[19] The leading search engines, Google, Bing, and Yahoo, do not disclose the algorithms they use to rank pages. Some SEO practitioners have studied different approaches to search engine optimization and have shared their personal opinions.[20] Patents related to search engines can provide information to better understand search engines.[21] In 2005, Google began personalizing search results for each user. Depending on their history of previous searches, Google crafted results for logged in users.[22]

In 2007, Google announced a campaign against paid links that transfer PageRank.[23] On June 15, 2009, Google disclosed that they had taken measures to mitigate the effects of PageRank sculpting by use of the nofollow attribute on links. Matt Cutts, a well-known software engineer at Google, announced that Google Bot would no longer treat any no follow links, in the same way, to prevent SEO service providers from using nofollow for PageRank sculpting.[24] As a result of this change, the usage of nofollow led to evaporation of PageRank. In order to avoid the above, SEO engineers developed alternative techniques that replace nofollowed tags with obfuscated JavaScript and thus permit PageRank sculpting. Additionally, several solutions have been suggested that include the usage of iframes, Flash, and JavaScript.[25]

In December 2009, Google announced it would be using the web search history of all its users in order to populate search results.[26] On June 8, 2010 a new web indexing system called Google Caffeine was announced. Designed to allow users to find news results, forum posts, and other content much sooner after publishing than before, Google Caffeine was a change to the way Google updated its index in order to make things show up quicker on Google than before. According to Carrie Grimes, the software engineer who announced Caffeine for Google, ""Caffeine provides 50 percent fresher results for web searches than our last index...""[27] Google Instant, real-time-search, was introduced in late 2010 in an attempt to make search results more timely and relevant. Historically site administrators have spent months or even years optimizing a website to increase search rankings. With the growth in popularity of social media sites and blogs, the leading engines made changes to their algorithms to allow fresh content to rank quickly within the search results.[28]

In February 2011, Google announced the Panda update, which penalizes websites containing content duplicated from other websites and sources. Historically websites have copied content from one another and benefited in search engine rankings by engaging in this practice. However, Google implemented a new system that punishes sites whose content is not unique.[29] The 2012 Google Penguin attempted to penalize websites that used manipulative techniques to improve their rankings on the search engine.[30] Although Google Penguin has been presented as an algorithm aimed at fighting web spam, it really focuses on spammy links[31] by gauging the quality of the sites the links are coming from. The 2013 Google Hummingbird update featured an algorithm change designed to improve Google's natural language processing and semantic understanding of web pages. Hummingbird's language processing system falls under the newly recognized term of ""conversational search"", where the system pays more attention to each word in the query in order to better match the pages to the meaning of the query rather than a few words.[32] With regards to the changes made to search engine optimization, for content publishers and writers, Hummingbird is intended to resolve issues by getting rid of irrelevant content and spam, allowing Google to produce high-quality content and rely on them to be 'trusted' authors.

In October 2019, Google announced they would start applying BERT models for English language search queries in the US. Bidirectional Encoder Representations from Transformers (BERT) was another attempt by Google to improve their natural language processing, but this time in order to better understand the search queries of their users.[33] In terms of search engine optimization, BERT intended to connect users more easily to relevant content and increase the quality of traffic coming to websites that are ranking in the Search Engine Results Page.

The leading search engines, such as Google, Bing, Brave Search and Yahoo!, use crawlers to find pages for their algorithmic search results. Pages that are linked from other search engine-indexed pages do not need to be submitted because they are found automatically. The Yahoo! Directory and DMOZ, two major directories which closed in 2014 and 2017 respectively, both required manual submission and human editorial review.[34] Google offers Google Search Console, for which an XML Sitemap feed can be created and submitted for free to ensure that all pages are found, especially pages that are not discoverable by automatically following links[35] in addition to their URL submission console.[36] Yahoo! formerly operated a paid submission service that guaranteed to crawl for a cost per click;[37] however, this practice was discontinued in 2009.

Search engine crawlers may look at a number of different factors when crawling a site. Not every page is indexed by search engines. The distance of pages from the root directory of a site may also be a factor in whether or not pages get crawled.[38]

Mobile devices are used for the majority of Google searches.[39] In November 2016, Google announced a major change to the way they are crawling websites and started to make their index mobile-first, which means the mobile version of a given website becomes the starting point for what Google includes in their index.[40] In May 2019, Google updated the rendering engine of their crawler to be the latest version of Chromium (74 at the time of the announcement).  Google indicated that they would regularly update the Chromium rendering engine to the latest version.[41]  In December 2019, Google began updating the User-Agent string of their crawler to reflect the latest Chrome version used by their rendering service.  The delay was to allow webmasters time to update their code that responded to particular bot User-Agent strings.  Google ran evaluations and felt confident the impact would be minor.[42]

To avoid undesirable content in the search indexes, webmasters can instruct spiders not to crawl certain files or directories through the standard robots.txt file in the root directory of the domain. Additionally, a page can be explicitly excluded from a search engine's database by using a meta tag specific to robots (usually <meta name=""robots"" content=""noindex""> ). When a search engine visits a site, the robots.txt located in the root directory is the first file crawled. The robots.txt file is then parsed and will instruct the robot as to which pages are not to be crawled. As a search engine crawler may keep a cached copy of this file, it may on occasion crawl pages a webmaster does not wish to crawl. Pages typically prevented from being crawled include login-specific pages such as shopping carts and user-specific content such as search results from internal searches. In March 2007, Google warned webmasters that they should prevent indexing of internal search results because those pages are considered search spam.[43]

In 2020, Google sunsetted the standard (and open-sourced their code) and now treats it as a hint rather than a directive. To adequately ensure that pages are not indexed, a page-level robot's meta tag should be included.[44]

A variety of methods can increase the prominence of a webpage within the search results. Cross linking between pages of the same website to provide more links to important pages may improve its visibility. Page design makes users trust a site and want to stay once they find it. When people bounce off a site, it counts against the site and affects its credibility.[45]

Writing content that includes frequently searched keyword phrases so as to be relevant to a wide variety of search queries will tend to increase traffic. Updating content so as to keep search engines crawling back frequently can give additional weight to a site. Adding relevant keywords to a web page's metadata, including the title tag and meta description, will tend to improve the relevancy of a site's search listings, thus increasing traffic. URL canonicalization of web pages accessible via multiple URLs, using the canonical link element[46] or via 301 redirects can help make sure links to different versions of the URL all count towards the page's link popularity score. These are known as incoming links, which point to the URL and can count towards the page link's popularity score, impacting the credibility of a website.[45]

SEO techniques can be classified into two broad categories: techniques that search engine companies recommend as part of good design (""white hat""), and those techniques of which search engines do not approve (""black hat""). Search engines attempt to minimize the effect of the latter, among them spamdexing. Industry commentators have classified these methods and the practitioners who employ them as either white hat SEO or black hat SEO.[47] White hats tend to produce results that last a long time, whereas black hats anticipate that their sites may eventually be banned either temporarily or permanently once the search engines discover what they are doing.[48]

An SEO technique is considered a white hat if it conforms to the search engines' guidelines and involves no deception. As the search engine guidelines[11][12][49] are not written as a series of rules or commandments, this is an important distinction to note. White hat SEO is not just about following guidelines but is about ensuring that the content a search engine indexes and subsequently ranks is the same content a user will see. White hat advice is generally summed up as creating content for users, not for search engines, and then making that content easily accessible to the online ""spider"" algorithms, rather than attempting to trick the algorithm from its intended purpose. White hat SEO is in many ways similar to web development that promotes accessibility,[50] although the two are not identical.

Black hat SEO attempts to improve rankings in ways that are disapproved of by the search engines or involve deception. One black hat technique uses hidden text, either as text colored similar to the background, in an invisible div, or positioned off-screen. Another method gives a different page depending on whether the page is being requested by a human visitor or a search engine, a technique known as cloaking. Another category sometimes used is grey hat SEO. This is in between the black hat and white hat approaches, where the methods employed avoid the site being penalized but do not act in producing the best content for users. Grey hat SEO is entirely focused on improving search engine rankings.

Search engines may penalize sites they discover using black or grey hat methods, either by reducing their rankings or eliminating their listings from their databases altogether. Such penalties can be applied either automatically by the search engines' algorithms or by a manual site review. One example was the February 2006 Google removal of both BMW Germany and Ricoh Germany for the use of deceptive practices.[51] Both companies subsequently apologized, fixed the offending pages, and were restored to Google's search engine results page.[52]

Companies that employ black hat techniques or other spammy tactics can get their client websites banned from the search results. In 2005, the Wall Street Journal reported on a company, Traffic Power, which allegedly used high-risk techniques and failed to disclose those risks to its clients.[53] Wired magazine reported that the same company sued blogger and SEO Aaron Wall for writing about the ban.[54] Google's Matt Cutts later confirmed that Google had banned Traffic Power and some of its clients.[55]

SEO is not an appropriate strategy for every website, and other Internet marketing strategies can be more effective, such as paid advertising through pay-per-click (PPC) campaigns, depending on the site operator's goals.[editorializing] Search engine marketing (SEM) is the practice of designing, running, and optimizing search engine ad campaigns. Its difference from SEO is most simply depicted as the difference between paid and unpaid priority ranking in search results. SEM focuses on prominence more so than relevance; website developers should regard SEM with the utmost importance with consideration to visibility as most navigate to the primary listings of their search.[56] A successful Internet marketing campaign may also depend upon building high-quality web pages to engage and persuade internet users, setting up analytics programs to enable site owners to measure results, and improving a site's conversion rate.[57][58] In November 2015, Google released a full 160-page version of its Search Quality Rating Guidelines to the public,[59] which revealed a shift in their focus towards ""usefulness"" and mobile local search. In recent years the mobile market has exploded, overtaking the use of desktops, as shown in by StatCounter in October 2016, where they analyzed 2.5 million websites and found that 51.3% of the pages were loaded by a mobile device.[60] Google has been one of the companies that are utilizing the popularity of mobile usage by encouraging websites to use their Google Search Console, the Mobile-Friendly Test, which allows companies to measure up their website to the search engine results and determine how user-friendly their websites are. The closer the keywords are together their ranking will improve based on key terms.[45]

SEO may generate an adequate return on investment. However, search engines are not paid for organic search traffic, their algorithms change, and there are no guarantees of continued referrals. Due to this lack of guarantee and uncertainty, a business that relies heavily on search engine traffic can suffer major losses if the search engines stop sending visitors.[61] Search engines can change their algorithms, impacting a website's search engine ranking, possibly resulting in a serious loss of traffic. According to Google's CEO, Eric Schmidt, in 2010, Google made over 500 algorithm changes – almost 1.5 per day.[62] It is considered a wise business practice for website operators to liberate themselves from dependence on search engine traffic.[63] In addition to accessibility in terms of web crawlers (addressed above), user web accessibility has become increasingly important for SEO.

Optimization techniques are highly tuned to the dominant search engines in the target market.
The search engines' market shares vary from market to market, as does competition.
In 2003, Danny Sullivan stated that Google represented about 75% of all searches.[64] In markets outside the United States, Google's share is often larger, and data showed Google was the dominant search engine worldwide as of 2007.[65] As of 2006, Google had an 85–90% market share in Germany.[66] While there were hundreds of SEO firms in the US at that time, there were only about five in Germany.[66]  As of March 2024, Google still had a significant market share of 89.85% in Germany.[67] As of June 2008, the market share of Google in the UK was close to 90% according to Hitwise.[68][obsolete source] As of March 2024, Google's market share in the UK was 93.61%.[69]

Successful search engine optimization (SEO) for international markets requires more than just translating web pages. It may also involve registering a domain name with a country-code top-level domain (ccTLD) or a relevant top-level domain (TLD) for the target market, choosing web hosting with a local IP address or server, and using a Content Delivery Network (CDN) to improve website speed and performance globally.  It is also important to understand the local culture so that the content feels relevant to the audience. This includes conducting keyword research for each market, using hreflang tags to target the right languages, and building local backlinks. However, the core SEO principles—such as creating high-quality content, improving user experience, and building links—remain the same, regardless of language or region.[66]

Regional search engines have a strong presence in specific markets:

By the early 2000s, businesses recognized that the web and search engines could help them reach global audiences. As a result, the need for multilingual SEO emerged.[74] In the early years of international SEO development, simple translation was seen as sufficient. However, over time, it became clear that localization and transcreation—adapting content to local language, culture, and emotional resonance—were far more effective than basic translation.[75]

On October 17, 2002, SearchKing filed suit in the United States District Court, Western District of Oklahoma, against the search engine Google. SearchKing's claim was that Google's tactics to prevent spamdexing constituted a tortious interference with contractual relations.  On May 27, 2003, the court granted Google's motion to dismiss the complaint because SearchKing ""failed to state a claim upon which relief may be granted.""[76][77]

In March 2006, KinderStart filed a lawsuit against Google over search engine rankings. KinderStart's website was removed from Google's index prior to the lawsuit, and the amount of traffic to the site dropped by 70%. On March 16, 2007, the United States District Court for the Northern District of California (San Jose Division) dismissed KinderStart's complaint without leave to amend and partially granted Google's motion for Rule 11 sanctions against KinderStart's attorney, requiring him to pay part of Google's legal expenses.[78][79]
"
Social Media Services,"

A social networking service (SNS), or social networking site, is a type of online social media platform which people use to build social networks or social relationships with other people who share similar personal or career content, interests, activities, backgrounds or real-life connections.[1][2]

Social networking services vary in format and the number of features. They can incorporate a range of new information and communication tools, operating on desktops and on laptops, on mobile devices such as tablet computers and smartphones. This may feature digital photo/video/sharing and diary entries online (blogging).[2] Online community services are sometimes considered social-network services by developers and users, though in a broader sense, a social-network service usually provides an individual-centered service whereas online community services are groups centered. Generally defined as ""websites that facilitate the building of a network of contacts in order to exchange various types of content online,"" social networking sites provide a space for interaction to continue beyond in-person interactions. These computer mediated interactions link members of various networks and may help to create, sustain and develop new social and professional relationships.[3]

Social networking sites allow users to share ideas, digital photos and videos, posts, and to inform others about online or real-world activities and events with people within their social network. While in-person social networking – such as gathering in a village market to talk about events – has existed since the earliest development of towns,[4] the web enables people to connect with others who live in different locations across the globe (dependent on access to an Internet connection to do so).

Depending on the platform, members may be able to contact any other member. In other cases, members can contact anyone they have a connection to, and subsequently anyone that contact has a connection to, and so on.

Facebook having a massive 2.13 billion active monthly users and an average of 1.4 billion daily active users in 2017.[5]

LinkedIn, a career-oriented social-networking service, generally requires that a member personally know another member in real life before they contact them online. Some services require members to have a preexisting connection to contact other members.

With COVID-19, Zoom, a videoconferencing platform, has taken an integral place to connect people located around the world and facilitate many online environments such as school, university, work and government meetings.

The main types of social networking services contain category places (such as age or occupation or religion), means to connect with friends (usually with self-description pages), and a recommendation system linked to trust. One can categorize social-network services into four types:[6]

There have been attempts to standardize these services to avoid the need to duplicate entries of friends and interests (see the FOAF standard). A study reveals that India recorded world's largest growth in terms of social media users in 2013.[7] A 2013 survey found that 73% of U.S. adults use social-networking sites.[8]

The potential for computer networking to facilitate newly improved forms of computer-mediated social interaction was suggested early on.[30] Efforts to support social networks via computer-mediated communication were made in many early online services, including Usenet,[31] ARPANET, LISTSERV, and bulletin board services (BBS). Many prototypical features of social networking sites were also present in online services such as The Source, Delphi, America Online, Prodigy, CompuServe, and The WELL.[32]

Early social networking on the World Wide Web began in the form of generalized online communities such as Theglobe.com (1995),[33] Geocities (1994) and Tripod.com (1995). Many of these early communities focused on bringing people together to interact with each other through chat rooms and encouraged users to share personal information and ideas via personal web pages by providing easy-to-use publishing tools and free or inexpensive web space. Some communities – such as Classmates.com – took a different approach by simply having people link to each other via email addresses. PlanetAll started in 1996.

In the late 1990s, user profiles became a central feature of social networking sites, allowing users to compile lists of ""friends"" and search for other users with similar interests. New social networking methods were developed by the end of the 1990s, and many sites began to develop more advanced features for users to find and manage friends.[34] Open Diary, a community for online diarists, invented both friends-only content and the reader comment, two features of social networks important to user interaction.[35]

This newer generation of social networking sites began to flourish with the emergence of SixDegrees in 1997,[2] Open Diary in 1998,[36] Mixi in 1999,[37] Makeoutclub in 2000,[38][39] Cyworld in 2001,[40][2] Hub Culture in 2002, and Friendster and Nexopia in 2003.[41] Cyworld also became one of the first companies to profit from the sale of virtual goods.[42][43] MySpace and LinkedIn were launched in 2003, and Bebo was launched in 2005. Orkut became the first popular social networking service in Brazil (although most of its very first users were from the United States) and quickly grew in popularity in India (Madhavan, 2007).[2] There was a rapid increase in social networking sites' popularity; in 2005, MySpace had more pageviews than Google.[44] Many of these services were displaced by Facebook, which launched in 2004 and became the largest social networking site in the world in 2009.[45][46]

The term social media was first used in 2004 and is often used to describe social networking services.[47][48]

Web-based social networking services make it possible to connect people who share interests and activities across political, economic, and geographic borders.[49] Through e-mail and instant messaging, online communities are created where a gift economy and reciprocal altruism are encouraged through cooperation. Information is suited to a gift economy, as information is a nonrival good and can be gifted at practically no cost.[50][51] Scholars have noted that the term ""social"" cannot account for technological features of the social network platforms alone.[52] Hence, the level of network sociability should determine by the actual performances of its users. According to the communication theory of uses and gratifications, an increasing number of individuals are looking to the Internet and social media to fulfill cognitive, affective, personal integrative, social integrative, and tension free needs. With Internet technology as a supplement to fulfill needs, it is in turn affecting everyday life, including relationships, school, church, entertainment, and family.[53] Companies are using social media as a way to learn about potential employees' personalities and behavior. In numerous situations, a candidate who might otherwise have been hired has been rejected due to offensive or otherwise unseemly photos or comments posted to social networks or appearing on a newsfeed.

Facebook and other social networking tools are increasingly the aims of scholarly research. Scholars in many fields have begun to investigate the impact of social networking sites, investigating how such sites may play into issues of identity, politics, privacy,[54]
social capital, youth culture, and education.[55] Research has also suggested that individuals add offline friends on Facebook to maintain contact and often this blurs the lines between work and home lives.[56]  Users from around the world also utilise social networking sites as an alternative news source.[57] While social networking sites have arguably changed how we access the news,[58] users tend to have mixed opinions about the reliability of content accessed through these sites.[59]

According to a study in 2015, 63% of the users of Facebook or Twitter in the USA consider these networks to be their main source of news, with entertainment news being the most seen. In the times of breaking news, Twitter users are more likely to stay invested in the story. In some cases when the news story is more political, users may be more likely to voice their opinion on a linked Facebook story with a comment or like, while Twitter users will just follow the site's feed and retweet the article.[60] In online social networks, the veracity and reliability of news may be diminished due to the absence of traditional media gatekeepers.[61]

A 2015 study shows that 85% of people aged 18 to 34 use social networking sites for their purchase decision making. While over 65% of people aged 55 and over-rely on word of mouth.[62] Several websites are beginning to tap into the power of the social networking model for philanthropy. Such models provide a means for connecting otherwise fragmented industries and small organizations without the resources to reach a broader audience with interested users.[63] Social networks are providing a different way for individuals to communicate digitally. These communities of hypertexts allow for the sharing of information and ideas, an old concept placed in a digital environment. In 2011, HCL Technologies conducted research that showed that 50% of British employers had banned the use of social networking sites/services during office hours.[64][65]

Research has provided us with mixed results as to whether or not a person's involvement in social networking can affect their feelings of loneliness. Studies have indicated that how a person chooses to use social networking can change their feelings of loneliness in either a negative or positive way. Some companies with mobile workers have encouraged their workers to use social networking to feel connected. Educators are using social networking to stay connected with their students whereas individuals use it to stay connected with their close relationships.[66]

Social networking sites can be used by consumers to create a social media firestorm which is ""A digital artifact created by large numbers of user comments of multiple purposes (condemnation and support) and tones (aggressive and cordial) that appear rapidly and recede shortly after”.[1]

Each social networking user is able to create a community that centers around a personal identity they choose to create online.[67] In his book Digital Identities: Creating and Communicating the Online Self,[68] Rob Cover argues that social networking's foundation in Web 2.0, high-speed networking shifts online representation to one which is both visual and relational to other people, complexifying the identity process for younger people and creating new forms of anxiety.[68] In 2016, news reports stated that excessive usage of SNS sites may be associated with an increase in the rates of depression, to almost triple the rate for non-SNS users. Experts worldwide[which?] have said that 2030 people who use SNS more have higher levels of depression than those who use SNS less.[69] At least one study went as far as to conclude that the negative effects of Facebook usage are equal to or greater than the positive effects of face-to-face interactions.[70]

According to a recent article from Computers in Human Behavior, Facebook has also been shown to lead to issues of social comparison. Users are able to select which photos and status updates to post, allowing them to portray their lives in acclamatory manners.[71]  These updates can lead to other users feeling like their lives are inferior by comparison.[72] Users may feel especially inclined to compare themselves to other users with whom they share similar characteristics or lifestyles, leading to a fairer comparison.[71] Motives for these comparisons can be associated with the goals of improving oneself by looking at profiles of people who one feels are superior, especially when their lifestyle is similar and possible.[71] One can also self-compare to make oneself feel superior to others by looking at the profiles of users who one believes to be worse off.[71] However, a study by the Harvard Business Review shows that these goals often lead to  negative consequences, as use of Facebook has been linked with lower levels of well-being; mental health has been shown to decrease due to the use of Facebook.[72] Computers in Human Behavior emphasizes that these feelings of poor mental health have been suggested to cause people to take time off from their Facebook accounts; this action is called ""Facebook Fatigue"" and has been common in recent years.[71]

Usage of social networking has contributed to a new form of abusive communication, and academic research has highlighted a number of social-technological explanations for this behaviour. These including the anonymity afforded by interpersonal communications,[73] factors that include boredom or attention seeking,[74] or the result of more polarised online debate.[75] The impact in this abuse has found impacts through the prevalence of online cyberbullying, and online trolling. There has also been a marked increase in political violence and abuse through social media platforms. For instance, one study by Ward and McLoughlin found that 2.57% of all messages sent to UK MPs on Twitter were found to contain abusive messages.[75]

According to boyd and Ellison's 2007 article, ""Why Youth (Heart) Social Network Sites: The Role of Networked Publics in Teenage Social Life"", social networking sites share a variety of technical features that allow individuals to: construct a public/semi-public profile, articulate a list of other users that they share a connection with, and view their list of connections within the system. The most basic of these are visible profiles with a list of ""friends"" who are also users of the site.[55] In an article entitled ""Social Network Sites: Definition, History, and Scholarship,"" boyd and Ellison adopt Sunden's (2003) description of profiles as unique pages where one can ""type oneself into being"".[2] A profile is generated from answers to questions, such as age, location, interests, etc. Some sites allow users to upload pictures, add multimedia content or modify the look and feel of the profile. Others, e.g., Facebook, allow users to enhance their profile by adding modules or ""Applications"".[2] Many sites allow users to post blog entries, search for others with similar interests and compile and share lists of contacts. User profiles often have a section dedicated to comments from friends and other users. To protect user privacy, social networks typically have controls that allow users to choose who can view their profile, contact them, add them to their list of contacts, and so on.[citation needed]

There is a trend towards more interoperability between social networks led by technologies such as OpenID and OpenSocial. In most mobile communities, mobile phone users can now create their own profiles, make friends, participate in chat rooms, create chat rooms, hold private conversations, share photos and videos, and share blogs by using their mobile phone. Some companies provide wireless services that allow their customers to build their own mobile community and brand it; one of the most popular wireless services for social networking in North America and Nepal is Facebook Mobile.
Recently, Twitter has also introduced fact check labels to combat misinformation which was primarily spread due to the coronavirus but also has had an impact on debunking false claims by Donald Trump in the 2020 election.[citation needed]

Social media platforms may allow users to change their user name (or ""handle"", distinct from the ""display name""), which could change the URL to their profile. Users are advised to do so with caution, since it could break back links from others' posts and comments depending on implementation, and external back links.[76]

The things you share are things that make you look good, things which you are happy to tie into your identity.
While the popularity of social networking consistently rises,[78] new uses for the technology are frequently being observed. Today's technologically savvy population requires convenient solutions to their daily needs.[79] At the forefront of emerging trends in social networking sites is the concept of ""real-time web"" and ""location-based"". Real-time allows users to contribute contents, which is then broadcast as it is being uploaded—the concept is analogous to live radio and television broadcasts. Twitter set the trend for ""real-time"" services, wherein users can broadcast to the world what they are doing, or what is on their minds within a 140-character limit. Facebook followed suit with their ""Live Feed"" where users' activities are streamed as soon as it happens. While Twitter focuses on words, Clixtr, another real-time service, focuses on group photo sharing wherein users can update their photo streams with photos while at an event. Facebook, however, remains the largest photo sharing site with over 250 billion photos as of September 2013.[80] In April 2012, the image-based social media network Pinterest had become the third largest social network in the United States.[81]

Companies have begun to merge business technologies and solutions, such as cloud computing, with social networking concepts. Instead of connecting individuals based on social interest, companies are developing interactive communities that connect individuals based on shared business needs or experiences. Many provide specialized networking tools and applications that can be accessed via their websites, such as LinkedIn. Others companies, such as Monster.com, have been steadily developing a more ""socialized"" feel to their career center sites to harness some of the power of social networking sites. These more business related sites have their own nomenclature for the most part but the most common naming conventions are ""Vocational Networking Sites"" or ""Vocational Media Networks"", with the former more closely tied to individual networking relationships based on social networking principles.[citation needed]

Foursquare gained popularity as it allowed for users to check into places that they are frequenting at that moment. Gowalla is another such service that functions in much the same way that Foursquare does, leveraging the GPS in phones to create a location-based user experience. Clixtr, though in the real-time space, is also a location-based social networking site, since events created by users are automatically geotagged, and users can view events occurring nearby through the Clixtr iPhone app. Recently, Yelp announced its entrance into the location-based social networking space through check-ins with their mobile app; whether or not this becomes detrimental to Foursquare or Gowalla is yet to be seen, as it is still considered a new space in the Internet technology industry.[82]

One popular use for this new technology is social networking between businesses. Companies have found that social networking sites such as Facebook and Twitter are great ways to build their brand image. According to Jody Nimetz, author of Marketing Jive,[83] there are five major uses for businesses and social media: to create brand awareness, as an online reputation management tool, for recruiting, to learn about new technologies and competitors, and as a lead generation tool to intercept potential prospects.[83] These companies are able to drive traffic to their own online sites while encouraging their consumers and clients to have discussions on how to improve or change products or services. As of September 2013, 71% of online adults use Facebook, 17% use Instagram, 21% use Pinterest, and 22% use LinkedIn.[84]

In 2012, it was reported that in the past few years, the niche social network has steadily grown in popularity, thanks to better levels of user interaction and engagement. In 2012, a survey by Reuters and research firm Ipsos[85]
found that one in three users were getting bored with Facebook and in 2014 the GlobalWebIndex found that this figured had risen to almost 50%. The niche social network offers a specialized space that's designed to appeal to a very specific market with a clearly defined set of needs. Where once the streams of social minutia on networks such as Facebook and Twitter were the ultimate in online voyeurism, now users are looking for connections, community and shared experiences. Social networks that tap directly into specific activities, hobbies, tastes, and lifestyles are seeing a consistent rise in popularity.[citation needed]

One other use that is being discussed is the use of social networks in the science communities. Julia Porter Liebeskind et al. have published a study on how new biotechnology firms are using social networking sites to share exchanges in scientific knowledge.[86] They state in their study that by sharing information and knowledge with one another, they are able to ""increase both their learning and their flexibility in ways that would not have been possible within a self-contained hierarchical organization"". Social networking is allowing scientific groups to expand their knowledge base and share ideas, and without these new means of communicating their theories might become ""isolated and irrelevant"". Researchers use social networks frequently to maintain and develop professional relationships.[87] They are interested in consolidating social ties and professional contact, keeping in touch with friends and colleagues and seeing what their own contacts are doing. This can be related to their need to keep updated on the activities and events of their friends and colleagues in order to establish collaborations on common fields of interest and knowledge sharing.[88]

Social networks are also used to communicate scientists research results[89] and as a public communication tool and to connect people who share the same professional interests, their benefits can vary according to the discipline.[90] The most interesting aspects of social networks for professional purposes are their potentialities in terms of dissemination of information and the ability to reach and multiple professional contacts exponentially. Social networks like Academia.edu, LinkedIn, Facebook, and ResearchGate give the possibility to join professional groups and pages, to share papers and results, publicize events, to discuss issues and create debates.[88] Academia.edu is extensively used by researchers, where they follow a combination of social networking and scholarly norms.[91] ResearchGate is also widely used by researchers, especially to disseminate and discuss their publications,[92] where it seems to attract an audience that it wider than just other scientists.[93] The usage of ResearchGate and Academia in different academic communities has increasingly been studied in recent years.[94]

The advent of social networking platforms may also be impacting the ways in which learners engage with technology in general. For a number of years, Prensky's (2001) dichotomy between Digital Natives and Digital Immigrants has been considered a relatively accurate representation of the ease with which people of a certain age range—in particular those born before and after 1980—use technology. Prensky's theory has been largely disproved, however, and not least on account of the burgeoning popularity of social networking sites and other metaphors such as White and Le Cornu's ""Visitors"" and ""Residents"" (2011) are greater currency. The use of online social networks by school libraries is also increasingly prevalent and they are being used to communicate with potential library users, as well as extending the services provided by individual school libraries. Social networks and their educational uses are of interest to many researchers. According to Livingstone and Brake (2010), ""Social networking sites, like much else on the Internet, represent a moving target for researchers and policymakers.""[95] Pew Research Center project, called Pew Internet, did a USA-wide survey in 2009 and in 2010 February published that 47% of American adults use a social networking website.[96] Same survey found that 73% of online teenagers use SNS, which is an increase from 65% in 2008, 55% in 2006.[96] Recent studies have shown that social network services provide opportunities within professional education, curriculum education, and learning. However, there are constraints in this area. Researches, especially in Africa, have disclosed that the use of social networks among students has been known to affect their academic life negatively. This is buttressed by the fact that their use constitutes distractions, as well as that the students tend to invest a good deal of time in the use of such technologies.[citation needed]

Albayrak and Yildirim (2015) examined the educational use of social networking sites. They investigated students' involvement in Facebook as a Course Management System (CMS) and the findings of their study support that Facebook as a CMS has the potential to increase student involvement in discussions and out-of-class communication among instructors and students.[97]

Professional use of social networking services refers to the employment of a network site to connect with other professionals within a given field of interest. These type of social networking services are referred to as ""Career-oriented social networking markets (CSNM)"".[9]
LinkedIn is one example and is a social networking website geared towards companies and industry professionals looking to make new business contacts or keep in touch with previous co-workers, affiliates, and clients. LinkedIn provides not only a professional social use but also encourages people to inject their personality into their profile – making it more personal than a resume.[98]
Similar websites to LinkedIn (also geared towards companies and industry professionals looking for work opportunities) to connect include AngelList, XING, Goodwall, The Dots,[99] Jobcase, Bark.com, ...[100] Various freelance marketplace websites (which focus on freelance work) also exist. There are also a number of other employment websites focused on international volunteering, notably VolunteerMatch, Idealist.org and All for Good.[101] National WWOOF networks finally allow for searching for homestays on organic farms.[102]

Now other social network sites are also being used in this manner. Twitter has become [a] mainstay for professional development as well as promotion[103] and online SNSs support both the maintenance of existing social ties and the formation of new connections. Much of the early research on online communities assume that individuals using these systems would be connecting with others outside their preexisting social group or location, liberating them to form communities around shared interests, as opposed to shared geography.[104] Other researchers have suggested that the professional use of network sites produce ""social capital"". For individuals, social capital allows a person to draw on resources from other members of the networks to which he or she belongs.[105] These resources can take the form of useful information, personal relationships, or the capacity to organize groups. As well, networks within these services also can be established or built by joining special interest groups that others have made, or creating one and asking others to join.[106]

According to Doering, Beach, and O'Brien, a future English curriculum needs to recognize a significant shift in how adolescents are communicating with each other.[107] Curriculum uses of social networking services can also include sharing curriculum-related resources. Educators tap into user-generated content to find and discuss curriculum-related content for students. Responding to the popularity of social networking services among many students, teachers are increasingly using social networks to supplement teaching and learning in traditional classroom environments. This way they can provide new opportunities for enriching existing curriculum through creative, authentic and flexible, non-linear learning experiences.[108] Some social networks, such as English, baby! and LiveMocha, are explicitly education-focused and couple instructional content with an educational peer environment.[109] The new Web 2.0 technologies built into most social networking services promote conferencing, interaction, creation, research on a global scale, enabling educators to share, remix, and repurpose curriculum resources. In short, social networking services can become research networks as well as learning networks.[110]

Educators and advocates of new digital literacies are confident that social networking encourages the development of transferable, technical, and social skills of value in formal and informal learning.[95] In a formal learning environment, goals or objectives are determined by an outside department or agency. Tweeting, instant messaging, or blogging enhances student involvement. Students who would not normally participate in class are more apt to partake through social network services. Networking allows participants the opportunity for just-in-time learning and higher levels of engagement.[111] The use of SNSs allow educators to enhance the prescribed curriculum. When learning experiences are infused into a website student utilize every day for fun, students realize that learning can and should be a part of everyday life.[112] It does not have to be separate and unattached.[113][unreliable source?]

Informal learning consists of the learner setting the goals and objectives. It has been claimed that media no longer just influence human culture; they are human culture.[114] With such a high number of users between the ages of 13 and 18, a number of skills are developed. Participants hone technical skills in choosing to navigate through social networking services. This includes elementary items such as sending an instant message or updating a status. The development of new media skills are paramount in helping youth navigate the digital world with confidence.

Social networking services foster learning through what Jenkins (2006) describes as a ""participatory culture"".[115] A participatory culture consists of a space that allows engagement, sharing, mentoring, and an opportunity for social interaction. Participants of social network services avail of this opportunity. Informal learning, in the forms of participatory and social learning online, is an excellent tool for teachers to sneak in material and ideas that students will identify with and therefore, in a secondary manner, students will learn skills that would normally be taught in a formal setting in the more interesting and engaging environment of social learning.[116][unreliable source?] Sites like Twitter provide students with the opportunity to converse and collaborate with others in real time.[citation needed]

Social networking services provide a virtual ""space"" for learners. James Gee (2004) suggests that affinity spaces instantiate participation, collaboration, distribution, dispersion of expertise, and relatedness.[117] Registered users share and search for knowledge which contributes to informal learning.[citation needed]

In the past, social networking services were viewed as a distraction and offered no educational benefit. Blocking these social networks was a form of protection for students against wasting time, bullying, and invasions of privacy. In an educational setting, Facebook, for example, is seen by many instructors and educators as a frivolous, time-wasting distraction from schoolwork, and it is not uncommon to be banned in junior high or high school computer labs.[113] Cyberbullying has become an issue of concern with social networking services. According to the UK Children Go Online survey of 9- to 19-year-olds, it was found that a third have received bullying comments online.[118] To avoid this problem, many school districts/boards have blocked access to social networking services such as Facebook, MySpace, and Twitter within the school environment. Social networking services often include a lot of personal information posted publicly, and many believe that sharing personal information is a window into privacy theft. Schools have taken action to protect students from this. It is believed that this outpouring of identifiable information and the easy communication vehicle that social networking services open the door to sexual predators, cyberbullying, and cyberstalking.[119] In contrast, however, 70% of social media using teens and 85% of adults believe that people are mostly kind to one another on social network sites.[96]

Recent research suggests that there has been a shift in blocking the use of social networking services. In many cases, the opposite is occurring as the potential of online networking services is being realized. It has been suggested that if schools block them [social networking services], they are preventing students from learning the skills they need.[120] Banning social networking [...] is not only inappropriate but also borderline irresponsible when it comes to providing the best educational experiences for students.[121] Schools and school districts have the option of educating safe media usage as well as incorporating digital media into the classroom experience, thus preparing students for the literacy they will encounter in the future.[citation needed]

A cyberpsychology research study conducted by Australian researchers demonstrated that a number of positive psychological outcomes are related to Facebook use. These researchers established that people can derive a sense of social connectedness and belongingness in the online environment. Importantly, this online social connectedness was associated with lower levels of depression and anxiety, and greater levels of subjective well-being. These findings suggest that the nature of online social networking determines the outcomes of online social network use.[122][123]

Social networks are being used by activists as a means of low-cost grassroots organizing. Extensive use of an array of social networking sites enabled organizers of 2009 National Equality March to mobilize an estimated 200,000 participants to march on Washington with a cost savings of up to 85% per participant over previous methods.[124]
The August 2011 England riots were similarly considered to have escalated and been fuelled by this type of grassroots organization.[citation needed]

A rise in social network use is being driven by college students using the services to network with professionals for internship and job opportunities. Many studies have been done on the effectiveness of networking online in a college setting, and one notable one is by Phipps Arabie and Yoram Wind published in Advances in Social Network Analysis.[125] Many schools have implemented online alumni directories which serve as makeshift social networks that current and former students can turn to for career advice. However, these alumni directories tend to suffer from an oversupply of advice-seekers and an undersupply of advice providers. One new social networking service, Ask-a-peer, aims to solve this problem by enabling advice seekers to offer modest compensation to advisers for their time. LinkedIn is also another great resource. It helps alumni, students and unemployed individuals look for work. They are also able to connect with others professionally and network with companies.

In addition, employers have been found to use social network sites to screen job candidates.[126]

A social network hosting service is a web hosting service that specifically hosts the user creation of web-based social networking services, alongside related applications.[citation needed]

A social trade network is a service that allows participants interested in specific trade sectors to share related contents and personal opinions.[citation needed]

Few social networks charge money for membership. In part, this may be because social networking is a relatively new service, and the value of using them has not been firmly established in customers' minds. Companies such as Myspace and Facebook sell online advertising on their site. Their business model is based upon large membership count, and charging for membership would be counterproductive.[127] Some believe that the deeper information that the sites have on each user will allow much better targeted advertising than any other site can currently provide.[128] In recent times, Apple has been critical of the Google and Facebook model, in which users are defined as product and a commodity, and their data being sold for marketing revenue.[129] Social networks operate under an autonomous business model, in which a social network's members serve dual roles as both the suppliers and the consumers of content. This is in contrast to a traditional business model, where the suppliers and consumers are distinct agents. Revenue is typically gained in the autonomous business model via advertisements, but subscription-based revenue is possible when membership and content levels are sufficiently high.[130]

People use social networking sites for meeting new friends, finding old friends, or locating people who have the same problems or interests they have, called niche networking. More and more relationships and friendships are being formed online and then carried to an offline setting. Psychologist and University of Hamburg professor Erich H. Witte says that relationships which start online are much more likely to succeed. In this regard, there are studies which predict tie strength among the friends[131] on social networking websites.  One online dating site claims that 2% of all marriages begin at its site, the equivalent of 236 marriages a day. Other sites claim one in five relationships begin online.[citation needed]

Users do not necessarily share with others the content which is of most interest to them, but rather that which projects a good impression of themselves.[77] While everyone agrees that social networking has had a significant impact on social interaction, there remains a substantial disagreement as to whether the nature of this impact is completely positive. A number of scholars have done research on the negative effects of Internet communication as well. These researchers have contended that this form of communication is an impoverished version of conventional face-to-face social interactions, and therefore produce negative outcomes such as loneliness and depression for users who rely on social networking entirely. By engaging solely in online communication, interactions between communities, families, and other social groups are weakened.[132]

Social networking services have led to many issues regarding privacy, bullying, social anxiety and potential for misuse.

Social networking services are increasingly being used in legal and criminal investigations. The information posted on sites such as MySpace and Facebook has been used by police (forensic profiling), probation, and university officials to prosecute users of said sites. In some situations, content posted on MySpace has been used in court.[133]

Facebook is increasingly being used by school administrations and law enforcement agencies as a source of evidence against student users. This site being the number one online destination for college students, allows users to create profile pages with personal details. These pages can be viewed by other registered users from the same school, which often include resident assistants and campus police who have signed up for the service.[134] One UK police force has sifted pictures from Facebook and arrested some people who had been photographed in a public place holding a weapon such as a knife (having a weapon in a public place is illegal).[135]

Social networking is more recently being used by various government agencies. Social networking tools serve as a quick and easy way for the government to get the suggestion of the public and to keep the public updated on their activity, however, this comes with a significant risk of abuse, for example, to cultivate a culture of fear such as that outlined in Nineteen Eighty-Four or THX-1138.

The Centers for Disease Control demonstrated the importance of vaccinations on the popular children's site Whyville and the National Oceanic and Atmospheric Administration has a virtual island on Second Life where people can explore caves or explore the effects of global warming.[136] Likewise, NASA has taken advantage of a few social networking tools, including Twitter and Flickr. The NSA is taking advantage of them all.[137] NASA is using such tools to aid the Review of U.S. Human Space Flight Plans Committee, whose goal it is to ensure that the nation is on a vigorous and sustainable path to achieving its boldest aspirations in space.[138]

The use of social networking services in an enterprise context presents the potential of having a major impact on the world of business and work.[139] Social networks connect people at low cost; this can be beneficial for entrepreneurs and small businesses looking to expand their contact bases. These networks often act as a customer relationship management tool for companies selling products and services. Companies can also use social networks for advertising in the form of banners and text ads. Since businesses operate globally, social networks can make it easier to keep in touch with contacts around the world. Applications for social networking sites have extended toward businesses and brands are creating their own, high functioning sites, a sector known as brand networking. It is the idea that a brand can build its consumer relationship by connecting their consumers to the brand image on a platform that provides them relative content, elements of participation, and a ranking or score system. Brand networking is a new way to capitalize on social trends as a marketing tool. The power of social networks is beginning to permeate into internal culture of businesses where they are finding uses for collaboration, file sharing and knowledge transfer. The term ""enterprise social software"" is becoming increasingly popular for these types of applications.[citation needed]

Many social networks provide an online environment for people to communicate and exchange personal information for dating purposes. Intentions can vary from looking for a one time date, short-term relationships, and long-term relationships.[140] Most of these social networks, just like online dating services, require users to give out certain pieces of information. This usually includes a user's age, gender, location, interests, and perhaps a picture. Releasing very personal information is usually discouraged for safety reasons.[141] This allows other users to search or be searched by some sort of criteria, but at the same time, people can maintain a degree of anonymity similar to most online dating services. Online dating sites are similar to social networks in the sense that users create profiles to meet and communicate with others, but their activities on such sites are for the sole purpose of finding a person of interest to date. Social networks do not necessarily have to be for dating; many users simply use it for keeping in touch with friends, and colleagues.[142]

However, an important difference between social networks and online dating services is the fact that online dating sites usually require a fee, where social networks are free.[143]
This difference is one of the reasons the online dating industry is seeing a massive decrease in revenue due to many users opting to use social networking services instead. Many popular online dating services such as Match.com, Yahoo Personals, and eHarmony.com are seeing a decrease in users, where social networks like MySpace and Facebook are experiencing an increase in users. The number of Internet users in the United States that visit online dating sites has fallen from a peak of 21% in 2003 to 10% in 2006.[144] Whether it is the cost of the services, the variety of users with different intentions, or any other reason, it is undeniable that social networking sites are quickly becoming the new way to find dates online.[citation needed]

The National School Boards Association reports that almost 60% of students who use social networking talk about education topics online, and more than 50% talk specifically about schoolwork. Yet the vast majority of school districts have stringent rules against nearly all forms of social networking during the school day—even though students and parents report few problem behaviors online. Social networks focused on supporting relationships between teachers and their students are now used for learning, educators professional development, and content sharing. HASTAC is a collaborative social network space for new modes of learning and research in higher education, K-12, and lifelong learning; Ning supports teachers; TermWiki, TeachStreet and other sites are being built to foster relationships that include educational blogs, portfolios, formal and ad hoc communities, as well as communication such as chats, discussion threads, and synchronous forums. These sites also have content sharing and rating features. Social networks are also emerging as online yearbooks, both public and private. One such service is MyYearbook, which allows anyone from the general public to register and connect. A new trend emerging is  private label yearbooks accessible only by students, parents, and teachers of a particular school, similar to Facebook's beginning within Harvard.[citation needed]

The use of virtual currency systems inside social networks create new opportunities for global finance. Hub Culture operates a virtual currency Ven used for global transactions among members, product sales[145] and financial trades in commodities and carbon credits.[146][147] In May 2010, carbon pricing contracts were introduced to the weighted basket of currencies and commodities that determine the floating exchange value of Ven. The introduction of carbon to the calculation price of the currency made Ven the first and only currency that is linked to the environment.[148]

Social networks are beginning to be adopted by healthcare professionals as a means to manage institutional knowledge, disseminate peer to peer knowledge and to highlight individual physicians and institutions. The advantage of using a dedicated medical social networking site is that all the members are screened against the state licensing board list of practitioners.[149] A new trend is emerging with social networks created to help its members with various physical and mental ailments.[150] For people suffering from life-altering diseases or chronic health conditions, companies such as HealthUnlocked and PatientsLikeMe offers their members the chance to connect with others dealing with similar issues and share experiences. For alcoholics and addicts, SoberCircle gives people in recovery the ability to communicate with one another and strengthen their recovery through the encouragement of others who can relate to their situation. DailyStrength is also a website that offers support groups for a wide array of topics and conditions, including the support topics offered by PatientsLikeMe and SoberCircle. Some social networks aim to encourage healthy lifestyles in their users. SparkPeople and HealthUnlocked offer community and social networking tools for peer support during weight loss. Fitocracy and QUENTIQ are focused on exercise, enabling users to share their own workouts and comment on those of other users. Other aspects of social network usage include the analysis of data coming from existing social networks (such as Twitter) to discover large crowd concentration events (based on tweets location statistical analysis) and disseminate the information to e.g. mobility-challenged individuals for e.g. avoiding the specific areas and optimizing their journey in an urban environment.[151]

Social networking sites have recently showed a value in social and political movements.[152] In the Egyptian revolution, Facebook and Twitter both played an allegedly pivotal role in keeping people connected to the revolt. Egyptian activists have credited social networking sites with providing a platform for planning protest and sharing news from Tahrir Square in real time. By presenting a platform for thousands of people to instantaneously share videos of mainly events featuring brutality, social networking can be a vital tool in revolutions.[153] On the flip side, social networks enable government authorities to easily identify, and repress, protestors and dissidents.[154] Another political application of social media is promoting the involvement of younger generations in politics and ongoing political issues.[155]

Perhaps the most significant political application of social media is Barack Obama's election campaign in 2008. It was the first of its kind, as it successfully incorporated social media into its campaign winning strategy, evolving the way of political campaigns forevermore in the ever-changing technological world we find ourselves in today. His campaign won by engaging everyday people and empowering volunteers, donors, and advocates, through social networks, text messaging, email messaging and online videos.[156] Obama's social media campaign was vast, with his campaign boasting 5 million 'friends' on over 15 social networking sites, with over 3 million friends just on Facebook.[157] Another significant success of the campaign was online videos, with nearly 2,000 YouTube videos being put online, receiving over 80 million views.[157]

In 2007, when Obama first announced his candidacy, there was no such thing as an iPhone or Twitter. However, a year later, Obama was sending out voting reminders to thousands of people through Twitter, showing just how fast social media moves. Obama's campaign was current and needed to be successful in incorporating social media, as social media acts best and is most effective in real time.[158]

Building up to the 2012 presidential election, it was interesting to see how strong the influence of social media would be following the 2008 campaigns, where Obama's winning campaign had been social media-heavy, whereas McCain's campaign did not really grasp social media. John F. Kennedy was the first president who really understood television, and similarly, Obama is the first president to fully understand the power of social media.[159] Obama has recognized social media is about creating relationships and connections and therefore used social media to the advantage of presidential election campaigns, in which Obama has dominated his opponents in terms of social media space.

Other political campaigns have followed on from Obama's successful social media campaigns, recognizing the power of social media and incorporating it as a key factor embedded within their political campaigns, for example, Donald Trump's presidential electoral campaign, 2016. Dan Pfeiffer, Obama's former digital and social media guru, commented that Donald Trump is ""way better at the internet than anyone else in the GOP which is partly why he is winning"".[160]

Research has shown that 66% of social media users actively engage in political activity online, and like many other behaviors, online activities translate into offline ones.[159] With research from the 'MacArthur Research Network on Youth and Participatory Politics' stating that young people who are politically active online are double as likely to vote than those who are not politically active online.[159] Therefore, political applications of social networking sites are crucial, particularly to engage with the youth, who perhaps are the least educated in politics and the most in social networking sites. Social media is, therefore, a very effective way in which politicians can connect with a younger audience through their political campaigns.[161]

On June 28, 2020, The New York Times released an article sharing the finding of two researchers who studied the impact of TikTok, a video-sharing and social networking application, on political expression. The application, besides being a creative space to express oneself, has been used maliciously to spread disinformation ahead of US President Donald Trump's Tulsa rally in Oklahoma and amplified footage of police brutality at Black Lives Matter protests.[162]

Crowdsourcing social media platform, such as Design Contest, Arcbazar, Tongal, combined group of professional freelancers, such as designers, and help them communicate with business owners interested in their suggestion. This process is often used to subdivide tedious work or to fund-raise startup companies and charities, and can also occur offline.[163]

There are a number of projects that aim to develop free and open source software to use for social networking services. These technologies are often referred to as social engine or social networking engine software.

The following is a list of the largest social networking services, in order by number of active users, as of January 2024, as published by Statista:[164]

*Platforms have not published updated user figures in the past 12 months, figures may be out of date and less reliable
**Figure uses daily active users, so monthly active user number is likely higher
"
Branding Services,"A branding agency is a firm that specializes in creating and launching brands and rebranding. The role of a branding agency is to create, plan, measure, and manage branding strategies for clients, including support in advertising and other forms of promotion.

Branding is the process of developing a company's brand, including name, identity system, brand architecture, and messaging platform. These aspects will lead to the development of a ""brand message"", applied to marketing campaigns. A brand represents a promise to the customer, reflecting the expectations they can have from the products and services offered, as well as the offering difference amongst the competitors.[1]

A branding agency allows organizations to gain a competitive advantage, to define a coherent brand communication strategy, and to reach the target market and expand it.

Although branding agencies and advertising agencies overlap in some aspects, they have different scopes and focus. 

Celebrity endorsement benefits both branding and advertising agencies, promoting them on social media, such as Instagram and Facebook. 'Firms invest significant monies in just a posting brands and organizations with endorser qualities such as attractiveness, likeability, and trustworthiness. They trust that these qualities operate in a transferable way, and, will generate desirable campaign outcomes' (Journal of marketing management, volume 15 1999). Examples of celebrities that act as ambassadors for obtaining ""likes"" for Coca-Cola and L'Oreal are Selena Gomez and Cheryl Cole.

The evolving economy with increasingly competitive markets is creating a greater stress on developing brands that aim at being widely recognized by the public. 

Coca-Cola is globally considered as one of the most successful and recognized brand. A survey conducted by Interbrand, an American branding consultancy, found that Coca-Cola's brand equity was valued at $63.5 billion, representing just under half of the company's true market value.[2] An executive at Coca-Cola stated, ""If Coca-Cola were to lose all of its production-related assets in a disaster, the company would survive. By contrast, if all consumers were to have a sudden lapse of memory and forget everything related to Coca-Cola, the company would go out of business.""[3]

Coca-Cola has collaborated with a number of branding agencies over the years. Most recently, in the light of a new global campaign, Coca-Cola has partnered with ten agencies (including WPP, Wieden+Kennedy and McCann) with the aim, as a Coca-Cola spokesperson stated, ""of uncovering the best ideas and marrying those to executional excellence"". The spokesperson added, ""this approach allows us to harness thinking from some of the best agency minds in the industry and see the great work that comes from collaborating"".[4] The statement acknowledges the collaborative and creative process involved in brand management, which is generally an integral concept involved in branding.

Even the most successful brands will seek branding support. This will often be more focused on specific areas, and in particular those related to reaching, attracting and retaining the target market.[5]

Despite having headed Interbrand's list of the 100 most valuable brands for 13 consecutive years, Coca-Cola has now been replaced by Apple, whose surge to the top highlights the dominance of technology today. Three of the top five most valuable brands are technology companies.[6]

This data stresses another key element that must be addressed by branding agencies: the value of innovation and the need to exploit trends. The rise of the technology domain has resulted in the emergence of new channels, such as Facebook, Instagram, Google and Twitter. 

This change in trends results in new approaches to branding.  Agencies must be naturally innovative and flexible in order to survive a fast-paced environment. With branding becoming an increasingly vital element to a firm's success, many organizations believe it is in their best interest to seek external support.

[7]

[1]
"
Graphic Design Services,"

Graphic design is a profession,[2] academic discipline[3][4][5] and applied art whose activity consists in projecting visual communications intended to transmit specific messages to social groups, with specific objectives.[6] Graphic design is an interdisciplinary branch of design[1] and of the fine arts. Its practice involves creativity, innovation and lateral thinking using manual or digital tools, where it is usual to use text and graphics to communicate visually.

The role of the graphic designer in the communication process is that of the encoder or interpreter of the message. They work on the interpretation, ordering, and presentation of visual messages. In its nature, design pieces can be philosophical, aesthetic, emotional and political.[7] Usually, graphic design uses the aesthetics of typography and the compositional arrangement of the text, ornamentation, and imagery to convey ideas, feelings, and attitudes beyond what language alone expresses. The design work can be based on a customer's demand, a demand that ends up being established linguistically, either orally or in writing, that is, that graphic design transforms a linguistic message into a graphic manifestation.[8]

Graphic design has, as a field of application, different areas of knowledge focused on any visual communication system. For example, it can be applied in advertising strategies, or it can also be applied in the aviation world[9] or space exploration.[10][11] In this sense, in some countries graphic design is related as only associated with the production of sketches and drawings, this is incorrect, since visual communication is a small part of a huge range of types and classes where it can be applied.

With origins in Antiquity and the Middle Ages,[12] graphic design as applied art was initially linked to the boom of the rise of printing in Europe in the 15th century and the growth of consumer culture in the Industrial Revolution. From there it emerged as a distinct profession in the West, closely associated with advertising in the 19th century[13] and its evolution allowed its consolidation in the 20th century. Given the rapid and massive growth in information exchange today, the demand for experienced designers is greater than ever, particularly because of the development of new technologies and the need to pay attention to human factors beyond the competence of the engineers who develop them.[14]

The term ""graphic design"" makes an early appearance in a 4 July 1908 issue (volume 9, number 27) of Organized Labor, a publication of the Labor Unions of San Francisco, in an article about technical education for printers:[15]

An Enterprising Trades Union
… The admittedly high standard of intelligence which prevails among printers is an assurance that with the elemental principles of design at their finger ends many of them will grow in knowledge and develop into specialists in graphic design and decorating. …

A decade later, the 1917–1918 course catalog of the California School of Arts & Crafts advertised a course titled Graphic Design and Lettering, which replaced one called Advanced Design and Lettering. Both classes were taught by Frederick Meyer.[16]

In both its lengthy history and in the relatively recent explosion of visual communication in the 20th and 21st centuries, the distinction between advertising, art, graphic design and fine art has disappeared. They share many elements, theories, principles, practices, languages and sometimes the same benefactor or client. In advertising, the ultimate objective is the sale of goods and services. In graphic design, ""the essence is to give order to information, form to ideas, expression, and feeling to artifacts that document the human experience.""[17]

The definition of the graphic designer profession is relatively recent concerning its preparation, activity, and objectives. Although there is no consensus on an exact date when graphic design emerged, some date it back to the Interwar period. Others understand that it began to be identified as such by the late 19th century.[12]

It can be argued that graphic communications with specific purposes have their origins in Paleolithic cave paintings and the birth of written language in the third millennium BCE. However, the differences in working methods, auxiliary sciences, and required training are such that it is not possible to clearly identify the current graphic designer with prehistoric man, the 15th-century xylographer, or the lithographer of 1890.

The diversity of opinions stems from some considering any graphic manifestation as a product of graphic design, while others only recognize those that arise as a result of the application of an industrial production model—visual manifestations that have been ""projected"" to address various needs: productive, symbolic, ergonomic, contextual, among others.

By the late 19th century, graphic design emerged as a distinct profession in the West, partly due to the process of labor specialization that occurred there and partly due to the new technologies and business possibilities brought about by the Industrial Revolution. New production methods led to the separation of the design of a communication medium (such as a poster) from its actual production. Increasingly, throughout the 19th and early 20th centuries, advertising agencies, book publishers, and magazines hired art directors who organized all visual elements of communication and integrated them into a harmonious whole, creating an expression appropriate to the content. In 1922, typographer William A. Dwiggins coined the term graphic design to identify the emerging field.[12]

Throughout the 20th century, the technology available to designers continued to advance rapidly, as did the artistic and commercial possibilities of design. The profession expanded greatly, and graphic designers created, among other things, magazine pages, book covers, posters, CD covers, postage stamps, packaging, brands, signs, advertisements, kinetic titles for TV programs and movies, and websites. By the early 21st century, graphic design had become a global profession as advanced technology and industry spread worldwide.[12]

In China, during the Tang dynasty (618–907) wood blocks were cut to print on textiles and later to reproduce Buddhist texts. A Buddhist scripture printed in 868 is the earliest known printed book. Beginning in the 11th century in China, longer scrolls and books were produced using movable type printing, making books widely available during the Song dynasty (960–1279).[18]

In Mesopotamia, writing (as an extension of graphic design) began with commerce. The earliest writing system, cuneiform, started out with basic pictograms, which were representations of houses, lambs, or grain.[19]

In the mid-15th century in Mainz, Germany, Johannes Gutenberg developed a way to reproduce printed pages at a faster pace using movable type made with a new metal alloy[20] that created a revolution in the dissemination of information.[21]

In 1849, Henry Cole became one of the major forces in design education in Great Britain, informing the government of the importance of design in his Journal of Design and Manufactures. He organized the Great Exhibition as a celebration of modern industrial technology and Victorian design.

From 1891 to 1896, William Morris' Kelmscott Press was a leader in graphic design associated with the Arts and Crafts movement, creating hand-made books in medieval and Renaissance era style,[22] in addition to wallpaper and textile designs.[23] Morris' work, along with the rest of the Private Press movement, directly influenced Art Nouveau.[24]

Will H. Bradley became one of the notable graphic designers in the late nineteenth-century due to creating art pieces in various Art Nouveau styles. Bradley created a number of designs as promotions for a literary magazine titled The Chap-Book.[25]

In 1917, Frederick H. Meyer, director and instructor at the California School of Arts and Crafts, taught a class entitled ""Graphic Design and Lettering"".[26] Raffe's Graphic Design, published in 1927, was the first book to use ""Graphic Design"" in its title.[27] In 1936, author and graphic designer Leon Friend published his book titled ""Graphic Design"" and it is known to be the first piece of literature to cover the topic extensively.[28]

The signage in the London Underground is a classic design example[29] of the modern era. Although he lacked artistic training, Frank Pick led the Underground Group design and publicity movement. The first Underground station signs were introduced in 1908 with a design of a solid red disk with a blue bar in the center and the name of the station. The station name was in white sans-serif letters. It was in 1916 when Pick used the expertise of Edward Johnston to design a new typeface for the Underground. Johnston redesigned the Underground sign and logo to include his typeface on the blue bar in the center of a red circle.[30]

In the 1920s, Soviet constructivism applied 'intellectual production' in different spheres of production. The movement saw individualistic art as useless in revolutionary Russia and thus moved towards creating objects for utilitarian purposes. They designed buildings, film and theater sets, posters, fabrics, clothing, furniture, logos, menus, etc.[31]

Jan Tschichold codified the principles of modern typography in his 1928 book, New Typography.[32] He later repudiated the philosophy he espoused in this book as fascistic, but it remained influential.[citation needed] Tschichold, Bauhaus typographers such as Herbert Bayer and László Moholy-Nagy and El Lissitzky greatly influenced graphic design. They pioneered production techniques[citation needed] and stylistic devices used throughout the twentieth century. The following years saw graphic design in the modern style gain widespread acceptance and application.[33]

The professional graphic design industry grew in parallel with consumerism. This raised concerns and criticisms, notably from within the graphic design community with the First Things First manifesto. First launched by Ken Garland in 1964, it was re-published as the First Things First 2000 manifesto in 1999 in the magazine Emigre 51[34] stating ""We propose a reversal of priorities in favor of more useful, lasting and democratic forms of communication – a mindshift away from product marketing and toward the exploration and production of a new kind of meaning. The scope of debate is shrinking; it must expand. Consumerism is running uncontested; it must be challenged by other perspectives expressed, in part, through the visual languages and resources of design.""[35]

Graphic design can have many applications, from road signs to technical schematics and reference manuals. It is often used in branding products and elements of company identity such as logos, colors, packaging, labelling and text.

From scientific journals to news reporting, the presentation of opinions and facts is often improved with graphics and thoughtful compositions of visual information – known as information design.  With the advent of the web, information designers with experience in interactive tools are increasingly used to illustrate the background to news stories. Information design can include Data and information visualization, which involves using programs to interpret and form data into a visually compelling presentation, and can be tied in with information graphics.

A graphic design project may involve the creative presentation of existing text, ornament, and images.

The ""process school"" is concerned with communication; it highlights the channels and media through which messages are transmitted and by which senders and receivers encode and decode these messages. The semiotic school treats a message as a construction of signs which through interaction with receivers, produces meaning; communication as an agent.[citation needed]

Typography includes type design, modifying type glyphs and arranging type. Type glyphs (characters) are created and modified using illustration techniques. Type arrangement is the selection of typefaces, point size, tracking (the space between all characters used), kerning (the space between two specific characters) and leading (line spacing).

Typography is performed by typesetters, compositors, typographers, graphic artists, art directors, and clerical workers. Until the digital age, typography was a specialized occupation. Certain fonts communicate or resemble stereotypical notions. For example, the 1942 Report is a font which types text akin to a typewriter or a vintage report.[36]

Page layout deals with the arrangement of elements (content) on a page, such as image placement, text layout and style. Page design has always been a consideration in printed material and more recently extended to displays such as web pages. Elements typically consist of type (text), images (pictures), and (with print media) occasionally place-holder graphics such as a dieline for elements that are not printed with ink such as die/laser cutting, foil stamping or blind embossing.

A grid serves as a method of arranging both space and information, allowing the reader to easily comprehend the overall project. Furthermore, a grid functions as a container for information and a means of establishing and maintaining order. Despite grids being utilized for centuries, many graphic designers associate them with Swiss design. The desire for order in the 1940s resulted in a highly systematic approach to visualizing information. However, grids were later regarded as tedious and uninteresting, earning the label of ""designersaur."" Today, grids are once again considered crucial tools for professionals, whether they are novices or veterans.[37]

In the mid-1980s desktop publishing and graphic art software applications introduced computer image manipulation and creation capabilities that had previously been manually executed. Computers enabled designers to instantly see the effects of layout or typographic changes, and to simulate the effects of traditional media. Traditional tools such as pencils can be useful even when computers are used for finalization; a designer or art director may sketch numerous concepts as part of the creative process.[38] Styluses can be used with tablet computers to capture hand drawings digitally.[39]

Designers disagree whether computers enhance the creative process.[40] Some designers argue that computers allow them to explore multiple ideas quickly and in more detail than can be achieved by hand-rendering or paste-up.[41] While other designers find the limitless choices from digital design can lead to paralysis or endless iterations with no clear outcome.

Most designers use a hybrid process that combines traditional and computer-based technologies. First, hand-rendered layouts are used to get approval to execute an idea, then the polished visual product is produced on a computer.

Graphic designers are expected to be proficient in software programs for image-making, typography and layout. Nearly all of the popular and ""industry standard"" software programs used by graphic designers since the early 1990s are products of Adobe Inc. Adobe Photoshop (a raster-based program for photo editing) and Adobe Illustrator (a vector-based program for drawing) are often used in the final stage. CorelDraw, a vector graphics editing software developed and marketed by Corel Corporation, is also used worldwide. Designers often use pre-designed raster images and vector graphics in their work from online design databases. Raster images may be edited in Adobe Photoshop, vector logos and illustrations in Adobe Illustrator and CorelDraw, and the final product assembled in one of the major page layout programs, such as Adobe InDesign, Serif PagePlus and QuarkXPress.

Many free and open-source programs are also used by both professionals and casual graphic designers. Inkscape uses Scalable Vector Graphics (SVG) as its primary file format and allows importing and exporting other formats. Other open-source programs used include GIMP for photo-editing and image manipulation, Krita for digital painting, and Scribus for page layout.

A specialized branch of graphic design and historically its earliest form, print design involves creating visual content intended for reproduction on physical substrates such as silk, paper, and later, plastic, for mass communication and persuasion (e.g., marketing, governmental publishing, propaganda). Print design techniques have evolved over centuries, beginning with the invention of movable type by the Chinese alchemist Pi Sheng, later refined by the German inventor Johannes Gutenberg. Over time, methods such as lithography, screen printing, and offset printing have been developed, culminating in the contemporary use of digital presses that integrate traditional print techniques with modern digital technology.

Since the advent of personal computers, many graphic designers have become involved in interface design, in an environment commonly referred to as a Graphical user interface (GUI). This has included web design and software design when end user-interactivity is a design consideration of the layout or interface. Combining visual communication skills with an understanding of user interaction and online branding, graphic designers often work with software developers and web developers to create the look and feel of a web site or software application. An important aspect of interface design is icon design.

User experience design (UX) is the study, analysis, and development of creating products that provide meaningful and relevant experiences to users. This involves the creation of the entire process of acquiring and integrating the product, including aspects of branding, design, usability, and function. UX design involves creating the interface and interactions for a website or application, and is considered both an act and an art. This profession requires a combination of skills, including visual design, social psychology, development, project management, and most importantly, empathy towards the end-users.[42]

Experiential graphic design is the application of communication skills to the built environment.[43] This area of graphic design requires practitioners to understand physical installations that have to be manufactured and withstand the same environmental conditions as buildings. As such, it is a cross-disciplinary collaborative process involving designers, fabricators, city planners, architects, manufacturers and construction teams.

Experiential graphic designers try to solve problems that people encounter while interacting with buildings and space (also called environmental graphic design). Examples of practice areas for environmental graphic designers are wayfinding, placemaking, branded environments, exhibitions and museum displays, public installations and digital environments.

Graphic design career paths cover all parts of the creative spectrum and often overlap. Workers perform specialized tasks, such as design services, publishing, advertising and public relations. As of 2023, median pay was $58,910 per year.[45] The main job titles within the industry are often country specific. They can include graphic designer, art director, creative director, animator and entry level production artist. Depending on the industry served, the responsibilities may have different titles such as ""DTP associate"" or ""Graphic Artist"". The responsibilities may involve specialized skills such as illustration, photography, animation, visual effects or interactive design.

Employment in design of online projects was expected to increase by 35% by 2026, while employment in traditional media, such as newspaper and book design, expect to go down by 22%. Graphic designers will be expected to constantly learn new techniques, programs, and methods.[46]

Graphic designers can work within companies devoted specifically to the industry, such as design consultancies or branding agencies, others may work within publishing, marketing or other communications companies. Especially since the introduction of personal computers, many graphic designers work as in-house designers in non-design oriented organizations.  Graphic designers may also work freelance, working on their own terms, prices, ideas, etc.

A graphic designer typically reports to the art director, creative director or senior media creative. As a designer becomes more senior, they spend less time designing and more time leading and directing other designers on broader creative activities, such as brand development and corporate identity development. They are often expected to interact more directly with clients, for example taking and interpreting briefs.

Jeff Howe of Wired Magazine first used the term ""crowdsourcing"" in his 2006 article, ""The Rise of Crowdsourcing.""[47][48] It spans such creative domains as graphic design, architecture, apparel design, writing, illustration, and others. Tasks may be assigned to individuals or a group and may be categorized as convergent or divergent. An example of a divergent task is generating alternative designs for a poster. An example of a convergent task is selecting one poster design. Companies, startups, small businesses and entrepreneurs have all benefitted from design crowdsourcing since it helps them source great graphic designs at a fraction of the budget they used to spend before. Getting a logo design through crowdsourcing being one of the most common. Major companies that operate in the design crowdsourcing space are generally referred to as design contest sites.[citation needed]]

Graphic design is essential for advertising, branding, and marketing, influencing how people act.  Good graphic design builds strong, recognizable brands, communicates messages clearly, and shapes how consumers see and react to things.

One way that graphic design influences consumer behavior is through the use of visual elements, such as color, typography, and imagery. Studies have shown that certain colors can evoke specific emotions and behaviors in consumers, and that typography can influence how information is perceived and remembered.[49] For example, serif fonts are often associated with tradition and elegance, while sans-serif fonts are seen as modern and minimalistic. These factors can all impact the way consumers perceive a brand and its messaging.[50]

Another way that graphic design impacts consumer behavior is through its ability to communicate complex information in a clear and accessible way. For example, infographics and data visualizations can help to distill complex information into a format that is easy to understand and engaging for consumers.[51] This can help to build trust and credibility with consumers, and encourage them to take action.

Ethics are an important consideration in graphic design, particularly when it comes to accurately representing information and avoiding harmful stereotypes. Graphic designers have a responsibility to ensure that their work is truthful, accurate, and free from any misleading or deceptive elements. This requires a commitment to honesty, integrity, and transparency in all aspects of the design process.

One of the key ethical considerations in graphic design is the responsibility to accurately represent information. This means ensuring that any claims or statements made in advertising or marketing materials are true and supported by evidence.[52] For example, a company should not use misleading statistics to promote their product or service, or make false claims about its benefits. Graphic designers must take care to accurately represent information in all visual elements, such as graphs, charts, and images, and avoid distorting or misrepresenting data.[53]

Another important ethical consideration in graphic design is the need to avoid harmful stereotypes. This means avoiding any images or messaging that perpetuate negative or harmful stereotypes based on race, gender, religion, or other characteristics.[54] Graphic designers should strive to create designs that are inclusive and respectful of all individuals and communities, and avoid reinforcing negative attitudes or biases.

The future of graphic design is likely to be heavily influenced by emerging technologies and social trends. Advancements in areas such as artificial intelligence, virtual and augmented reality, and automation are likely to transform the way that graphic designers work and create designs. Social trends, such as a greater focus on sustainability and inclusivity, are also likely to impact the future of graphic design.[55]

One area where emerging technologies are likely to have a significant impact on graphic design is in the automation of certain tasks. Easily accessible computer software using AI algorithms will complete many practical tasks performed by graphic designers, allowing clients to bypass human designers altogether. [56]Machine learning algorithms, for example, can analyze large datasets and create designs based on patterns and trends, freeing up designers to focus on more complex and creative tasks. Virtual and augmented reality technologies may also allow designers to create immersive and interactive experiences for users, blurring the lines between the digital and physical worlds.[57] Artificial intelligence has also led to many challenges within the world of graphic design. Some of those challenges include maintaining brand authenticity, ensuring quality, issues of bias, and preserving creative control.[58]

Visual communication design education is ill prepared for automation, artificial intelligence and machine learning.[59]

Social trends are also likely to shape the future of graphic design. As consumers become more conscious of environmental issues, for example, there may be a greater demand for designs that prioritize sustainability and minimize waste. Similarly, there is likely to be a growing focus on inclusivity and diversity in design, with designers seeking to create designs that are accessible and representative of a wide range of individuals and communities.[60]
"
Advertising Services,"

An advertising agency, often referred to as a creative agency or an ad agency, is a business dedicated to creating, planning, and handling advertising and sometimes other forms of promotion and marketing for its clients. An ad agency is generally independent of the client; it may be an internal department or agency that provides an outside point of view to the effort of selling the client's products or services, or an outside firm. An agency can also handle overall marketing and branding strategies promotions for its clients, which may include sales as well.

Typical ad agency clients include businesses and corporations, non-profit organizations and private agencies. Agencies may be hired to produce television advertisements, radio advertisements, online advertising, out-of-home advertising, mobile marketing, and AR advertising, as part of an advertising campaign.

The first acknowledged advertising agency was William Taylor in 1786. Another early agency, started by James 'Jem' White in 1800 at Fleet Street, London, eventually evolved into White Bull Holmes, a recruitment advertising agency, that went out of business in the late 1980s.[1][2] In 1812 George Reynell, an officer at the London Gazette, set up another of the early advertising agencies, also in London.[1] This remained a family business until 1993, as 'Reynell & Son,' and is now part of the TMP Worldwide agency (UK and Ireland) under the brand TMP Reynell.[1] Another early agency that traded until recently, was founded by Charles Barker, and the firm he established traded as 'Barkers' until 2009 when it went into Administration.

Volney B. Palmer opened the first American advertising agency, in Philadelphia in 1850. This agency placed ads produced by its clients in various newspapers.[3]

In 1856, Mathew Brady created the first modern advertisement when he placed an ad in the New York Herald paper offering to produce ""photographs, ambrotypes, and daguerreotypes."" His ads were the first whose typeface and fonts were distinct from the text of the publication and from that of other advertisements. At that time all newspaper ads were set in agate and only agate. His use of larger distinctive fonts caused a sensation.[3] Later that same year Robert E. Bonner ran the first full-page ad in a newspaper.[3]

In 1864, William James Carlton began selling advertising space in religious magazines. In 1869, Francis Ayer, at the age of 20, created the first full-service advertising agency in Philadelphia, called N.W. Ayer & Son. It was the oldest advertising agency in America and dissolved in 2002.
James Walter Thompson joined Carlton's firm in 1868. Thompson rapidly became their best salesman, purchasing the company in 1877 and renaming it the James Walter Thompson Company. Realizing that he could sell more space if the company provided the service of developing content for advertisers, Thompson hired writers and artists to form the first known Creative Department in an advertising agency. He is credited as the ""father of modern magazine advertising"" in the US.[3] Advertising Age commemorated the first 100 years of the agency in 1964, noting that its  ""history and expansion overseas seems peculiarly to match the whole history of modern advertising.""[4]

Globalization of advertising originates in earlier days of the twentieth century. American advertising agencies began as the process of opening overseas offices before the two World Wars and accelerated their globalization throughout the latter part of the twentieth century.

McCann,  an agency established in New York City in 1902, opened its first European offices by 1927. It was followed up with offices opening in South America in 1935 and in Australia in 1959.[5]

Companies such as J. Walter Thompson adopted a strategy to expand in order to provide advertising services wherever clients operated.

In the 1960s and 1970s, English agencies began to realize the overseas opportunities associated with globalization.[6] Expanding overseas gives potential to wider markets.

In the early 21st century, management consulting firms such as PwC Digital and Deloitte Digital began competing with ad agencies by emphasizing data analytics. As of 2017[update], Accenture Interactive was the world's sixth-largest ad agency, behind WPP, Omnicom, Publicis, Interpublic, and Dentsu. In 2019, it purchased David Droga's Droga5 agency, the first major consultant acquisition of an ad agency.[7]

Studies show that successful advertising agencies tend to have a shared sense of purpose with their clients through collaboration. This includes a common set of client objectives where agencies feel a shared sense of ownership of the strategic process. Successful advertisements start with clients building a good relationship with the agencies and work together to figure out what their objectives are. Clients must trust the agencies to do their jobs correctly and accordingly with the resources they have provided. Breakdowns in relationships were more likely to occur when agencies felt undermined, subjugated, or even feel they do not have equal status. Traditionally advertising agencies tend to be in a position to take the lead on projects[8] but results are best when there is a more collaborative relationship.

Stronger collaboration happens in situations where a personal chemistry has been established between both parties. Finding out similar likes and dislikes points of view, and even hobbies and passions. Personal chemistry builds with the length of the client relationship, frequency of meetings, and how far mutual respect goes between parties. This was one trait that advertising agencies were perceived to not always have. It was suggested that on occasions media planners and researchers were more closely involved in the project because of their personal relationships with their clients.[9] Successful strategic planning is best when both parties are involved due to the bond between sides by understanding each other's views and mindset.

Involved advertising account planners are seen to contribute towards successful agency-client collaboration. Planners of advertising agencies tend to be capable of creating a very powerful, trusting relationship with their clients because they were seen as intellectual prowess, seniority, and have empathy in the creative process.

All advertising agencies are called that because they are acting as agents for their principals which were the media. They were then, and are now, paid by the media to sell advertising space to clients. Originally, in the 18th century, and the first half of the 19th, advertising agencies made all of their income from commissions paid by the media for selling space to the client.[10]

Although it is still the case that the majority of their income comes from the media, in the middle of the 19th century, agencies began to offer additional services which they sold directly to the client. Such services can include writing the text of the advertisement.[10]

The use of creativity by agencies is ""unexpected"" because so much advertising today is expected.  This will capture the attention of audiences, therefore the message is more likely to get through. There have been many advertisements that have surprised audiences because it was not normal for them to see that in an advertisement of that nature. The best use of creativity is when the agencies make consumers think about the product or brand. The type of creativity is distinctive communication which is breaking through the clutter.[11]

The ""big six"" largest agencies, with their estimated worldwide revenues in quarter three of 2021 were:[12]
"
Corporate Training Services,"Corporate education refers to a system of professional development activities provided to educate employees.  It may consist of formal university or college training or informal training provided by non-collegiate institutions.  The simplest form of corporate education may be training programs designed ""in-house"" for an organization that may wish to train their employees on specific aspects of their job processes or responsibilities.  More formal relationships may further exist where corporate training is provided to employees through contracts or relationships with educational institutions who may award credit, either at the institution or through a system of CEUs (Continuing Education Units).

Many institutions or trainers offering corporate education will provide certificates or diplomas verifying the attendance of the employee.  Some employers use corporate and continuing education as part of a holistic human resources effort to determine the performance of the employee and as part of their review systems.

Increasingly organisations appear to be using corporate education and training as an incentive to retain managers and key employees within their organisation. This win-win arrangement creates better educated managers for the organisation and provides the employees with a more marketable portfolio of skills and, in many cases, recognised qualifications. 

Most organisations tend to think of corporate education as corporate training. Corporate training programs are often competency based and related to the essential training employees need to operate certain equipment or perform certain tasks in a competent, safe and effective manner. The outcome of a corporate training program is a participant who is either able to operate a piece of equipment or perform a specific task in an effective manner according to pre-determined training criteria. 

The primary role of corporate training is to ensure an employee has the knowledge and skills to undertake a specific operation to enable an organisation can continue to operate. Fundamentally, corporate training is centred on knowledge transfer, with an instructor teaching or demonstrating a particular function and the student learning and demonstrating they can apply what they have learnt to a particular operation.

Corporate education, however, adds another dimension and depth to training by involving learners as participants in generating new knowledge that assists an organisation to develop and evolve, rather than maintain the status quo. Corporate education focuses on developing the capability of an organisation to be able to do things and, in particular, the right things in order to be a sustainable and successful organisation.

Corporate education involves a facilitator, rather than an instructor or trainer, to engage participants and encourage them to think about the what, how and why of what they are doing and to challenge their current paradigms. Corporate education is centred on introducing learning techniques to stimulate employees to think about what their organisation does, where it is heading, potential new opportunities for the organisation and new and better ways of doing things. While the role of corporate training is to develop the operational competency of individuals, the purpose of corporate education is to promote the development of capability of both an individual and their organisation.[1]

Increasingly organisations  appear to be using corporate education as an incentive to retain managers and key employees within their organisation. This win-win arrangement creates better educated managers and employees for the organisation and gives individual employees a more marketable portfolio of skills and, in many cases, recognised qualifications.[2]
"
Health and Safety Consulting,"

Occupational safety and health (OSH) or occupational health and safety (OHS) is a multidisciplinary field concerned with the safety, health, and welfare of people at work (i.e., while performing duties required by one's occupation). OSH is related to the fields of occupational medicine and occupational hygiene[a] and aligns with workplace health promotion initiatives. OSH also protects all the general public who may be affected by the occupational environment.[4]

According to the official estimates of the United Nations, the WHO/ILO Joint Estimate of the Work-related Burden of Disease and Injury, almost 2 million people die each year due to exposure to occupational risk factors.[5] Globally, more than 2.78 million people die annually as a result of workplace-related accidents or diseases, corresponding to one death every fifteen seconds. There are an additional 374 million non-fatal work-related injuries annually. It is estimated that the economic burden of occupational-related injury and death is nearly four per cent of the global gross domestic product each year. The human cost of this adversity is enormous.[6]

In common-law jurisdictions, employers have the common law duty (also called duty of care) to take reasonable care of the safety of their employees.[7] Statute law may, in addition, impose other general duties, introduce specific duties, and create government bodies with powers to regulate occupational safety issues. Details of this vary from jurisdiction to jurisdiction.

Prevention of workplace incidents and occupational diseases is addressed through the implementation of occupational safety and health programs at company level.[8]

The International Labour Organization (ILO) and the World Health Organization (WHO) share a common definition of occupational health.[b] It was first adopted by the Joint ILO/WHO Committee on Occupational Health at its first session in 1950:[10][11]

Occupational health should aim at the promotion and maintenance of the highest degree of physical, mental and social well-being of workers in all occupations; the prevention amongst workers of departures from health caused by their working conditions; the protection of workers in their employment from risks resulting from factors adverse to health; the placing and maintenance of the worker in an occupational environment adapted to his physiological and psychological capabilities and; to summarize: the adaptation of work to man and of each man to his job.
In 1995, a consensus statement was added:[10][11]

The main focus in occupational health is on three different objectives: (i) the maintenance and promotion of workers' health and working capacity; (ii) the improvement of working environment and work to become conducive to safety and health and (iii) development of work organizations and working cultures in a direction which supports health and safety at work and in doing so also promotes a positive social climate and smooth operation and may enhance productivity of the undertakings. The concept of working culture is intended in this context to mean a reflection of the essential value systems adopted by the undertaking concerned. Such a culture is reflected in practice in the managerial systems, personnel policy, principles for participation, training policies and quality management of the undertaking.
An alternative definition for occupational health given by the WHO is: ""occupational health deals with all aspects of health and safety in the workplace and has a strong focus on primary prevention of hazards.""[12]

The expression ""occupational health"", as originally adopted by the WHO and the ILO, refers to both short- and long-term adverse health effects. In more recent times, the expressions ""occupational safety and health"" and ""occupational health and safety"" have come into use (and have also been adopted in works by the ILO),[13] based on the general understanding that occupational health refers to hazards associated to disease and long-term effects, while occupational safety hazards are those associated to work accidents causing injury and sudden severe conditions.[14]

Research and regulation of occupational safety and health are a relatively recent phenomenon. As labor movements arose in response to worker concerns in the wake of the industrial revolution, workers' safety and health entered consideration as a labor-related issue.[15]

Written works on occupational diseases began to appear by the end of the 15th century, when demand for gold and silver was rising due to the increase in trade and iron, copper, and lead were also in demand from the nascent firearms market. Deeper mining became common as a consequence. In 1473, Ulrich Ellenbog [de], a German physician, wrote a short treatise On the Poisonous Wicked Fumes and Smokes, focused on coal, nitric acid, lead, and mercury fumes encountered by metal workers and goldsmiths. In 1587, Paracelsus (1493–1541) published the first work on the mine and smelter workers diseases. In it, he gave accounts of miners' ""lung sickness"". In 1526, Georgius Agricola's (1494–1553) De re metallica, a treaty on metallurgy, described accidents and diseases prevalent among miners and recommended practices to prevent them. Like Paracelsus, Agricola mentioned the dust that ""eats away the lungs, and implants consumption.""[16]

The seeds of state intervention to correct social ills were sown during the reign of Elizabeth I by the Poor Laws, which originated in attempts to alleviate hardship arising from widespread poverty. While they were perhaps more to do with a need to contain unrest than morally motivated, they were significant in transferring responsibility for helping the needy from private hands to the state.[15]

In 1713, Bernardino Ramazzini (1633–1714), often described as the father of occupational medicine and a precursor to occupational health, published his De morbis artificum diatriba (Dissertation on Workers' Diseases), which outlined the health hazards of chemicals, dust, metals, repetitive or violent motions, odd postures, and other disease-causative agents encountered by workers in more than fifty occupations. It was the first broad-ranging presentation of occupational diseases.[16][17][18]

Percivall Pott (1714–1788), an English surgeon, described cancer in chimney sweeps (chimney sweeps' carcinoma), the first recognition of an occupational cancer in history.[16]

The United Kingdom was the first nation to industrialize. Soon shocking evidence emerged of serious physical and moral harm suffered by children and young persons in the cotton textile mills, as a result of exploitation of cheap labor in the factory system. Responding to calls for remedial action from philanthropists and some of the more enlightened employers, in 1802 Sir Robert Peel, himself a mill owner, introduced a bill to Parliament with the aim of improving their conditions. This would engender the Health and Morals of Apprentices Act 1802, generally believed to be the first attempt to regulate conditions of work in the United Kingdom. The act applied only to cotton textile mills and required employers to keep premises clean and healthy by twice yearly washings with quicklime, to ensure there were sufficient windows to admit fresh air, and to supply ""apprentices"" (i.e., pauper and orphan employees) with ""sufficient and suitable"" clothing and accommodation for sleeping.[15] It was the first of the 19th century Factory Acts.

Charles Thackrah (1795–1833), another pioneer of occupational medicine, wrote a report on The State of Children Employed in Cotton Factories, which was sent to the Parliament in 1818. Thackrah recognized issues of inequalities of health in the workplace, with manufacturing in towns causing higher mortality than agriculture.[16]

The Factory Act 1833 created a dedicated professional Factory Inspectorate.[19] The initial remit of the Inspectorate was to police restrictions on the working hours in the textile industry of children and young persons (introduced to prevent chronic overwork, identified as leading directly to ill-health and deformation, and indirectly to a high accident rate).[15]

In 1840 a royal commission published its findings on the state of conditions for the workers of the mining industry that documented the appallingly dangerous environment that they had to work in and the high frequency of accidents. The commission sparked public outrage which resulted in the Mines and Collieries Act 1842. The act set up an inspectorate for mines and collieries which resulted in many prosecutions and safety improvements, and by 1850, inspectors were able to enter and inspect premises at their discretion.[20]

On the urging of the Factory Inspectorate, a further Factories Act 1844 giving similar restrictions on working hours for women in the textile industry introduced a requirement for machinery guarding (but only in the textile industry, and only in areas that might be accessed by women or children).[21] The latter act was the first to take a significant step toward improvement of workers' safety, as the former focused on health aspects alone.[15]

The first decennial British Registrar-General's mortality report was issued in 1851. Deaths were categorized by social classes, with class I corresponding to professionals and executives and class V representing unskilled workers. The report showed that mortality rates increased with the class number.[16]

Otto von Bismarck inaugurated the first social insurance legislation in 1883 and the first worker's compensation law in 1884 – the first of their kind in the Western world. Similar acts followed in other countries, partly in response to labor unrest.[16]

The United States are responsible for the first health program focusing on workplace conditions. This was the Marine Hospital Service, inaugurated in 1798 and providing care for merchant seamen. This was the beginning of what would become the US Public Health Service (USPHS).[16]

The first worker compensation acts in the United States were passed in New York in 1910 and in Washington and Wisconsin in 1911. Later rulings included occupational diseases in the scope of the compensation, which was initially restricted to accidents.[16]

In 1914 the USPHS set up the Office of Industrial Hygiene and Sanitation, the ancestor of the current National Institute for Safety and Health (NIOSH). In the early 20th century, workplace disasters were still common. For example, in 1911 a fire at the Triangle Shirtwaist Company in New York killed 146 workers, mostly women and immigrants. Most died trying to open exits that had been locked. Radium dial painter cancers,""phossy jaw"", mercury and lead poisonings, silicosis, and other pneumoconioses were extremely common.[16]

The enactment of the Federal Coal Mine Health and Safety Act of 1969 was quickly followed by the 1970 Occupational Safety and Health Act, which established the Occupational Safety and Health Administration (OSHA) and NIOSH in their current form`.[16]

A wide array of workplace hazards can damage the health and safety of people at work. These include but are not limited to, ""chemicals, biological agents, physical factors, adverse ergonomic conditions, allergens, a complex network of safety risks,"" as well a broad range of psychosocial risk factors.[23] Personal protective equipment can help protect against many of these hazards.[24] A landmark study conducted by the World Health Organization and the International Labour Organization found that exposure to long working hours is the occupational risk factor with the largest attributable burden of disease, i.e. an estimated 745,000 fatalities from ischemic heart disease and stroke events in 2016.[25] This makes overwork the globally leading occupational health risk factor.[26]

Physical hazards affect many people in the workplace. Occupational hearing loss is the most common work-related injury in the United States, with 22 million workers exposed to hazardous occupational noise levels at work and an estimated $242 million spent annually on worker's compensation for hearing loss disability.[27] Falls are also a common cause of occupational injuries and fatalities, especially in construction, extraction, transportation, healthcare, and building cleaning and maintenance.[28] Machines have moving parts, sharp edges, hot surfaces and other hazards with the potential to crush, burn, cut, shear, stab or otherwise strike or wound workers if used unsafely.[29]

Biological hazards (biohazards) include infectious microorganisms such as viruses, bacteria and toxins produced by those organisms such as anthrax. Biohazards affect workers in many industries; influenza, for example, affects a broad population of workers.[30] Outdoor workers, including farmers, landscapers, and construction workers, risk exposure to numerous biohazards, including animal bites and stings,[31][32][33] urushiol from poisonous plants,[34] and diseases transmitted through animals such as the West Nile virus and Lyme disease.[35][36] Health care workers, including veterinary health workers, risk exposure to blood-borne pathogens and various infectious diseases,[37][38] especially those that are emerging.[39]

Dangerous chemicals can pose a chemical hazard in the workplace. There are many classifications of hazardous chemicals, including neurotoxins, immune agents, dermatologic agents, carcinogens, reproductive toxins, systemic toxins, asthmagens, pneumoconiotic agents, and sensitizers.[40] Authorities such as regulatory agencies set occupational exposure limits to mitigate the risk of chemical hazards.[41] International investigations are ongoing into the health effects of mixtures of chemicals, given that toxins can interact synergistically instead of merely additively. For example, there is some evidence that certain chemicals are harmful at low levels when mixed with one or more other chemicals. Such synergistic effects may be particularly important in causing cancer. Additionally, some substances (such as heavy metals and organohalogens) can accumulate in the body over time, thereby enabling small incremental daily exposures to eventually add up to dangerous levels with little overt warning.[42]

Psychosocial hazards include risks to the mental and emotional well-being of workers, such as feelings of job insecurity, long work hours, and poor work-life balance.[43] Psychological abuse has been found present within the workplace as evidenced by previous research. A study by Gary Namie on workplace emotional abuse found that 31% of women and 21% of men who reported workplace emotional abuse exhibited three key symptoms of post-traumatic stress disorder (hypervigilance, intrusive imagery, and avoidance behaviors).[44] Sexual harassment is a serious hazard that can be found in workplaces.[45]

Specific occupational safety and health risk factors vary depending on the specific sector and industry. Construction workers might be particularly at risk of falls, for instance, whereas fishermen might be particularly at risk of drowning. Similarly psychosocial risks such as workplace violence are more pronounced for certain occupational groups such as health care employees, police, correctional officers and teachers.[46]

Agriculture workers are often at risk of work-related injuries, lung disease, noise-induced hearing loss, skin disease, as well as certain cancers related to chemical use or prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery. The most common cause of fatal agricultural injuries in the United States is tractor rollovers, which can be prevented by the use of roll over protection structures which limit the risk of injury in case a tractor rolls over.[47] Pesticides and other chemicals used in farming can also be hazardous to worker health,[48] and workers exposed to pesticides may experience illnesses or birth defects.[49] As an industry in which families, including children, commonly work alongside their families, agriculture is a common source of occupational injuries and illnesses among younger workers.[50] Common causes of fatal injuries among young farm worker include drowning, machinery and motor vehicle-related accidents.[51]

The 2010 NHIS-OHS found elevated prevalence rates of several occupational exposures in the agriculture, forestry, and fishing sector which may negatively impact health. These workers often worked long hours. The prevalence rate of working more than 48 hours a week among workers employed in these industries was 37%, and 24% worked more than 60 hours a week.[52] Of all workers in these industries, 85% frequently worked outdoors compared to 25% of all US workers. Additionally, 53% were frequently exposed to vapors, gas, dust, or fumes, compared to 25% of all US workers.[53]

The mining industry still has one of the highest rates of fatalities of any industry.[54] There are a range of hazards present in surface and underground mining operations. In surface mining, leading hazards include such issues as geological instability,[55] contact with plant and equipment, rock blasting, thermal environments (heat and cold), respiratory health (black lung), etc.[56] In underground mining, operational hazards include respiratory health, explosions and gas (particularly in coal mine operations), geological instability, electrical equipment, contact with plant and equipment, heat stress, inrush of bodies of water, falls from height, confined spaces, ionising radiation, etc.[57]

According to data from the 2010 NHIS-OHS, workers employed in mining and oil and gas extraction industries had high prevalence rates of exposure to potentially harmful work organization characteristics and hazardous chemicals. Many of these workers worked long hours: 50% worked more than 48 hours a week and 25% worked more than 60 hours a week in 2010. Additionally, 42% worked non-standard shifts (not a regular day shift). These workers also had high prevalence of exposure to physical/chemical hazards. In 2010, 39% had frequent skin contact with chemicals. Among nonsmoking workers, 28% of those in mining and oil and gas extraction industries had frequent exposure to secondhand smoke at work. About two-thirds were frequently exposed to vapors, gas, dust, or fumes at work.[58]

Construction is one of the most dangerous occupations in the world, incurring more occupational fatalities than any other sector in both the United States and in the European Union.[59][60] In 2009, the fatal occupational injury rate among construction workers in the United States was nearly three times that for all workers.[59] Falls are one of the most common causes of fatal and non-fatal injuries among construction workers.[59] Proper safety equipment such as harnesses and guardrails and procedures such as securing ladders and inspecting scaffolding can curtail the risk of occupational injuries in the construction industry.[61] Due to the fact that accidents may have disastrous consequences for employees as well as organizations, it is of utmost importance to ensure health and safety of workers and compliance with HSE construction requirements. Health and safety legislation in the construction industry involves many rules and regulations. For example, the role of the Construction Design Management (CDM) Coordinator as a requirement has been aimed at improving health and safety on-site.[62]

The 2010 National Health Interview Survey Occupational Health Supplement (NHIS-OHS) identified work organization factors and occupational psychosocial and chemical/physical exposures which may increase some health risks. Among all US workers in the construction sector, 44% had non-standard work arrangements (were not regular permanent employees) compared to 19% of all US workers, 15% had temporary employment compared to 7% of all US workers, and 55% experienced job insecurity compared to 32% of all US workers. Prevalence rates for exposure to physical/chemical hazards were especially high for the construction sector. Among nonsmoking workers, 24% of construction workers were exposed to secondhand smoke while only 10% of all US workers were exposed. Other physical/chemical hazards with high prevalence rates in the construction industry were frequently working outdoors (73%) and frequent exposure to vapors, gas, dust, or fumes (51%).[63]

The service sector comprises diverse workplaces. Each type of workplace has its own health risks. While some occupations have become mobile, others still require desk work. As the number of service sector jobs has risen in developed countries, many jobs have turned sedentary, presenting an array of health problems that differ from previous health concerns associated with manufacturing and the primary sector. Contemporary health problems include obesity. Some working conditions, such as occupational stress, workplace bullying, and overwork, have negative consequences for physical and mental health.[64][65]

Tipped wage workers are at a higher risk of negative mental health outcomes like addiction or depression. The higher rates of mental health issues may be attributed to the precarious nature of their employment, characterized by low and unpredictable incomes, inadequate access to benefits, wage exploitation, and minimal control over work schedules and assigned shifts.[66] Close to 70% of tipped wage workers are women.[67] Additionally, ""almost 40 percent of people who work for tips are people of color: 18 percent are Latino, 10 percent are African American, and 9 percent are Asian. Immigrants are also overrepresented in the tipped workforce.""[68] According to data from the 2010 NHIS-OHS, hazardous physical and chemical exposures in the service sector were lower than national averages. However, harmful organizational practices and psychosocial risks were fairly prevalent in this sector. Among all workers in the service industry, 30% experienced job insecurity in 2010, 27% worked non-standard shifts (not a regular day shift), 21% had non-standard work arrangements (were not regular permanent employees).[69]

In addition to these organizational risks, some industries pose significant physical dangers due to the manual labor involved. For instance, on a per employee basis, the US Postal Service, UPS and FedEx are the 4th, 5th and 7th most dangerous companies to work for in the United States, respectively.[70]

In general, healthcare workers are exposed to many hazards that can adversely affect their health and well-being.[71] Long hours, changing shifts, physically demanding tasks, violence, and exposures to infectious diseases and harmful chemicals are examples of hazards that put these workers at risk for illness and injury. Musculoskeletal injury (MSI) is the most common health hazard in for healthcare workers and in workplaces overall.[72] Injuries can be prevented by using proper body mechanics.[73]

According to the Bureau of Labor statistics, US hospitals recorded 253,700 work-related injuries and illnesses in 2011, which is 6.8 work-related injuries and illnesses for every 100 full-time employees.[74] The injury and illness rate in hospitals is higher than the rates in construction and manufacturing – two industries that are traditionally thought to be relatively hazardous.[citation needed]

An estimated 2.90 million work-related deaths occurred in 2019, increased from 2.78 million death from 2015. About, one-third of the total work-related deaths (31%) were due to circulatory diseases, while cancer contributed 29%, respiratory diseases 17%, and occupational injuries contributed 11% (or about 319,000 fatalities). Other diseases such as work-related communicable diseases contributed 6%, while neuropsychiatric conditions contributed 3% and work-related digestive disease and genitourinary diseases contributed 1% each. The contribution of cancers and circulatory diseases to total work-related deaths increased from 2015, while deaths due to occupational injuries decreased. Although work-related injury deaths and non-fatal injuries rates were on a decreasing trend, the total deaths and non-fatal outcomes were on the rise. Cancers represented the most significant cause of mortality in high-income countries. The number of non-fatal occupational injuries for 2019 was estimated to be 402 million.[75]

Mortality rate is unevenly distributed, with male mortality rate (108.3 per 100,000 employed male individuals) being significantly higher than female rate (48.4 per 100,000). 6.7% of all deaths globally are represented by occupational fatalities.[76]

Certain EU member states admit to having lacking quality control in occupational safety services, to situations in which risk analysis takes place without any on-site workplace visits and to insufficient implementation of certain EU OSH directives. Disparities between member states result in different impact of occupational hazards on the economy. In the early 2000s, the total societal costs of work-related health problems and accidents varied from 2.6% to 3.8% of the national GDPs across the member states.[77]

In 2021, in the EU-27 as a whole, 93% of deaths due to injury were of males.[78]

One of the decisions taken by the communist regime under Stalin was to reduce the number of accidents and occupational diseases to zero.[80] The tendency to decline remained in the Russian Federation in the early 21st century. However, as in previous years, data reporting and publication was incomplete and manipulated, so that the actual number of work-related diseases and accidents are unknown.[81] The ILO reports that, according to the information provided by the Russian government, there are 190,000 work-related fatalities each year, of which 15,000 due to occupational accidents.[82]

After the demise of the USSR, enterprises became owned by oligarchs who were not interested in upholding safe and healthy conditions in the workplace. Expenditure on equipment modernization was minimal and the share of harmful workplaces increased.[83] The government did not interfere in this, and sometimes it helped employers.[citation needed] At first, the increase in occupational diseases and accidents was slow, due to the fact that in the 1990s it was compensated by mass deindustrialization.[citation needed] However, in the 2000s deindustrialization slowed and occupational diseases and injuries started to rise in earnest. Therefore, in the 2010s the Ministry of Labor adopted federal law no. 426-FZ. This piece of legislation has been described as ineffective and based on the superficial assumption that the issuance of personal protective equipment to the employee means real improvement of working conditions. Meanwhile, the Ministry of Health made significant changes in the methods of risk assessment in the workplace.[84] However, specialists from the Izmerov Research Institute of Occupational Health found that the post-2014 apparent decrease in the share of employees engaged in hazardous working conditions is due to the change in definitions consequent to the Ministry of Health's decision, but does not reflect actual improvements. This was most clearly shown in the results for the aluminum industry.[85]

Further problems in the accounting of workplace fatalities arise from the fact that multiple Russian federal entities collect and publish records, a practice that should be avoided. In 2008 alone, 2074 accidents at work may have not been reported in official government sources.[86]

In the UK there were 135 fatal injuries at work in financial year 2022–2023, compared with 651 in 1974 (the year when the Health and Safety at Work Act was promulgated). The fatal injury rate declined from 2.1 fatalities per 100,000 workers in 1981 to 0.41 in financial year 2022–2023.[87] Over recent decades reductions in both fatal and non-fatal workplace injuries have been very significant. However, illnesses statistics have not uniformly improved: while musculoskeletal disorders have diminished, the rate of self-reported work-related stress, depression or anxiety has increased, and the rate of mesothelioma deaths has remained broadly flat (due to past asbestos exposures).[88]

The Occupational Safety and Health Statistics (OSHS) program in the Bureau of Labor Statistics of the United States Department of Labor compiles information about workplace fatalities and non-fatal injuries in the United States. The OSHS program produces three annual reports:


The Bureau also uses tools like AgInjuryNews.org to identify and compile additional sources of fatality reports for their datasets.[90][91]
Between 1913 and 2013, workplace fatalities dropped by approximately 80%.[92] In 1970, an estimated 14,000 workers were killed on the job. By 2021, in spite of the workforce having since more than doubled, workplace deaths were down to about 5,190.[93] According to the census of occupational injuries 5,486 people died on the job in 2022, up from the 2021 total of 5,190. The fatal injury rate was 3.7 per 100,000 full-time equivalent workers.[94] The decrease in the mortality rate is only partly (about 10–15%) explained by the deindustrialization of the US in the last 40 years.[95]

About 3.5 million nonfatal workplace injuries and illnesses were reported by private industry employers in 2022, occurring at a rate of 3.0 cases per 100 full-time workers.[96][97]

employees

Companies may adopt a safety and health management system (SMS),[c] either voluntarily or because required by applicable regulations, to deal in a structured and systematic way with safety and health risks in their workplace. An SMS provides a systematic way to assess and improve prevention of workplace accidents and incidents based on structured management of workplace risks and hazards. It must be adaptable to changes in the organization's business and legislative requirements. It is usually based on the Deming cycle, or plan-do-check-act (PDCA) principle.[98] An effective SMS should:

Management standards across a range of business functions such as environment, quality and safety are now being designed so that these traditionally disparate elements can be integrated and managed within a single business management system and not as separate and stand-alone functions. Therefore, some organizations dovetail other management system functions, such as process safety, environmental resource management or quality management together with safety management to meet both regulatory requirements, industry sector requirements and their own internal and discretionary standard requirements.

The ILO published ILO-OSH 2001 on Guidelines on Occupational Safety and Health Management Systems to assist organizations with introducing OSH management systems. These guidelines encouraged continual improvement in employee health and safety, achieved via a constant process of policy; organization; planning and implementation; evaluation; and action for improvement, all supported by constant auditing to determine the success of OSH actions.[99]

From 1999 to 2018, OHSAS 18001 was adopted and widely used internationally. It was developed by a selection of national standards bodies, academic bodies, accreditation bodies, certification bodies and occupational health and safety institutions to address a gap where no third-party certifiable international standard existed.[100] It was designed for integration with ISO 9001 and ISO 14001.[101]

OHSAS 18001 was replaced by ISO 45001, which was published in March 2018 and implemented in March 2021.[citation needed]

National management system standards for occupational health and safety include AS/NZS 4801 for Australia and New Zealand (now superseded by ISO 45001),[102][103] CSA Z1000:14 for Canada (which is due to be discontinued in favor of CSA Z45001:19, the Canadian adoption of ISO 45000)[104] and ANSI/ASSP Z10 for the United States.[105] In Germany, the Bavarian state government, in collaboration with trade associations and private companies, issued their OHRIS standard for occupational health and safety management systems. A new revision was issued in 2018.[106] The Taiwan Occupational Safety and Health Management System (TOSHMS) was issued in 1997 under the auspices of Taiwan's Occupational Safety and Health Administration.[107]

The terminology used in OSH varies between countries, but generally speaking:

""Hazard"", ""risk"", and ""outcome"" are used in other fields to describe e.g., environmental damage or damage to equipment. However, in the context of OSH, ""harm"" generally describes the direct or indirect degradation, temporary or permanent, of the physical, mental, or social well-being of workers. For example, repetitively carrying out manual handling of heavy objects is a hazard. The outcome could be a musculoskeletal disorder (MSD) or an acute back or joint injury. The risk can be expressed numerically (e.g., a 0.5 or 50/50 chance of the outcome occurring during a year), in relative terms (e.g., ""high/medium/low""), or with a multi-dimensional classification scheme (e.g., situation-specific risks).[citation needed]

Hazard identification is an important step in the overall risk assessment and risk management process. It is where individual work hazards are identified, assessed and controlled or eliminated as close to source (location of the hazard) as reasonably practicable. As technology, resources, social expectation or regulatory requirements change, hazard analysis focuses controls more closely toward the source of the hazard. Thus, hazard control is a dynamic program of prevention. Hazard-based programs also have the advantage of not assigning or implying there are ""acceptable risks"" in the workplace.[109] A hazard-based program may not be able to eliminate all risks, but neither does it accept ""satisfactory"" – but still risky – outcomes. And as those who calculate and manage the risk are usually managers, while those exposed to the risks are a different group, a hazard-based approach can bypass conflict inherent in a risk-based approach.[citation needed]

The information that needs to be gathered from sources should apply to the specific type of work from which the hazards can come from. Examples of these sources include interviews with people who have worked in the field of the hazard, history and analysis of past incidents, and official reports of work and the hazards encountered. Of these, the personnel interviews may be the most critical in identifying undocumented practices, events, releases, hazards and other relevant information. Once the information is gathered from a collection of sources, it is recommended for these to be digitally archived (to allow for quick searching) and to have a physical set of the same information in order for it to be more accessible. One innovative way to display the complex historical hazard information is with a historical hazards identification map, which distills the hazard information into an easy-to-use graphical format.[citation needed]

Modern occupational safety and health legislation usually demands that a risk assessment be carried out prior to making an intervention. This assessment should:

The calculation of risk is based on the likelihood or probability of the harm being realized and the severity of the consequences. This can be expressed mathematically as a quantitative assessment (by assigning low, medium and high likelihood and severity with integers and multiplying them to obtain a risk factor), or qualitatively as a description of the circumstances by which the harm could arise.[citation needed]

The assessment should be recorded and reviewed periodically and whenever there is a significant change to work practices. The assessment should include practical recommendations to control the risk. Once recommended controls are implemented, the risk should be re-calculated to determine if it has been lowered to an acceptable level. Generally speaking, newly introduced controls should lower risk by one level, i.e., from high to medium or from medium to low.[110]

Occupational safety and health practice vary among nations with different approaches to legislation, regulation, enforcement, and incentives for compliance. In the EU, for example, some member states promote OSH by providing public monies as subsidies, grants or financing, while others have created tax system incentives for OSH investments. A third group of EU member states has experimented with using workplace accident insurance premium discounts for companies or organizations with strong OSH records.[111][112]

In Australia, four of the six states and both territories have enacted and administer harmonized work health and safety legislation in accordance with the Intergovernmental Agreement for Regulatory and Operational Reform in Occupational Health and Safety.[113] Each of these jurisdictions has enacted work health and safety legislation and regulations based on the Commonwealth Work Health and Safety Act 2011 and common codes of practice developed by Safe Work Australia.[114] Some jurisdictions have also included mine safety under the model approach. However, most have retained separate legislation for the time being. In August 2019, Western Australia committed to join nearly every other state and territory in implementing the harmonized Model WHS Act, Regulations and other subsidiary legislation.[115] Victoria has retained its own regime, although the Model WHS laws themselves drew heavily on the Victorian approach.[citation needed]

In Canada, workers are covered by provincial or federal labor codes depending on the sector in which they work. Workers covered by federal legislation (including those in mining, transportation, and federal employment) are covered by the Canada Labour Code; all other workers are covered by the health and safety legislation of the province in which they work.[116][117] The Canadian Centre for Occupational Health and Safety (CCOHS), an agency of the Government of Canada, was created in 1978 by an act of parliament. CCOHS is mandated to promote safe and healthy workplaces and help prevent work-related injuries and illnesses.[118]

There are significant common elements across relevant provincial OHS legislation. The foundation of each of these legislative frameworks is the belief that all Canadians have ""a fundamental right to a healthy and safe working environment."" In general, provincial workplace safety laws in Canada are designed to promote shared responsibility, prevent accidents, and ensure accountability at all levels of an organization. Employers, supervisors, and workers are expected to work together to minimize risks.  Employers, in particular, are legally obligated to take every reasonable precaution to protect workers. If the workplace has more than a few employees, they are required to develop written health and safety policies and procedures. Employers must also provide and maintain equipment and machinery in a safe working condition. Additionally, employers must inform, instruct, and supervise workers to ensure safe work practices are followed. Employers are also responsible for supplying necessary protective equipment and ensuring it is used correctly, whether it involves machine guards or personal protective equipment (PPE). Supervisors have a duty to ensure that workers use all required safety devices and comply with established procedures. They must also communicate information about existing or potential hazards and provide guidance on how to work safely. Workers also have the right to refuse work if they believe it is unsafe and poses a danger to themselves or others.[119][additional citation(s) needed]

In workplaces with a set minimum number of employees (twenty in the case of workplaces under federal jurisdiction[120]), it is mandatory to have a health and safety committee. This, made up of both worker and management representatives, meets regularly to identify hazards, investigate incidents, and make recommendations to improve workplace safety. These committees are crucial for fostering collaboration and addressing safety concerns in a timely manner.[121]

Law also requires employers to take defined steps to prevent workplace violence and harassment. They must create a workplace violence policy along with a program that identifies risks and outlines procedures for addressing them. A separate workplace harassment policy must explain how complaints should be reported and investigated. Employers are required to train employees on these policies to ensure awareness and compliance. All incidents involving violence, threats, or persistent harassment must be taken seriously and handled appropriately.[122][123]

In severe cases involving serious injury or death due to negligence, organizations and individuals can be prosecuted under the Criminal Code of Canada through the provisions introduced by Bill C-45. In some provinces, like Ontario, this introduces serious criminal consequences for safety violations.[124]

Workplaces are also subject to federal regulations under WHMIS, the Workplace Hazardous Materials Information System. WHMIS governs the labeling, documentation, and communication of hazardous materials. Employers must ensure that all hazardous substances are properly labeled, that material safety data sheets are readily available, and that workers are trained on how to handle these materials safely.[125]

As an example of arrangements at a provincial level, Ontario's primary workplace safety legislation is the Occupational Health and Safety Act (OHSA). This law sets out the responsibilities of employers, supervisors, and workers to promote a safe and healthy work environment. Ontario's occupational health and safety framework is built around the concept known as the ""Internal Responsibility System,"" which means that everyone in the workplace shares responsibility for recognizing and addressing safety concerns. The OHSA is enforced by Ontario’s Ministry of Labour, Immigration, Training and Skills Development. Ministry inspectors have the authority to visit workplaces, investigate complaints, and issue orders. Failure to comply with the law can lead to substantial fines and penalties, and individual supervisors or managers may also be held personally liable.[126][127]

In China, the Ministry of Health is responsible for occupational disease prevention and the State Administration of Work Safety workplace safety issues.[citation needed] The Work Safety Law (安全生产法) was issued on 1 November 2002.[128][129] The Occupational Disease Control Act came into force on 1 May 2002.[130] In 2018, the National Health Commission (NHC) was formally established to formulating national health policies. The NHC formulated the ""National Occupational Disease Prevention and Control Plan (2021–2025)"" in the context of the activities leading to the ""Healthy China 2030"" initiative.[128]

The European Agency for Safety and Health at Work was founded in 1994. In the European Union, member states have enforcing authorities to ensure that the basic legal requirements relating to occupational health and safety are met. In many EU countries, there is strong cooperation between employer and worker organizations (e.g., unions) to ensure good OSH performance, as it is recognized this has benefits for both the worker (through maintenance of health) and the enterprise (through improved productivity and quality).[citation needed]

Member states have all transposed into their national legislation a series of directives that establish minimum standards on occupational health and safety. These directives (of which there are about 20 on a variety of topics) follow a similar structure requiring the employer to assess workplace risks and put in place preventive measures based on a hierarchy of hazard control. This hierarchy starts with elimination of the hazard and ends with personal protective equipment.[citation needed]

per 10,000 full-time employees[131]

In Denmark, occupational safety and health is regulated by the Danish Act on Working Environment and Cooperation at the Workplace.[132] The Danish Working Environment Authority (Arbejdstilsynet) carries out inspections of companies, draws up more detailed rules on health and safety at work and provides information on health and safety at work.[133] The result of each inspection is made public on the web pages of the Danish Working Environment Authority so that the general public, current and prospective employees, customers and other stakeholders can inform themselves about whether a given organization has passed the inspection.[citation needed]

In the Netherlands, the laws for safety and health at work are registered in the Working Conditions Act (Arbeidsomstandighedenwet and Arbeidsomstandighedenbeleid). Apart from the direct laws directed to safety and health in working environments, the private domain has added health and safety rules in Working Conditions Policies (Arbeidsomstandighedenbeleid), which are specified per industry. The Ministry of Social Affairs and Employment (SZW) monitors adherence to the rules through their inspection service. This inspection service investigates industrial accidents and it can suspend work and impose fines when it deems the Working Conditions Act has been violated. Companies can get certified with a VCA certificate for safety, health and environment performance. All employees have to obtain a VCA certificate too, with which they can prove that they know how to work according to the current and applicable safety and environmental regulations.[citation needed]

The main health and safety regulation in Ireland is the Safety, Health and Welfare at Work Act 2005,[134] which replaced earlier legislation from 1989. The Health and Safety Authority, based in Dublin, is responsible for enforcing health and safety at work legislation.[134]

In Spain, occupational safety and health is regulated by the Spanish Act on Prevention of Labor Risks. The Ministry of Labor is the authority responsible for issues relating to labor environment.[citation needed] The National Institute for Safety and Health at Work (Instituto Nacional de Seguridad y Salud en el Trabajo, INSST) is the government's scientific and technical organization specialized in occupational safety and health.[135]

In Sweden, occupational safety and health is regulated by the Work Environment Act.[136] The Swedish Work Environment Authority (Arbetsmiljöverket) is the government agency responsible for issues relating to the working environment. The agency works to disseminate information and furnish advice on OSH, has a mandate to carry out inspections, and a right to issue stipulations and injunctions to any non-compliant employer.[137]

In India, the Ministry of Labour and Employment formulates national policies on occupational safety and health in factories and docks with advice and assistance from its Directorate General Factory Advice Service and Labour Institutes (DGFASLI), and enforces its policies through inspectorates of factories and inspectorates of dock safety. The DGFASLI provides technical support in formulating rules, conducting occupational safety surveys and administering occupational safety training programs.[138]

In Indonesia, the Ministry of Manpower (Kementerian Ketenagakerjaan, or Kemnaker) is responsible to ensure the safety, health and welfare of workers. Important OHS acts include the Occupational Safety Act 1970 and the Occupational Health Act 1992.[139] Sanctions, however, are still low (with a maximum of 15 million rupiahs fine and/or a maximum of one year in prison) and violations are still very frequent.[140]

The Japanese Ministry of Health, Labor and Welfare (MHLW) is the governmental agency overseeing occupational safety and health in Japan. The MHLW is responsible for enforcing Industrial Safety and Health Act of 1972 – the key piece of OSH legislation in Japan –, setting regulations and guidelines, supervising labor inspectors who monitor workplaces for compliance with safety and health standards, investigating accidents, and issuing orders to improve safety conditions. The Labor Standards Bureau is an arm of MHLW tasked with supervising and guiding businesses, inspecting manufacturing facilities for safety and compliance, investigating accidents, collecting statistics, enforcing regulations and administering fines for safety violations, and paying accident compensation for injured workers.[141][142]

The Japan Industrial Safety and Health Association [jp] (JISHA) is a non-profit organization established under the Industrial Safety and Health Act of 1972. It works closely with MHLW, the regulatory body, to promote workplace safety and health. The responsibilities of JISHA include: Providing education and training on occupational safety and health, conducting research and surveys on workplace safety and health issues, offering technical guidance and consultations to businesses, disseminating information and raising awareness about occupational safety and health, and collaborating with international organizations to share best practices and improve global workplace safety standards.[143]

The Japan National Institute of Occupational Safety and Health [jp] (JNIOSH) conducts research to support governmental policies in occupational safety and health. The organization categorizes its research into project studies, cooperative research, fundamental research, and government-requested research. Each category focuses on specific themes, from preventing accidents and ensuring workers' health, to addressing changes in employment structure. The organization sets clear goals, develops road maps, and collaborates with the Ministry of Health, Labor and Welfare to discuss progress and policy contributions.[144]

In Malaysia, the Department of Occupational Safety and Health (DOSH) under the Ministry of Human Resources is responsible to ensure that the safety, health and welfare of workers in both the public and private sector is upheld. DOSH is responsible to enforce the Factories and Machinery Act 1967 and the Occupational Safety and Health Act 1994. Malaysia has a statutory mechanism for worker involvement through elected health and safety representatives and health and safety committees.[145] This followed a similar approach originally adopted in Scandinavia.[citation needed]

In Saudi Arabia, the Ministry of Human Resources and Social Development administrates workers' rights and the labor market as a whole, consistent with human rights rules upheld by the Human Rights Commission of the kingdom.[146]

In Singapore, the Ministry of Manpower (MOM) is the government agency in charge of OHS policies and enforcement. The key piece of legislation regulating aspects of OHS is the Workplace Safety and Health Act.[147] The MOM promotes and manages campaigns against unsafe work practices, such as when working at height, operating cranes and in traffic management. Examples include Operation Cormorant and the Falls Prevention Campaign.[148]

In South Africa the Department of Employment and Labour is responsible for occupational health and safety inspection and enforcement in the commercial and industrial sectors, with the exclusion of mining, where the Department of Mineral Resources is responsible.[149][150] The main statutory legislation on health and safety in the jurisdiction of the Department of Employment and Labour is the OHS Act or OHSA (Act No. 85 of 1993: Occupational Health and Safety Act, as amended by the Occupational Health and Safety Amendment Act, No. 181 of 1993).[149] Regulations implementing the OHS Act include:[151]

In Syria, health and safety is the responsibility of the Ministry of Social Affairs and Labor (Arabic: وزارة الشؤون الاجتماعية والعمل, romanized: Wizārat al-Shuʼūn al-ijtimāʻīyah wa-al-ʻamal).[159]

In Taiwan, the Occupational Safety and Health Administration [zh] of the Ministry of Labor is in charge of occupational safety and health.[160] The matter is governed under the Occupational Safety and Health Act [zh].[161]

In the United Arab Emirates, national OSH legislation is based on the Federal Law on Labor (1980). Order No. 32 of 1982 on Protection from Hazards and Ministerial Decision No. 37/2 of 1982 are also of importance.[162] The competent authority for safety and health at work at the federal level is the Ministry of Human Resources and Emiratisation (MoHRE).[163]

Health and safety legislation in the UK is drawn up and enforced by the Health and Safety Executive and local authorities under the Health and Safety at Work etc. Act 1974 (HASAWA or HSWA).[164][165] HASAWA introduced (section 2) a general duty on an employer to ensure, so far as is reasonably practicable, the health, safety and welfare at work of all his employees, with the intention of giving a legal framework supporting codes of practice not in themselves having legal force but establishing a strong presumption as to what was reasonably practicable (deviations from them could be justified by appropriate risk assessment). The previous reliance on detailed prescriptive rule-setting was seen as having failed to respond rapidly enough to technological change, leaving new technologies potentially unregulated or inappropriately regulated.[166] HSE has continued to make some regulations giving absolute duties (where something must be done with no ""reasonable practicability"" test) but in the UK the regulatory trend is away from prescriptive rules, and toward goal setting and risk assessment. Recent major changes to the laws governing asbestos and fire safety management embrace the concept of risk assessment. The other key aspect of the UK legislation is a statutory mechanism for worker involvement through elected health and safety representatives and health and safety committees. This followed a similar approach in Scandinavia, and that approach has since been adopted in countries such as Australia, Canada, New Zealand and Malaysia.[citation needed]

The Health and Safety Executive service dealing with occupational medicine has been the Employment Medical Advisory Service. In 2014 a new occupational health organization, the Health and Work Service, was created to provide advice and assistance to employers in order to get back to work employees on long-term sick-leave.[167] The service, funded by the government, offers medical assessments and treatment plans, on a voluntary basis, to people on long-term absence from their employer; in return, the government no longer foots the bill for statutory sick pay provided by the employer to the individual.[citation needed]

In the United States, President Richard Nixon signed the Occupational Safety and Health Act into law on 29 December 1970. The act created the three agencies which administer OSH: the Occupational Safety and Health Administration (OSHA), the National Institute for Occupational Safety and Health (NIOSH), and the Occupational Safety and Health Review Commission (OSHRC).[168] The act authorized OSHA to regulate private employers in the 50 states, the District of Columbia, and territories.[169] It includes a general duty clause (29 U.S.C. §654, 5(a)) requiring an employer to comply with the Act and regulations derived from it, and to provide employees with ""employment and a place of employment which are free from recognized hazards that are causing or are likely to cause [them] death or serious physical harm.""[170]

OSHA was established in 1971 under the Department of Labor. It has headquarters in Washington, DC, and ten regional offices, further broken down into districts, each organized into three sections: compliance, training, and assistance. Its stated mission is ""to ensure safe and healthful working conditions for workers by setting and enforcing standards and by providing training, outreach, education and assistance.""[169] The original plan was for OSHA to oversee 50 state plans with OSHA funding 50% of each plan, but this did not work out that way: As of 2023[update] there are 26 approved state plans (with four covering only public employees) and OSHA manages the plan in the states not participating.[93]

OSHA develops safety standards in the Code of Federal Regulations and enforces those safety standards through compliance inspections conducted by Compliance Officers; enforcement resources are focused on high-hazard industries. Worksites may apply to enter OSHA's Voluntary Protection Program (VPP). A successful application leads to an on-site inspection; if this is passed, the site gains VPP status and OSHA no longer inspect it annually nor (normally) visit it unless there is a fatal accident or an employee complaint until VPP revalidation (after three–five years). VPP sites generally have injury and illness rates less than half the average for their industry.[citation needed]

OSHA has a number of specialists in local offices to provide information and training to employers and employees at little or no cost.[4] Similarly OSHA produces a range of publications and funds consultation services available for small businesses.[citation needed]

OSHA has strategic partnership and alliance programs to develop guidelines, assist in compliance, share resources, and educate workers in OHS.[93] OSHA manages Susan B. Harwood grants to non-profit organizations to train workers and employers to recognize, avoid, and prevent safety and health hazards in the workplace.[171] Grants focus on small business, hard-to-reach workers and high-hazard industries.[172]

The National Institute for Occupational Safety and Health (NIOSH), also created under the Occupational Safety and Health Act, is the federal agency responsible for conducting research and making recommendations for the prevention of work-related injury and illness. NIOSH is part of the Centers for Disease Control and Prevention (CDC) within the Department of Health and Human Services.[173]

Those in the field of occupational safety and health come from a wide range of disciplines and professions including medicine, occupational medicine, epidemiology, physiotherapy and rehabilitation, psychology, human factors and ergonomics, and many others. Professionals advise on a broad range of occupational safety and health matters. These include how to avoid particular pre-existing conditions causing a problem in the occupation, correct posture, frequency of rest breaks, preventive actions that can be undertaken, and so forth. The quality of occupational safety is characterized by (1) the indicators reflecting the level of industrial injuries, (2) the average number of days of incapacity for work per employer, (3) employees' satisfaction with their work conditions and (4) employees' motivation to work safely.[174]

The main tasks undertaken by the OSH practitioner include:

OSH specialists examine worksites for environmental or physical factors that could harm employee health, safety, comfort or performance. They then find ways to improve potential risk factors. For example, they may notice potentially hazardous conditions inside a chemical plant and suggest changes to lighting, equipment, materials, or ventilation. OSH technicians assist specialists by collecting data on work environments and implementing the worksite improvements that specialists plan. Technicians also may check to make sure that workers are using required protective gear, such as masks and hardhats. OSH specialists and technicians may develop and conduct employee training programs. These programs cover a range of topics, such as how to use safety equipment correctly and how to respond in an emergency. In the event of a workplace safety incident, specialists and technicians investigate its cause. They then analyze data from the incident, such as the number of people impacted, and look for trends in occurrence. This evaluation helps them to recommend improvements to prevent future incidents.[175]

Given the high demand in society for health and safety provisions at work based on reliable information, OSH professionals should find their roots in evidence-based practice. A new term is ""evidence-informed decision making"". Evidence-based practice can be defined as the use of evidence from literature, and other evidence-based sources, for advice and decisions that favor the health, safety, well-being, and work ability of workers. Therefore, evidence-based information must be integrated with professional expertise and the workers' values. Contextual factors must be considered related to legislation, culture, financial, and technical possibilities. Ethical considerations should be heeded.[176]

The roles and responsibilities of OSH professionals vary regionally but may include evaluating working environments, developing, endorsing and encouraging measures that might prevent injuries and illnesses, providing OSH information to employers, employees, and the public, providing medical examinations, and assessing the success of worker health programs.[citation needed]

In the Netherlands, the required tasks for health and safety staff are only summarily defined and include:[177]

Dutch law influences the job of the safety professional mainly through the requirement on employers to use the services of a certified working-conditions service for advice. A certified service must employ sufficient numbers of four types of certified experts to cover the risks in the organizations which use the service:

In 2004, 14% of health and safety practitioners in the Netherlands had an MSc and 63% had a BSc. 23% had training as an OSH technician.[178]

In Norway, the main required tasks of an occupational health and safety practitioner include:

In 2004, 37% of health and safety practitioners in Norway had an MSc and 44% had a BSc. 19% had training as an OSH technician.[178]

There are multiple levels of training applicable to the field of occupational safety and health. Programs range from individual non-credit certificates and awareness courses focusing on specific areas of concern, to full doctoral programs. The University of Southern California was one of the first schools in the US to offer a PhD program focusing on the field. Further, multiple master's degree programs exist, such as that of the Indiana State University who offer MSc and MA programs. Other masters-level qualifications include the MSc and Master of Research (MRes) degrees offered by the University of Hull in collaboration with the National Examination Board in Occupational Safety and Health (NEBOSH). Graduate programs are designed to train educators, as well as high-level practitioners.[citation needed]

Many OSH generalists focus on undergraduate studies; programs within schools, such as that of the University of North Carolina's online BSc in environmental health and safety, fill a large majority of hygienist needs. However, smaller companies often do not have full-time safety specialists on staff, thus, they appoint a current employee to the responsibility. Individuals finding themselves in positions such as these, or for those enhancing marketability in the job-search and promotion arena, may seek out a credit certificate program. For example, the University of Connecticut's online OSH certificate[179] provides students familiarity with overarching concepts through a 15-credit (5-course) program. Programs such as these are often adequate tools in building a strong educational platform for new safety managers with a minimal outlay of time and money. Further, most hygienists seek certification by organizations that train in specific areas of concentration, focusing on isolated workplace hazards. The American Society of Safety Professionals (ASSP), Board for Global EHS Credentialing (BGC), and American Industrial Hygiene Association (AIHA) offer individual certificates on many different subjects from forklift operation to waste disposal and are the chief facilitators of continuing education in the OSH sector.[citation needed]

In the US, the training of safety professionals is supported by NIOSH through their NIOSH Education and Research Centers.

In the UK, both NEBOSH and the Institution of Occupational Safety and Health (IOSH) develop health and safety qualifications and courses which cater to a mixture of industries and levels of study. Although both organizations are based in the UK, their qualifications are recognized and studied internationally as they are delivered through their own global networks of approved providers. The Health and Safety Executive has also developed health and safety qualifications in collaboration with the NEBOSH.[citation needed]

In Australia, training in OSH is available at the vocational education and training level, and at university undergraduate and postgraduate level. Such university courses may be accredited by an accreditation board of the Safety Institute of Australia. The institute has produced a Body of Knowledge which it considers is required by a generalist safety and health professional and offers a professional qualification.[180] The Australian Institute of Health and Safety has instituted the national Eric Wigglesworth OHS Education Medal to recognize achievement in OSH doctorate education.[181]


Informal or field training may be delivered in the workplace or during off-site training sessions. One form of training delivered in the workplace is known as a toolbox talk. According to the UK's Health and Safety Executive, a toolbox talk is a short presentation to the workforce on a single aspect of health and safety.[182] Such talks are often used, especially in the construction industry, by site supervisors, frontline managers and owners of small construction firms to prepare and deliver advice on matters of health, safety and the environment and to obtain feedback from the workforce.[183]

Virtual reality is a novel tool to deliver safety training in many fields. Some applications have been developed and tested especially for fire and construction safety training.[184][185] Preliminary findings seem to support that virtual reality is more effective than traditional training in knowledge retention.[186]

On an international scale, the World Health Organization (WHO) and the International Labour Organization (ILO) have begun focusing on labor environments in developing nations with projects such as Healthy Cities.[187] Many of these developing countries are stuck in a situation in which their relative lack of resources to invest in OSH leads to increased costs due to work-related illnesses and accidents.[citation needed] The ILO estimates that work-related illness and accidents cost up to 10% of GDP in Latin America, compared with just 2.6% to 3.8% in the EU.[188] There is continued use of asbestos, a notorious hazard, in some developing countries. So asbestos-related disease is expected to continue to be a significant problem well into the future.[citation needed]

There are several broad aspects of artificial intelligence (AI) that may give rise to specific hazards.

Many hazards of AI are psychosocial in nature due to its potential to cause changes in work organization.[189] For example, AI is expected to lead to changes in the skills required of workers, requiring retraining of existing workers, flexibility, and openness to change.[190] Increased monitoring may lead to micromanagement or perception of surveillance, and thus to workplace stress. There is also the risk of people being forced to work at a robot's pace, or to monitor robot performance at nonstandard hours. Additionally, algorithms may show algorithmic bias through being trained on past decisions may mimic undesirable human biases, for example, past discriminatory hiring and firing practices.[191] Some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead.[192]

Physical hazards in the form of human–robot collisions may arise from robots using AI, especially collaborative robots (cobots). Cobots are intended to operate in close proximity to humans, which makes it impossible to implement the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots. Automated guided vehicles are a type of cobot in common use, often as forklifts or pallet jacks in warehouses or factories.[193]

Both applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management. As with all hazards, risk identification is most effective and least costly when done in the design phase.[189] AI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions,[194] as well as information privacy measures.[195] Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues.[195] Workplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate, does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs.[196]

The National Institute of Occupational Safety and Health (NIOSH) National Occupational Research Agenda Manufacturing Council established an externally-lead COVID-19 workgroup to provide exposure control information specific to working in manufacturing environments. The workgroup identified disseminating information most relevant to manufacturing workplaces as a priority, and that would include providing content in Wikipedia. This includes evidence-based practices for infection control plans,[197] and communication tools.

Nanotechnology is an example of a new, relatively unstudied technology. A Swiss survey of 138 companies using or producing nanoparticulate matter in 2006 resulted in forty completed questionnaires. Sixty-five per cent of respondent companies stated they did not have a formal risk assessment process for dealing with nanoparticulate matter.[198] Nanotechnology already presents new issues for OSH professionals that will only become more difficult as nanostructures become more complex. The size of the particles renders most containment and personal protective equipment ineffective. The toxicology values for macro sized industrial substances are rendered inaccurate due to the unique nature of nanoparticulate matter. As nanoparticulate matter decreases in size its relative surface area increases dramatically, increasing any catalytic effect or chemical reactivity substantially versus the known value for the macro substance. This presents a new set of challenges in the near future to rethink contemporary measures to safeguard the health and welfare of employees against a nanoparticulate substance that most conventional controls have not been designed to manage.[199]

Occupational health inequalities refer to differences in occupational injuries and illnesses that are closely linked with demographic, social, cultural, economic, and/or political factors.[200] Although many advances have been made to rectify gaps in occupational health within the past half century, still many persist due to the complex overlapping of occupational health and social factors.[201] There are three main areas of research on occupational health inequities:

Immigrant worker populations often are at greater risk for workplace injuries and fatalities. For example within the United States, immigrant Mexican workers have one of the highest rates of fatal workplace injuries out of all of the working population. Statistics like these are explained through a combination of social, structural, and physical aspects of the workplace. These workers struggle to access safety information and resources in their native languages because of lack of social and political inclusion. In addition to linguistically tailored interventions, it is also critical for the interventions to be culturally appropriate.[205]

Those residing in a country to work without a visa or other formal authorization may also not have access to legal resources and recourse that are designed to protect most workers. Health and Safety organizations that rely on whistleblowers instead of their own independent inspections may be especially at risk of having an incomplete picture of worker health.

Comprehensive Employment and Training Act
"
Food Safety Services,"

Food safety (or food hygiene) is used as a scientific method/discipline describing handling, preparation, and storage of food in ways that prevent foodborne illness. The occurrence of two or more cases of a similar illness resulting from the ingestion of a common food is known as a food-borne disease outbreak.[1] This includes a number of routines that should be followed to avoid potential health hazards. In this way, food safety often overlaps with food defense to prevent harm to consumers. The tracks within this line of thought are safety between industry and the market and then between the market and the consumer. In considering industry-to-market practices, food safety[2] considerations include the origins of food including the practices relating to food labeling, food hygiene, food additives and pesticide residues, as well as policies on biotechnology and food and guidelines for the management of governmental import and export inspection and certification systems for foods. In considering market-to-consumer practices, the usual thought is that food ought to be safe in the market and the concern is safe delivery and preparation of the food for the consumer. Food safety, nutrition and food security are closely related. Unhealthy food creates a cycle of disease and malnutrition that affects infants and adults as well.[3]

Food can transmit pathogens, which can result in the illness or death of the person or other animals. The main types of pathogens are bacteria, viruses, parasites, and fungus. The WHO Foodborne Disease Epidemiology Reference Group conducted the only study that solely and comprehensively focused on the global health burden of foodborne diseases. This study, which involved the work of over 60 experts for a decade, is the most comprehensive guide to the health burden of foodborne diseases. The first part of the study revealed that 31 foodborne hazards considered priority accounted for roughly 420,000 deaths in LMIC and posed a burden of about 33 million disability adjusted life years in 2010.[4] Food can also serve as a growth and reproductive medium for pathogens. In developed countries there are intricate standards for food preparation, whereas in lesser developed countries there are fewer standards and less enforcement of those standards. Even so, in the US, in 1999, 5,000 deaths per year were related to foodborne pathogens. [5] Another main issue is simply the availability of adequate safe water, which is usually a critical item in the spreading of diseases.[6] In theory, food poisoning[7] is 100% preventable. However this cannot be achieved due to the number of persons involved in the supply chain,[8] as well as the fact that pathogens can be introduced into foods no matter how many precautions are taken.[9] [contradictory]

Food safety issues and regulations concern:

Food contamination happens when foods are corrupted with another substance. It can happen In the process of production, transportation, packaging,[11] storage, sales, and cooking process. Contamination can be physical, chemical, or biological.[12]

Physical contaminants (or 'foreign bodies') are objects such as hair, plant stalks or pieces of plastic and metal.[13] When a foreign object enters food, it is a physical contaminant.[13] If the foreign objects contain bacteria, both a physical and biological contamination will occur.

Common sources of physical contaminations are: hair, glass or metal, pests, jewelry, dirt, and fingernails.[13]

Physical food contamination is a hazardous yet natural accident of contaminating food with dangerous objects around the kitchen or production base when being prepared. If kitchens or other places where food may be prepared are unsanitary, it is very likely that physical contamination will occur and cause negative consequences.[14] Dangerous objects such as glass and wire may be found in food which can cause many issues with the individuals who consume it including choking, breaking of teeth and cutting the insides of the body.[15] Children and the elderly are at the highest risk of being harmed by food contamination due to their weaker immune systems and fragile structures.[15] The most common reasoning for physical contamination to occur is when the food is left uncovered without lids.[14] To prevent such contamination and harm to those consuming food from restaurants, cooks are recommended to wear hair nets, remove jewelry, and wear gloves when necessary, especially over wounds with bandages.[16]

Chemical contamination happens when food is contaminated with a natural or artificial chemical substance.[12] Common sources of chemical contamination can include: pesticides, herbicides, veterinary drugs, contamination from environmental sources (water, air or soil pollution), cross-contamination during food processing, migration from food packaging materials,[11] presence of natural toxins, or use of unapproved food additives and adulterants.[17]

It happens when the food has been contaminated by substances produced by living creatures, such as humans, rodents, pests or microorganisms.[18] This includes bacterial contamination, viral contamination, or parasite contamination that is transferred through saliva, pest droppings, blood or fecal matter.[18] Bacterial contamination is the most common cause of food poisoning worldwide.[18] If an environment is high in starch or protein, water, oxygen, has a neutral pH level, and maintains a temperature between 5°C and 60°C (danger zone) for even a brief period of time (~0–20 minutes),[19] bacteria are likely to survive.[20]

In April and May 2018, 26 states in the United States suffered an outbreak of the bacteria strain E. coli O157:H7.[21] Several investigations show the contamination might have come from the Yuma, Arizona, growing region.[22] This outbreak, which began 10 April, is the largest US flare-up of E. coli in a decade.[23] One person in California has died.[21] At least 14 of the people affected developed kidney failure.[21] The most common symptoms of E. coli include diarrhea, bloody diarrhea, abdominal pain, nausea and vomiting.[21]

The five key principles of food hygiene, according to WHO, are:[24]

Proper storage, sanitary tools and work spaces, heating and cooling properly and to adequate temperatures, and avoiding contact with other uncooked foods can greatly reduce the chances of contamination. Tightly sealed water and air proof containers are good measures to limit the chances of both physical and biological contamination during storage. Using clean, sanitary surfaces and tools, free of debris, chemicals, standing liquids, and other food types (different from the kind currently being prepared, i.e. mixing vegetables/meats or beef/poultry) can help reduce the chance of all forms of contamination. However, even if all precautions have been taken and the food has been safely prepared and stored, bacteria can still form over time during storage. Food should be consumed within one to seven (1–7) days while it has been stored in a cold environment, or one to twelve (1–12) months if it was in a frozen environment (if it was frozen immediately after preparation).[25][26] The length of time before a food becomes unsafe to eat depends on the type of food it is, the surrounding environment, and the method with which it is kept out of the danger zone.

ISO 22000 is a standard developed by the International Organization for Standardization dealing with food safety. This is a general derivative of ISO 9000.
The ISO 22000 international standard specifies the requirements for a food safety management system that involves interactive communication, system management, prerequisite programs, and hazard analysis and critical control points principles. ISO 22000 was first published in 2005. It is the culmination of all previous attempts from many sources and areas of food safety concern to provide an end product that is safe as possible from pathogens and other contaminants. Every 5 years standards are reviewed to determine whether a revision is necessary, to ensure that the standards remain as relevant and useful to businesses as possible.[28]

A 2003 WHO report concluded that about 30% of reported food poisoning outbreaks in the WHO European Region occur in private homes.[29] According to the WHO and CDC, in the USA alone, annually, there are 76 million cases of foodborne illness leading to 325,000 hospitalizations and 5,000 deaths.[30]

Health protection measures, specifically food safety inspections, play a crucial role in preventing foodborne illnesses and are implemented by governments globally. The challenge of quantifying the impact of foodborne illness arises from disparities in the effectiveness of public health surveillance systems, variations in reporting, co-morbidities, under-reporting, diagnostic uncertainties, healthcare accessibility, and individual experiences of illness. In 2010, the WHO estimated approximately 600 million cases of foodborne illness worldwide, resulting in an estimated 420,000 deaths.[31]

Governments worldwide have introduced health protection measures and regulatory systems to address foodborne illness. One such method is food safety inspection, which can take different forms at various stages of the food production system. Food safety inspection involves analyzing food samples to determine composition, contamination levels, or quality, particularly in the case of imported foods or those entering the consumer market. Additionally, traditional food safety inspection includes evaluating food handling practices and production environments, commonly applied in the food retail sector, food manufacturing, farms, and slaughterhouses. Inspectors often employ observational and qualitative methods to assess food handling practices and identify potential contamination risks.[31]

The implementation of food safety inspection varies across jurisdictions. Some jurisdictions follow a prescriptive approach, setting specific food safety requirements for businesses and using inspection to measure compliance. This compliance-check approach typically involves inspection checklists and numerical scoring or grading systems, which may carry different weights for various non-compliances. Regulatory systems often prescribe actions based on score ranges, such as enforcement measures or adjustments to inspection frequency. The application of food safety inspection also differs in motivating compliance. Traditionally, inspections aimed to identify and report safety deficiencies to food business operators for resolution within a specified timeframe. Regulatory frameworks may include compliance motivators such as monetary penalties and other enforcement measures triggered by inspection findings.[31]

In 1963, the WHO and FAO published the Codex Alimentarius which serves as an guideline to food safety.[32]

However, according to Unit 04 - Communication of Health & Consumers Directorate-General of the European Commission:
""The Codex, while being recommendations for voluntary application by members, Codex standards serve in many cases as a basis for national legislation. The reference made to Codex food safety standards in the World Trade Organizations' Agreement on Sanitary and Phytosanitary measures means that Codex has far reaching implications for resolving trade disputes. WTO members that wish to apply stricter food safety measures than those set by Codex may be required to justify these measures scientifically.""
So, an agreement made in 2003, signed by all member states, inclusive all EU, in the codex Stan Codex 240 – 2003 for coconut milk, sulphite containing additives like E223 and E 224 are allowed till 30 mg/kg, does NOT mean, they are allowed into the EU, see Rapid Alert System for Food and Feed entries from Denmark: 2012.0834; 2011.1848; en 2011.168, ""sulphite unauthorised in coconut milk from Thailand "".[33][34] Same for polysorbate E 435: see 2012.0838 from Denmark, unauthorised polysorbates in coconut milk and, 2007.AIC from France. Only for the latter the EU amended its regulations with (EU) No 583/2012 per 2 July 2012 to allow this additive, already used for decades and absolutely necessary.[35]

Food Standards Australia New Zealand requires all food businesses to implement food safety systems. These systems are designed to ensure food is safe to consume and halt the increasing incidence of food poisoning, and they include basic food safety training for at least one person in each business.
Food safety training is delivered in various forms by, among other organisations, Registered Training Organisations (RTOs), after which staff are issued a nationally recognised unit of competency code on their certificate.

Food safety is a growing concern in Chinese agriculture. The Chinese government oversees agricultural production as well as the manufacture of food packaging, containers, chemical additives, drug production, and business regulation. In recent years, the Chinese government attempted to consolidate food regulation with the creation of the State Food and Drug Administration in 2003, and officials have also been under increasing public and international pressure to solve food safety problems. However, it appears that regulations are not well known by the trade. Labels used for ""green"" food, ""organic"" food and ""pollution-free"" food are not well recognized by traders and many are unclear about their meaning. A survey by the World Bank found that supermarket managers had difficulty in obtaining produce that met safety requirements and found that a high percentage of produce did not comply with established standards.[36]

Traditional marketing systems, whether in China or the rest of Asia, presently provide little motivation or incentive for individual farmers to make improvements to either quality or safety as their produce tends to get grouped together with standard products as it progresses through the marketing channel. Direct linkages between farmer groups and traders or ultimate buyers, such as supermarkets, can help avoid this problem. Governments need to improve the condition of many markets through upgrading management and reinvesting market fees in physical infrastructure. Wholesale markets need to investigate the feasibility of developing separate sections to handle fruits and vegetables that meet defined safety and quality standards.[37]

The parliament of the European Union (EU) makes legislation in the form of directives and regulations, many of which are mandatory for member states and which therefore must be incorporated into individual countries' national legislation.  As a very large organisation that exists to remove barriers to trade between member states, and into which individual member states have only a proportional influence, the outcome is often seen as an excessively bureaucratic 'one size fits all' approach.  However, in relation to food safety the tendency to err on the side of maximum protection for the consumer may be seen as a positive benefit.  The EU parliament is informed on food safety matters by the European Food Safety Authority.

Individual member states may also have other legislation and controls in respect of food safety, provided that they do not prevent trade with other states, and can differ considerably in their internal structures and approaches to the regulatory control of food safety.

From 13 December 2014, new legislation - the EU Food Information for Consumers Regulation 1169/2011 - require food businesses to provide allergy information on food sold unpackaged, in for example catering outlets, deli counters, bakeries and sandwich bars.[38] A further addition to the 2014 legislation, named 'Natasha's Law', was to come into force on 1 October in the UK and NI. Following the death of Natasha Ednan-Laperouse, who died after eating a sandwich containing the allergen sesame, foods pre-packed on premises for direct sale will require individual ingredients labelling - this replaces the historic requirement for outlets to provide ingredients information for these types of food upon request.[39]

Agence nationale de sécurité sanitaire de l'alimentation, de l'environnement et du travail is a French governmental agency dealing with food safety.

The Federal Ministry of Food, Agriculture and Consumer Protection[40] is a Federal Ministry of the Federal Republic of Germany.
History: Founded as Federal Ministry of Food, Agriculture and Foresting in 1949, this name did not change until 2001. Then the name changed to Federal Ministry of Consumer Protection, Food and Agriculture. At 22 November 2005, the name got changed again to its current state: Federal Ministry of Food, Agriculture and Consumer Protection. The reason for this last change was that all the resorts should get equal ranking which was achieved by sorting the resorts alphabetically.
Vision: A balanced and healthy diet with safe food, distinct consumer rights and consumer information for various areas of life, and a strong and sustainable agriculture as well as perspectives for our rural areas are important goals of the Federal Ministry of Food, Agriculture and Consumer Protection.
The Federal Office of Consumer Protection and Food Safety is under the control of the Federal Ministry of Food, Agriculture and Consumer Protection. It exercises several duties, with which it contributes to safer food and thereby intensifies health-based consumer protection in Germany. Food can be manufactured and sold within Germany without a special permission, as long as it does not cause any damage on consumers' health and meets the general standards set by the legislation. However, manufacturers, carriers, importers and retailers are responsible for the food they pass into circulation. They are obliged to ensure and document the safety and quality of their food with the use of in-house control mechanisms.

In Greece, the Hellenic Food Authority governing body supervised by the Ministry of the Environment and Energy (Greek: Υπουργείο Περιβάλλοντος και Ενέργειας), it is in charge of ensuring food sold is safe and fit for consumption. It controls the food business operators including agricultural producers, food processors, retailers, caterers, input material suppliers and private laboratories.

In Hong Kong SAR, the Food and Environmental Hygiene Department is in charge of ensuring food sold is safe and fit for consumption.

In Hungary, the National Food Chain Safety Office controls the food business operators including agricultural producers, food processors, retailers, caterers, input material suppliers and private laboratories.[41] Its activities also cover risk assessment, risk communication and related research.[42][43]

Food Safety and Standards Authority of India, established under the Food Safety and Standards Act, 2006, is the regulating body related to food safety and laying down of standards of food in India. Hence, it regulates the manufacture, storage, distribution, sale, and import of food articles, while also establishing strict standards to ensure food safety.[44]

To ensure the safety of imported foods and related products, Article 27 of the Food Sanitation Act obliges importers to submit import notification. As Article 27 of the Food Sanitation Act states that ""Those who wish to import food, food additives, apparatuses, or container/packages for sale or for use in business, shall notify the Minister of Health, Labour, and Welfare on each occasion as prescribed by the Ministerial Ordinance,"" the imported foods and related products must not be used for sale without import notification.[45]

The New Zealand Food Safety Authority (NZFSA), or Te Pou Oranga Kai O Aotearoa is the New Zealand government body responsible for food safety. NZFSA is also the controlling authority for imports and exports of food and food-related products.  The NZFSA as of 2012 is now a division of the Ministry for Primary Industries and is no longer its own organization.

The Pure Food Ordinance 1960 consolidates and amends the law in relation to the preparation and the sale of foods. Its aim is to ensure purity of food being supplied to people in the market and, therefore, provides for preventing adulteration.

Pakistan Hotels and Restaurant Act, 1976 applies to all hotels and restaurants in Pakistan and seeks to control and regulate the standard of service(s) by hotels and restaurants. In addition to other provisions, under section 22(2), the sale of food or beverages that are contaminated, not prepared hygienically or served in utensils that are not hygienic or clean is an offense.[46]

The Ministry of Food and Drug Safety [47] has been working for food safety since 1945.[vague] It is part of the Government of South Korea.

IOAS[48]-Organic Certification Bodies Registered in KFDA: ""Organic"" or related claims can be labelled on food products when organic certificates are considered as valid by KFDA. KFDA admits organic certificates which can be issued by 1) IFOAM (International Federation of Organic Agriculture Movement) accredited certification bodies 2) Government accredited certification bodies – 328 bodies in 29 countries have been registered in KFDA.

Food Import Report: According to Food Import Report,[49] it is supposed to report or register what you import. Competent authority is as follows:

National Institute of Food and Drug Safety Evaluation (NIFDS)[50] is functioning as well.
The National Institute of Food and Drug Safety Evaluation is a national organization for toxicological tests and research. Under the Korea Food & Drug Administration, the Institute performs research on toxicology, pharmacology, and risk analysis of foods, drugs, and their additives. The Institute strives primarily to understand important biological triggering mechanisms and improve assessment methods of human exposure, sensitivities, and risk by (1) conducting basic, applied, and policy research that closely examines biologically triggering harmful effects on the regulated products such as foods, food additives, and drugs, and  operating the national toxicology program for the toxicological test development and inspection of hazardous chemical substances assessments. The Institute ensures safety by  investigation and research on safety by its own researchers,  contract research by external academicians and research centers.

In Taiwan, the Ministry of Health and Welfare in charge of Food and Drug Safety, also evaluate the catering industry to maintenance the food product quality.[51] Currently, US $29.01 million budget is allocated each year for food safety-related efforts.[52]

In Turkey, the Ministry of Agriculture and Forestry, is in charge of food safety and they provide their mission as ""to ensure access to safe food and high-quality agricultural products needed by Turkey and world markets"" among other responsibilities. The institution itself has research and reference laboratories across the country helping the control and inspection of food safety as well as reviewing and updating the current regulations and laws about food safety constantly.[53]

In the UK the Food Standards Agency is an independent government department responsible for food safety and hygiene across the England, Wales and Northern Ireland, while Food Standards Scotland is responsible for Scotland.[54][55] They work with businesses to help them produce safe food, and with local authorities to enforce food safety regulations. In 2006 food hygiene legislation changed and new requirements came into force. The main requirement resulting from this change is that anyone who owns or run a food business in the UK must have a documented Food Safety Management System, which is based on the principles of Hazard Analysis Critical Control Point.[56] Furthermore, according to UK legislation, food handlers and their supervisors must be adequately trained in food safety. Although food handlers are not legally obliged to hold a certificate they must be able to demonstrate to a health officer that they received training on the job, have prior experience, and have completed self-study. In practice, the self-study component is covered via a food hygiene and safety certificate.[57] Common occupations which fall under this obligation are Nannys, childminders, teachers, food manufacturers, chefs, cooks and catering staff.[58]

In early 2019, as part of US-UK negotiations to arrive at a trade deal prior to Brexit, the Trump administration asked the UK to eliminate its existing ban on chlorinated chicken, genetically modified plants and hormone-injected beef, products that the US would like to sell in the UK.[59]

The US food system is regulated by numerous federal, state and local officials. Since 1906 tremendous progress has been made in producing safer foods as can be seen in the section below. Still, it has been criticized as lacking in ""organization, regulatory tools, and not addressing food borne illness"".[60]

The Food and Drug Administration (FDA) publishes the Food Code, a model set of guidelines and procedures that assists food control jurisdictions by providing a scientifically sound technical and legal basis for regulating the retail and food service industries, including restaurants, grocery stores and institutional foodservice providers such as nursing homes. Regulatory agencies at all levels of government in the United States use the FDA Food Code to develop or update food safety rules in their jurisdictions that are consistent with national food regulatory policy. According to the FDA, 48 of 56 states and territories, representing 79% of the US population, have adopted food codes patterned after one of the five versions of the Food Code, beginning with the 1993 edition.[61]

In the United States, federal regulations governing food safety are fragmented and complicated, according to a February 2007 report from the Government Accountability Office.[62] There are 15 agencies sharing oversight responsibilities in the food safety system, although the two primary agencies are the US Department of Agriculture (USDA) Food Safety and Inspection Service (FSIS), which is responsible for the safety of meat, poultry, and processed egg products, and the FDA, which is responsible for virtually all other foods.

The Food Safety and Inspection Service has approximately 7,800 inspection program personnel working in nearly 6,200 federally inspected meat, poultry and processed egg establishments. FSIS is charged with administering and enforcing the Federal Meat Inspection Act, the Poultry Products Inspection Act, the Egg Products Inspection Act, portions of the Agricultural Marketing Act, the Humane Slaughter Act, and the regulations that implement these laws. FSIS inspection program personnel inspect every animal before slaughter, and each carcass after slaughter to ensure public health requirements are met.  In fiscal year (FY) 2008, this included about 50 billion pounds of livestock carcasses, about 59 billion pounds of poultry carcasses, and about 4.3 billion pounds of processed egg products. At US borders, they also inspected 3.3 billion pounds of imported meat and poultry products.[63]

Recognition of food safety issues and attempts to address them began after Upton Sinclair published the novel The Jungle in 1906. It was a fictional account of the lives of immigrants in the industrial cities in the US around this time. Sinclair spent nine months undercover as an employee in a Chicago meat plant doing research. The book inadvertently raised public concern about food safety and sanatization of the Chicago meat packing industry. Upon reading The Jungle, President Theodore Roosevelt called on Congress to pass the Pure Food and Drug Act and the Federal Meat Inspection Act (FMIA), which passed in 1906 and 1907 respectively.[64] These laws were the first to address food safety in the US Misbranding and adulteration were defined as they concerned food additives and truth in labeling. Food preservatives such as formaldehyde and borax used to disguise unsanitary production processes were also addressed.

The first test and major court battle involving the Pure Food and Drug Act was United States v. Forty Barrels and Twenty Kegs of Coca-Cola, an attempt to outlaw Coca-Cola due to its excessive caffeine content. The Meat Inspection Act led to the formation of the Food and Drug Administration (FDA). Between 1906 and 1938, acts were created that monitored food coloration additives, and other chemical additives such as preservatives, as well as food labeling and food marketing.

During the winter of 1924–1925, the worst food-borne illness to date in the US occurred because of improper handling of oysters.[65] This produced a typhoid fever epidemic, and food-borne illness outbreaks gained national attention. Unfortunately, it was not until 1969 that the FDA began sanitization programs specifically for shellfish and milk, and began its focus and implementation on the food service industry as a whole.

In 1970 the Centers for Disease Control and Prevention (CDC) began keeping records on food-borne illness deaths. This was the beginning of effective record keeping that could be used to control and prevent similar outbreaks in the future. The first major food recall in the US was caused by canned mushrooms in 1973.[66] This outbreak of botulism produced the National Botulism Surveillance System. This system collected the data on all confirmed cases of botulism in the US This led to processing regulations for low-acid foods to ensure proper heat treating of canned foods. The Jack in the Box E. coli outbreak of 1993 led the Clinton administration to put $43 million into the Food Safety Initiative to create many of the common specific regulations in place today. This initiative produced regulations on seafood, meat, poultry, and shell-eggs. This initiative produced a program for DNA fingerprinting to help track outbreaks and to determine their source. It also called for a cooperative detection and response effort between the CDC, FDA, USDA and local agencies called FoodNet.[67]

In 2011 the Food Safety Modernization Act (FSMA) produced what is considered the most significant food safety legislation in over 70 years. The significant difference between this and previous acts was that it shifted to focus from response and containment of food-borne disease outbreaks to their prevention. This act is still in the early implementation phase but gives the FDA authority to regulate the way foods are grown, processed, and harvested.

There have been concerns over the efficacy of safety practices and food industry pressure on US regulators. A study reported by Reuters found that ""the food industry is jeopardizing US public health by withholding information from food safety investigators or pressuring regulators to withdraw or alter policy designed to protect consumers"". A 2010 survey found that 25% of US government inspectors and scientists surveyed had experienced during the past year corporate interests forcing their food safety agency to withdraw or to modify agency policy or action that protects consumers. Scientists observed that management undercuts field inspectors who stand up for food safety against industry pressure. According to Dr. Dean Wyatt, a USDA veterinarian who oversees federal slaughterhouse inspectors, ""Upper level management does not adequately support field inspectors and the actions they take to protect the food supply. Not only is there lack of support, but there's outright obstruction, retaliation and abuse of power.""[68] A growing number of food and beverage manufacturers are improving food safety standards by incorporating a food safety management system which automates all steps in the food quality management process.[69]

A number of US states have their own meat inspection programs that substitute for USDA inspection for meats that are sold only in-state.[70] Certain state programs have been criticized for undue leniency to bad practices.[71] Contrastingly, there are some state-level programs that supplement Federal inspections rather than replacing them. Said programs generally operate with the goal of increasing consumer confidence in their state's produce, play a role in investigating outbreaks of food-borne disease bacteria- such as in the 2006 outbreak of pathogenic Escherichia coli O157:H7[72]- and promote better food processing practices to eliminate food-borne threats.[73] Additionally, several states which are major producers of fresh fruits and vegetables (including California, Arizona and Florida) have their own state programs to test produce for pesticide residues.[74]

The food system represents one of the most significant components of the U.S. economy. It affects the social and economic well-being of nearly all Americans and plays a significant role in the well-being of the global community. The U.S. food and fiber system accounted for 18 percent of employment 4 percent of imported goods, and 11 percent of exports in 2011. The relative economic contribution of each various step of the U.S. food supply chain has changed significantly over the past 100 years. Generally speaking, the economic importance of the farm production subsector has steadily diminished relative to the shares of the other components of the food supply chain.

Restaurants and other retail food establishments fall under state law and are regulated by state or local health departments. Typically these regulations require official inspections of specific design features, best food-handling practices, and certification of food handlers.[75][76] In some places a letter grade or numerical score must be prominently posted following each inspection.[77] In some localities, inspection deficiencies and remedial action are posted on the Internet.[78] In addition, states may maintain and enforce their own model of the FDA Food Code. For example, California maintains the California Retail Food Code (CalCode), which is part of the Health and Safety Code and is based on most current and safe food handling practices in the retail industry.[79]
It has been argued that restaurant hygiene ratings, though useful at times, are not informative enough for consumers.[80]

The Vietnam Food Administration manages food hygiene, safety, and quality and has made significant progress since its establishment in 1999. Food safety remains a high priority in Vietnam with the growth of export markets and increasing food imports raising the need to rapidly build capacity of the Food Administration in order to reduce threats of foodborne disease. The Food Administration has demonstrated commitment to the food safety challenges it faces, and has embarked on an innovative capacity building activity with technical assistance from the WHO.[81]

Meat and Poultry manufacturers are required to have a HACCP plan in accordance with 9 CFR part 417.[82]

Juice manufacturers are required to have a HACCP plan in accordance with 21 CFR part 120.[82]

Seafood manufacturers are required to have a HACCP plan in accordance with 21 CFR part 123.[82]

With the exception of infant formula and baby foods which must be withdrawn by their expiration date, Federal law does not require expiration dates. For all other foods, except dairy products in some states, freshness dating is strictly voluntary on the part of manufacturers. In response to consumer demand, perishable foods are typically labelled with a 'SELL BY' date.[83] It is up to the consumer to decide how long after the 'SELL BY' date a package is usable. Other common dating statements are 'BEST IF USED BY' date, 'USE BY' date, 'EXPIRES/EXPIRATION' date, 'GUARANTEED FRESH' date, and 'PACKED/PACKED ON' dating.[84] When used, freshness dating must be validated using AOAC International (Association of Official Analytical Collaboration International) guidelines.[85] Although this dating requires product testing throughout the entire timeframe, accelerated shelf life testing, using elevated temperatures and humidity, can be used to determine shelf life before the long-term results can be completed.[citation needed]

In the United States a study showed that most adults, over the age of 18, did not fully understand what the terms ""BEST BY"", ""SELL BY"" or ""USE BY"" meant. Over the years this had led to billions of pounds of food being discarded prematurely. The primary reason the prevention of foodborne illness, which affects 48 million people annually in the United States.[86] With lack of federal regulation, and standardization of date labeling those from low socioeconomic backgrounds showed to be most affected, often lacking the tools and awareness to safely handle and store food.[87]

The Natural Resource Defense Council and Harvard University Food Law and Clinic Policy have both stated the importance of food date regulation needing to be standardized so consumers are able to make more informed decision on food safety. Most of the packaging dates from the manufacturer are intended for store use, to reflect when an item is at peak quality. Not to inform consumers when food is no longer safe to eat.[88] A study conducted in 2019 found that 86% of adults discarded food near the packaging date occasionally. Over a third of the participants also believed that date labeling is federally regulated. The results also showed that adults ranging from 18-34 more frequently misunderstood and relied on the date labeling when deciding to discard food, showing that consumer education is needed for adults in this range.[89] Families from low socioeconomic backgrounds have been shown to have less knowledge about food safety.[90] With food security being an issues for millions of americans[91] it is important for such individuals to be educated on food safety practices.

Guide to Food Labelling and Other Information Requirements: This guide provides background information on the general labelling requirements in the Code. The information in this guide applies both to food for retail sale and to food for catering purposes. Foods for catering purposes means those foods for use in restaurants, canteens, schools, caterers or self-catering institutions, where food is offered for immediate consumption. Labelling and information requirements in the new Code apply both to food sold or prepared for sale in Australia and New Zealand and food imported into Australia and New Zealand.[citation needed]
Warning and Advisory Declarations, Ingredient Labelling, Date Marking, Nutrition Information Requirements, Legibility Requirements for Food Labels, Percentage Labelling, Information Requirements for Foods Exempt from Bearing a Label.[92][93]

Food recalls are typically initiated by the manufacturer, distributor of the product, or by a government agency responsible for food safety. Once a safety or quality concern with food products that are already on the market has been recognized, a recall is issued to prevent further damage to the public.[94]

The batch number tracking technique is one of the methods which can be used by manufacturers to recall contaminated food products. In 2015, 19 people in the US suffered food poisoning caused by E. coli O157:H7 after consuming Costco rotisserie chicken salad. Health officials issued a recall on all the uneaten salads with batch number 37719.[95]




"
Quality Assurance Services,"Quality assurance (QA) is the term used in both manufacturing and service industries to describe the systematic efforts taken to assure that the product(s) delivered to customer(s) meet with the contractual and other agreed upon performance, design, reliability, and maintainability expectations of that customer. The core purpose of Quality Assurance is to prevent mistakes and defects in the development and production of both manufactured products, such as automobiles and shoes, and delivered services, such as automotive repair and athletic shoe design. Assuring quality and therefore avoiding problems and delays when delivering products or services to customers is what ISO 9000 defines as that ""part of quality management focused on providing confidence that quality requirements will be fulfilled"".[1] This defect prevention aspect of quality assurance differs from the defect detection  aspect of quality control and has been referred to as a shift left since it focuses on quality efforts earlier in product development and production (i.e., a shift to the left of a linear process diagram reading left to right)[2] and on avoiding defects in the first place rather than correcting them after the fact.

The terms ""quality assurance"" and ""quality control"" are often used interchangeably to refer to ways of ensuring the quality of a service or product.[3] For instance, the term ""assurance"" is often used in a context such as: Implementation of inspection and structured testing as a measure of quality assurance in a television set software project at Philips Semiconductors is described.[4] where inspection and structured testing are the measurement phase of a quality assurance strategy referred to as the DMAIC model (define, measure, analyze, improve, control). DMAIC is a data-driven quality strategy used to improve processes.[5] The term ""control"" is the fifth phase of this strategy.

Quality assurance comprises administrative and procedural activities implemented in a quality system so that requirements and goals for a product, service or activity will be accomplished.[3] It is the systematic measurement, comparison with a standard, and monitoring of processes in an associated feedback loop that confers error prevention.[6] This can be contrasted with quality control, which is focused on process output.[7]

Quality assurance includes two principles: ""fit for purpose"" (the product should be suitable for the intended purpose); and ""right first time"" (mistakes should be eliminated). QA includes management of the quality of raw materials, assemblies, products and components, services related to production, and management, production and inspection processes.[8] The two principles also manifest before the background of developing (engineering) a novel technical product: The task of engineering is to make it work once, while the task of quality assurance is to make it work all the time.[9]

Historically, defining what suitable product or service quality means has been a more difficult process, determined in many ways, from the subjective user-based approach that contains ""the different weights that individuals normally attach to quality characteristics,"" to the value-based approach which finds consumers linking quality to price and making overall conclusions of quality based on such a relationship.[10]

During the Middle Ages, guilds adopted responsibility for the quality of goods and services offered by their members, setting and maintaining certain standards for guild membership.[11]

Royal governments purchasing material were interested in quality control as customers. For this reason, King John of England appointed William de Wrotham to report about the construction and repair of ships.[12] Centuries later, Samuel Pepys, Secretary to the British Admiralty, appointed multiple such overseers to standardize sea rations and naval training.[13]

Prior to the extensive division of labor and mechanization resulting from the Industrial Revolution, it was possible for workers to control the quality of their own products. The Industrial Revolution led to a system in which large groups of people performing a specialized type of work were grouped together under the supervision of a foreman who was appointed to control the quality of work manufactured.

During the time of the First World War, manufacturing processes typically became more complex, with larger numbers of workers being supervised. This period saw the widespread introduction of mass production and piece work, which created problems as workmen could now earn more money by the production of extra products, which in turn occasionally led to poor quality workmanship being passed on to the assembly lines. Pioneers such as Frederick Winslow Taylor and Henry Ford recognized the limitations of the methods being used in mass production at the time and the subsequent varying quality of output. Taylor, utilizing the concept of scientific management, helped separate production tasks into many simple steps (the assembly line) and limited quality control to a few specific individuals, limiting complexity.[14] Ford emphasized standardization of design and component standards to ensure a standard product was produced, while quality was the responsibility of machine inspectors, ""placed in each department to cover all operations ... at frequent intervals, so that no faulty operation shall proceed for any great length of time.""[15]

Out of this also came statistical process control (SPC), which was pioneered by Walter A. Shewhart at Bell Laboratories in the early 1920s. Shewhart developed the control chart in 1924 and the concept of a state of statistical control. Statistical control is equivalent to the concept of exchangeability[16][17] developed by logician William Ernest Johnson, also in 1924, in his book Logic, Part III: The Logical Foundations of Science.[18] Along with a team at AT&T that included Harold Dodge and Harry Romig, he worked to put sampling inspection on a rational statistical basis as well. Shewhart consulted with Colonel Leslie E. Simon in the application of control charts to munitions manufacture at the Army's Picatinny Arsenal in 1934.[19] That successful application helped convince Army Ordnance to engage AT&T's George Edwards to consult on the use of statistical quality control among its divisions and contractors at the outbreak of World War II.[20]

After World War II, many countries' manufacturing capabilities that had been destroyed during the war were rebuilt. General Douglas MacArthur oversaw the rebuilding of Japan. He involved two key people in the development of modern quality concepts: W. Edwards Deming and Joseph Juran. They and others promoted the collaborative concepts of quality to Japanese business and technical groups, and these groups used these concepts in the redevelopment of the Japanese economy.[21]

Although there were many people trying to lead United States industries toward a more comprehensive approach to quality, the US continued to apply the Quality Control (QC) concepts of inspection and sampling to remove defective products from production lines, essentially unaware of or ignoring advances in QA for decades.[22]

It is valuable to failure test or stress test a complete consumer product. In mechanical terms this is the operation of a product until it fails, often under stresses such as increasing vibration, temperature, and humidity. This may expose many unanticipated weaknesses in the product, and the data is used to drive engineering and manufacturing process improvements. Often quite simple changes can dramatically improve product service, such as changing to mold-resistant paint or adding lock-washer placement to the training for new assembly personnel.

Statistical control is based on analyses of objective and subjective data.[23] Many organizations use statistical process control as a tool in any quality improvement effort[24] to track quality data. Product quality data is statistically charted to distinguish between common cause variation or special cause variation.[25]

Walter Shewart of Bell Telephone Laboratories recognized that when a product is made, data can be taken from scrutinized areas of a sample lot of the part and statistical variances are then analyzed and charted. Control can then be implemented on the part in the form of rework or scrap, or control can be implemented on the process that made the part, ideally eliminating the defect before more parts can be made like it.[23]

The quality of products is dependent upon that of the participating constituents,[26] some of which are sustainable and effectively controlled while others are not. The process(es) which are managed with QA pertain to Total quality management.

If the specification does not reflect the true quality requirements, the product's quality cannot be guaranteed. For instance, the parameters for a pressure vessel should cover not only the material and dimensions but operating, environmental, safety, reliability and maintainability requirements.

ISO 17025 is an international standard that specifies the general requirements for the competence to carry out tests and or calibrations. There are 15 management requirements and 10 technical requirements. These requirements outline what a laboratory must do to become accredited.
Management system refers to the organization's structure for managing its processes or activities that transform inputs of resources into a product or service which meets
the organization's objectives, such as satisfying the customer's quality requirements, complying with regulations, or meeting environmental objectives. WHO has developed several tools and offers training courses for quality assurance in public health laboratories.[27]

The Capability Maturity Model Integration (CMMI) model is widely used to implement Process and Product Quality Assurance (PPQA) in an organization. The CMMI maturity levels can be divided into 5 steps, which a company can achieve by performing specific activities within the organization.

During the 1980s, the concept of ""company quality"" with the focus on management and people came to the fore in the U.S.[22] It was considered that, if all departments approached quality with an open mind, success was possible if management led the quality improvement process.

The company-wide quality approach places an emphasis on four aspects (enshrined in standards such as ISO 9001):[28]

The quality of the outputs is at risk if any of these aspects is deficient.

The importance of actually measuring Quality Culture throughout the organization is illustrated by a survey that was done by Forbes Insights in partnership with the American Society for Quality. 75% of senior or C-suite titles believed that their organization exhibits ""a comprehensive, group-wide culture of quality."" But agreement with that response dropped to less than half among those with quality job titles. In other words, the further from the C-suite, the less favorable the view of the culture of quality.[29] A survey of more than 60 multinational companies found that those companies whose employees rated as having a low quality culture had increased costs of $67 million/year for every 5000 employees compared to those rated as having a high quality culture.[30]

QA is not limited to manufacturing, and can be applied to any business or non-business activity, including: design, consulting, banking, insurance, computer software development, retailing, investment, transportation, education, and translation.

It comprises a quality improvement process, which is generic in the sense that it can be applied to any of these activities and it establishes a quality culture, which supports the achievement of quality.[31]

This in turn is supported by quality management practices which can include a number of business systems and which are usually specific to the activities of the business unit concerned.

In manufacturing and construction activities, these business practices can be equated to the models for quality assurance defined by the International Standards contained in the ISO 9000 series and the specified specifications for quality systems.

In the system of Company Quality, the work being carried out was shop floor inspection which did not reveal the major quality problems. This led to quality assurance or total quality control, which has come into being recently.

QA is very important in the medical field because it helps to identify the standards of medical equipment and services.[32][33] Hospitals and laboratories make use of external agencies in order to ensure standards for equipment such as X-ray machines, Diagnostic Radiology and AERB. QA is particularly applicable throughout the development and introduction of new medicines and medical devices. The Research Quality Association (RQA) supports and promotes the quality of research in life sciences, through its members and regulatory bodies.

The term product assurance (PA) is often used instead of quality assurance and is, alongside project management and engineering, one of the three primary project functions. Quality assurance is seen as one part of product assurance. Due to the sometimes catastrophic consequences a single failure can have for human lives, the environment, a device, or a mission, product assurance plays a particularly important role here. It has organizational, budgetary and product developmental independence meaning that it reports to highest management only, has its own budget, and does not expend labor to help build a product. Product assurance stands on an equal footing with project management but embraces the customer's point of view.[9]

Software quality assurance refers to monitoring the software engineering processes and methods used to ensure quality. Various methods or frameworks are employed for this, such as ensuring conformance to one or more standards, e.g. ISO 25010 (which supersede ISO/IEC 9126) or process models such as CMMI, or SPICE. In addition, enterprise quality management software is used to correct issues such as supply chain disaggregation and to ensure regulatory compliance; these are vital for medical device manufacturers.[34]

Consultants and contractors are sometimes employed when introducing new quality practices and methods, particularly where the relevant skills and expertise and resources are not available within the organization. Consultants and contractors will often employ Quality Management Systems (QMS), auditing and procedural documentation writing CMMI, Six Sigma, Measurement Systems Analysis (MSA), Quality Function Deployment (QFD), Failure Mode and Effects Analysis (FMEA), and Advance Product Quality Planning (APQP).
"
Compliance Services,"In general, compliance means conforming to a rule, such as a specification, policy, standard or law. Compliance has traditionally been explained by reference to deterrence theory, according to which punishing a behavior will decrease the violations both by the wrongdoer (specific deterrence) and by others (general deterrence). This view has been supported by economic theory, which has framed punishment in terms of costs and has explained compliance in terms of a cost-benefit equilibrium (Becker 1968). However, psychological research on motivation provides an alternative view: granting rewards (Deci, Koestner and Ryan, 1999) or imposing fines (Gneezy Rustichini 2000) for a certain behavior is a form of extrinsic motivation that weakens intrinsic motivation and ultimately undermines compliance. 

Regulatory compliance describes the goal that organizations aspire to achieve in their efforts to ensure that they are aware of and take steps to comply with relevant laws, policies, and regulations.[1] Due to the increasing number of regulations and need for operational transparency, organizations are increasingly adopting the use of consolidated and harmonized sets of compliance controls.[2] This approach is used to ensure that all necessary governance requirements can be met without the unnecessary duplication of effort and activity from resources.

Regulations and accrediting organizations vary among fields, with examples such as PCI-DSS and GLBA in the financial industry, FISMA for U.S. federal agencies, HACCP for the food and beverage industry, and the Joint Commission and HIPAA in healthcare. In some cases other compliance frameworks (such as COBIT) or even standards (NIST) inform on how to comply with regulations.

Some organizations keep compliance data—all data belonging or pertaining to the enterprise or included in the law, which can be used for the purpose of implementing or validating compliance—in a separate store for meeting reporting requirements. Compliance software is increasingly being implemented to help companies manage their compliance data more efficiently. This store may include calculations, data transfers, and audit trails.[3][4]

The International Organization for Standardization (ISO) and its ISO 37301:2021 (which deprecates ISO 19600:2014) standard is one of the primary international standards for how businesses handle regulatory compliance, providing a reminder of how compliance and risk should operate together, as ""colleagues"" sharing a common framework with some nuances to account for their differences. The ISO also produces international standards such as ISO/IEC 27002 to help organizations meet regulatory compliance with their security management and assurance best practices.[5]

Some local or international specialized organizations such as the American Society of Mechanical Engineers (ASME) also develop standards and regulation codes. They thereby provide a wide range of rules and directives to ensure compliance of the products to safety, security or design standards.[6]

Regulatory compliance varies not only by industry but often by location. The financial, research, and pharmaceutical regulatory structures in one country, for example, may be similar but with particularly different nuances in another country. These similarities and differences are often a product ""of reactions to the changing objectives and requirements in different countries, industries, and policy contexts"".[7]

Australia's major financial services regulators of deposits, insurance, and superannuation include the Reserve Bank of Australia (RBA), the Australian Prudential Regulation Authority (APRA), the Australian Securities & Investments Commission (ASIC), and the Australian Competition & Consumer Commission (ACCC).[8] These regulators help to ensure financial institutes meet their promises, that transactional information is well documented, and that competition is fair while protecting consumers. The APRA in particular deals with superannuation and its regulation, including new regulations requiring trustees of superannuation funds to demonstrate to APRA that they have adequate resources (human, technology and financial), risk management systems, and appropriate skills and expertise to manage the superannuation fund, with individuals running them being ""fit and proper"".[8]

Other key regulators in Australia include the Australian Communications & Media Authority (ACMA) for broadcasting, the internet, and communications;[9] the Clean Energy Regulator for ""monitoring, facilitating and enforcing compliance with"" energy and carbon emission schemes;[10] and the Therapeutic Goods Administration for drugs, devices, and biologics;[11]

Australian organisations seeking to remain compliant with various regulations may turn to AS ISO 19600:2015 (which supersedes AS 3806-2006). This standard helps organisations with compliance management, placing ""emphasis on the organisational elements that are required to support compliance"" while also recognizing the need for continual improvement.[12][13]

In Canada, federal regulation of deposits, insurance, and superannuation is governed by two independent bodies: the OSFI through the Bank Act, and FINTRAC, mandated by the Proceeds of Crime (Money Laundering) and Terrorist Financing Act, 2001 (PCMLTFA).[14][15] These groups protect consumers, regulate how risk is controlled and managed, and investigate illegal action such as money laundering and terrorist financing.[14][15] On a provincial level, each province maintain individuals laws and agencies. Unlike any other major federation, Canada does not have a securities regulatory authority at the federal government level. The provincial and territorial regulators work together to coordinate and harmonize regulation of the Canadian capital markets through the Canadian Securities Administrators (CSA).[16]

Other key regulators in Canada include the Canadian Food Inspection Agency (CFIA) for food safety, animal health, and plant health; Health Canada for public health; and Environment and Climate Change Canada for environment and sustainable energy.[17]

Canadian organizations seeking to remain compliant with various regulations may turn to ISO 19600:2014, an international compliance standard that ""provides guidance for establishing, developing, implementing, evaluating, maintaining and improving an effective and responsive compliance management system within an organization"".[18] For more industry specific guidance, e.g., financial institutions, Canada's E-13 Regulatory Compliance Management provides specific compliance risk management tactics.[19]

Regulatory compliance in the European Union (EU) is governed by a harmonized legal framework designed to ensure consistency across member states while allowing for national implementation. EU compliance regulations cover various industries, including consumer product safety, financial services, environmental protection, and data privacy.

The General Product Safety Regulation (GPSR) establishes a unified safety framework for consumer products across the EU, requiring manufacturers to conduct risk assessments, maintain traceability documentation, and meet safety compliance standards before placing products on the market.[20][21] The GPSR applies to all consumer products made available in the EU unless covered by sector-specific regulations, such as medical devices or food products. The regulation extends to products sold through e-commerce platforms, requiring online marketplaces to ensure that only compliant products are listed. Fulfillment service providers are also included as economic operators, making them responsible for product safety compliance in certain cases.

For business compliance, the EU’s regulatory approach is guided by the New Legislative Framework (NLF) and various sector-specific directives and regulations. Businesses must comply with EU product conformity assessments and affix the CE marking to indicate compliance with essential safety and performance standards.[22]

Financial compliance is enforced through regulations such as the Markets in Financial Instruments Directive (MiFID II) and the General Data Protection Regulation (GDPR), which set strict requirements for financial transparency, consumer protection, and data security.

The EU Legislation Compliance framework ensures that organizations operate within the legal boundaries of EU directives, helping public and private entities manage regulatory risks efficiently.[23]

Companies operating in the EU must stay updated on evolving compliance requirements, as non-compliance can lead to fines, product recalls, or restrictions on market access.

The financial sector in the Netherlands is heavily regulated. The Dutch Central Bank (De Nederlandsche Bank N.V.) is the prudential regulator while the Netherlands Authority for Financial Markets (AFM) is the regulator for behavioral supervision of financial institutions and markets. A common definition of compliance is:'Observance of external (international and national) laws and regulations, as well as internal norms and procedures, to protect the integrity of the organization, its management and employees with the aim of preventing and controlling risks and the possible damage resulting from these compliance and integrity risks'.[24]

In India, compliance regulation takes place across three strata: Central, State, and Local regulation. India veers towards central regulation, especially of financial organizations and foreign funds. Compliance regulations vary based on the industry segment in addition to the geographical mix. Most regulation comes in the following broad categories: economic regulation, regulation in the public interest, and environmental regulation.[25] India has also been characterized by poor compliance - reports suggest that only around 65% of companies are fully compliant to norms.[26]

The Monetary Authority of Singapore is Singapore's central bank and financial regulatory authority. It administers the various statutes pertaining to money, banking, insurance, securities and the financial sector in general, as well as currency issuance.[27]

There is considerable regulation in the United Kingdom, some of which is derived from European Union legislation. Various areas are policed by different bodies, such as the Financial Conduct Authority (FCA),[28] Environment Agency,[29] Scottish Environment Protection Agency,[30] Information Commissioner's Office,[31] Care Quality Commission,[32] and others: see List of regulators in the United Kingdom.

Important compliance issues for all organizations large and small include the Data Protection Act 2018[33] and, for the public sector, Freedom of Information Act 2000.[34]

Corporate scandals and breakdowns such as the Enron case of reputational risk in 2001 have increased calls for stronger compliance and regulations, particularly for publicly listed companies.[1] The most significant recent statutory changes in this context have been the Sarbanes–Oxley Act developed by two U.S. congressmen, Senator Paul Sarbanes and Representative Michael Oxley in 2002 which defined significantly tighter personal responsibility of corporate top management for the accuracy of reported financial statements; and the Dodd-Frank Wall Street Reform and Consumer Protection Act.

The Office of Foreign Assets Control (OFAC) is an agency of the United States Department of the Treasury under the auspices of the Under Secretary of the Treasury for Terrorism and Financial Intelligence. OFAC administers and enforces economic and trade sanctions based on U.S. foreign policy and national security goals against targeted foreign states, organizations, and individuals.

Compliance in the U.S. generally means compliance with laws and regulations. These laws and regulations can have criminal or civil penalties. The definition of what constitutes an effective compliance plan has been elusive. Most authors, however, continue to cite the guidance provided by the United States Sentencing Commission in Chapter 8 of the Federal Sentencing Guidelines.[35][36]

On October 12, 2006, the U.S. Small Business Administration re-launched Business.gov (later Business.USA.gov and finally SBA.Gov)[37] which provides a single point of access to government services and information that help businesses comply with government regulations.

The U.S. Department of Labor, Occupational Health and Safety Administration (OSHA) was created by Congress to assure safe and healthful working conditions for working men and women by setting and enforcing standards and by providing training, outreach, education, and assistance. OSHA implements laws and regulations regularly in the following areas, construction, maritime, agriculture, and recordkeeping.[38]

The United States Department of Transportation also has various laws and regulations requiring that prime contractors when bidding on federally funded projects engage in good faith effort compliance, meaning they must document their outreach to certified disadvantaged business enterprises.[39]

Data retention is a part of regulatory compliance that is proving to be a challenge in many instances. The security that comes from compliance with industry regulations can seem contrary to maintaining user privacy. Data retention laws and regulations ask data owners and other service providers to retain extensive records of user activity beyond the time necessary for normal business operations. These requirements have been called into question by privacy rights advocates.[40]

Compliance in this area is becoming very difficult. Laws like the CAN-SPAM Act and Fair Credit Reporting Act in the U.S. require that businesses give people the right to be forgotten.[41][42] In other words, they must remove individuals from marketing lists if it is requested, tell them when and why they might share personal information with a third party, or at least ask permission before sharing that data. Now, with new laws coming out that demand longer data retention despite the individual’s desires, it can create some real difficulties.

Money laundering and terrorist financing pose significant threats to the integrity of the financial system and national security. To combat these threats, the EU has adopted a risk-based approach to Anti-Money Laundering and Combating the Financing of Terrorism (AML/CFT) that relies on cooperation and coordination between EU and national authorities. In this context, risk-based regulation refers to the approach of identifying and assessing potential risks of money laundering and terrorist financing and implementing regulatory measures proportional to those risks. However, the shared enforcement powers between EU and national authorities in the implementation and enforcement of AML/CFT regulations can create legal implications and challenges. The potential for inconsistent application of AML regulations across different jurisdictions can create regulatory arbitrage and undermine the effectiveness of AML efforts. Additionally, a lack of clear and consistent legal frameworks defining the roles and responsibilities of EU and national authorities in AML enforcement can lead to situations where accountability is difficult to establish. 

The U.K. Corporate Governance Code (formerly the Combined Code) is issued by the Financial Reporting Council (FRC) and ""sets standards of good practice in relation to board leadership and effectiveness, remuneration, accountability, and relations with shareholders"".[43] All companies with a Premium Listing of equity shares in the U.K. are required under the Listing Rules to report on how they have applied the Combined Code in their annual report and accounts.[44] (The Codes are therefore most similar to the U.S.' Sarbanes–Oxley Act.)

The U.K.'s regulatory framework requires that all its publicly listed companies should provide specific content in the core financial statements that must appear in a yearly report, including balance sheet, comprehensive income statement, and statement of changes in equity, as well as cash flow statement as required under international accounting standards.[45] It further demonstrates the relationship that subsists among shareholders, management, and the independent audit teams. Financial statements must be prepared using a particular set of rules and regulations hence the rationale behind allowing the companies to apply the provisions of company law, international financial reporting standards (IFRS), as well as the U.K. stock exchange rules as directed by the FCA.[46] It is also possible that shareholders may not understand the figures as presented in the various financial statements, hence it is critical that the board should provide notes on accounting policies as well as other explanatory notes to help them understand the report better.
"
Environmental Health Services,"Environmental health is the branch of public health concerned with all aspects of the natural and built environment affecting human health. To effectively control factors that may affect health, the requirements that must be met to create a healthy environment must be determined.[1] The major sub-disciplines of environmental health are environmental science, toxicology, environmental epidemiology, and environmental and occupational medicine.[2]

Environmental health was defined in a 1989 document by the World Health Organization (WHO) as:
Those aspects of human health and disease that are determined by factors in the environment.[3] It is also referred to as the theory and practice of accessing and controlling factors in the environment that can potentially affect health.[4]

A 1990 WHO document states that environmental health, as used by the WHO Regional Office for Europe, ""includes both the direct pathological effects of chemicals, radiation and some biological agents, and the effects (often indirect) on health and well being of the broad physical, psychological, social and cultural environment, which includes housing, urban development, land use and transport.""[5]

As of 2016[update], the WHO website on environmental health states that ""Environmental health addresses all the physical, chemical, and biological factors external to a person, and all the related factors impacting behaviours. It encompasses the assessment and control of those environmental factors that can potentially affect health. It is targeted towards preventing disease and creating health-supportive environments. This definition excludes behaviour not related to environment, as well as behaviour related to the social and cultural environment, as well as genetics.""[6]

The WHO has also defined environmental health services as ""those services which implement environmental health policies through monitoring and control activities. They also carry out that role by promoting the improvement of environmental parameters and by encouraging the use of environmentally friendly and healthy technologies and behaviors. They also have a leading role in developing and suggesting new policy areas.""[7][8]

The term environmental medicine may be seen as a medical specialty, or branch of the broader field of environmental health.[9][10] Terminology is not fully established, and in many European countries they are used interchangeably.[11]

Other terms referring to or concerning environmental health include environmental public health and health protection.[12]

Children's environmental health is the academic discipline that studies how environmental exposures in early life—chemical, biological, nutritional, and social—influence health and development in childhood and across the entire human life span.[13] Pediatric environmental health is based on the recognition that children are not “little adults.” Infants and children have unique patterns of exposure and vulnerabilities. Environmental risks of infants and children are qualitatively and quantitatively different from those of adults. Pediatric environmental health is highly interdisciplinary. It spans and brings together general pediatrics and numerous pediatric subspecialties as well as epidemiology, occupational and environmental medicine, medical toxicology, industrial hygiene, and exposure science.

Five basic disciplines generally contribute to the field of environmental health: environmental epidemiology, toxicology, exposure science, environmental engineering, and environmental law. Each of these five disciplines contributes different information to describe problems and solutions in environmental health. However, there is some overlap among them.

Information from epidemiology, toxicology, and exposure science can be combined to conduct a risk assessment for specific chemicals, mixtures of chemicals or other risk factors to determine whether an exposure poses significant risk to human health (exposure would likely result in the development of pollution-related diseases). This can in turn be used to develop and implement environmental health policy that, for example, regulates chemical emissions, or imposes standards for proper sanitation.[21] Actions of engineering and law can be combined to provide risk management to minimize, monitor, and otherwise manage the impact of exposure to protect human health to achieve the objectives of environmental health policy.

Environmental health addresses all human-health-related aspects of the natural environment and the built environment. Environmental health concerns include:

According to recent estimates, about 5 to 10% of disability-adjusted life years (DALYs) lost are due to environmental causes in Europe. By far the most important factor is fine particulate matter pollution in urban air.[25] Similarly, environmental exposures have been estimated to contribute to 4.9 million (8.7%) deaths and 86 million (5.7%) DALYs globally.[26] In the United States, Superfund sites created by various companies have been found to be hazardous to human and environmental health in nearby communities. It was this perceived threat, raising the specter of miscarriages, mutations, birth defects, and cancers that most frightened the public.[27]

Air quality includes ambient outdoor air quality and indoor air quality. Large concerns about air quality include environmental tobacco smoke, air pollution by forms of chemical waste, and other concerns.

Air pollution is globally responsible for over 6.5 million deaths each year.[28] Air pollution is the contamination of an atmosphere due to the presence of substances that are harmful to the health of living organisms, the environment or climate.[29] These substances concern environmental health officials since air pollution is often a risk-factor for diseases that are related to pollution, like lung cancer, respiratory infections, asthma, heart disease, and other forms of respiratory-related illnesses.[30] Reducing air pollution, and thus developing air quality, has been found to decrease adult mortality.[31]

Common products responsible for emissions include road traffic, energy production, household combustion, aviation and motor vehicles, and other forms of pollutants.[32][33] These pollutants are responsible for the burning of fuel, which can release harmful particles into the air that humans and other living organisms can inhale or ingest.[34]

Air pollution is associated with adverse health effects like respiratory and cardiovascular diseases, cancer, related illnesses, and even death.[35] The risk of air pollution is determined by the pollutant's hazard and the amount of exposure that affects a person.[36] For example, a child who plays outdoor sports will have a higher likelihood of outdoor air pollution exposure than an adult who tends to spend more time indoors, whether at work or elsewhere.[36] Environmental health officials work to detect individuals who are at higher risks of consuming air pollution, work to decrease their exposure, and detect risk factors present in communities.[37]

However, as shown in research by Ernesto, Sánchez-Triana in the case of Pakistan. After identifying the main sources of air pollution, such as mobile sources, such as heavy-duty vehicles and motorized 2–3 wheelers; stationary sources, such as power plants and burning of waste; and natural dust. The country implemented a clean air policy to reduce the road transport sector, which is responsible for 85% of particulate matter of less than 2.5 microns (PM2.5) total emissions and 72% of particulate matter of less than 10 microns (PM10)[38] Most successful policies were:

Household air pollution contributes to diseases that kill almost 4.3 million people every year.[39] Indoor air pollution contributes to risk factors for diseases like heart disease, pulmonary disease, stroke, pneumonia, and other associated illnesses.[39] For vulnerable populations, such as children and elderly populations, who spend large amounts of their time indoors or indoor air quality can be dangerous.[40]

Burning fuels like coal or kerosene inside homes can cause dangerous chemicals to be released into the air.[39] Dampness and mold in houses can cause diseases, but few studies have been performed on mold in schools and workplaces.[41] Environmental tobacco smoke is considered to be a leading contributor to indoor air pollution since exposure to second and third-hand smoke is a common risk factor.[42] Tobacco smoke contains over 60 carcinogens, where 18% are known human carcinogens.[43] Exposure to these chemicals can lead to exacerbation of asthma, the development of cardiovascular diseases and cardiopulmonary diseases, and an increase in the likelihood of cancer development.[44]

Climate change makes extreme weather events more likely, including ozone smog events, dust storms, and elevated aerosol levels, all due to extreme heat, drought, winds, and rainfall.[45][46] These extreme weather events can increase the likelihood of undernutrition, mortality, food insecurity, and climate-sensitive infectious diseases in vulnerable populations.[47] The effects of climate change are felt by the whole world, but disproportionately affect disadvantaged populations who are subject to climate change vulnerability.[48]

Climate impacts can affect exposure to water-borne pathogens through increased rates of runoff, frequent heavy rains, and the effects of severe storms.[49] Extreme weather events and storm surges can also exceed the capacity of water infrastructure, which can increase the likelihood that populations will be exposed to these contaminants.[49][50] Exposure to these contaminants are more likely in low-income communities, where they have inadequate infrastructure to respond to climate disasters and are less likely to recover from infrastructure damage as quickly.[51]

Problems like the loss of homes, loved ones, and previous ways of life, are often what people face after a climate disaster occurs. These events can lead to vulnerability in the form of housing affordability stress, lower household income, lack of community attachment, grief, and anxiety around another disaster occurring.[48]

Certain groups of people can be put at a higher risk for environmental hazards like air, soil and water pollution. This often happens due to marginalization, economic and political processes, and racism. Environmental racism uniquely affects different groups globally, however generally the most marginalized groups of any region are affected. These marginalized groups are frequently put next to pollution sources like major roadways, toxic waste sites, landfills, and chemical plants.[52] In a 2021 study, it was found that racial and ethnic minority groups in the United States are exposed to disproportionately high levels of particulate air pollution.[53] Racial housing policies that exist in the United States continue to exacerbate racial minority exposure to air pollution at a disproportionate rate, even as overall pollution levels have declined.[53] Likewise, in a 2022 study, it was shown that implementing policy changes that favor wealth redistribution could double as climate change mitigation measures.[54] For populations who are not subject to wealth redistribution measures, this means more money will flow into their communities while climate effects are mitigated.[53][54]

Noise pollution is usually environmental, machine-created sound that can disrupt activities or communication between humans and other forms of life.[citation needed] Exposure to persistent noise pollution can cause numerous ailments like hearing impairment, sleep disturbances, cardiovascular problems, annoyance, problems with communication and other diseases.[55] For American minorities that live in neighborhoods of low socioeconomic status, they often experience higher levels of noise pollution compared to their higher socioeconomic counterparts.[56]

Noise pollution can cause or exacerbate cardiovascular diseases, which can further attribute to a larger range of diseases, increase stress levels, and cause sleep disturbances.[56] Noise pollution is also responsible for many reported cases of hearing loss, tinnitus, and other forms of hypersensitivity(stress/irritability) or lack thereof to sound(present or subconscious from continuous exposure).[56] These conditions can be dangerous to children and young adults who consistently experience noise pollution, as many of these conditions can develop into long-term problems, including physical and mental health issues.[56]

Children who attend school in noisy traffic zones have shown to have 15% lower memory development compared to other students who attended schools in quiet traffic zones, according to a Barcelona study.[57] This is consistent with research that suggests that children who are exposed to regular aircraft noise ""have inadequate performance on standardised achievement tests.""[58]

Exposure to persistent noise pollution can cause one to develop hearing impairments, like tinnitus or impaired speech discrimination.[59] One of the largest factors in worsened mental health due to noise pollution is annoyance.[60][61] Annoyance due to environmental factors has been found to increase stress reactions and overall feelings of stress among adults.[55] The level of annoyance felt by an individual varies, but contributes to worsened mental health significantly.[61]

Noise exposure also contributes to sleep disturbances, which can cause daytime sleepiness and an overall lack of sleep, which contributes to worsened health.[61] Daytime sleepiness has been linked to several reports of declining mental health and other health issues, job insecurities and further social and environmental factors declining.

Access to safe drinking water is considered a ""basic human need for health and well-being"" by the United Nations.[62] According to their reports, over 2 billion people worldwide live without access to safe drinking water.[63] In 2017, almost 22 million Americans drank from water systems that were in violation of public health standards.[64] Globally, over 2 billion people drink feces-contaminated water, which poses the greatest threat to drinking water safety.[65] Contaminated drinking water could transmit diseases like cholera, dysentery, typhoid, diarrhea and polio.[65]

Harmful chemicals in drinking water can negatively affect health. Unsafe water management practices can increase the prevalence of water-borne diseases and sanitation-related illnesses.[66][67] Inadequate disinfecting of wastewater in industrial and agricultural centers can also infect hundreds of millions of people with contaminated water.[65] Chemicals like fluoride and arsenic can benefit humans when the levels of these chemicals are controlled;but other, more dangerous chemicals like lead and metals can be harmful to humans.[65]

In America, communities of color can be subject to poor-quality water.[68] In communities in America with large Hispanic and black populations, there is a correlated rise in SDWA health violations.[68] Populations who have experienced lack of safe drinking water, like populations in Flint, Michigan, are more likely to distrust tap water in their communities.[51] Populations to experience this are commonly low-income, communities of color.[69]

Hazardous materials management, including hazardous waste management, contaminated site remediation, the prevention of leaks from underground storage tanks and the prevention of hazardous materials releases to the environment and responses to emergency situations resulting from such releases. When hazardous materials are not managed properly, waste can pollute nearby water sources and reduce air quality.[70]

According to a study done in Austria, people who live near industrial sites are ""more often unemployed, have lower educations levels, and are twice as likely to be immigrants.[71] With the interest of environmental health in mind, the Resource Conservation and Recovery Act was passed in the United States in 1976 that covered how to properly manage hazardous waste.[72]

There are a variety of occupations that work with hazardous materials and help manage them so that everything is disposed of correctly. These professionals work in various sectors, including government agencies, private industry, consulting firms, and non-profit organizations, all with the common goal of ensuring the safe handling of hazardous materials and waste. These positions include but are not limited to Environmental Health and Safety Specialists, Waste collectors, Medical Professionals, and Emergency Responders.[73] Handling waste, especially hazardous materials is considered one of the most dangerous occupations in the world.[74] Often, these workers may not have all of information about the specific hazardous materials they encounter, making their jobs even more dangerous. The sudden exposure to materials they are not properly prepared to handle can lead to severe consequences.[75] This emphasizes the importance of training, safety protocols, and the use of personal protective equipment for those working with hazardous waste.

The effects of microplastics on human health are a growing concern and an actively increasing area of research. Tiny particles known as microplastics (MPs), have been found in various environmental and biological matrices, including air, water, food, and human tissues. MPs, defined as plastic fragments smaller than 5 millimeter (mm), and even smaller particles such as nanoplastics (NPs), particles smaller than 1000 nanometer (nm) in diameter (0.001 mm or 1 micrometer [μm]), have raised concerns impacting human health.[76][77] The pervasive presence of plastics in our environment has raised concerns about their long-term impacts on human health. While visible pollution caused by larger plastic items is well-documented, the hidden threat posed by NPs remains under-explored. These particles originate from the degradation of larger plastics and are now found in various environmental matrices, including water, soil, and air. Given their minute size, NPs can penetrate biological barriers and accumulate in human tissues, potentially leading to adverse health effects.[78][79]

Plastics continue to accumulate in landfills and oceans, leading to pollution that negatively affects both human and animal health. Notably, MPs and NPs are now ubiquitous, infiltrating our food chain and water supplies. Studies indicate that humans ingest significant amounts of MPs daily through food, especially seafood[80] and inhalation, with estimates ranging from 39,000 to 52,000 particles per person annually.[81] Additionally, the presence of MPs in human feces suggests widespread exposure and absorption.[82] In scientific literature, combined MPs and NPs are referred to as micro- and nanoplastics (MNPs), nano- and microplastics (NMPs), or nano-and microplastic particles (NMPPs).

Contaminated or polluted soil directly affects human health through direct contact with soil or via inhalation of soil contaminants that have vaporized; potentially greater threats are posed by the infiltration of soil contamination into groundwater aquifers used for human consumption, sometimes in areas apparently far removed from any apparent source of above-ground contamination. Toxic metals can also make their way up the food chain through plants that reside in soils containing high concentrations of heavy metals.[85] This tends to result in the development of pollution-related diseases.

Most exposure is accidental, and exposure can happen through:[86]

The Toxicology and Environmental Health Information Program (TEHIP)[87] is a comprehensive toxicology and environmental health web site, that includes open access to resources produced by US government agencies and organizations, and is maintained under the umbrella of the Specialized Information Service at the United States National Library of Medicine. TEHIP includes links to technical databases, bibliographies, tutorials, and consumer-oriented resources. TEHIP is responsible for the Toxicology Data Network (TOXNET),[88] an integrated system of toxicology and environmental health databases including the Hazardous Substances Data Bank, that are open access, i.e. available free of charge. TOXNET was retired in 2019.[89]

There are many environmental health mapping tools. TOXMAP is a geographic information system (GIS) from the Division of Specialized Information Services[90] of the United States National Library of Medicine (NLM) that uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs. TOXMAP is a resource funded by the US federal government. TOXMAP's chemical and environmental health information is taken from the NLM's Toxicology Data Network (TOXNET)[91] and PubMed, and from other authoritative sources.

Environmental health professionals may be known as environmental health officers, public health inspectors, environmental health specialists or environmental health practitioners. Researchers and policy-makers also play important roles in how environmental health is practiced in the field. In many European countries, physicians and veterinarians are involved in environmental health.[92] In the United Kingdom, practitioners must have a graduate degree in environmental health and be certified and registered with the Chartered Institute of Environmental Health or the Royal Environmental Health Institute of Scotland.[93] In Canada, practitioners in environmental health are required to obtain an approved bachelor's degree in environmental health along with the national professional certificate, the Certificate in Public Health Inspection (Canada), CPHI(C).[94] Many states in the United States also require that individuals have a bachelor's degree and professional licenses to practice environmental health.[95] California state law defines the scope of practice of environmental health as follows:[96]

The environmental health profession had its modern-day roots in the sanitary and public health movement of the United Kingdom. This was epitomized by Sir Edwin Chadwick, who was instrumental in the repeal of the poor laws, and in 1884 was the founding president of the Association of Public Sanitary Inspectors, now called the Chartered Institute of Environmental Health.[97]

Journals:
"
Risk Assessment Services,"

Risk assessment determines possible mishaps, their likelihood and consequences, and the tolerances for such events.[1][2]
The results of this process may be expressed in a quantitative or qualitative fashion. Risk assessment is an inherent part of a broader risk management strategy to help reduce any potential risk-related consequences.[1][3]

More precisely, risk assessment identifies and analyses potential (future) events that may negatively impact individuals, assets, and/or the environment (i.e. hazard analysis). It also makes judgments ""on the tolerability of the risk on the basis of a risk analysis"" while considering influencing factors (i.e. risk evaluation).[1][3]

Risk assessments can be done in individual cases, including in patient and physician interactions.[4] In the narrow sense chemical risk assessment is the assessment of a health risk in response to environmental exposures.[5]
The ways statistics are expressed and communicated to an individual, both through words and numbers impact his or her interpretation of benefit and harm. For example, a fatality rate may be interpreted as less benign than the corresponding survival rate.[4]
A systematic review of patients and doctors from 2017 found that overstatement of benefits and understatement of risks occurred more often than the alternative.[4][6]
A systematic review from the Cochrane collaboration suggested ""well-documented decision aids"" are helpful in reducing effects of such tendencies or biases.[4][7] Aids may help people come to a decision about their care based on evidence informed information that align with their values.[7] Decision aids may also help people understand the risks more clearly, and they empower people to take an active role when making medical decisions.[7] The systematic review did not find a difference in people who regretted their decisions between those who used decision aids and those who had the usual standard treatment.[7]

An individual's own risk perception may be affected by psychological, ideological, religious or otherwise subjective factors, which impact rationality of the process.[4] Individuals tend to be less rational when risks and exposures concern themselves as opposed to others.[4] There is also a tendency to underestimate risks that are voluntary or where the individual sees themselves as being in control, such as smoking.[4]

Risk assessment can also be made on a much larger systems theory scale, for example assessing the risks of an ecosystem or an interactively complex mechanical, electronic, nuclear, and biological system or a hurricane (a complex meteorological and geographical system). Systems may be defined as linear and nonlinear (or complex), where linear systems are predictable and relatively easy to understand given a change in input, and non-linear systems unpredictable when inputs are changed.[8] As such, risk assessments of non-linear/complex systems tend to be more challenging.

In the engineering of complex systems, sophisticated risk assessments are often made within safety engineering and reliability engineering when it concerns threats to life, natural environment, or machine functioning. The agriculture, nuclear, aerospace, oil, chemical, railroad, and military industries have a long history of dealing with risk assessment.[9] Also, medical, hospital, social service,[10] and food industries control risks and perform risk assessments on a continual basis. Methods for assessment of risk may differ between industries and whether it pertains to general financial decisions or environmental, ecological, or public health risk assessment.[9]

Rapid technological change, increasing scale of industrial complexes, increased system integration, market competition, and other factors have been shown to increase societal risk in the past few decades.[1] As such, risk assessments become increasingly critical in mitigating accidents, improving safety, and improving outcomes. Risk assessment consists of an objective evaluation of risk in which assumptions and uncertainties are clearly considered and presented. This involves identification of risk (what can happen and why), the potential consequences, the probability of occurrence, the tolerability or acceptability of the risk, and ways to mitigate or reduce the probability of the risk.[3] Optimally, it also involves documentation of the risk assessment and its findings, implementation of mitigation methods, and review of the assessment (or risk management plan), coupled with updates when necessary.[1] Sometimes risks can be deemed acceptable, meaning the risk ""is understood and tolerated ... usually because the cost or difficulty of implementing an effective countermeasure for the associated vulnerability exceeds the expectation of loss.""[11]

Benoit Mandelbrot distinguished between ""mild"" and ""wild"" risk and argued that risk assessment and risk management must be fundamentally different for the two types of risk.[12] Mild risk follows normal or near-normal probability distributions, is subject to regression to the mean and the law of large numbers, and is therefore relatively predictable. Wild risk follows fat-tailed distributions, e.g., Pareto or power-law distributions, is subject to regression to the tail (infinite mean or variance, rendering the law of large numbers invalid or ineffective), and is therefore difficult or impossible to predict. A common error in risk assessment and management is to underestimate the wildness of risk, assuming risk to be mild when in fact it is wild, which must be avoided if risk assessment and management are to be valid and reliable, according to Mandelbrot.

To see the risk management process expressed mathematically, one can define expected risk as the sum over individual risks, 




R

i




{\displaystyle R_{i}}

, which can be computed as the product of potential losses, 




L

i




{\displaystyle L_{i}}

, and their probabilities, 



p
(

L

i


)


{\displaystyle p(L_{i})}

:

Even though for some risks 




R

i


,

R

j




{\displaystyle R_{i},R_{j}}

, we might have 




R

i


=

R

j




{\displaystyle R_{i}=R_{j}}

, if the probability 



p
(

L

j


)


{\displaystyle p(L_{j})}

 is small compared to 



p
(

L

i


)


{\displaystyle p(L_{i})}

, its estimation might be based only on a smaller number of prior events, and hence, more uncertain. On the other hand, since 




R

i


=

R

j




{\displaystyle R_{i}=R_{j}}

, 




L

j




{\displaystyle L_{j}}

 must be larger than 




L

i




{\displaystyle L_{i}}

, so decisions based on this uncertainty would be more consequential, and hence, warrant a different approach.

This becomes important when we consider the variance of risk

as a large 




L

i




{\displaystyle L_{i}}

 changes the value.

Financial decisions, such as insurance, express loss in terms of dollar amounts.  When risk assessment is used for public health or environmental decisions, the loss can be quantified in a common metric such as a country's currency or some numerical measure of a location's quality of life.  For public health and environmental decisions, the loss is simply a verbal description of the outcome, such as increased cancer incidence or incidence of birth defects. In that case, the ""risk"" is expressed as

If the risk estimate takes into account information on the number of individuals exposed, it is termed a ""population risk"" and is in units of expected increased cases per time period. If the risk estimate does not take into account the number of individuals exposed, it is termed an ""individual risk"" and is in units of incidence rate per time period.  Population risks are of more use for cost/benefit analysis; individual risks are of more use for evaluating whether risks to individuals are ""acceptable"".

In quantitative risk assessment, an annualized loss expectancy (ALE) may be used to justify the cost of implementing countermeasures to protect an asset. This may be calculated by multiplying the single loss expectancy (SLE), which is the loss of value based on a single security incident, with the annualized rate of occurrence (ARO), which is an estimate of how often a threat would be successful in exploiting a vulnerability.

The usefulness of quantitative risk assessment has been questioned, however. Barry Commoner, Brian Wynne and other critics have expressed concerns that risk assessment tends to be overly quantitative and reductive. For example, they argue that risk assessments ignore qualitative differences among risks. Some charge that assessments may drop out important non-quantifiable or inaccessible information, such as variations among the classes of people exposed to hazards, or social amplification.[13] Furthermore, Commoner[14] and O'Brien[15] claim that quantitative approaches divert attention from precautionary or preventative measures.[16] Others, like Nassim Nicholas Taleb consider risk managers little more than ""blind users"" of statistical tools and methods.[17]

Older textbooks distinguish between the term risk analysis and risk evaluation;
a risk analysis includes the following 4 steps:[1]

A risk evaluation means that judgements are made on the tolerability of the identified risks, leading to risk acceptance. When risk analysis and risk evaluation are made at the same time, it is called risk assessment.[1]

As of 2023, chemical risk assessment follows these 4 steps:[5]

There is tremendous variability in the dose-response relationship between a chemical and human health outcome in particularly susceptible subgroups, such as pregnant women, developing fetuses, children up to adolescence, people with low socioeconomic status, those with preexisting diseases, disabilities, genetic susceptibility, and those with other environmental exposures.[5]

The process of risk assessment may be somewhat informal at the individual social level, assessing economic and household risks,[18][19] or a sophisticated process at the strategic corporate level. However, in both cases, ability to anticipate future events and create effective strategies for mitigating them when deemed unacceptable is vital.

At the individual level, identifying objectives and risks, weighing their importance, and creating plans, may be all that is necessary.
At the strategic organisational level, more elaborate policies are necessary, specifying acceptable levels of risk, procedures to be followed within the organisation, priorities, and allocation of resources.[20]: 10 

At the strategic corporate level, management involved with the project produce project level risk assessments with the assistance of the available expertise as part of the planning process and set up systems to ensure that required actions to manage the assessed risk are in place. At the dynamic level, the personnel directly involved may be required to deal with unforeseen problems in real time. The tactical decisions made at this level should be reviewed after the operation to provide feedback on the effectiveness of both the planned procedures and decisions made in response to the contingency.

The results of these steps are combined to produce an estimate of risk. Because of the different susceptibilities and exposures, this risk will vary within a population. An uncertainty analysis is usually included in a health risk assessment.

During an emergency response, the situation and hazards are often inherently less predictable than for planned activities (non-linear). In general, if the situation and hazards are predictable (linear), standard operating procedures should deal with them adequately. In some emergencies, this may also hold true, with the preparation and trained responses being adequate to manage the situation. In these situations, the operator can manage risk without outside assistance, or with the assistance of a backup team who are prepared and available to step in at short notice.

Other emergencies occur where there is no previously planned protocol, or when an outsider group is brought in to handle the situation, and they are not specifically prepared for the scenario that exists but must deal with it without undue delay. Examples include police, fire department, disaster response, and other public service rescue teams. In these cases, ongoing risk assessment by the involved personnel can advise appropriate action to reduce risk.[20] HM Fire Services Inspectorate has defined dynamic risk assessment (DRA) as:

The continuous assessment of risk in the rapidly changing circumstances of an operational incident, in order to implement the control measures necessary to ensure an acceptable level of safety.[20]
Dynamic risk assessment is the final stage of an integrated safety management system that can provide an appropriate response during changing circumstances. It relies on experience, training and continuing education, including effective debriefing to analyse not only what went wrong, but also what went right, and why, and to share this with other members of the team and the personnel responsible for the planning level risk assessment.[20]

The application of risk assessment procedures is common in a wide range of fields, and these may have specific legal obligations, codes of practice, and standardised procedures. Some of these are listed here.

There are many resources that provide human health risk information:

The National Library of Medicine provides risk assessment and regulation information tools for a varied audience.[21] These include:

The United States Environmental Protection Agency provides basic information about environmental health risk assessments for the public for a wide variety of possible environmental exposures.[24]

The Environmental Protection Agency began actively using risk assessment methods to protect drinking water in the United States after the passage of the Safe Drinking Water Act of 1974. The law required the National Academy of Sciences to conduct a study on drinking water issues, and in its report, the NAS described some methodologies for doing risk assessments for chemicals that were suspected carcinogens, recommendations that top EPA officials have described as perhaps the study's most important part.[25]

Considering the increase in junk food and its toxicity, FDA required in 1973 that cancer-causing compounds must not be present in meat at concentrations that would cause a cancer risk greater than 1 in a million over a lifetime. The US Environmental Protection Agency provides extensive information about ecological and environmental risk assessments for the public via its risk assessment portal.[26] The Stockholm Convention on persistent organic pollutants (POPs) supports a qualitative risk framework for public health protection from chemicals that display environmental and biological persistence, bioaccumulation, toxicity (PBT) and long range transport; most global chemicals that meet this criterion have been previously assessed quantitatively by national and international health agencies.[27]

For non-cancer health effects, the terms reference dose (RfD)   or reference concentration (RfC) are used to describe the safe level of exposure in a dichotomous fashion. Newer ways of communicating the risk is the probabilistic risk assessment.[28]

When risks apply mainly to small sub-populations, it can be difficult to determine when intervention is necessary. For example, there may be a risk that is very low for everyone, other than 0.1% of the population. It is necessary to determine whether this 0.1% is represented by:

If the risk is higher for a particular sub-population because of abnormal exposure rather than susceptibility, strategies to further reduce the exposure of that subgroup are considered.  If an identifiable sub-population is more susceptible due to inherent genetic or other factors, public policy choices must be made. The choices are:

Acceptable risk is a risk that is understood and tolerated usually because the cost or difficulty of implementing an effective countermeasure for the associated vulnerability exceeds the expectation of loss.[29]

The idea of not increasing lifetime risk by more than one in a million has become commonplace in public health discourse and policy.[30] It is a heuristic measure. It provides a numerical basis for establishing a negligible increase in risk.

Environmental decision making allows some discretion for deeming individual risks potentially ""acceptable"" if less than one in ten thousand chance of increased lifetime risk. Low risk criteria such as these provide some protection for a case where individuals may be exposed to multiple chemicals e.g. pollutants, food additives, or other chemicals.[citation needed]

In practice, a true zero-risk is possible only with the suppression of the risk-causing activity.[citation needed]

Stringent requirements of 1 in a million may not be technologically feasible or may be so prohibitively expensive as to render the risk-causing activity unsustainable, resulting in the optimal degree of intervention being a balance between risks vs. benefit.[citation needed] For example, emissions from hospital incinerators result in a certain number of deaths per year. However, this risk must be balanced against the alternatives.  There are public health risks, as well as economic costs, associated with all options. The risk associated with no incineration is the potential spread of infectious diseases or even no hospitals. Further investigation identifies options such as separating noninfectious from infectious wastes, or air pollution controls on a medical incinerator.

Intelligent thought about a reasonably full set of options is essential.  Thus, it is not unusual for there to be an iterative process between analysis, consideration of options, and follow up analysis.[citation needed]

In the context of public health, risk assessment is the process of characterizing the nature and likelihood of a harmful effect to individuals or populations from certain human activities. Health risk assessment can be mostly qualitative or can include statistical estimates of probabilities for specific populations. In most countries, the use of specific chemicals or the operations of specific facilities (e.g. power plants, manufacturing plants) is not allowed unless it can be shown that they do not increase the risk of death or illness above a specific threshold. For example, the American Food and Drug Administration (FDA) regulates food safety through risk assessment, while the EFSA does the same in EU.[31]

An occupational risk assessment is an evaluation of how much potential danger a hazard can have to a person in a workplace environment. The assessment takes into account possible scenarios in addition to the probability of their occurrence and the results.[32] The six types of hazards to be aware of are safety (those that can cause injury), chemicals, biological, physical, psychosocial (those that cause stress, harassment) and ergonomic (those that can cause musculoskeletal disorders).[33] To appropriately access hazards there are two parts that must occur. Firstly, there must be an ""exposure assessment"" which measures the likelihood of worker contact and the level of contact. Secondly, a ""risk characterization"" must be made which measures the probability and severity of the possible health risks.[34]



The importance of risk assessments to manage the consequences of climate change and variability is recalled in the global frameworks for disaster risk reduction, adopted by the member countries of the United Nations at the end of the World Conferences held in Kobe (2005) and Sendai (2015). The Sendai Framework for Disaster Risk Reduction brings attention to the local scale and encourages a holistic risk approach, which should consider all the hazards to which a community is exposed, the integration of technical-scientific knowledge with local knowledge, and the inclusion of the concept of risk in local plans to achieve a significant disaster reduction by 2030. Taking these principles into daily practice poses a challenge for many countries. The Sendai framework monitoring system highlights how little is known about the progress made from 2015 to 2019 in local disaster risk reduction.[35]

As of 2019, in the South of the Sahara, risk assessment is not yet an institutionalized practice. The exposure of human settlements to multiple hazards (hydrological and agricultural drought, pluvial, fluvial and coastal floods) is frequent and requires risk assessments on a regional, municipal, and sometimes individual human settlement scale. The multidisciplinary approach and the integration of local and technical-scientific knowledge are necessary from the first steps of the assessment. Local knowledge remains unavoidable to understand the hazards that threaten individual communities, the critical thresholds in which they turn into disasters, for the validation of hydraulic models, and in the decision-making process on risk reduction. On the other hand, local knowledge alone is not enough to understand the impacts of future changes and climatic variability and to know the areas exposed to infrequent hazards.
The availability of new technologies and open access information (high resolution satellite images, daily rainfall data) allow assessment today with an accuracy that only 10 years ago was unimaginable. The images taken by unmanned vehicle technologies allow to produce very high resolution digital elevation models and to accurately identify the receptors.[36] Based on this information, the hydraulic models allow the identification of flood areas with precision even at the scale of small settlements.[37] The information on loss and damages and on cereal crop at individual settlement scale allow to determine the level of multi-hazard risk on a regional scale.The multi-temporal high-resolution satellite images allow to assess the hydrological drought and the dynamics of human settlements in the flood zone.[38]
Risk assessment is more than an aid to informed decision making about risk reduction or acceptance.[39] It integrates early warning systems by highlighting the hot spots where disaster prevention and preparedness are most urgent.[40] When risk assessment considers the dynamics of exposure over time, it helps to identify risk reduction policies that are more appropriate to the local context.
Despite these potentials, the risk assessment is not yet integrated into the local planning in the South of the Sahara which, in the best of cases, uses only the analysis of vulnerability to climate change and variability.[40]

For audits performed by an outside audit firm, risk assessment is a crucial stage before accepting an audit engagement. According to ISA315 Understanding the Entity and its Environment and Assessing the Risks of Material Misstatement, ""the auditor should perform risk assessment procedures to obtain an understanding of the entity and its environment, including its internal control"". Evidence relating to the auditor's risk assessment of a material misstatement in the client's financial statements. Then, the auditor obtains initial evidence regarding the classes of transactions at the client and the operating effectiveness of the client's internal controls. Audit risk is defined as the risk that the auditor will issue a clean unmodified opinion regarding the financial statements, when in fact the financial statements are materially misstated, and therefore do not qualify for a clean unmodified opinion. As a formula, audit risk is the product of two other risks: Risk of Material Misstatement and Detection Risk. This formula can be further broken down as follows:  inherent risk × control risk × detection risk.

In project management, risk assessment is an integral part of the risk management plan, studying the probability, the impact, and the effect of every known risk on the project, as well as the corrective action to take should an incident be implied by a risk occur.[41] Of special consideration in this area are the relevant codes of practice that are enforced in the specific jurisdiction. Understanding the regime of regulations that risk management must abide by is integral to formulating safe and compliant risk assessment practices.

Information technology risk assessment can be performed by a qualitative or quantitative approach, following different methodologies. One important difference[clarification needed] in risk assessments in information security is modifying the threat model to account for the fact that any adversarial system connected to the Internet has access to threaten any other connected system.[42] Risk assessments may therefore need to be modified to account for the threats from all adversaries, instead of just those with reasonable access as is done in other fields.

NIST Definition: The process of identifying risks to organizational operations (including mission, functions, image, reputation), organizational assets, individuals, other organizations, and the Nation, resulting from the operation of an information system. Part of risk management incorporates threat and vulnerability analyses and considers mitigations provided by security controls planned or in place.[43]

There are various risk assessment methodologies and frameworks available which include NIST Risk Management Framework (RMF),[44] Control Objectives for Information and Related Technologies (COBIT),[45] Factor Analysis of Information Risk (FAIR),[46] Operationally Critical Threat, Asset, and Vulnerability Evaluation (OCTAVE),[47] The Center for Internet Security Risk Assessment Method (CIS RAM),[48] and The Duty of Care Risk Analysis (DoCRA) Standard,[49] which helps define 'reasonable' security.

The Threat and Risk Assessment (TRA) process is part of risk management referring to risks related to cyber threats. The TRA process will identify cyber risks, assess risks' severities, and may recommend activities to reduce risks to an acceptable level.

There are different methodologies for performing TRA (e.g., Harmonized TRA Methodology[50]), all utilize the following elements:[51][52][53] identifying of assets (what should be protected), identifying and assessing of the threats and vulnerabilities for the identified assets, determining the exploitability of the vulnerabilities, determining the levels of risk associated with the vulnerabilities (what are the implications if the assets were damaged or lost), and recommending a risk mitigation program.

Megaprojects (sometimes also called ""major programs"") are extremely large-scale investment projects, typically costing more than US$1 billion per project. They include bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, coastal flood protection, oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, and defence systems. Megaprojects have been shown to be particularly risky in terms of finance, safety, and social and environmental impacts.

Studies have shown that early parts of the system development cycle such as requirements and design specifications are especially prone to error. This effect is particularly notorious in projects involving multiple stakeholders with different points of view. Evolutionary software processes offer an iterative approach to requirement engineering to alleviate the problems of uncertainty, ambiguity, and inconsistency inherent in software developments, including uncertainty, ambiguity, and inconsistency inherent in software developments.[clarification needed]

In July 2010, shipping companies agreed to use standardized procedures in order to assess risk in key shipboard operations. These procedures were implemented as part of the amended ISM Code.[54]

Formal risk assessment is a required component of most professional dive planning, but the format and methodology may vary. Consequences of an incident due to an identified hazard are generally chosen from a small number of standardised categories, and probability is estimated based on statistical data on the rare occasions when it is available, and on a best guess estimate based on personal experience and company policy in most cases. A simple risk matrix is often used to transform these inputs into a level of risk, generally expressed as unacceptable, marginal or acceptable. If unacceptable, measures must be taken to reduce the risk to an acceptable level, and the outcome of the risk assessment must be accepted by the affected parties before a dive commences. Higher levels of risk may be acceptable in special circumstances, such as military or search and rescue operations when there is a chance of recovering a survivor. Diving supervisors are trained in the procedures of hazard identification and risk assessment, and it is part of their planning and operational responsibility. Both health and safety hazards must be considered. Several stages may be identified. There is risk assessment done as part of the diving project planning, on site risk assessment which takes into account the specific conditions of the day, and dynamic risk assessment which is ongoing during the operation by the members of the dive team, particularly the supervisor and the working diver.[55][56]

In recreational scuba diving, the extent of risk assessment expected of the diver is relatively basic and is included in the pre-dive checks. Several mnemonics have been developed by diver certification agencies to remind the diver to pay some attention to risk, but the training is rudimentary. Diving service providers are expected to provide a higher level of care for their customers, and diving instructors and divemasters are expected to assess risk on behalf of their customers and warn them of site-specific hazards and the competence considered appropriate for the planned dive. Technical divers are expected to make a more thorough assessment of risk, but as they will be making an informed choice for a recreational activity, the level of acceptable risk may be considerably higher than that permitted for occupational divers under the direction of an employer.[57][58]

In outdoor activities including commercial outdoor education, wilderness expeditions, and outdoor recreation, risk assessment refers to the analysis of the probability and magnitude of unfavorable outcomes such as injury, illness, or property damage due to environmental and related causes, compared to the human development or other benefits of outdoor activity. This is of particular importance as school programs and others weigh the benefits of youth and adult participation in various outdoor learning activities against the inherent and other hazards present in those activities.  Schools, corporate entities seeking team-building experiences, parents/guardians, and others considering outdoor experiences expect or require[59] organizations to assess the hazards and risks of different outdoor activities—such as sailing, target shooting, hunting, mountaineering, or camping—and select activities with acceptable risk profiles.

Outdoor education, wilderness adventure, and other outdoor-related organizations should, and are in some jurisdictions required, to conduct risk assessments prior to offering programs for commercial purposes.[60][61][62]

Such organizations are given guidance on how to provide their risk assessments.[63]

Risk assessments for led outdoor activities form only one component of a comprehensive risk management plan, as many risk assessments use a basic linear-style thinking that does not employ more modern risk management practice employing complex socio-technical systems theory.[64][65]

Environmental Risk Assessment (ERA) aims to assess the effects of stressors, usually chemicals, on the local environment. A risk is an integrated assessment of the likelihood and severity of an undesired event. In ERA, the undesired event often depends on the chemical of interest and on the risk assessment scenario.[66] This undesired event is usually a detrimental effect on organisms, populations or ecosystems. Current ERAs usually compare an exposure to a no-effect level, such as the Predicted Environmental Concentration/Predicted No-Effect Concentration (PEC/PNEC) ratio in Europe. Although this type of ratio is useful and often used in regulation purposes, it is only an indication of an exceeded apparent threshold.[67] New approaches start to be developed in ERA in order to quantify this risk and to communicate effectively on it with both the managers and the general public.[66]

Ecological risk assessment is complicated by the fact that there are many nonchemical stressors that substantially influence ecosystems, communities, and individual plants and animals, as well as across landscapes and regions.[68][69]  Defining the undesired (adverse) event is a political or policy judgment, further complicating applying traditional risk analysis tools to ecological systems.  Much of the policy debate surrounding ecological risk assessment is over defining precisely what is an adverse event.[70]

Biodiversity Risk Assessments evaluate risks to biological diversity, specially the risk of species extinction or the risk of ecosystem collapse. The units of assessments are the biological (species, subspecies or populations) or ecological entities (habitats, ecosystems, etc.), and the risk are often related to human actions and interventions (threats and pressures). Regional and national protocols have been proposed by multiple academic or governmental institutions and working groups,[71] but global standards such as the Red List of Threatened Species and the IUCN Red List of Ecosystems have been widely adopted, and are recognized or proposed as official indicators of progress toward international policy targets and goals, such as the Aichi targets and the Sustainable Development Goals.[72][73]

Risk assessments are used in numerous stages during the legal process and are developed to measure a wide variety of items, such as recidivism rates, potential pretrial issues, probation/parole, and to identify potential interventions for defendants.[74] Clinical psychologists, forensic psychologists, and other practitioners are responsible for conducting risk assessments.[74][75][76] Depending on the risk assessment tool, practitioners are required to gather a variety of background information on the defendant or individual being assessed. This information includes their previous criminal history (if applicable) and other records (i.e. Demographics, Education, Job Status, Medical History), which can be accessed through direct interview with the defendant or on-file records.[74]

In the pre-trial stage, a widely used risk assessment tool is the Public Safety Assessment,[77] which predicts failure to appear in court, likelihood of a new criminal arrest while on pretrial release, and likelihood of a new violent criminal arrest while on pretrial release. Multiple items are observed and taken into account based on which aspect of the PSA is being focused, and like all other actuarial risk assessments, each item is assigned a weighted amount to produce a final score.[74] Detailed information such as transparency on the items the PSA factors and how scores are distributed are accessible online.[78]

For defendants who have been incarcerated, risk assessments are used to determine their likelihood of recidivism and inform sentence length decisions. Risk assessments also aid parole/probation officers in determining the level of supervision a probationer should be subjected to and what interventions could be implemented to improve offender risk status.[75] The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) is a risk assessment too designed to measure pretrial release risk, general recidivism risk, and violent recidivism risk. Detailed information on scoring and algorithms for COMPAS are not accessible to the general public.




"
Crisis Management Services,"

Crisis management is the process by which an organization deals with a disruptive and unexpected event that threatens to harm the organization or its stakeholders.[1] The study of crisis management originated with large-scale industrial and environmental disasters in the 1980s.[2][3] It is considered to be the most important process in public relations.[3]

Three elements are common to a crisis: (a) a threat to the organization, (b) the element of surprise, and (c) a short decision time.[4] Venette argues that ""crisis is a process of transformation where the old system can no longer be maintained"".[5] Therefore, the fourth defining quality is the need for change. If change is not needed, the event could more accurately be described as a failure or incident.[6]

In contrast to risk management, which involves assessing potential threats and finding the best ways to avoid those threats, crisis management involves dealing with threats before, during, and after they have occurred. It is a discipline within the broader context of management consisting of skills and techniques required to identify, assess, understand, and cope with a serious situation, especially from the moment it first occurs to the point that recovery procedures start.

Crisis management is a situation-based management system that includes clear roles and responsibilities and processes related organizational requirements company-wide. The response shall include action in the following areas: crisis prevention, crisis assessment, crisis handling, and crisis termination. The aim of crisis management is to be well prepared for crisis, ensure a rapid and adequate response to the crisis, maintaining clear lines of reporting and communication in the event of crisis and agreeing rules for crisis termination.

The techniques of crisis management include a number of consequent steps from the understanding of the influence of the crisis on the corporation to preventing, alleviating, and overcoming the different types of crisis.[7] Crisis management consists of different aspects including: 

Crisis-management methods of a business or an organization are called a crisis-management plan. A British Standard BS11200:2014 provides a useful foundation for understanding terminology and frameworks relating to crisis, in this document the focus is on the corporate exposure to risks in particular to the black swan events that result in significant strategic threats to organizations. Currently there is work on-going to develop an International standard.

Crisis management is occasionally referred to as incident management, although several industry specialists such as Peter Power argue that the term ""crisis management"" is more accurate.[8]

A crises mindset requires the ability to think of the worst-case scenario while simultaneously suggesting numerous solutions. Trial and error is an accepted discipline, as the first line of defense might not work. It is necessary to maintain a list of contingency plans and to be always on alert. Organizations and individuals should always be prepared with a rapid response plan to emergencies which would require analysis, drills and exercises.[9]

The credibility and reputation of organizations is heavily influenced by the perception of their responses during crisis situations. The organization and communication involved in responding to a crisis in a timely fashion makes for a challenge in businesses. There must be open and consistent communication throughout the hierarchy to contribute to a successful crisis-communication process.

The related terms emergency management and business continuity management focus respectively on the prompt but short lived ""first aid"" type of response (e.g. putting the fire out) and the longer-term recovery and restoration phases (e.g. moving operations to another site). Crisis is also a facet of risk management, although it is probably untrue to say that crisis management represents a failure of risk management, since it will never be possible to totally mitigate the chances of catastrophes' occurring.

An organizational crisis is described as a rare, high-impact event that jeopardizes the organization's survival. It is marked by uncertainty regarding the cause, effects, and solutions, along with the need for rapid decision-making.[10]

During the crisis management process, it is important to identify types of crises in that different crises necessitate the use of different crisis management strategies.[11] Potential crises are enormous, but crises can be clustered.[11]

Lerbinger[12] categorized eight types of crises

Natural disaster related crises, typically natural disasters, are such environmental phenomena as earthquakes, volcanic eruptions, tornadoes and hurricanes, floods, landslides, tsunamis, storms, and droughts that threaten life, property, and the environment itself.[11][12]

Technological crises are caused by human application of science and technology. Technological accidents inevitably occur when technology becomes complex and coupled and something goes wrong in the system as a whole (Technological breakdowns). Some technological crises occur when human error causes disruptions (Human breakdowns[11]). People tend to assign blame for a technological disaster because technology is subject to human manipulation. That said, a decent amount of research demonstrates that the remedy or response strategy after a technological crisis (such as a data breach) can contribute significantly to an organization's overall reputation. There is also a cross-sectional difference among firms – firms with higher reputations tend to be less impacted by the blame on the technological crisis.[13] In contrast, people do not hold anyone responsible for natural disaster. When an accident creates significant environmental damage, the crisis is categorized as megadamage.[11] Samples include software failures, industrial accidents, and oil spills.[11][12]

Confrontation crisis occur when discontented individuals and/or groups fight businesses, government, and various interest groups to win acceptance of their demands and expectations. The common type of confrontation crisis is boycotts, and other types are picketing, sit-ins, ultimatums to those in authority, blockade or occupation of buildings, and resisting or disobeying police.

An organization faces a crisis of malevolence when opponents or miscreant individuals use criminal means or other extreme tactics for the purpose of expressing hostility or anger toward, or seeking gain from, a company, country, or economic system, perhaps with the aim of destabilizing or destroying it. Sample crises include product tampering, kidnapping, malicious rumors, terrorism, cybercrime and espionage.[11][12]

Example: Chicago Tylenol murders

Crises occur when management takes actions it knows will harm or place stakeholders at risk for harm without adequate precautions.[11] Lerbinger[12] specified three different types of crises of organizational misdeeds: crises of skewed management values, crises of deception, and crises of management misconduct.

Crises stemming from skewed management values occur when leaders prioritize short-term financial gains while disregarding broader social responsibilities and key stakeholders beyond investors. This imbalance often originates from the traditional business mindset that prioritizes shareholder interests at the expense of customers, employees, and the community. A structured framework for corporate recovery following ethical failures involves four key actions: Replace, Restructure, Redevelop, Rebrand. These steps help organizations restore integrity, rebuild trust, and realign their values with ethical and sustainable business practices.[14]

Crisis of deception occur when management conceals or misrepresents information about itself and its products in its dealing with consumers and others.

Some crises are caused not only by skewed values and deception but deliberate amorality and illegality.

Crises occur when an employee or former employee commits violence against other employees on organizational grounds.

False information about an organization or its products creates crisis hurting the organization's reputation. Sample is linking the organization to radical groups or stories that their products are contaminated.[11]

These occur when the crisis was triggered by people, for example global financial crises, transportation accidents, massive destruction.

Alan Hilburg, a pioneer in crisis management, defines organizational crises as categorized as either acute crises or chronic crises. Hilburg also created the concept of the Crisis Arc. Erika Hayes James, an organizational psychologist at the University of Virginia's Darden Graduate School of Business, identifies two primary types of organizational crisis.[17] James defines organizational crisis as ""any emotionally charged situation that, once it becomes public, invites negative stakeholder reaction and thereby has the potential to threaten the financial well-being, reputation, or survival of the firm or some portion thereof"".[18]

Sudden crises are circumstances that occur without warning and beyond an institution's control.  Consequently, sudden crises are most often situations for which the institution and its leadership are not blamed.

Smoldering crises differ from sudden crises in that they begin as minor internal issues that, due to manager's negligence, develop to crisis status. These are situations when leaders are blamed for the crisis and its subsequent effect on the institution in question.[18]

James categorises five phases of crisis that require specific crisis leadership competencies.[18] Each phase contains an obstacle that a leader must overcome to improve the structure and operations of an organization. James's case study on crisis in the financial services sector, for example, explores why crisis events erode public trust in leadership.  James's research demonstrates how leadership competencies of integrity, positive intent, capability, mutual respect, and transparency impact the trust-building process.[19]

Signal detection is the stage in a crisis in which leaders should, but do not always, sense early warning signals (red flags) that suggest the possibility of a crisis. The detection stages of a crisis include:

It is during this stage that crisis handlers begin preparing for or averting the crisis that had been foreshadowed in the signal detection stage. Hilburg has demonstrated that using an impact/probability model allows organizations to fairly accurately predict crisis scenarios.  He's recognized the greatest organizational challenge is 'speaking truth to power' to predict truly worst-case scenarios.  Organizations such as the Red Cross's primary mission is to prepare for and prevent the escalation of crisis events. Companies like Walmart have been recognized for their role in providing emergency relief [20] after having witnessed the incredibly speedy and well-coordinated effort to get supplies to the Gulf Coast of the United States in anticipation of Hurricane Katrina.

Usually the most vivid stage, the goal of crisis containment and damage control is to limit the reputational, financial, safety, and other threats to firm survival.  Crisis handlers work diligently during this stage to bring the crisis to an end as quickly as possible to limit the negative publicity to the organization, and move into the business recovery phase.

When a crisis hits, organizations must be able to carry on with their business in the midst of the crisis while simultaneously planning for how they will recover from the damage the crisis caused. Crisis handlers not only engage in continuity planning (determining the people, financial, and technology resources needed to keep the organization running), but will also actively pursue organizational resilience.

In the wake of a crisis, organizational decision makers adopt a learning orientation and use prior experience to develop new routines and behaviors that ultimately change the way the organization operates. The best leaders recognize this and are purposeful and skillful in finding the learning opportunities inherent in every crisis situation.

The effort taken by an organization to communicate with the public and stakeholders when an unexpected event occurs that could have a negative impact on the organization's reputation. This can also refer to the efforts to inform employees or the public of a potential hazard which could have a catastrophic impact.  There are 3 essential steps that an organization can take to prepare for and withstand a communications crisis: 1) Define your philosophy; 2) Assess your vulnerabilities; 3) Develop a protocol.[21]

Crisis management strategy (CMS)[22] is corporate development strategy designed primarily to prevent crisis for follow-up company advancement. Thus, CMS is synthesis of strategic management. It includes projection of the future based on ongoing monitoring of business internal and external environment, as well as selection and implementation of crisis prevention strategy and operating management. This is including current status control based on ongoing monitoring of the internal and external environment, as well as crisis-coping strategy selection and implementation.

Successfully managing a crisis requires an understanding of how to handle a crisis – beginning with before they occur. Alan Hilburg speaks about a crisis arc.  The arc consists of crisis avoidance, crisis mitigation and crisis recovery. Gonzalez-Herrero and Pratt found the different phases of Crisis Management.

There are 3 phases in any crisis management as shown below

No corporation looks forward to facing a situation that causes a significant disruption to their business, especially one that stimulates extensive media coverage. Public scrutiny can result in a negative financial, political, legal and government impact. Crisis management planning deals with providing the best response to a crisis.[23]

With the growing threat of cyber attacks, ""traditional information technology incident response plans often fail to consider the cross-organizational activities that need to be performed to remain resilient when a major cyber crisis occurs, resulting in a delayed, chaotic, unstructured, and fragmented response.  A cyber crisis management plan is designed to reduce these risks through careful pre-planning; therefore, developing a cyber crisis management plan requires organizations to take a holistic approach to cyber crisis planning.  By proactively acting to build a cyber crisis management plan, a broader, carefully considered, integrated and validated plan can be developed to meet an organization's unique demands before the crisis strikes.""[24]

Preparing contingency plans in advance, as part of a crisis-management plan, is the first step to ensuring an organization is appropriately prepared for a crisis. Crisis-management teams can rehearse a crisis plan by developing a simulated scenario to use as a drill. The plan should clearly stipulate that the only people to speak to publicly about the crisis are the designated persons, such as the company spokesperson or crisis team members. Ideally it should be one spokesperson who can be available on call at any time. Cooperation with media is crucial in crisis situation, assure that all questions are answered on time and information on what was done to resolve the situation is provided. The first hours after a crisis breaks are the most crucial, so working with speed and efficiency is important, and the plan should indicate how quickly each function should be performed. When preparing to offer a statement externally as well as internally, information should be accurate and transparent. Providing incorrect or manipulated information has a tendency to backfire and will greatly exacerbate the situation. The contingency plan should contain information and guidance that will help decision makers to consider not only the short-term consequences, but the long-term effects of every decision.[23]

When a crisis will undoubtedly cause a significant disruption to an organization, a business continuity plan can help minimize the disruption. First, one must identify the critical functions and processes that are necessary to keep the organization running.[25] This part of the planning should be conducted in the earliest stages, and is part of a business impact analysis phase that will signpost ""How much does the organization stand to lose?” (Osborne, 2007).  Practical Business Continuity Management.  Business Management: Top tips for effective, real-world Business Continuity Management).

Each critical function and or/process must have its own contingency plan in the event that one of the functions/processes ceases or fails, then the business/organization is more resilient, which in itself provides a mechanism to lessen the possibility of having to invoke recovery plans (Osborne, 2007). Testing these contingency plans by rehearsing the required actions in a simulation will allow those involved to become more acutely aware of the possibility of a crisis. As a result, and in the event of an actual crisis, the team members will act more quickly and effectively.[23]

A note of caution when planning training scenarios, all too often simulations can lack ingenuity, an appropriate level of realism and as a consequence potentially lose their training value.   This part can be improved by employing external exercise designers who are not part of the organisational culture and are able to test an organizations response to crisis, in order to bring about a crisis of confidence for those who manage vital systems.[26]

Following a simulation exercise, a thorough and systematic debriefing must be conducted as a key component of any crisis simulation.  The purpose of this is to create a link and draw lessons from the reality of the simulated representation and the reality of the real world.[27]

The whole process relating to business continuity planning should be periodically reviewed to identify any number of changes that may invalidate the current plan.[28]

Providing information to an organization in a time of crisis is critical to effective crisis management. Structural-functional systems theory addresses the intricacies of information networks and levels of command making up organizational communication. The structural-functional theory identifies information flow in organizations as ""networks"" made up of members "". Information in organizations flow in patterns called networks.[29]

Another theory that can be applied to the sharing of information is diffusion of innovation theory. Developed by Everett Rogers, the theory describes how innovation is disseminated and communicated through certain channels over a period of time. Diffusion of innovation in communication occurs when an individual communicates a new idea to one or several others. At its most elementary form, the process involves: (1) an innovation, (2) an individual or other unit of adoption that has knowledge of or experience with using the innovation, (3) another individual or other unit that does not yet have knowledge of the innovation, and (4) a communication channel connecting the two units. A communication channel is the means by which messages get from one individual to another.

There has been debate about the role of apologies in crisis management, and some argue that apology opens an organization up for possible legal consequences.  ""However some evidence indicates that compensation and sympathy, two less expensive strategies, are as effective as an apology in shaping people's perceptions of the organization taking responsibility for the crisis because these strategies focus on the victims' needs.  The sympathy response expresses concern for victims while compensation offers victims something to offset the suffering.""[30]

James identifies five leadership competencies which facilitate organizational restructuring during and after a crisis. 

Crisis leadership research concludes that leadership action in crisis reflects the competency of an organization, because the test of crisis demonstrates how well the institution's leadership structure serves the organization's goals and withstands crisis. 
[18]
Developing effective human resources is vital when building organizational capabilities through crisis management executive leadership.[31]

James postulates that organizational crisis can result from discrimination lawsuits. 
[32] James's theory of unequal human capital and social position derives from economic theories of human and social capital concluding that minority employees receive fewer organizational rewards than those with access to executive management. In a recent study of managers in a Fortune 500 company, race was found to be a predictor of promotion opportunity or lack thereof.[33] Thus, discrimination lawsuits can invite negative stakeholder reaction, damage the company's reputation, and threaten corporate survival.

Social media has accelerated the speed that information about a crisis can spread. The viral effect of social networks such as Twitter means that stakeholders can break news faster than traditional media – making managing a crisis harder.[34] This can be mitigated by having the right training and policy in place as well as the right social media monitoring tools to detect signs of a crisis breaking.[35] Social media also gives crisis management teams access to real-time information about how a crisis is impacting stakeholder sentiment and the issues that are of most concern to them.

The advent of social media changed the field of crisis management dramatically, empowering stakeholders and making organisations more accountable for their actions. By creating a platform for two-way symmetrical communication between an organisation and its stakeholders, social media facilitated a rise in organisational crises, allowing for stakeholders anywhere in the world – providing they have an internet connection – to communicate publicly with organisations. The publishing unfavourable behaviour on social media, combined with the immense speed that information can be shared online, created a need for social media strategy to be included within the crisis management planning process. Stakeholders expect organisations to respond quickly and effectively to crises that transpire online.[36]

Organisations should have a planned approach to releasing information to the media in the event of a crisis. A media reaction plan should include a company media representative as part of the Crisis Management Team (CMT). Since there is always a degree of unpredictability during a crisis, it is best that all CMT members understand how to deal with the media and be prepared to do so, should they be thrust into such a situation.[37]

In 2010 Procter & Gamble Co called reports that its new Pampers with Dry Max caused rashes and other skin irritations ""completely false"" as it aimed to contain a public relations threat to its biggest diaper innovation in 25 years. A Facebook group called ""Pampers bring back the OLD CRUISERS/SWADDLERS"" rose to over 4,500 members. Pampers denied the allegation and stated that only two complaints had been received for every one million diapers sold.[38] Pampers quickly reached out to people expressing their concerns via social media, Pampers even held a summit with four influential ""mommy bloggers"", to help dispel the rumour. Pampers acted quickly and decisively to an emerging crisis, before competitors and critics alike could fuel the fire further.

In the fall of 1982, a murderer added 65 milligrams of cyanide to some Tylenol capsules on store shelves, killing seven people, including three in one family. Johnson & Johnson recalled and destroyed 31 million capsules at a cost of $100 million. The affable CEO, James Burke, appeared in television ads and at news conferences informing consumers of the company's actions. Tamper-resistant packaging was rapidly introduced, and Tylenol sales swiftly bounced back to near pre-crisis levels.[39]

When another bottle of tainted Tylenol was discovered in a store, it took only a matter of minutes for the manufacturer to issue a nationwide warning that people should not use the medication in its capsule form.[40]

When Odwalla's apple juice was thought to be the cause of an outbreak of E. coli infection, the company lost a third of its market value. In October 1996, an outbreak of E. coli bacteria in Washington state, California, Colorado and British Columbia was traced to unpasteurized apple juice manufactured by natural juice maker Odwalla Inc. Forty-nine cases were reported, including the death of a small child. Within 24 hours, Odwalla conferred with the FDA and Washington state health officials; established a schedule of daily press briefings; sent out press releases which announced the recall; expressed remorse, concern and apology, and took responsibility for anyone harmed by their products; detailed symptoms of E. coli poisoning; and explained what consumers should do with any affected products. Odwalla then developed – through the help of consultants – effective thermal processes that would not harm the products' flavors when production resumed. All of these steps were communicated through close relations with the media and through full-page newspaper ads.

Mattel Inc., the toy maker, has been plagued with more than 28 product recalls and in the summer of 2007, among problems with exports from China, faced two product recalls in two weeks. The company ""did everything it could to get its message out, earning high marks from consumers and retailers. Though upset by the situation, they were appreciative of the company's response. At Mattel, just after the 7 a.m. recall announcement by federal officials, a public relations staff of 16 was set to call reporters at the 40 biggest media outlets. They told each to check their e-mail for a news release outlining the recalls, invited them to a teleconference call with executives and scheduled TV appearances or phone conversations with Mattel's chief executive. The Mattel CEO Robert Eckert did 14 TV interviews on a Tuesday in August and about 20 calls with individual reporters. By the week's end, Mattel had responded to more than 300 media inquiries in the U.S. alone.""[41]

The Pepsi Corporation faced a crisis in 1993 which started with claims of syringes being found in cans of diet Pepsi. Pepsi urged stores not to remove the product from shelves while it had the cans and the situation investigated. This led to an arrest, which Pepsi made public and then followed with their first video news release, showing the production process to demonstrate that such tampering was impossible within their factories. A second video news release displayed the man arrested.  A third video showed surveillance from a convenience store where a woman was caught inserting a syringe into a can.[42] The company simultaneously publicly worked with the FDA during the crisis. This made public communications effective throughout the crisis.  After the crisis had been resolved, the corporation ran a series of special campaigns designed to thank the public for standing by the corporation, along with coupons for further compensation. This case served as a design for how to handle other crisis situations.[43]

The Bhopal disaster in which poor communication before, during, and after the crisis cost thousands of lives, illustrates the importance of incorporating cross-cultural communication in crisis management plans. According to American University's Trade Environmental Database Case Studies (1997), local residents were not sure how to react to warnings of potential threats from the Union Carbide plant. Operating manuals printed only in English is an extreme example of mismanagement but indicative of systemic barriers to information diffusion. According to Union Carbide's own chronology of the incident (2006), a day after the crisis Union Carbide's upper management arrived in India but was unable to assist in the relief efforts because they were placed under house arrest by the Indian government. Symbolic intervention can be counter productive; a crisis management strategy can help upper management make more calculated decisions in how they should respond to disaster scenarios. The Bhopal incident illustrates the difficulty in consistently applying management standards to multi-national operations and the blame shifting that often results from the lack of a clear management plan.[44]

The Ford-Firestone Tire and Rubber Company dispute transpired in August 2000. In response to claims that their 15-inch Wilderness AT, radial ATX and ATX II tire treads were separating from the tire core—leading to crashes—Bridgestone/Firestone recalled 6.5 million tires. These tires were mostly used on the Ford Explorer, the world's top-selling sport utility vehicle (SUV).[45]

The two companies committed three major blunders early on, say crisis experts. First, they blamed consumers for not inflating their tires properly. Then they blamed each other for faulty tires and faulty vehicle design. Then they said very little about what they were doing to solve a problem that had caused more than 100 deaths—until they got called to Washington to testify before Congress.[46]

On 24 March 1989, a tanker belonging to the Exxon Corporation ran aground in the Prince William Sound in Alaska. The Exxon Valdez spilled millions of gallons of crude oil into the waters off Valdez, killing thousands of fish, fowl, and sea otters. Hundreds of miles of coastline were polluted and salmon spawning runs disrupted; numerous fishermen, especially Native Americans, lost their livelihoods. Exxon, by contrast, did not react quickly in terms of dealing with the media and the public; the CEO, Lawrence Rawl, did not become an active part of the public relations effort and actually shunned public involvement; the company had neither a communication plan nor a communication team in place to handle the event—in fact, the company did not appoint a public relations manager to its management team until 1993, 4 years after the incident; Exxon established its media center in Valdez, a location too small and too remote to handle the onslaught of media attention; and the company acted defensively in its response to its publics, even laying blame, at times, on other groups such as the Coast Guard. These responses also happened within days of the incident.[47]

One of the foremost recognized studies conducted on the impact of a catastrophe on the stock value of an organization was completed by Dr Rory Knight and Dr Deborah Pretty (1996, Templeton College, University of Oxford – commissioned by the Sedgewick Group). This study undertook a detailed analysis of the stock price (post impact) of organizations that had experienced catastrophes. The study identified organizations that recovered and even exceeded pre-catastrophe stock price, (Recoverers), and those that did not recover on stock price, (Non-recoverers). The average cumulative impact on shareholder value for the recoverers was 5% plus on their original stock value. So the net impact on shareholder value by this stage was actually positive. The non-recoverers remained more or less unchanged between days 5 and 50 after the catastrophe, but suffered a net negative cumulative impact of almost 15% on their stock price up to one year afterwards.

One of the key conclusions of this study is that ""Effective management of the consequences of
catastrophes would appear to be a more significant factor than whether catastrophe insurance hedges the economic impact of the catastrophe"".

While there are technical elements to this report it is highly recommended to those who wish to engage their senior management in the value of crisis management.[48]

Hilburg proffers that every crisis is an opportunity to showcase an institution's character, its commitment to its brand promise and its institutional values.  To address such shareholder impact, management must move from a mindset that manages crisis to one that generates crisis leadership. 
[17] Research shows that organizational contributory factors affect the tendency of executives to adopt an effective ""crisis as opportunity"" mindset.[49] Since pressure is both a precipitator and consequence of crisis, leaders who perform well under pressure can effectively guide the organization through such crisis.
[50]

James contends that most executives focus on communications and public relations as a reactive strategy. While the company's reputation with shareholders, financial well-being, and survival are all at stake, potential damage to reputation can result from the actual management of the crisis issue.[17] Additionally, companies may stagnate as their risk management group identifies whether a crisis is sufficiently ""statistically significant"".
[51] 
Crisis leadership, on the other hand, immediately addresses both the damage and implications for the company's present and future conditions, as well as opportunities for improvement.
[18]

Corporate America is not the only community that is vulnerable to the perils of a crisis. Whether a school shooting, a public health crisis or a terrorist attack that leaves the public seeking comfort in the calm, steady leadership of an elected official, no sector of society is immune to crisis. In response to that reality, crisis management policies, strategies and practices have been developed and adapted across multiple disciplines.

In the wake of the Columbine High School Massacre, the September 11 attacks in 2001, and shootings on college campuses including the Virginia Tech massacre, educational institutions at all levels are now focused on crisis management.[52]

A national study conducted by the University of Arkansas for Medical Sciences (UAMS) and Arkansas Children's Hospital Research Institute (ACHRI) has shown that many public school districts have important deficiencies in their emergency and disaster plans (The School Violence Resource Center, 2003). In response the Resource Center has organized a comprehensive set of resources to aid schools is the development of crisis management plans.[citation needed] A study conducted by researchers Min Liu, Isaac Blankson and Laurel Servies Brooks in regards to emergency response plans resulted in many findings including affirmative information that emergency and crisis training in institutions of higher education is lacking. They also found that college and university staff's knowledge and self-efficacy are positively correlated, meaning that the more knowledgeable they are, the more confident they feel in responding efficiently to various crisis events, further backing the need for crisis management plans and communication in educational institutions.[53]

Crisis-management plans cover a wide variety of incidents including bomb threats, child abuse, natural disasters, suicide, drug abuse and gang activities – just to list a few.[54] In a similar fashion the plans aim to address all audiences in need of information including parents, the media and law enforcement officials.[55]

Historically, government at all levels—local, state, and national—has played a large role in crisis management. Indeed, many political philosophers have considered this to be one of the primary roles of government. Emergency services, such as fire and police departments at the local level, and the United States National Guard at the federal level, often play integral roles in crisis situations.

To help coordinate communication during the response phase of a crisis, the U.S. Federal Emergency Management Agency (FEMA) within the Department of Homeland Security administers the National Response Plan (NRP). This plan is intended to integrate public and private response by providing a common language and outlining a chain-of-command when multiple parties are mobilized. It is based on the premise that incidents should be handled at the lowest organizational level possible. The NRP recognizes the private sector as a key partner in domestic incident management, particularly in the area of critical infrastructure protection and restoration.[56]

The NRP is a companion to the National Incidence Management System, which acts as a more general template for incident management regardless of cause, size, or complexity.[56]

FEMA offers free web-based training on the National Response Plan through the Emergency Management Institute.[57]

Common Alerting Protocol (CAP) is a relatively recent mechanism that facilitates crisis communication across different mediums and systems. CAP helps create a consistent emergency alert format to reach geographically and linguistically diverse audiences through both audio and visual mediums.[58]

A group of international psychoanalysts started in 1994 with a project to contribute to crisis management in the sense of managing conflicts between national groups. They called themselves Partners in confronting collective atrocities.[59] They began their work with the so-called Nazareth-Conferences –  based on the model of Leicesterconferences having been developed by the Tavistock Institute.

Historically, politics and crisis go hand in hand. In describing crisis, President Abraham Lincoln said,
""We live in the midst of alarms, anxiety beclouds the future; we expect some new disaster with each newspaper we read"".[citation needed]

Crisis management has become a defining feature of contemporary governance. In times of crisis, communities and members of organizations expect their public leaders to minimize the impact of the crisis at hand, while critics and bureaucratic competitors try to seize the moment to blame incumbent rulers and their policies. In this extreme environment, policymakers must somehow establish a sense of normality, and foster collective learning from the crisis experience.[60]

In the face of crisis, leaders must deal with the strategic challenges they face, the political risks and opportunities they encounter, the errors they make, the pitfalls they need to avoid, and the paths away from crisis they may pursue. The necessity for management is even more significant with the advent of a 24-hour news cycle and an increasingly internet-savvy audience with ever-changing technology at its fingertips.[60]

Public leaders have a special responsibility to help safeguard society from the adverse consequences of crisis. Experts in crisis management note that leaders who take this responsibility seriously would have to concern themselves with all crisis phases: the incubation stage, the onset, and the aftermath. Crisis leadership then involves five critical tasks: sense making, decision making, meaning making, terminating, and learning.[60]

A brief description of the five facets of crisis leadership includes:[61]

While all countries encounter various crises over the course of their existence, they utilize different strategies that reflect their particular structure and circumstances.[62] Autocratic and more centralized governments may address crises in ways that democratic countries could not. These distinctions present opportunities to compare numerous approaches to similar emergencies.

The COVID-19 crisis has proved to be a useful lens to view how crisis management differs by country.[63] In democratic countries like Italy and Spain for example, health care is more decentralized and management is shared between the central and regional governments.[64] While this decentralization may allow for greater autonomy and flexibility, during a major worldwide crisis, coordinating a response may become difficult. Several institutional challenges and political conflicts emerged because limited communication between agencies and two opposing parties controlling different regions and different levels of government. This was especially true in areas of Northern Italy where the regionalist Lega party had a significant influence.[64] In Spain, basic coordination between different governments was not established until a second COVID wave struck.[64]

Iran, an autocratic country, was able to manage their COVID-19 crisis centrally while allowing for flexibility on the ground. it maintained its system of central control but also utilized behavioral economics to motivate citizen compliance and participation.[63] After years of sanctions, Iran was able to develop a level of self-reliance that proved useful during this public health crisis.[63] The government implemented universal measures that did not vary from region to region and developed a citizen focused approach that encouraged engagement.[65] The Iranians produced their own medical supplies and developed and distributed domestic vaccines. Iran enjoyed a greater degree of success in managing the crisis than in Italy and Spain.[63] However, Iran was not immune to some of the issues occurring elsewhere. The government still struggled to enforce compliance measures and some citizens did engage in activities that were restricted.[63]

One noteworthy challenge in the COVID-19 crisis was the impact of Ideological differences on the decision-making and implementation processes of crisis management. Ideology plays a significant role in how institutions are constructed and in the degree of coordination between rival factions in government.[62] The Western emphasis on decentralization has in some ways hindered the effectiveness of government responses to the spread of COVID-19. By relegating the management of health care to regions, political parties are able to implement conflicting measures that fail to achieve the greater national or international goals.[62]

In 2022, the Scientific Advice Mechanism to the European Commission published a major piece of advice on strategic crisis management in the European Union, which included a detailed survey of existing emergency management institutions and mechanisms in Europe as well as recommendations for updating and improving the Union's resilience to and preparedness for emergencies. One of the scientists' key recommendations was that ""existing and future legislation and instruments should be integrated in a framework that is capable of dealing with increasingly systemic and large-scale crises in a structural way"".[66]

Although most studies focus on one or two country-specific examples, it is also important to consider crisis management at a global level.[67] Because each state pursues its own style of crisis management, developing international coordination has proven to be complicated. International organizations and multinational corporations will struggle to adjust their operations to conflicting practices in different countries.[67] Global health organizations have also received scrutiny from around the world for falling short of expectations during previous crises which has deepened the distrust of these international groups.[68] These problems are alarming because global markets continue to integrate and supply chains grow between countries. Crises that impact more than one country will occur more often and will have greater consequences.[67]

There are a number of professional industry associations that provide advice, literature and contacts to turnaround professionals and academics.
Some are:

1. International Association of Emergency Managers (International)

2. Turnaround Management Society (International / Focus on Europe)

3. Institute for Turnaround (England)

4. Turnaround Management Association (International)

5. Institut für die Standardisierung von Unternehmenssanierungen (Germany)

6. Disaster Recovery Institute (International)
"
Community Engagement Services,"Community engagement is involvement and participation in an organization for the welfare of the community.

Volunteering, which involves giving personal time to projects in humanitarian NGOs  or religious groups, are forms of community involvement.[1] The engagement is generally motivated by values and ideals of social justice[2] Community engagement can be volunteering at food banks, homeless shelters, emergency assistance programs, neighborhood cleanup programs, etc.[3][4][5]

It is also defined as ""a dynamic relational process that facilitates communication, interaction, involvement, and exchange between an organization and a community for a range of social and organizational outcomes"".[6]  As a concept, engagement features attributes of connection, interaction, participation, and involvement, designed to achieve or elicit an outcome at individual, organization, or social levels.[7] Current research acknowledges engagement's socially-situated nature. Community engagement therefore offers an ethical, reflexive, and socially responsive approach to community-organizational relationships with engagement practices that aim to both understand and be responsive to community needs, views, and expectations.[8][9] For academic research to have impact, community engagement is essential, especially for the research around population health and wellness issues.[10][11] It is imperative that the researchers employ community-engaged approaches where community members[12] and organizations[13] and researchers work hand-in-hand to identify the problems, co-develop solutions and recommend policy changes.

Community engagement is a community-centered orientation based in dialogue.[14] Community engagement enables a more contextualized understanding of community members’ perceptions of the topics and contexts, and facilitates stronger relationships among and between community members.[15][16] The outcome of community engagement is ultimately social capital and stronger relational networks.[17] While community organizing involves the process of building a grassroots movement involving communities, community engagement primarily deals with the practice of moving communities toward change, usually from a stalled or similarly suspended position.
"
Stakeholder Services,"In a corporation, a stakeholder is a member of ""groups without whose support the organization would cease to exist"",[1] as defined in the first usage of the word in a 1963 internal memorandum at the Stanford Research Institute. The theory was later developed and championed by R. Edward Freeman in the 1980s. Since then it has gained wide acceptance in business practice and in theorizing relating to strategic management, corporate governance, business purpose and corporate social responsibility (CSR). The definition of corporate responsibilities through a classification of stakeholders to consider has been criticized as creating a false dichotomy between the ""shareholder model"" and the ""stakeholder model"",[2] or a false analogy of the obligations towards shareholders and other interested parties.[3]

Any action taken by any organization or any group might affect those people who are linked with them in the private sector. For examples these are parents, children, customers, owners, employees, associates, partners, contractors, and suppliers, people that are related or located nearby. Broadly speaking there are three types of stakeholders:

A narrow mapping of a company's stakeholders might identify the following stakeholders:[4]

A broader mapping of a company's stakeholders may also include:[citation needed]

In the field of corporate governance and corporate responsibility, a debate[5][6] is ongoing about whether the firm or company should be managed primarily for stakeholders, stockholders (shareholders), customers, or others.[7] Proponents in favor of stakeholders may base their arguments on the following four key assertions:

A corporate stakeholder can affect or be affected by the actions of a business as a whole. Whereas shareholders are often the party with the most direct and obvious interest at stake in business decisions, they are one of various subsets of stakeholders, as customers and employees also have stakes in the outcome. In the most developed sense of stakeholders in terms of real corporate responsibility, the bearers of externalities are included in stakeholdership.

In the last decades of the 20th century, the word ""stakeholder"" became more commonly used to mean a person or organization that has a legitimate interest in a project or entity. In discussing the decision-making process for institutions—including large business corporations, government agencies, and non-profit organizations—the concept has been broadened to include everyone with an interest (or ""stake"") in what the entity does. This includes not only vendors, employees, and customers, but even members of a community where its offices or factory may affect the local economy or environment. In this context, a ""stakeholder"" includes not only the directors or trustees on its governing board (who are stakeholders in the traditional sense of the word) but also all persons who paid into the figurative stake and the persons to whom it may be ""paid out"" (in the sense of a ""payoff"" in game theory, meaning the outcome of the transaction). Therefore, in order to effectively engage with a community of stakeholders, the organisation's management needs to be aware of the stakeholders, understand their wants and expectations, understand their attitude (supportive, neutral or opposed), and be able to prioritize the members of the overall community to focus the organisation's scarce resources on the most significant stakeholders.[8]

Example

The holders of each separate kind of interest in the entity's affairs are called a constituency, so there may be a constituency of stockholders, a constituency of adjoining property owners, a constituency of banks the entity owes money to, and so on. In that usage, ""constituent"" is a synonym for ""stakeholder"".[9]

Post, Preston, Sachs (2002), use the following definition of the term ""stakeholder"":
""A person, group or organization that has interest or concern in an organization.
Stakeholders can affect or be affected by the organization's actions, objectives and policies. Some examples of key stakeholders are creditors, directors, employees, government (and its agencies), owners (shareholders), suppliers, unions, and the community from which the business draws its resources.

Not all stakeholders are equal. A company's customers are entitled to fair trading practices but they are not entitled to the same consideration as the company's employees. The stakeholders in a corporation are the individuals and constituencies that contribute, either voluntarily or involuntarily, to its wealth-creating capacity and activities, and that are therefore its potential beneficiaries and/or risk bearers.""[10] This definition differs from the older definition of the term stakeholder in Stakeholder theory (Freeman, 1983) that also includes competitors as stakeholders of a corporation. Robert Allen Phillips provides a moral foundation for stakeholder theory in Stakeholder Theory and Organizational Ethics. There he defends a ""principle of stakeholder fairness"" based on the work of John Rawls, as well as a distinction between normative and derivative legitimate stakeholders. Real stakeholders, labelled stakeholders: genuine stakeholders with a legitimate stake, the loyal partners who strive for mutual benefits. Stake owners own and deserve a stake in the firm. Stakeholder reciprocity could be an innovative criterion in the corporate governance debate as to who should be accorded representation on the board. Corporate social responsibility should imply a corporate stakeholder responsibility.
"
Corporate Responsibility Services,"Corporate social responsibility (CSR) or corporate social impact is a form of international private business self-regulation[1] which aims to contribute to societal goals of a philanthropic, activist, or charitable nature by engaging in, with, or supporting professional service volunteering through pro bono programs, community development, administering monetary grants to non-profit organizations for the public benefit, or to conduct ethically oriented business and investment practices.[2][3] While CSR could have previously been described as an internal organizational policy or a corporate ethic strategy,[4] similar to what is now known today as environmental, social, and governance (ESG), that time has passed as various companies have pledged to go beyond that or have been mandated or incentivized by governments to have a better impact on the surrounding community. In addition, national and international standards, laws, and business models have been developed to facilitate and incentivize this phenomenon. Various organizations have used their authority to push it beyond individual or industry-wide initiatives. In contrast, it has been considered a form of corporate self-regulation[5] for some time, over the last decade or so it has moved considerably from voluntary decisions at the level of individual organizations to mandatory schemes at regional, national, and international levels. Moreover, scholars and firms are using the term ""creating shared value"", an extension of corporate social responsibility, to explain ways of doing business in a socially responsible way while making profits (see the detailed review article of Menghwar and Daood, 2021).[6]

Considered at the organisational level, CSR is generally understood as a strategic initiative that contributes to a brand's reputation.[7] As such, social responsibility initiatives must coherently align with and be integrated into a business model to be successful. With some models, a firm's implementation of CSR goes beyond compliance with regulatory requirements and engages in ""actions that appear to further some social good, beyond the interests of the firm and that which is required by law"".[8]

Furthermore, businesses may engage in CSR for strategic or ethical purposes. From a strategic perspective, CSR can contribute to firm profits, particularly if brands voluntarily self-report both the positive and negative outcomes of their endeavors.[9] In part, these benefits accrue by increasing positive public relations and high ethical standards to reduce business and legal risk by taking responsibility for corporate actions. CSR strategies encourage the company to make a positive impact on the environment and stakeholders including consumers, employees, investors, communities, and others.[10] From an ethical perspective, some businesses will adopt CSR policies and practices because of the ethical beliefs of senior management: for example, the CEO of outdoor-apparel company Patagonia, Inc. argues that harming the environment is ethically objectionable.[11]


Proponents argue that corporations increase long-term profits by operating with a CSR perspective, while critics argue that CSR distracts from businesses' economic role. A 2000 study compared existing econometric studies of the relationship between social and financial performance, concluding that the contradictory results of previous studies reporting positive, negative, and neutral financial impact were due to flawed empirical analysis and claimed when the study is properly specified, CSR has a neutral impact on financial outcomes.[12] Critics[13][14] have questioned the ""lofty"" and sometimes ""unrealistic expectations"" of CSR,[15] or observed that CSR is merely window-dressing, or an attempt to pre-empt the role of governments as a watchdog over powerful multinational corporations. In line with this critical perspective, political and sociological institutionalists became interested in CSR in the context of theories of globalization, neoliberalism, and late capitalism.
Since the 1960s,[16] corporate social responsibility has attracted attention from a range of businesses, academics and stakeholders and been referred to by a number of other terms, including ""corporate sustainability"", ""sustainable business"", ""corporate conscience"", ""corporate citizenship"",[17] ""purpose"", ""social impact"", ""conscious capitalism"",[18] and ""responsible business"".[19][20]

A wide variety of definitions have been developed, but with little consensus. Part of the definition problem has arisen because of the different interests represented. A business person may define CSR as a business strategy, an NGO activist may see it as 'greenwash' while a government official may see it as voluntary regulation.[1] ""In addition, disagreement about the definition will arise from the disciplinary approach.""[1] For example, while an economist might consider the director's discretion necessary for CSR to be implemented a risk of agency costs, a law academic may consider that discretion to be an appropriate expression of what the law demands from directors. In the 1930s, two law professors, A. A. Berle and Merrick Dodd, famously debated how directors should be made to uphold the public interest: Berle believed there had to be legally enforceable rules in favor of labor, customers and the public equal to or ahead of shareholders, while Dodd argued that powers of directors were simply held on trust.[21][22]

Corporate social responsibility was defined by Sheehy as ""international private business self-regulation"".[1] Sheehy examined a range of different disciplinary approaches to defining CSR. The definitions reviewed included the economic definition of ""sacrificing profits"", a management definition of ""beyond compliance"", institutionalist views of CSR as a ""socio-political movement,"" and the law's focus on directors' duties. Further, Sheehy considered Archie B. Carroll's description of CSR as a pyramid of responsibilities, namely, economic, legal, ethical, and philanthropic responsibilities.[23] While Carroll was not defining CSR, but simply arguing for the classification of activities, Sheehy developed a definition differently following the philosophy of science—the branch of philosophy used for explaining phenomena.

Carroll extended corporate social responsibility from the traditional economic and legal responsibility to ethical and philanthropic responsibility in response to the rising concerns on ethical issues in businesses.[23] A review of 14,523 articles found that stakeholder perspective is the most prevalent dimension of corporate social responsibility.[24] This view is reflected in the Business Dictionary that defines CSR as ""a company's sense of responsibility towards the community and environment (both ecological and social) in which it operates. Companies express this citizenship (1) through their waste and pollution reduction processes, (2) by contributing educational and social programs, and (3) by earning adequate returns on the employed resources.""[25][26]

Businesses have changed when the public came to expect and require different behavior [...] I predict that in the future, just as in the past, changes in public attitudes will be essential for changes in businesses' environmental practices.
Most consumers agree that while achieving business targets, companies should engage in CSR efforts at the same time.[28] Most consumers believe companies doing charity work will receive a positive response.[29] Somerville also found that consumers are loyal and willing to spend more on retailers that support charity. Consumers also believe that retailers selling local products will gain loyalty.[30] Smith (2013)[31] shares the belief that marketing local products will gain consumer trust. However, environmental efforts are receiving negative views, given the belief that this would affect customer service.[30] Oppewal et al. (2006) found that not all CSR activities are attractive to consumers.[32] They recommended that retailers focus on one activity.[33] Becker-Olsen (2006)[34] found that if the social initiative done by the company is not aligned with other company goals it will have a negative impact. Mohr et al. (2001)[35] and Groza et al. (2011)[36] also emphasise the importance of reaching the consumer.

Some commentators have identified a difference between the Canadian (Montreal school of CSR), the Continental European, and the Anglo-Saxon approaches to CSR.[37] It has been described that for Chinese consumers a socially responsible company makes safe, high-quality products;[38] for Germans it provides secure employment; in South Africa it makes a positive contribution to social needs such as health care and education.[39] Even within Europe, the discussion about CSR is very heterogeneous.[40]

A more common approach to CSR is corporate philanthropy. This includes monetary donations and aid given to nonprofit organizations and communities. Donations are made in areas such as the arts, education, housing, health, social welfare, and the environment, among others, but excluding political contributions and commercial event sponsorship.[41]

Another approach to CSR is to incorporate the CSR strategy directly into operations, such as procurement of Fair Trade tea and coffee.

Creating shared value, or CSV, is based on the idea that corporate success and social welfare are interdependent. A business needs a healthy, educated workforce, sustainable resources, and an adept government to compete effectively. For society to thrive, profitable and competitive businesses must be developed and supported to create income, wealth, tax revenues, and philanthropy.[dubious – discuss] The Harvard Business Review article ""Strategy & Society: The Link between Competitive Advantage and Corporate Social Responsibility"" provided examples of companies that have developed deep linkages between their business strategies and CSR.[42] CSV acknowledges trade-offs between short-term profitability and social or environmental goals, but emphasizes the opportunities for competitive advantage from building a social value proposition into corporate strategy. CSV gives the impression that only two stakeholders are essential – shareholders and consumers.

Many companies employ benchmarking to assess their CSR policy, implementation, and effectiveness. Benchmarking involves reviewing competitor initiatives, measuring and evaluating the impact those policies have on society and the environment, and how others perceive competitor CSR strategy.[43]

Meehan, Meehan and Richard developed a model known as the 3C-SR model, published in a frequently cited article in 2006, which aimed to offer ""a new strategic approach to corporate responsibility"".[44] Their model sought to fill the gap between corporate social responsibility definitions and strategy, which the authors perceived to be an issue, and to provide guidance to managers on connecting businesses with ethically-aware consumers.[45][46]

An approach described by Tóth Gergely and published by the Hungarian Association for Environmentally Aware Management (KÖVET) refers to ""Deep CSR"" and the role of a ""Truly Responsible Enterprise"". Gergely's definition of ""Deep CSR"" is the behaviour displayed by a ""Truly Responsible Enterprise"" (TRE), which:

The five principles of the TRE are:

In competitive markets, the cost-benefit analysis of CSR initiatives can be examined using a resource-based view (RBV). According to Barney (1990), ""formulation of the RBV, sustainable competitive advantage requires that resources be valuable (V), rare (R), inimitable (I) and non-substitutable (S)"".[48][49] A firm introducing a CSR-based strategy might only sustain high returns on their investment if their CSR-based strategy could not be copied (I). However, should competitors imitate such a strategy that might increase overall social benefits? Firms that choose CSR for strategic financial gain are also acting responsibly.

RBV presumes that firms are bundles of heterogeneous resources and capabilities that are imperfectly mobile across firms. This imperfect mobility can produce competitive advantages for firms that acquire immobile resources. McWilliams and Siegel (2001) examined CSR activities and attributes as a differentiation strategy. They concluded that managers could determine the appropriate level of investment in CSR by conducting a cost-benefit analysis in the same way they analyze other investments. Reinhardt (1998) found that a firm engaging in a CSR-based strategy could only sustain an abnormal return if it could prevent competitors from imitating it.[50]

The relationship between corporate social responsibility and a firm's corporate financial performance is a phenomenon that is being explored in a variety of research studies that are being conducted across the world. Based on these research studies, including those undertaken by Sang Jun Cho, Chune Young Chung, and Jason Young, a positive relationship exists between a firm's corporate social responsibility policies and corporate financial performance. To investigate this relationship, the researchers conducted a regression analysis and preceded the analysis with the provision of several measures that they utilized to serve as proxies for key financial performance indicators (i.e. return on assets serves as a proxy for profitability).[52]

Initially, CSR emphasized the official behaviour of individual firms. Later, it expanded to include supplier behaviour, the uses to which products were put, and how articles were disposed of after they lost value. Malcolm McIntosh notes also that focussing on the identifiable behaviour of individual businesses risks not including what he calls ""unincorporated market behaviour"" within the scope of CSR - actions attributable to market processes, and also calls for other factors including ""brand citizenship"" and ""illegitimate, informal or illegal activity"" to be considered as part of a more complete picture.[53]

The term ""brand citizenship"" has been put forward because the public perception of an organisation may be associated with its branding rather than its corporate identity: McIntosh uses Virgin as an example.[53] Similarly, Anne Bahr Thompson uses the same term and observes that companies adopting socially responsible behaviours are primarily investing in their reputations.[54]

In the 21st century, corporate social responsibility in the supply chain has attracted attention from businesses and stakeholders. A corporation's supply chain is the process by which several organizations, including suppliers, customers, and logistics providers, work together to provide a value package of products and services to the end-user, who is the customer.[55]

Corporate social irresponsibility in the supply chain has greatly affected the reputation of companies, leading to many costs to solve the problems. For instance, incidents like the 2013 Savar building collapse, which killed over 1000 people, pushed companies to consider the impacts of their operations on society and the environment. On the other hand, the horsemeat scandal of 2013 in Europe affected many food retailers, including Tesco, the largest retailer in the United Kingdom,[56] leading to the dismissal of the supplier. Corporate social irresponsibility from suppliers and retailers has greatly affected the stakeholders who lost trust in the affected business entities. Although sometimes it is not directly undertaken by the companies, they become accountable to the stakeholders. These surrounding issues have prompted supply chain management to consider the corporate social responsibility context. Wieland and Handfield (2013) suggested that companies must include social responsibility in their reviews of component quality. They highlighted the use of technology to improve visibility across the supply chain.[57]

Corporate social responsibility includes six types of corporate social initiatives:[2]

All six of the corporate initiatives are forms of corporate citizenship. However, only some of these CSR activities rise to the level of cause marketing, defined as ""a type of corporate social responsibility (CSR) in which a company's promotional campaign has the dual purpose of increasing profitability while bettering society.""[58]

Companies generally do not have a profit motive when participating in corporate philanthropy and community volunteering. On the other hand, the remaining corporate social initiatives can be examples of cause marketing, in which there is both a societal interest and a profit motive.

CSR may be based within the human resources, business development or public relations departments of an organisation,[59] or may be a separate unit reporting to the CEO or the board of directors.

An engagement plan can assist in reaching the desired audience. A corporate social responsibility individual or team plans the goals and objectives of the organization. As with any corporate activity, a defined budget demonstrates commitment and scales the program's relative importance.

Social accounting is the communication of social and environmental effects of a company's economic actions to particular interest groups within society and to society at large.[60]

Social accounting emphasizes the notion of corporate accountability. Crowther defines social accounting as ""an approach to reporting a firm's activities which stresses the need for the identification of socially relevant behavior, the determination of those to whom the company is accountable for its social performance and the development of appropriate measures and reporting techniques.""[61]

Modern CSR has a wide range of different standards, frameworks and metrics for reporting and disclosing the social, environmental and economic issues. However, there is no single, fixed standard and the complex, dynamic and contextual nature of CSR means different companies and stakeholders adopt different approaches depending on their needs.[62]

There are a range of reporting guidelines and standards that serve as frameworks for social accounting, auditing, and reporting:

Legal requirements for social accounting, auditing, and reporting exist in nations like France. However, international or national agreement on meaningful social and environmental performance measurements has not been achieved. Many companies produce externally audited annual reports that cover Sustainable Development and CSR issues (""Triple Bottom Line Reports""), but the reports vary widely in format, style, and evaluation methodology (even within the same industry). Critics dismiss these reports as lip service, citing examples such as Enron's yearly ""Corporate Responsibility Annual Report"" and tobacco companies' social reports.

In South Africa, as of June 2010, all companies listed on the Johannesburg Stock Exchange (JSE) were required to produce an integrated report in place of an annual financial report and sustainability report.[73] An integrated report reviews environmental, social, and economic performance alongside financial performance. This requirement was implemented in the absence of formal or legal standards. An Integrated Reporting Committee (IRC) was established to issue guidelines for good practice.

One of the reputable institutions that capital markets turn to for credible sustainability reports is the Carbon Disclosure Project, or CDP.

Consumers of goods and services should verify corporate social responsibility and the results of reports and efforts.[74] The accounting, auditing, and reporting resources provide a foundation for consumers to verify that their products are socially sustainable. Due to an increased awareness of the need for CSR, many industries have their verification resources.[75] They include organizations such as the Forest Stewardship Council (paper and forest products), International Cocoa Initiative,[76] and Kimberly Process (diamonds). The United Nations Global Compact provides frameworks not only for verification, but also for reporting human rights violations in corporate supply chains.[77]

The rise of ethics training inside corporations, some of which are required by government regulation, has helped CSR to spread. Such training aims to help employees make ethical decisions when the answers are unclear.[78] The most direct benefit is reducing the likelihood of ""dirty hands"",[79] fines, and damaged reputations for breaching laws or moral norms. Organizations see increased employee loyalty and pride in the organization.[80]

Common CSR actions include:[81]

The term ""social license"" was introduced in 1997 and has since been applied in multiple resource extraction industries to describe changes in company-community interactions.[91] This use of social license has included an understanding of how acceptance levels impact resource development operations within these industries.[91] Gunningham et al.[92] state corporations comply with their social license by operating within societal expectations and avoiding activities (or influential elements within them) considered unacceptable, and define social license it as ""the demands on and expectations for a business enterprise that emerge from neighborhoods, environmental groups, local stakeholders, and other elements of the surrounding civil society"".[92]

Social License to Operate can be determined as contractual grounds for the legitimacy of activities and projects a company is involved in.[93] It refers to the level of support and approval of a company's activities by its stakeholders.[94] Displaying commitment to CSR is one way to achieve a social license, by enhancing a company's reputation.[95]

As stated in Enduring value: the Australian minerals industry framework for sustainable development the concept of the 'social license to operate', then defined simply as obtaining and maintaining broad community support and acceptance. Unless a company earns and maintains that license, social license holders may intend to block project developments; employees may leave the company for a company that is a better corporate citizen: and companies may be under ongoing legal challenge.[96] Issues related to the government's measurement of corporations' social license include its role in licensure processes, the penalties for non-compliance, or the community's ability to halt a project if a corporation is not responsive to their concerns, are still subject to global concern.[97][98] Regardless of government involvement, social license is achieved within and given by communities, which is defined as ""a social unit of any size that shares common values, or that is situated in a given geographical area"".[99] Lacey[100] suggested that social license can take a long time for a corporation or industry to achieve, but social license can be lost very quickly for a variety of factors, including changes in stakeholder expectations, technology, or other disturbances. Gunningham et al.[92] stated that meeting and exceeding regulations to build reputational capital is economically vital, saying: ""in certain circumstances, [natural resource-based industries] cannot afford to do otherwise"". In communities with a diverse economy, achieving social license is often much more complex than in local communities, which depend economically on the natural resource industry.[92]

In research undertaken by Ketola et al., the writers believed that the forest products industry in rural Michigan in the United States may have received a social license through the channels that mining corporations initially established and the long history of logging and copper mining in the area continued to shape the attitudes and identities of industry participants to present day.[101] They found that local stakeholders and local industry operators have shared history and experience as having limited power to control the larger economic forces acting upon them. Local actors are more likely to have values similar to those of stakeholders, have established some history in the area, and have had the time to develop meaningful relationships within the community. This shared experience shaped the process of acquiring a social license. Nonlocal actors are likely to experience a much lesser degree of social license than local actors. Furthermore, many of the resources affected by forest management are held in the public trust,[102] so it is essential for both industry actors and community stakeholders to feel engaged and involved in decisions regarding local natural resource management. Baines and Edwards shared similar findings in New Zealand's aquaculture sector regarding the importance of relationships and communication between industry and local stakeholders.[103] They find that social license depends on relationships and building trust. Smaller, local companies tend towards relationships that are relational as opposed to transactional, possibly due to their ongoing community presences and communication abilities, which are better for fostering these relationships and trust building.

In research of Requisite Organization, Elliott Jaques defines Social License to Operate for the company as the social contract the company has with the social license holders (employees, trade unions, communities, government) for them to manifest positive intention to support the business short- and long-term objectives by ""providing managerial leadership that nurtures the social good and also gives the foundation for sustainable growth in organizational results.""[104]

The primary objective for the companies is to obtain and maintain the Social License to Operate. Based on the Requisite Organization, to achieve this goal, a company needs to:

A positive relationship has been shown to exist between CSR and a firm's corporate financial performance. However, results from these analyses may need to be examined under different lenses for emerging and developed economies, especially since firms based in emerging economies oftentimes have weak firm-level governance.[52]

For companies operating in emerging markets, engaging in CSR practices enables widespread reach into a variety of outside markets, an improved reputation, and stakeholder relationships.[105] In all cases (emerging markets vs. developed economies), implementing CSR policies into the daily activities and framework of a company has been shown to allow for a competitive advantage versus other companies, including the creation of a positive image for the company, improved stakeholder relationships, increased employee morale, and attraction of new consumers who are committed to social responsibility.[105] Despite all of the benefits, it is important to note that several drawbacks exist, including possible accusations of hypocrisy, the difficulty of measuring the social impact of CSR policies, and oftentimes placing companies at a disadvantage against competitors when prioritizing CSR ahead of advancing a company's R&D.[105]

A large body of literature urges businesses to adopt non-financial measures of success (e.g., Deming's Fourteen Points, balanced scorecards). While CSR benefits are hard to quantify, Orlitzky, Schmidt and Rynes[106] found a correlation between social/environmental performance and financial performance. However, across the empirical literature this correlation is on average relatively small.[107]

The business case for CSR[108] within a company employs one or more of these arguments:

""People, planet, and profit"", also known as the triple bottom line, form one way to evaluate CSR.
""People"" refers to fair labour practices, the community, and the region where the business operates.
""Planet"" refers to sustainable environmental practices. Profit is the economic value created by the organization after deducting the cost of all inputs, including the cost of the capital (unlike accounting definitions of profit).[109][110]

Balancing economic, ecological, and social goals is at the heart of the triple bottom line.[111]

This measure was claimed to help some companies be more conscious of their social and moral responsibilities.[112] However, critics claim that it is selective and substitutes a company's perspective for that of the community. Another criticism is about the absence of a standard auditing procedure.[113]

The term was coined by John Elkington in 1994[110] who re-evaluated it in 2018 and called for more urgent action toward sustainability [114]

A CSR program can be an aid to recruitment and retention,[115][116] particularly within the competitive graduate student market. Potential recruits often consider a firm's CSR policy. CSR can also help improve the perception of a company among its staff, particularly when staff can become involved through payroll giving, fundraising activities, or community volunteering. CSR has been credited with encouraging customer orientation among customer-facing employees.[117]

CSR is known for impacting employee turnover. Several executives suggest that employees are their most valuable asset and that the ability to retain them leads to organizational success. Socially responsible activities promote fairness, which in turn generates lower employee turnover. On the other hand, if a firm demonstrates irresponsible behavior, employees may view this behavior as negative. Proponents argue that treating employees well with competitive pay and good benefits is seen as socially responsible behavior and, therefore, reduces employee turnover.[118] Executives have a strong desire for building a positive work context that benefits CSR and the company as a whole. This interest is driven particularly by the realization that a positive work environment can result in desirable outcomes such as more favorable job attitudes and increased work performance.[119]

The IBM Institute for Business Value surveyed 250 business leaders worldwide in 2008.[120] The survey showed that businesses have assimilated a much more strategic view with 68% of companies utilizing CSR as an opportunity and part of a sustainable growth strategy.[120] Developing and implementing a CSR strategy represents a unique opportunity to benefit the company. However, only 31% of businesses surveyed engaged their employees on the company's CSR objectives and initiatives.[120] Employee engagement on CSR initiatives can be a powerful recruitment and retention tool. Moreover, employees tend to avoid employers with a bad reputation.[120]

Managing risk is an important executive responsibility. Reputations that take decades to build up can be ruined in hours through corruption scandals or environmental accidents.[121] These situations draw unwanted attention from regulators, courts, governments, and the media. CSR can limit these risks.[122]

Sustainability is key to resilience across value chains. As companies prefer working with long-lasting partners, those that have implemented CSR practices will be preferred over ones that have not in order to minimize reputational as well as other damages.[123] High levels of CSR compliance within supply chains (including Tier 1 and beyond) will also help reduce vulnerabilities and eliminate environmental, social, and economic risks through implementing a sustainability-focused procurement strategy.

With effective CSR policies, a company can better mitigate legislative and legal risks by complying with emerging CSR-related laws and regulations, preempting costly lawsuits and non-compliance actions, and addressing sources of non-compliance by fostering corporate alignment around the relevant issues.[124]

CSR can enhance a brand's reputation by ""inducing a desire to support and help the company that has acted to benefit consumers"".[7] In this way, CSR serves to enhance brand perceptions, which can lead to positive product evaluations,[125] though this effect is dependent upon a variety of factors, including the degree to which consumers value close relationships or believe that the CSR initiative is self-serving,[7] whether the CSR program may be perceived to affect product quality negatively,[126] consumers' consumption-related goals (i.e., whether their consumption is socially versus product-motivated),[127] or consumers' attributions toward the motives of the CSR endeavor.[128]

Some companies use their commitment to CSR as their primary positioning tool, e.g., The Co-operative Group, The Body Shop, and American Apparel.[129] Others use CSR methodologies as a strategic tactic to gain public support for their presence in global markets, helping them sustain a competitive advantage by using their social contributions as another form of advertising.[130]

Companies that operate strong CSR activities tend to drive customers' attention to buy products or services regardless of the price. As a result, this increases competition among firms since customers are aware of the company's CSR practices. These initiatives serve as a potential differentiator because they not only add value to the company, but also to the products or services. Furthermore, firms under intense competition can leverage CSR to increase the impact of their distribution on the firm's performance. Lowering the carbon footprint of a firm's distribution network or engaging in fair trade are potential differentiators to lower costs and increase profits. In this scenario, customers can observe the company's commitment to CSR while increasing company sales.[131]

Whole Foods' marketing and promotion of organic foods have had a positive effect on the supermarket industry. Proponents assert that Whole Foods has been able to work with its suppliers to improve animal treatment and the quality of meat offered in their stores. They also promote local agriculture in over 2,400 independent farms to maintain their line of sustainable organic produce. As a result, Whole Foods' high prices do not turn customers away from shopping. They are pleased to buy organic products that come from sustainable practices.[132]

A Harvard Business Review article proposes three stages of practice in which CSR can be divided. Stage one focuses on philanthropy, including donations of money or equipment to non-profit organizations, engagement with community initiatives, and employee volunteering. This is characterized as the ""soul"" of a company, expressing the social and environmental priorities of the founders. The authors assert that companies engage in CSR because they are an integral part of society. The Coca-Cola Company contributes $88.1 million annually to various environmental, educational, and humanitarian organizations. Another example is PNC Financial Services' ""Grow Up Great"" childhood education program. This program provides critical school readiness resources to underserved communities where PNC operates.[133]

On the other hand, stage two focuses on improving operational effectiveness in the workplace. The researchers assert that programs in this stage strive to deliver social or environmental benefits to support a company's operation across the value chain by improving efficiency. Some examples include sustainability initiatives to reduce resource use, waste, and emissions that could reduce costs. It also calls for investing in employee work conditions such as health care and education, which may enhance productivity and retention. Unlike philanthropic giving, which is evaluated by its social and environmental return, initiatives in the second stage are predicted to improve the corporate bottom line with social value. Grupo Bimbo, the largest bakery in Mexico, is an excellent example of this. The company strives to meet social welfare needs. It offers free educational services to help employees complete high school. Bimbo also provides supplementary medical care and financial assistance to close government health coverage gaps.[133]

Moreover, the third stage of the program aims to transform the business model. Companies create new forms of business to address social or environmental challenges that will lead to financial returns in the long run. One example can be seen in Unilever's Project Shakti in India. The authors describe that the company hires women in villages and provides them with micro-finance loans to sell soaps, oils, detergents, and other products door-to-door. This research indicates that more than 65,000 women entrepreneurs are doubling their incomes while increasing rural access and hygiene in Indian villages. Another example is IKEA's ""People and Planet"" initiative, which is to be 100% sustainable by 2020. Consequently, the company wants to introduce a new model to collect and recycle old furniture.[133]

Corporations are keen to avoid interference in their business through taxation or regulations. A CSR program can persuade governments and the public that a company takes health and safety, diversity, and the environment seriously, reducing the likelihood that company practices will be closely monitored.

Appropriate CSR programs can increase the attractiveness of supplier firms to potential customer corporations. For example, a fashion merchandiser may find value in an overseas manufacturer that uses CSR to establish a positive image and to reduce the risks of bad publicity from uncovered misbehavior.

A price-focused procurement strategy has limitations for companies with high CSR expectations. According to Boston Consulting Group, “businesses that are considered leaders in environmental, social and governance criteria have an 11% valuation premium over their competitors.”[134] Such companies look for suppliers who share their social, environmental, and business ethics values, which in turn would trigger common innovations that would attract more customers and generate further value for the whole supply chain for a win-win business relationship through a series of interconnected activities implemented holistically.[citation needed] Furthermore, supplier relations are crucial for a company's CSR profile as 70% of businesses’ social and environmental impacts occur in their supply chain.[135] Through the spillover effect, CSR programs encourage sustainable practices within different industries. Additionally, a CSR-focused supplier relations management improves collaboration with suppliers, increases the satisfaction of customers’ expectations and requirements, expands market opportunities, enhances investor relations, and promotes the development of more sustainable products.[136] Furthermore, CSR supply chain imperatives can leverage their responsible commitment to forge robust and lasting relationships with important stakeholders and positively influence the decision-making of consumers, partners, investors, and talent. By constructing this reputational capital, companies can earn consumer loyalty, attract top talent, and strengthen employee morale and commitment.[137]

CSR strategies or behaviors related to CSR were discussed by many scholars in terms of crisis management, such as responses to boycotts in an international context.[138] Ang found that relationship building through providing additional services rather than price-cutting is what businesses in Asia feel more comfortable with as a strategy during an economic crisis.[139] Regarding direct research about strategies in cross-cultural crisis management, scholars found that CSR strategies could make effects through empirical case studies involving multinational businesses in China.[140] They found that meeting local stakeholders' social expectations can mitigate the risk of crises. The strategy utilized by Arla Foods works and has helped the company regain most of its lost market share among many countries in the Middle East. Arla Foods founded funding for children with cancer, and they donated ambulances to refugees in Lebanon. As Arla Foods did, they tried to contribute to solving social problems of children's access to health care, which were local priorities. Other researchers analyzed the case of multinational enterprise strategies during the conflict between Lebanon and Israel. During the conflict, many companies stressed seeking to help the local community.[141] In the post-conflict stage, managers highlighted their philanthropic programs and contributions, in terms of monetary in-kind donations to the refugees or businesses that were directly affected. For example, Citibank has provided monetary assistance to some local businesses affected by the war. Another activity done by a Lebanese company was a fund-raising campaign.

CSR concerns include its relationship to the purpose of business and the motives for engaging in it.

Milton Friedman and others argued that a corporation's purpose is to maximize returns to its shareholders and that obeying the laws of the jurisdictions within which it operates constitutes socially responsible behavior.[142] Friedman argued each person should be free to spend their own money on social causes if they wished, but that business owners should avoid putting a ""tax"" on consumers as ""unwitting puppets"" of socialism by raising prices to support business practices with social goals unrelated to profit.[143][144]

While some CSR supporters claim that companies practicing CSR, especially in developing countries, are less likely to exploit workers and communities, critics claim that CSR itself imposes outside values on local communities with unpredictable outcomes.[145]

Rather than voluntary measures, better governmental regulation and enforcement are an alternative to CSR that moves decision-making and resource allocation from public to private bodies.[146] However, critics claim that effective CSR must be voluntary as mandatory social responsibility programs regulated by the government interferes with people's plans and preferences, distorts the allocation of resources, and increases the likelihood of irresponsible decisions.[147]

Some critics believe that companies undertake CSR programs to distract the public from ethical questions posed by their core operations. They argue that the reputational benefits that CSR companies receive (cited above as a benefit to the corporation) demonstrate the hypocrisy of the approach.[149] Moreover, some studies find that CSR programs are motivated by corporate managers' interests at the cost of the shareholders so they are a type of an agency problem in corporations.[150][151]

Others have argued that the primary purpose of CSR is to provide legitimacy to the power of businesses.[152] As wealth inequality is perceived to be increasing[153] it has become increasingly necessary for businesses to justify their position of power.

Joel Bakan is one of the prominent critics of the conflict of interest between private profit and a public good, characterizing corporate officials of publicly listed corporations as constrained by law to maximize the wealth of their shareholders.[154] This argument is summarised by Haynes that ""a corporate calculus exists in which costs are pushed onto both workers, consumers and the environment"".[155] CSR spending may be seen in these financial terms, whereby the higher costs of socially undesirable behaviour are offset by a CSR spending of a lower amount. Indeed, it has been argued that there is a ""halo effect"" in terms of CSR spending. Research has found that firms that had been convicted of bribery in the US under the Foreign Corrupt Practices Act (FCPA) received more lenient fines if they had been seen to be actively engaging in comprehensive CSR practices. It was found that typically either a 20% increase in corporate giving or a commitment to eradicating a significant labour issue, such as child labour, was equated to a 40% lower fine in the case of bribing foreign officials.[156]

Aguinis and Glavas conducted a comprehensive review of CSR literature, covering 700 academic sources from numerous fields, including organizational behavior, corporate strategy, marketing, and HRM. It was found that the primary reason for firms to engage in CSR was the expected financial benefits associated with CSR, rather than being motivated by a desire to be responsible to society.[157] Consistent with this analysis, consumers respond less favorably to CSR initiatives that they believe are tainted with self-serving motives"".[7]

CEOs' political ideologies are evident manifestations of their different personal views. Each CEO may exercise different powers according to their organizational outcomes. Their political ideologies are expected to influence their preferences for CSR outcomes. Proponents argue that politically liberal CEOs will envision the practice of CSR as beneficial and desirable to increase a firm's reputation. They tend to focus more on how the firm can meet the needs of society. Consequently, they will advance with the practice of CSR while adding value to the firm. On the other hand, property rights may be more relevant to conservative CEOs. Since conservatives tend to value free markets, individualism, and the call for respect of authority, they will not likely envision this practice as often as those identifying as liberals might.[158]

The company's financials and the CSR practice also have a positive relationship. Moreover, a company's performance tends to influence conservatives more likely than liberals. While not seeing it from the financial performance point of view, liberals tend to believe that CSR adds to the business's triple bottom line. For example, when the company performs well, it will promote CSR. If the company is not performing as expected, they will rather tend to emphasize this practice because it will potentially envision it as a way to add value to the business. In contrast, politically conservative CEOs will tend to support the practice of CSR if they believe that it will provide a good return to the company's financials. In other words, these types of executives tend to not see the outcome of CSR as a value to the company if it does not provide anything in exchange.[158]

There have been unsubstantiated social efforts, ethical claims, and outright greenwashing by some companies that have resulted in increasing consumer cynicism and mistrust.[159] Sometimes companies use CSR to direct public attention away from other, harmful business practices. For example, McDonald's Corporation positioned its association with Ronald McDonald House and other children's charities as CSR[160] while its meals have been accused of promoting poor eating habits.[citation needed]

Acts that may initially appear to be altruistic CSR may have ulterior motives. The funding of scientific research projects has been used as a source of misdirection by firms. Stanley B. Prusiner, who discovered the protein responsible for Creutzfeldt–Jakob disease (CJD) and won the 1997 Nobel Prize in Medicine, thanked the tobacco company RJ Reynolds for their crucial support. RJ Reynolds funded the research into CJD. Proctor states that ""the tobacco industry was the leading funder of research into genetics, viruses, immunology, air pollution"",[161] anything which formed a distraction from the well-established research linking smoking and cancer.

Research has also found that corporate social marketing, a form of CSR promoting societal good, is being used to direct criticism away from the damaging practices of the alcohol industry.[162] It has been shown that advertisements that supposedly encourage responsible drinking simultaneously aim to promote drinking as a social norm. In this case, companies may engage in CSR and social marketing to prevent more stringent government legislation on alcohol marketing.

Industries such as tobacco, alcohol, or munitions firms make products that damage their consumers or the environment. Such firms may engage in the same philanthropic activities as other industries. This duality complicates assessments of such firms concerning CSR.[163]

To fully observe the impact of corporate social responsibility practices on a firm's corporate financial performance, it is crucial to delve into a concrete example, such as the study conducted by researchers from the Global Conference on Business, Economics, Management, and Tourism. In this study, Mocan, Draghici, Ivascu, and Turi examined the correlation between CSR policies and value creation/financial performance in the banking industry specifically and found that various benefits include greater economic efficiency, improved company reputation, and employee loyalty, better communication streamline between the industry and individuals, and the opportunity to attract new opportunities (i.e. attract new investments, or remain competitive) and improve organizational commitment.[164] However, before discussing these effects, the researchers preceded the analysis by stating that typically implementing CSR and other ethical principles within the framework of a financial institution such as banks make it seem as if these are marketing tools for attracting and communicating with stakeholders rather than serving as tools that allow banks and other financial institutions to benefit the individuals that they serve. Another interesting case study is provided by Hein Schreuder (2022), who discusses the evolution of CSR at the Dutch multinational corporation DSM.[111]

One motivation for corporations to adopt CSR is to satisfy stakeholders beyond those of a corporation's shareholders.

Branco and Rodrigues (2007) describe the stakeholder perspective of CSR as the set of views of corporate responsibility held by all groups or constituents with a relationship to the firm.[165] In their normative model, the company accepts these views as long as they do not hinder the organization. The stakeholder perspective fails to acknowledge the complexity of network interactions in cross-sector partnerships. It relegates communication to a maintenance function, similar to the exchange perspective.[166]

The rise in popularity of ethical consumerism over the last two decades can be linked to the rise of CSR.[167] Consumers are becoming more aware of the environmental and social implications of their day-to-day consumption decisions and in some cases make purchasing decisions related to their environmental and ethical concerns.[168]

One issue with the consumer's relationship with CSR is that it is much more complex than it first appears. This phenomenon could be described as the ""CSR-Consumer Paradox"" or mismatch, where consumers report that they would only buy from companies with good social responsibility. Many consumers want to buy from responsible companies, but surveys indicate that ""ethical purchases"" are a small percentage of household expenditures. The discrepancy between consumer beliefs and intentions, and actual consumer behaviour, means CSR has a much lesser impact than consumers initially say.

One theory for explaining this discrepancy is the ""bystander apathy"" or the bystander effect. This theory stems from social psychology and states that the likelihood of an individual acting in a given situation is significantly reduced if other bystanders do nothing, even if that individual strongly believes in a specific course of action. It would suggest an ""If they do not care then why should I?"" mentality. Even if consumers are against the use of sweatshops or want to support green causes, they may continue to make purchases from companies that are socially irresponsible just because other consumers seem apathetic towards the issue.

Another explanation is that of reciprocal altruism. In the evolutionary psychology of human behaviour: people only do something if they can get something back in return. In the case of CSR and ethical consumerism, however, consumers get very little in return for their investment. Ethically sourced or manufactured products are typically higher in price due to greater costs. However, the reward for consumers is not much different from that of a non-ethical counterpart. Therefore, evolutionary speaking, making an ethical purchase is not worth the higher cost to the individual, even if they believe in supporting ethical, environmental, and socially beneficial causes.

Shareholders and investors, through socially responsible investing (SRI), are using their capital to encourage behavior they consider responsible. However, definitions of what constitutes ethical behavior vary. For example, some religious investors in the United States have withdrawn investments from companies that violate their religious views. In contrast, secular investors divest from companies that they see as imposing religious views on workers or customers.[169]

Some national governments promote socially and environmentally responsible corporate practices. The heightened role of government in CSR has facilitated the development of numerous CSR programs and policies.[170] Various European governments have pushed companies to develop sustainable corporate practices.[171] CSR critics such as Robert Reich argued that governments should set the agenda for social responsibility with laws and regulations that describe how to conduct business responsibly.

Collective bargaining is a way nations promote CSR. In Germany, CSR is kept at the industry level instead of the workplace; this has been viewed as one of the strengths of the German government's push for CPR.[172] Germany also established the German Trade Union Confederation in 1949 to further advance CSR; the confederation represents the interests of 45 million workers in Germany.[173] Job security, wage increases with industry growth are key aspects of collective bargaining in the German labor system.

There is a higher percentage of workers in unions in countries like Sweden and Iceland which have more Social-Democratic elements in their Nordic Model than the U.S. and the U.K.[174]

The U.S. and the U.K. are Liberal Market Economies (LMEs), and the German economy falls under Coordinated Market Economy (CMEs), which are Varieties of Capitalism. In comparison with the U.S. that covers 25.5% of its blue and white-collar workforce under collective bargaining[175] and the U.K. that covers 29% of its workforce,[176] Germany covers a significantly higher 57% of its workforce under collective bargaining.[172]

Fifteen European Union countries are actively engaged in CSR regulation and public policy development.[171] CSR efforts and policies are different among countries, responding to the complexity and diversity of governmental, corporate, and societal roles. Some studies have claimed that the role and effectiveness of these actors were case-specific.[170] This variety among company approaches to CSR can complicate regulatory processes.[177]

Canada adopted CSR in 2007. Prime Minister Harper encouraged Canadian mining companies to meet Canada's newly developed CSR standards.[178]

The 'Heilbronn Declaration' is a voluntary agreement between enterprises and institutions in Germany, especially in the Heilbronn-Franconia region, signed on 15 September 2012. The approach of the Heilbronn Declaration targets the decisive factors of success or failure, the achievements of the implementation, and best practices regarding CSR. A form of responsible entrepreneurship shall be initiated to meet the requirements of stakeholders' trust in the economy. It is an approach to make voluntary commitments more binding.[179]

In opposition to mandated CSR regulation, researchers Armstrong and Green suggest that all regulation is ""harmful"", citing regulation as the cause of North Korea's low economic freedom and per capita GDP. They further claim without source that ""There is no form of market failure, however egregious, which is not eventually made worse by the political interventions intended to fix it"", and conclude ""there is no need for further research on regulation in the name of social responsibility.""[180]

In the 1800s, the US government could take away a firm's license if it acted irresponsibly. Corporations were viewed as ""creatures of the state"" under the law. In 1819, the United States Supreme Court in Dartmouth College vs. Woodward established a corporation as a legal person in specific contexts. This ruling allowed corporations to be protected under the Constitution and prevented states from regulating firms.[181] Countries have included CSR policies in government agendas.[171]

On 16 December 2008, the Danish parliament adopted a bill that required the 1,100 largest Danish companies, investors, and state-owned companies to include CSR information in their financial reports. The reporting requirements became effective on 1 January 2009.[182] The required information included:

CSR/SRI is voluntary in Denmark, but if a company has no policy on this, it must state its position on CSR in financial reports.[183]

In 1995, item S50K of the Income Tax Act of Mauritius mandated that companies registered in Mauritius pay 2% of their annual book profit to contribute to the social and environmental development of the country.[184] In 2014, India also enacted a mandatory minimum CSR spending law. Under Companies Act, 2013, any company having a net worth of 500 crore or more or a turnover of 1,000 crore or a net profit of 5 crore must spend 2% of their net profits on CSR activities.[185] The rules came into effect on 1 April 2014.[186]

The only mandatory CSR Act in the world thus far was passed by the Indian parliament in 2013 as Section 135 of the Companies Act. According to that bill, all firms with a net worth above 5 billion rupees (approx. $75 million), turnover over 10 billion rupees (approx. $150 million), or net profit over 50 million rupees (approx. $750,000) are required to spend at least 2% of their annual profits (averaged over three years). The Act requires that all businesses affected establish a CSR committee to oversee the spending. Before this law was passed, CSR laws were applied only to public sector companies.[187]

Unlike global definitions of CSR in the triple bottom line, corporate citizenship, sustainable business, business responsibility, and closed-loop realm, in India, CSR is a philanthropic activity. What has changed since formalizing it in 2014 is the shift in focus from institution building (schools, hospitals, etc.) to focus on community development.[188]

Crises have encouraged the adoption of CSR. The CERES principles were adopted following the 1989 Exxon Valdez incident.[79] Other examples include the lead paint used by toymaker Mattel, which required the recall of millions of toys and caused the company to initiate new risk management and quality control processes. Magellan Metals was found responsible for lead contamination, which killed thousands of birds in Australia. The company ceased business immediately and had to work with independent regulatory bodies to execute a cleanup. Odwalla experienced a crisis, with sales dropping 90% and its stock price dropping 34% due to cases of E. coli. The company recalled all apple or carrot juice products and introduced a new process called ""flash pasteurization"", as well as maintaining lines of communication constantly open with customers.

Corporations that employ CSR behaviors do not always behave consistently in all parts of the world.[189] Conversely, a single behavior may not be considered ethical in all jurisdictions, e.g., some jurisdictions forbid women from driving,[190] while others require women to be treated equally in employment decisions.


The European Commission presented a green paper for the European Communities, as the EU was then called, ""promoting a European framework for Corporate Social Responsibility"" in 2001.[191] In that document CSR was defined as 
a concept whereby companies integrate social and environmental concerns in their business operations and in their interaction with their stakeholders on a voluntary basis.
By 2011, the Commission recognised a ""strategic approach"" to CSR as ""increasingly important too the competitiveness of enterprises"".[192] Believing that enterprises can ""significantly contribute to the European Union's treaty objectives of sustainable development and a highly competitive social market economy"", therefore presented a revised strategy in October 2011, A renewed EU strategy 2011–14 for Corporate Social Responsibility. In this document, CSR was defined more briefly as 
the responsibility of enterprises for their impact on society.[192]: 6 
 In parallel with CSR, the alternative term ""responsible business conduct"" (RBC) introduced by the OECD (see OECD Guidelines for Multinational Enterprises) was also adopted.[193] No longer treating CSR as a ""voluntary"" or is some sense ""additional"" aspect of managing an enterprise, the commission now stated that 
enterprises should have in place a process to integrate social, environmental, ethical, human rights and consumer concerns into their business operations and core strategy in close collaboration with their stakeholders.[192]: 6 
 Actions planned under the 2011–2014 agenda were aimed at:

Subsequently, the Commission published a staff working document in March 2019 that examined the progress in implementing CSR/RBC as well as business and human rights.[194]

A 2006 study found that the UK retail sector showed the greatest rate of CSR involvement.[32] Many of the big retail companies in the UK joined the Ethical Trading Initiative,[195] an association established to improve working conditions and worker health.

Tesco (2013)[196] reported that their 'essentials' are 'Trading responsibility', 'Reducing our Impact on the Environment', 'Being a Great Employer' and 'Supporting Local Communities'. J Sainsbury[197] employs the headings 'Best for food and health', 'Sourcing with integrity', 'Respect for our environment', 'Making a difference to our community', and 'A great place to work', etc. The four main issues to which UK retail companies are committed: environment, social welfare, ethical trading, and becoming an attractive workplace.[198][199]

Anselmsson and Johansson (2007)[200] assessed three areas of CSR performance: human responsibility, product responsibility, and environmental responsibility. Martinuzzi et al. described the terms, writing that human responsibility is ""the company deals with suppliers who adhere to principles of natural and good breeding and farming of animals, and also maintains fair and positive working conditions and workplace environments for their employees. Product responsibility means that all products come with a complete list of content, that country of origin is stated, and that the company will uphold its declarations of intent and assume liability for its products. Environmental responsibility means that a company is perceived to produce environmental-friendly, ecological, and non-harmful products"".[201] Jones et al. (2005) found that environmental issues are the most commonly reported CSR programs among top retailers.[202]

An article published in Forbes.com in September 2017 mentioned the yearly study of Boston-based reputation management consulting company Reputation Institute (RI)[203] which rates the top 10 US corporations in terms of corporate social responsibility. RI monitors social responsibility reputations by focusing on the perception of consumers regarding company governance,[204] positive impact on the community and society, and treatment of the workforce. It rates each criterion with the firm's proprietary RepTrak Pulse platform.[205] Forbes identified companies like Lego, Microsoft, Google, Walt Disney Company, BMW Group, Intel, Robert Bosch, Cisco Systems, Rolls-Royce Aerospace, and Colgate-Palmolive.[206]

According to the CSR Journal, the millennial generation worldwide helps propel brands toward social responsibility. Many millennials want to conduct business with companies and trademarks that employ pro-social themes,[207] sustainable manufacturing processes,[208] and ethical business practices.[209] Nielsen Holdings published its Annual Global Corporate Sustainability Report in 2017 concentrating on global responsibility as well as sustainability.[210] Nielsen's 2015 report showed that 66 percent of consumers will spend more on products that come from sustainable brands.[211] Another 81 percent expect their preferred corporate institutions to reveal in public their statements about corporate citizenship[212]

The National Association on the Advancement of Colored People (NAACP), through its chief executive officer Derrick Johnson, shared the organization's insights on how American corporations can help in the realization of social justice.[213] According to the article from Yahoo News, the NAACP has been engaged in a crusade for racial justice and economic opportunities during the last 109 years. This organization believes all citizens in the United States must be held liable in ensuring democracy works for all people.[214]

From an Indian perspective, corporate social responsibility is not just about how an organization distributes its profits, but is also about how it generates its revenue. Corporate social responsibility is defined as the ethos and practice of discovering, invoking, infusing, evoking, and radiating the human values of 'righteousness' (dharma) and 'love' (Prema) in an organisation's interactions with its stakeholders.[215] Stakeholders, in this definition, refers to the organisation's customers, employees, shareholders, society, natural environment, business associates, regulatory agencies, future generations, etc.[216]

International field policy expert Edmond Fernandes has provided various ways in which Indian companies can increase their investment efficiency and investment sufficiency by integrating social health in all policies. Non-government organizations like CHD Group and SEEDS have demonstrated ways in which smart and efficient CSR is possible.[217]
"
Fundraising Services,"Fundraising or fund-raising is the process of seeking and gathering voluntary financial contributions by engaging individuals, businesses, charitable foundations, or governmental agencies. Although fundraising typically refers to efforts to gather money for non-profit organizations, it is sometimes used to refer to the identification and solicitation of investors or other sources of capital for for-profit enterprises.

Traditionally, fundraising has consisted mostly of asking for donations through face-to-face fundraising, such as door-knocking. In recent years, though, new forms such as online fundraising or  grassroots fundraising have emerged.[1]

Fundraising is a significant way that non-profit organizations may obtain the money for their operations. These operations can involve a very broad array of concerns such as religious or philanthropic groups such as research organizations, public broadcasters, political campaigns and environmental issues.

Some examples of charitable organizations include student scholarship merit awards for athletic or academic achievements, humanitarian and ecological concerns, disaster relief, human rights, research, and other social issues.

Some of the most substantial fundraising efforts in the United States are conducted by colleges and universities. Commonly the fundraising, or ""development"" / ""advancement,"" program, makes a distinction between annual fund appeals and major campaigns. Most institutions use professional development officers to conduct superior fundraising appeals for both the entire institution or individual colleges and departments (e.g. School of Art, School of Math, School of Science, etc... as well as campus institutions like athletics[2] and libraries.[3]). The number of people involved, often having socialized at such ""fund-raisings"",[4] will vary widely depending on the size of the institution they sponsor.

Equally important are fundraising efforts by virtually all recognized religious groups throughout the world. These efforts are organized on a local, national, and global level. Sometimes, such funds will go exclusively toward assisting the basic needs of others, while money may at other times be used only for evangelism or proselytism. Usually, religious organizations mix the two, which can sometimes cause tension.

Fundraising also plays a major role in political campaigns.  This fact, despite numerous campaign finance reform laws, continues to be a highly controversial topic in American politics. Political action committees (PACs) are the best-known organizations that back candidates and political parties, though others such as 527 groups also have an impact. Some advocacy organizations conduct fundraising for-or-against policy issues in an attempt to influence legislation.

While public broadcasters are completely government-funded in much of the world, there are many countries where some funds must come from donations from the public. In the United States less than 15% of local public broadcasting stations' funding comes from the federal government. Pledge drives, a type of annual giving, commonly occur about three times each year, usually lasting one to two weeks each time. Viewership and listenership often decline significantly during funding periods, so special programming may be aired in order to keep regular viewers and listeners interested.

The techniques of direct marketing have been adapted in the nonprofit sector. The alignment of direct marketing approaches with fundraising is believed to have facilitated its adoption by associations in the mid-1970s. The results obtained through the intensive use of direct marketing techniques triggered increasing interest from associations. While only a few dozen were using major direct marketing channels in 1975, several hundred commonly utilize them today.[5]

In the case of associations, direct marketing enhances the anonymity and intimacy of donations, but it can also serve as a tool for developing visibility and image.[citation needed]

Many associations have transformed their one-time fundraising into automatic withdrawal collections following the example of ""Doctors Without Borders."" This progression is similar to that resulting from another form of fundraising introduced in France in the early 21st century, ""street fundraising"". 

Street fundraising involves recruiting new donors in public places, approaching them to introduce the association and propose support for its actions through assured automatic withdrawal donations. Unlike traditional fundraising, it does not collect cash or checks. It usually involves promises of donations. Street fundraising aims to establish a dialogue, create a connection with donors, often younger, engaged, and loyal.[6]

There are two dimensions to Fundraising – Donor Acquisition and Donor Retention.

Donor Acquisition refers to acquiring or addition of new donors.

Donor Retention, as the name implies, refers to retaining of existing donors.

The efforts of the Fundraising team should be primarily focused on acquisition, while also ensuring that acquired donors are retained as happy repeat donors. It is a common mistake to either over-burden existing donors with funding requests or under-serve them to the point of disengagement. For effective fundraising to occur, the systems and operations in place, including reporting, should be organized to ensure that both new and existing donors are not just satisfied, but delighted to support.[citation needed]

According to Giving USA 2019, a study of charitable giving in the United States, the sources of funds donated to charities are as follows:

*NOTE – This chart does not include government grants, which are technically contracts to perform a service, not a charitable gift.

Fundraising is just one of several revenue sources for a nonprofit organization. Additional revenue can come in the form of grants from government agencies, endowments, and sales and services. Income from an endowment is not strictly considered fundraising, but rather the result of previous fundraising efforts' investment.

The donor base (often called a ""donor file"" or simply ""constituents"") for higher education includes alumni, parents, friends, private foundations, and corporations. Gifts of appreciated property are important components of such efforts because the tax advantage they confer on the donor encourages larger gifts. The process of soliciting appreciated assets is called planned giving. Charitable giving by individuals in the U.S. was estimated to be $286.65 billion in 2017.[7]

The established development programs at institutions of higher learning include prospect identification, prospect research and verification of the prospect's viability, cultivation, solicitation, and finally stewardship, the latter being the process of keeping donors informed about how past support has been used. When goods or professional services are donated to an organization rather than cash, this is called an in-kind gift.

A number of charities and non-profit organizations are increasingly using the internet as a means to raise funds; this practice is referred to as online fundraising. In addition, crowdfunding has begun to be used as a method to engage small-donation donors for small, specific opportunities.

Comparing traditional and online fundraising, 55% of donors worldwide prefer to give online with a credit or debit card, while 12% prefer to give by bank/wire transfer, and only 8% choose to donate in cash. 51% of donors are enrolled in a recurring giving program with 87% of recurring donors opting to give monthly. Worldwide, 45% of donors donate to crowdfunding campaigns that benefit NPOs, 13% create online peer-to-peer fundraising campaigns to benefit NPOs, and 60% have donated to an NPO in response to the COVID-19 pandemic.[8]

Non-profit organizations also raise funds through competing for grant funding. Grants are offered by governmental units and private foundations/charitable trusts to non-profit organizations for the benefit of all parties to the transaction. Charitable giving by foundations in the U.S. was estimated to be $66.90 billion in 2017.[7]

Charitable giving by corporations in the U.S. was estimated to be $20.77 billion in 2017.[7] This consists of corporate grants as well as matching gift and volunteer grants. 65% of Fortune 500 companies offer employee matching gift programs and 40% offer volunteer grant programs.[9] These are charitable giving programs set up by corporations in which the company matches donations made by employees to eligible nonprofit organizations or provides grants to eligible nonprofit organizations as a way to recognize and promote employee volunteerism.

A bequest is a gift that is written into a donor's will that is fulfilled after their death. These gifts can be written in the will itself or added as a codicil (addendum) after the main will has been ratified. These gifts are separated from individual giving by Giving USA to illustrate the importance of Planned Giving, which is a type of fundraising that focuses on asking donors to include charitable gifts in their estate plans.[citation needed]

While fundraising often involves the donation of money as an outright gift, money may also be generated by selling a product of some kind, also known as product fundraising. Girl Scouts of the USA are well known for selling cookies in order to generate funds. It is also common to see on-line impulse sales links to be accompanied by statements that a proportion of proceeds will be directed to a particular charitable foundation.  Tax law may require differentiating between the cost of an item versus its gift value, such as a $100.00 per person dinner, for a $25.00 cost meal.  Fundraising often involves recognition to the donor, such as naming rights or adding donors to an honor roll or other general recognition.  Charity Ad Books are another form of donation for recognition, sponsorship or selling of ads often in an event related program or group directory.[citation needed]

Fund raising is typically undertaken for one of two broad objectives: Opex (Operational Expenditure) or Capex (Capital Expenditure). Opex includes salary, overheads such as electricity, rent and transport, whereas Capex includes expenses such as infrastructure, equipment or supplies. Therefore, organizations raise funds to support capital projects, endowments, or operating expenses of current programs.

Capital fundraising is when fundraising is undertaken to raise major sums for a building or endowment; generally such funds are kept separate from operating funds. This is often done over a period of time (in a capital campaign) to encourage donors to give more than they would normally give and tap donors, especially corporations and foundations who would not otherwise give. A capital campaign normally begins with a private phase before launching a public appeal.

Many non-profit organizations solicit funds for a financial endowment, which is a sum of money that is invested to generate an annual return. Although endowments may be created when a sizable gift is received from an individual or family, often as directed in a will upon the death of a family member, they are more typically the result of many gifts over time from a variety of sources.

A fundraising event (also called a fundraiser) is an event or campaign whose primary purpose is to raise money for a cause, charity or non-profit organization. Fundraisers often benefit charitable, non-profit, religious, or non-governmental organizations, though there are also fundraisers that benefit for-profit companies and individuals.

Special events are another method of raising funds. These range from formal dinners to benefit concerts to walkathons. Events are used to increase visibility and support for an organization as well as raising funds.[10] Events can feature activities for the group such as speakers, a dance, an outing or entertainment, to encourage group participation and giving. Events can also include fundraising methods such as a raffle or charity auction. Events often feature notable sponsors or honoree. Events often feature a charity ""ad book"" as a program guide for the event. This can also be another fundraiser providing members, supporters and vendors an opportunity to show their support of the group at the event by way of placing an ad-like page. Events and their associated fundraisers can be a major source of a group's revenue, visibility and donor relations.

One specific type of event is the ""ad book"" fundraiser, where those who wish to give funds to a fundraising group do so through the sponsorship or statement within a book of advertisements.

Online fundraising pages have become very popular for people taking part in activities such as charities and crowdfunding. Those pages facilitate online payments in support of the charity.

Popular charity fundraisers in major American cities include lavish black-tie gala benefit dinners that honor celebrities, philanthropists, and business leaders who help to fundraise for the event's goals through solicitations of their social and business connections.[11]

Often called donor cultivation, relationship building is the foundation on which most fundraising takes place.[12] Most fundraising development strategies divide donors into a series of categories based on the amount and frequency of donations. For instance, annual giving and recurring gifts represent the base of a fundraising pyramid. This would be followed by mid-level gifts, planned gifts, major gifts, and principal gifts.

More sophisticated strategies use tools to overlay demographic and other market segmentation data against their database of donors in order to more precisely customize communication and more effectively target resources.[13] Research by Peter Maple in the UK[14] shows that charities generally underinvest in good marketing research spending around a quarter of what an equivalent sized for profit company might spend.

Donor relations and stewardship[15] professionals support fundraisers by recognizing and thanking donors, and demonstrating the impact of their donations in a fashion that will cultivate future giving to nonprofit organizations.

Recent research by Adrian Sargeant and the Association of Fundraising Professionals' Fundraising Effectiveness Project suggests the sector has a long way to go in improving the quality of donor relations. The sector generally loses 50–60% of its newly acquired donors between their first and second donations and one in three, year on year thereafter. The economics of regular or sustained giving are rather different, but even then organizations routinely lose 30% of their donors from one year to the next.[16]

A capital campaign is ""an intensive fundraising effort designed to raise a specified sum of money within a defined time period to meet the varied asset-building needs of an organization"". Asset-building activities include the construction, renovation or expansion of facilities (for example, a new building), the acquisition or improvement of land, equipment, or other items, and additions to a financial endowment. Two characteristics set capital campaigns apart from other forms of fundraising activities. First, ""the gifts solicited are much larger than those generally sought during an annual fund"". Second, ""pledges are emphasized as commitments payable over a number of years convenient to the donor or through the transfer of appreciated real or personal property"".[17]

Various types of capital campaigns have been identified. The traditional ""brick and mortar"" campaign, focused on building construction or improvements, was considered a ""once in a lifetime"" campaign in the past because of the ambitious goals of the campaign. Today, however, organizations frequently schedule capital campaigns every five to ten years, and ""the megagoals announced by large institutions often are the result of 'counting everything' during a five-to seven-year campaign period"".[17]

A second type of campaign is the comprehensive, integrated, or total development campaign, which aims for a longer fundraising program based on a long-term analysis of the organization's needs and direction. This form of campaign can wrap together capital projects, endowment and operating expenses as its purpose, and use a variety of fundraising activities, such as annual gift drives, which are ""slower-paced and lack the intensity of the traditional capital campaign"".[17]

Some non-profit organizations demonstrate greater accountability by showing donors the direct impact of their fundraising efforts. This accountability may comes in the form of a vote, where the members select a specific program or charity that they would like their money to go to. Another example is put in place a mechanism which allows donors to contraint usage of funds toward a specific purpose and closely monitor/allow spending to ensure proper usage.

Many non-profit organizations take advantage of the services of professional fundraisers. These fundraisers may be paid for their services either through fees unrelated to the amounts of money to be raised, or by retaining a percentage of raised funds (percentage-based compensation). The latter approach is expressly forbidden under the Code of Ethics of the Association of Fundraising Professionals (AFP), a professional membership body.[18]
However, by far the most common practice of American non-profits is to employ a staff person whose main responsibility is fund raising.  This person is paid a salary like any other employee, and is usually a part of the top management staff of the organization.

Some non-profit organizations nonetheless engage fundraisers who are paid a percentage of the funds they raise. In the United States, this ratio of funds retained to funds passed on to the non-profit is subject to reporting to a number of state's Attorneys General or Secretaries of state.[19]  This ratio is highly variable and subject to change over time and place, and it is a point of contention between a segment of the general public and the non-profit organizations.

The term 'professional fundraiser' is often legislated, referring to third-party firms whose services are contracted, whereas 'fundraising professionals' or development officers are typically individuals or staff at charitable non-profits. Although potentially confusing, it's important to note this distinction.

Online and mobile fundraising had become a popular fundraising method over the last few years due to its accessibility. Fundraising organizations are developing technical options like mobile apps and donate buttons to attract donors around the globe. Common online and mobile fundraising methods include online donation pages, text to give, mobile silent auctions, and peer to peer fundraising.

Since 2016, online giving has grown by 17% in the United States. In 2018, digital fundraising accounted for 8.5% percent of charitable donations and 24% of online donations were made on a mobile device in the United States.[20]

Organizations in the United States established for charitable purposes are allowed to raise funds from many sources. They are given a specific designation by the Internal Revenue Service (IRS), commonly noted as 501(c)(3) organizations. Other nonprofits such as fraternal associations have different IRS designations, and may or may not be eligible to raise funds. Financial information on many nonprofits, including all nonprofits that file annual IRS 990 forms is available from GuideStar.

 The dictionary definition of fundraising at Wiktionary
"
Volunteer Services,"

Volunteering is an elective and free-choice act of an individual or group freely giving time and labor, often for community service.[1][2] Many volunteers are specifically trained in the areas they work, such as medicine, education, or emergency rescue. Others serve on an as-needed basis, such as in response to a natural disaster.

The verb was first recorded in 1755. It was derived from the noun volunteer, in c. 1600, ""one who offers himself for military service,"" from the Middle French voluntaire.[3] In the non-military sense, the word was first recorded during the 1630s. The word volunteering has more recent usage—still predominantly military—coinciding with the phrase community service.[3][4]
In a military context, a volunteer army is a military body whose soldiers chose to enter service, as opposed to having been conscripted. Such volunteers do not work ""for free"" and are given regular pay.

During this time, America experienced the Great Awakening. People became aware of the disadvantaged and realized the cause for movement against slavery.[5] In 1851, the first YMCA in the United States was started, followed seven years later by the first YWCA. During the American Civil War, women volunteered their time to sew supplies for the soldiers and the ""Angel of the Battlefield"" Clara Barton and a team of volunteers began providing aid to servicemen.  Barton founded the American Red Cross in 1881 and began mobilizing volunteers for disaster relief operations, including relief for victims of the Johnstown Flood in 1889.

The Salvation Army is one of the oldest and largest organizations working for disadvantaged people. Though it is a charity organization, it has organized a number of volunteering programs since its inception.[6]
Prior to the 19th century, few formal charitable organizations existed to assist people in need.

In the first few decades of the 20th century, several volunteer organizations were founded, including the Rotary International, Kiwanis International, Association of Junior Leagues International, and Lions Clubs International.

The Great Depression saw one of the first large-scale efforts to coordinate volunteering for a specific need in the US. During World War II, thousands of volunteer offices supervised the volunteers who helped with the many needs of the military and the home front, including collecting supplies, entertaining soldiers on leave, and caring for the injured.[6]

After World War II, people shifted the focus of their altruistic passions to other areas, including helping the poor and volunteering overseas. A major development was the Peace Corps in the United States in 1960. When President Lyndon B. Johnson declared a War on Poverty in 1964, volunteer opportunities started to expand and continued into the next few decades. The process for finding volunteer work became more formalized, with more volunteer centers forming and new ways to find work appearing on the World Wide Web through organizations like JustServe and AmeriCorps.[6][7]

According to the Corporation for National and Community Service (in 2012), about 64.5 million Americans, or 26.5 percent of the adult population, gave 7.9 billion hours of volunteer service worth $175 billion.  This calculates at about 125–150 hours per year or 3 hours per week at a rate of $22 per hour. Volunteer hours in the UK are similar; the data for other countries is unavailable.

Many schools on all education levels offer service-learning programs, which allow students to serve the community through volunteering while earning educational credit.[8] According to Alexander Astin in the foreword to Where's the Learning in Service-Learning? by Janet Eyler and Dwight E. Giles, Jr., ""...we promote more wide-spread adoption of service-learning in higher education because we see it as a powerful means of preparing students to become more caring and responsible parents and citizens and of helping colleges and universities to make good on their pledge to 'serve society.'""[9] When describing service learning, the Medical Education at Harvard says, ""Service learning unites academic study and volunteer community service in mutually reinforcing ways. ...service learning is characterized by a relationship of partnership: the student learns from the service agency and from the community and, in return, gives energy, intelligence, commitment, time and skills to address human and community needs.""[8] Volunteering in service learning seems to have the result of engaging both mind and heart, thus providing a more powerful learning experience; according to Janet Eyler and Dwight E. Giles, it succeeds by the fact that it ""...fosters student development by capturing student interest...""[9]: 1–2, 8  
More recent scholarship has found shortcomings in the early assumptions of mutual benefit, since early studies were interested in educational benefits rather than community outcomes. An Indiana study found that the nonprofit agencies hosting student service-learners do not report a positive impact on service capacity, although service-learners do help to increase agency visibility.[10] In the end, service-learning must follow other principles of effective volunteer management such as screening, training, and supervising.[editorializing]

Skills-based volunteering is leveraging the specialized skills and the talents of individuals to strengthen the infrastructure of nonprofits, helping them build and sustain their capacity to successfully achieve their missions.[11] This is in contrast to traditional volunteering, where volunteers do something other than their professional work.[12] The average hour of traditional volunteering is valued by the Independent Sector at between $18–20 an hour.[13] Skills-based volunteering is valued at $40–500 an hour, depending on the market value of the time.[14][failed verification]

Also called e-volunteering or online volunteering, virtual volunteering is a volunteer who completes tasks, in whole or in part, offsite from the organization being assisted. They use the Internet and a home, school, telecenter or work computer, or other Internet-connected device, such as a PDA or smartphone. Virtual volunteering is also known as cyber service, telementoring, and teletutoring, as well as various other names. Virtual volunteering is similar to remote work, except that instead of online employees who are paid, these are online volunteers who are not paid.[15][16] Contributing to free and open source software projects or editing Wikipedia are examples of virtual volunteering.[17]

Micro-volunteering is a task performed via an internet-connected device. An individual typically does this task in small, un-paid increments of time. Micro-volunteering is distinct from ""virtual volunteering"" in that it typically does not require the individual volunteer to go through an application process, screening process, or training period.[18][19]

Environmental volunteering refers to the volunteers who contribute towards environmental management or conservation. Volunteers conduct a range of activities including environmental monitoring, ecological restoration such as re-vegetation and weed removal, protecting endangered animals, and educating others about the natural environment.[20]

Volunteering often plays a pivotal role in the recovery effort following natural disasters, such as tsunamis, floods, droughts, hurricanes, and earthquakes. For example, the 1995 Great Hanshin-Awaji earthquake in Japan was a watershed moment, bringing in many first-time volunteers for earthquake response. The 2004 Indian Ocean earthquake and tsunami attracted a large number of volunteers worldwide, deployed by non-governmental organizations, government agencies, and the United Nations.[21][22]

During the 2012 hurricane Sandy emergency, Occupy Sandy volunteers formed a laterally organized rapid-response team that provided much needed help during and after the storm, from food to shelter to reconstruction.  It is an example of mutualism at work, pooling resources and assistance and leveraging social media.

Resource poor schools around the world rely on government support or on efforts from volunteers and private donations, in order to run effectively. In some countries, whenever the economy is down, the need for volunteers and resources increases greatly.[23] School systems offer many volunteer opportunities with minimal requirements. Whether one is a high school or TEFL (Teaching English as a Foreign Language) graduate or college student, most schools require just voluntary and selfless effort.[24]

Much like the benefits of any type of volunteering there are great rewards for the volunteer, student, and school. In addition to intangible rewards, volunteers can add relevant experience to their resumes. Volunteers who travel to assist may learn foreign culture and language. ""Volunteering can give the students the sufficient experience in order to support and strengthen their CVs and resumes.""[25]

Volunteering in schools can be an additional teaching guide for the students and help to fill the gap of local teachers. Cultural and language exchange during teaching and other school activities can be the most essential learning experience for both students and volunteers.[24]

Benefacto, a volunteering brokerage, describe corporate volunteering as ""Companies giving their employees an allowance of paid time off annually, which they use to volunteer at a charity of their choice.""[26]

A majority of the companies at the Fortune 500 allow their employees to volunteer during work hours. These formalized Employee Volunteering Programs (EVPs), also called Employer Supported Volunteering (ESV), are regarded as a part of the companies' sustainability efforts and their social responsibility activities.[27] About 40% of Fortune 500 companies provide monetary donations, also known as volunteer grants, to nonprofits as a way to recognize employees who dedicate significant amounts of time to volunteering in the community.[28]

According to the information from VolunteerMatch, a service that provides Employee Volunteering Program solutions, the key drivers for companies that produce and manage EVPs are building brand awareness and affinity, strengthening trust and loyalty among consumers, enhancing corporate image and reputation, improving employee retention, increasing employee productivity and loyalty, and providing an effective vehicle to reach strategic goals.[29]

In April 2015, David Cameron pledged to give all UK workers employed by companies with more 250 staff mandatory three days' paid volunteering leave, which if implemented will generate an extra 360 million volunteering hours a year.[30]

Community volunteering, in the US called ""community service"", refers globally to those who work to improve their local community. This activity commonly occurs through not for profit organizations, local governments and churches; but also encompasses ad-hoc or informal groups such as recreational sports teams.[31]

In some European countries government organisations and non-government organisations provide auxiliary positions for a certain period in institutions like hospitals, schools, memorial sites and welfare institutions. The difference to other types of volunteering is that there are strict legal regulations, what organisation is allowed to engage volunteers and about the period a volunteer is allowed to work in a voluntary position. Due to that fact, the volunteer is getting a limited amount as a pocket money from the government. Organizations having the biggest manpower in Europe are the Voluntary social year (German: Freiwilliges Soziales Jahr), with more than 50.000 volunteers per year, and the Federal volunteers service (German: Bundesfreiwilligendienst), with about 30.000 to 40.000 volunteers per year.[32][33][34]

25,000 volunteers worked at the 2014 Sochi Winter Olympics. They supported the organisers in more than 20 functional areas: meeting guests, assisting navigation, organising the opening and closing ceremonies, organising food outlets, etc. Volunteer applications were open to any nationals of Russia and other countries. The Sochi 2014 Organising Committee received about 200,000 applications, 8 applicants per place. Volunteers received training over the course of more than a year at 26 volunteer centres in 17 cities across Russia. The majority of participants were between 17 and 22 years old. At the same time, 3000 applications were submitted from people over 55 years old. Some of them worked as volunteers during the 1980 Olympics in Moscow. It was the first experience with such a large-scale volunteer program in the contemporary Russia.

The FIFA World Cup in 2018 was supported by 17,040 volunteers of the Russia 2018 Local Organising Committee.[35]

Volunteering in the context of delivering medical care is referred to as medical volunteering. In general, medical volunteering has been lauded as a ""ethical responsibility to aid the needy. The activities are often offered by both for profit and not for profit associations. Medical volunteers typically participate in unpaid medical volunteer programs in hospitals, clinics, and underserved areas. Typically, these regions are in underdeveloped nations or nations battling natural disasters, sickness, or violence. These activities typically involves volunteer physicians and nurses. Dental volunteering is a part of medical volunteering which predominantly focused on dental care.[36]

In Hinduism, seva means selfless service and is often associated with karma yoga, disciplined action, and bhakti yoga, disciplined devotion. Seva is also connected to other Sanskrit concepts such as dāna (gift giving), karunā (compassion), and preman (kindness). Seva is also performed as a form of ego-transcending spiritual practise known as Sadhana, and plays a large role in modern Hinduism. This is because a key concept in Hinduism is liberation (Moksha) from the cycle of births and deaths (Saṃsāra), and sadhana is the effort one makes to strive for liberation, highlighting the importance of service to others.[37]

In Sikhism, the word seva also means ""to worship, to adore, to pay homage through the act of love."" In the writings of Sikh gurus, these two meanings of seva (service and worship) have been merged. Seva is expected to be a labour of love performed without desire and intention, and with humility.[37]

Designated days, weeks and years observed by a country or as designated by the United Nations to encourage volunteering / community service 

Modern societies share a common value of people helping each other; not only do volunteer acts assist others, but they also benefit the volunteering individual on a personal level.[38] Despite having similar objectives, tension can arise between volunteers and state-provided services. In order to curtail this tension, most countries develop policies and enact legislation to clarify the roles and relationships among governmental stakeholders and their voluntary counterparts; this regulation identifies and allocates the necessary legal, social, administrative, and financial support of each party. This is particularly necessary when some voluntary activities are seen as a challenge to the authority of the state (e.g., on 29 January 2001, President Bush cautioned that volunteer groups should supplement—not replace—government agencies' work).[39]

Volunteering that benefits the state but challenges paid counterparts angers labor unions that represent those who are paid for their volunteer work; this is particularly seen in combination departments, such as volunteer fire departments.

Difficulties in the cross-national aid model of volunteering can arise when it is applied across national borders. The presence of volunteers who are sent from one state to another can be viewed as a breach of sovereignty and showing a lack of respect towards the national government of the proposed recipients. Thus, motivations are important when states negotiate offers to send aid and when these proposals are accepted, particularly if donors may postpone assistance or stop it altogether. Three types of conditionality have evolved:

Some international volunteer organizations define their primary mission as being altruistic: to fight poverty and improve the living standards of people in the developing world, (e.g. Voluntary Services Overseas has almost 2,000 skilled professionals working as volunteers to pass on their expertise to local people so that the volunteers' skills remain long after they return home). When these organizations work in partnership with governments, the results can be impressive. However, when other organizations or individual First World governments support the work of volunteer groups, there can be questions as to whether the organizations' or governments' real motives are poverty alleviation. Instead, a focus on creating wealth for some of the poor or developing policies intended to benefit the donor states is sometimes reported.[40] Many low-income countries' economies suffer from industrialization without prosperity and investment without growth. One reason for this is that development assistance guides many Third World governments to pursue development policies that have been wasteful, ill-conceived, or unproductive; some of these policies have been so destructive that the economies could not have been sustained without outside support.[41]

Indeed, some offers of aid have distorted the general spirit of volunteering, treating local voluntary action as contributions in kind, i.e., existing conditions requiring the modification of local people's behavior in order for them to earn the right to donors' charity. This can be seen as patronizing and offensive to the recipients because the aid expressly serves the policy aims of the donors rather than the needs of the recipients.

Based on a case study in China, Xu and Ngai (2011) revealed that the developing grassroots volunteerism can be an enclave among various organizations and may be able to work toward the development of civil society in the developing countries. The researchers developed a ""Moral Resources and Political Capital"" approach to examine the contributions of volunteerism in promoting the civil society. Moral resource means the available morals could be chosen by NGOs. Political capital means the capital that will improve or enhance the NGOs' status, possession or access in the existing political system.[42]

Moreover, Xu and Ngai (2011) distinguished two types of Moral Resources: Moral Resource-I and Moral Resource-II (ibid).

Thanks to the intellectual heritage of Blau and Duncan (1967), two types of political capital were identified:

Obviously, ""Moral resource-I itself contains the self-determination that gives participants confidence in the ethical beliefs they have chosen"",[46] almost any organizations may have Moral Resource-I, while not all of them have the societal recognized Moral Resource-II. However, the voluntary service organizations predominantly occupy Moral Resource-II because a sense of moral superiority makes it possible that for parties with different values, goals and cultures to work together in promoting the promotion of volunteering. Thus the voluntary service organizations are likely to win the trust and support of the masses as well as the government more easily than will the organizations whose morals are not accepted by mainstream society. In other words, Moral Resource II helps the grassroots organizations with little Political Capital I to win Political Capital-II, which is a crucial factor for their survival and growth in developing countries such as China. Therefore, the voluntary service realm could be an enclave of the development of civil society in the developing nations.[42]

Volunteering for community service as part of a college curriculum (service-learning) provides opportunities for students to surround themselves with new people which helps them learn how to work together as a group, improve teamwork and relational skills, reduce stereotypes, and increases appreciation of other cultures.[9] Students participating in service-learning programs are shown to have more positive attitudes toward self, attitudes toward school and learning, civic engagement, social skills, and academic performance.[47][48] They are also more likely to complete their degree.[49][50]

Volunteers are observed to have a reduced mortality risk compared to non-volunteers.[51] Therefore, the various types of work as a volunteer and psychological effects of such altruistic work may produce enough side-effects to contribute to a longer and more fulfilling life. A systematic review shows that adults over age of 65 years who volunteer may experience improved physical and mental health and potentially reduced mortality.[52]

A worldwide survey was conducted in a study, suggesting that people who experience the highest levels of happiness are the most successful in terms of close relationships and volunteer work.[53] In comparison, charity in the form of monetary donations, which is another form of altruism (volunteering being one of them) is also known to have a similar effect.[54][55] Another study finds that helping others is associated with higher levels of mental health, above and beyond the benefits of receiving help.[56] This is true across age groups. Observational evidence indicates that volunteering helps improve the mental health of adolescents.[57] Moreover, on the subject of service-learning, undergraduate students who volunteered 1 to 9 hours per week were less likely to feel depressed than students who did not volunteer.[58] Among people aged 65 years old or above, volunteering may reduce the risk of depression.[52]

Volunteering in the aftermath of the 2011 Christchurch Earthquake was found to build social capital, increasing the social connectedness of individuals as well as community wellbeing. The researchers suggested healthcare professionals could prescribe volunteering to improve the health of individuals.[59]

In the United States, statistics on volunteering have historically been limited, according to volunteerism expert Susan J. Ellis.[60] In 2013, the U.S. Current Population Survey included a volunteering supplement which produced statistics on volunteering.[61]

In the 1960s, Ivan Illich offered an analysis of the role of American volunteers in Mexico in his speech entitled ""To Hell With Good Intentions"". His concerns, along with those of critics such as Paulo Freire and Edward Said, revolve around the notion of altruism as an extension of Christian missionary ideology. In addition, he mentions the sense of responsibility/obligation as a factor, which drives the concept of noblesse oblige—first developed by the French aristocracy as a moral duty derived from their wealth. Simply stated, these apprehensions propose the extension of power and authority over indigenous cultures around the world. Recent critiques of volunteering come from Westmier and Kahn (1996) and bell hooks (née Gloria Watkins) (2004). Also, Georgeou (2012) has critiqued the impact of neoliberalism on international aid volunteering.

The field of the medical tourism (referring to volunteers who travel overseas to deliver medical care) has recently attracted negative criticism when compared to the alternative notion of sustainable capacities, i.e., work done in the context of long-term, locally-run, and foreign-supported infrastructures.  A preponderance of this criticism appears largely in scientific and peer-reviewed literature.[62][63][64] Recently, media outlets with more general readerships have published such criticisms as well.[65] This type of volunteering is pejoratively referred to as ""medical voluntourism"".[66]

Another problem noted with volunteering is that it can be used to replace low paid entry positions. This can act to decrease social mobility, with only those capable of affording to work without payment able to gain the experience.[67] Trade unions in the United Kingdom (UK) have warned that long term volunteering is a form of exploitation, used by charities to avoid minimum wage legislation.[68] Some sectors now expect candidates for paid roles to have undergone significant periods of volunteer experience whether relevant to the role or not, setting up 'Volunteer Credentialism'.[69]

Volunteers can be exposed to stressful situations and attitudes, which can cause them to suffer from burnout which in turn reduces their activism and overall well-being.[70] There is also a clear evidence that volunteering can become a moral obligation that prompts feelings of guilt when not performed.[71]

Comprehensive Employment and Training Act
"
Non-Profit Management,"

A nonprofit organization (NPO), also known as a nonbusiness entity,[1] nonprofit institution,[2] not-for-profit organisation,[3] or simply a nonprofit,[a] is a non-governmental (private) legal entity organized and operated for a collective, public or social benefit, as opposed to an entity that operates as a business aiming to generate a profit for its owners. A nonprofit organization is subject to the non-distribution constraint: any revenues that exceed expenses must be committed to the organization's purpose, not taken by private parties. Depending on the local laws, charities are regularly organized as non-profits. A host of organizations may be nonprofit, including some political organizations, schools, hospitals, business associations, churches, foundations, social clubs, and consumer cooperatives. Nonprofit entities may seek approval from governments to be tax-exempt, and some may also qualify to receive tax-deductible contributions, but an entity may incorporate as a nonprofit entity without having tax-exempt status.

Key aspects of nonprofits are their ability to fulfill their mission with respect to accountability, integrity, trustworthiness, honesty, and openness to every person who has invested time, money, and faith into the organization. Nonprofit organizations are accountable to the donors, founders, volunteers, program recipients, and the public community. Theoretically, for a nonprofit that seeks to finance its operations through donations, public confidence is a factor in the amount of money that a nonprofit organization is able to raise. Presumably, the more a nonprofit focuses on their mission, the more public confidence they will gain. This will result in more money for the organization.[1] The activities a nonprofit is partaking in can help build the public's confidence in nonprofits, as well as how ethical the standards and practices are.

There is an important distinction in the US between non-profit and not-for-profit organizations (NFPOs);  while an NFPO does not profit its owners, and money goes into running the organization, it is not required to operate for the public good. An example is a sports club, whose purpose is its members' enjoyment.[4] The names used and precise regulations vary from one jurisdiction to another.

Nonprofit organizations are not driven by generating profit, but they must bring in enough income to pursue their social goals. Nonprofits are able to raise money in different ways. This includes income from donations from individual donors or foundations; sponsorship from corporations; government funding; programs, services or merchandise sales, and investments.[5] Each NPO is unique in which source of income works best for them. With an increase in NPOs since 2010, organizations have adopted competitive advantages to create revenue for themselves to remain financially stable. Donations from private individuals or organizations can change each year and government grants have diminished. With changes in funding from year to year, many nonprofit organizations have been moving toward increasing the diversity of their funding sources. For example, many nonprofits that have relied on government grants have started fundraising efforts to appeal to individual donors.[6]

Most nonprofits have staff that work for the company, possibly using volunteers to perform the nonprofit's services under the direction of the paid staff. Nonprofits must be careful to balance the salaries paid to staff against the money paid to provide services to the nonprofit's beneficiaries. Organizations whose salary expenses are too high relative to their program expenses may face regulatory scrutiny.[7]

A second misconception is that nonprofit organizations may not make a profit. Although the goal of nonprofits is not specifically to maximize profits, they still have to operate as a fiscally responsible business. They must manage their income (both grants and donations, and income from services) and expenses so as to remain a fiscally viable entity. Nonprofits have the responsibility of focusing on being professional and financially responsible, replacing self-interest and profit motive with mission motive.[8]

Though nonprofits are managed differently from for-profit businesses, they have felt pressure to be more businesslike. To combat private and public business growth in the public service industry, nonprofits have modeled their business management and mission, shifting their reason of existing to establish sustainability and growth.[9]

Setting effective missions is a key for the successful management of nonprofit organizations.[10] There are three important conditions for effective mission: opportunity, competence, and commitment.[10]

One way of managing the sustainability of nonprofit organizations is to establish strong relations with donor groups.[10] This requires a donor marketing strategy, something many nonprofits lack.[10] Nonprofit organizations need to motivate staff, maintain and communicate a vision and set a strategic direction, manage change effectively, and provide a safe working environment for employees, volunteers and visitors. These issues are comparable to those affecting a commercial business but factors which place a nonprofit organization at risk have been identified such as lack of direction or clear purpose, over-centralized management and decision-making, and lack of engagement with staff.[11]

Nonprofit organizations provide public goods that are undersupplied by government.[12] NPOs have a wide diversity of structures and purposes. For legal classification, there are, nevertheless, some elements of importance:

Some of the above must be (in most jurisdictions in the US at least) expressed in the organization's charter of establishment or constitution. Others may be provided by the supervising authority at each particular jurisdiction.

While affiliations will not affect a legal status, they may be taken into consideration by legal proceedings as an indication of purpose. Most countries have laws that regulate the establishment and management of NPOs and that require compliance with corporate governance regimes. Most larger organizations are required to publish their financial reports detailing their income and expenditure publicly.

In many aspects, they are similar to corporate business entities though there are often significant differences. Both not-for-profit[clarification needed] and for-profit corporate entities must have board members, steering-committee members, or trustees who owe the organization a fiduciary duty of loyalty and trust. A notable exception to this involves churches, which are often not required to disclose finances to anyone, including church members.[13]

In the United States, nonprofit organizations are formed by filing bylaws, articles of incorporation, or both in the state in which they expect to operate. The act of incorporation creates a legal entity enabling the organization to be treated as a distinct body (corporation) by law and to enter into business dealings, form contracts, and own property as individuals or for-profit corporations can.

Nonprofits can have members, but many do not. The nonprofit may also be a trust or association of members. The organization may be controlled by its members who elect the board of directors, board of governors or board of trustees. A nonprofit may have a delegate structure to allow for the representation of groups or corporations as members. Alternatively, it may be a non-membership organization and the board of directors may elect its own successors.

The two major types of nonprofit organizations are membership and board-only. A membership organization elects the board and has regular meetings and the power to amend the bylaws. A board-only organization typically has a self-selected board and a membership whose powers are limited to those delegated to it by the board. A board-only organization's bylaws may even state that the organization does not have any membership, although the organization's literature may refer to its donors or service recipients as 'members'; examples of such organizations are FairVote[14][15] and the National Organization for the Reform of Marijuana Laws.[16] The Model Nonprofit Corporation Act imposes many complexities and requirements on membership decision-making.[17] Accordingly, many organizations, such as the Wikimedia Foundation,[18] have formed board-only structures. The National Association of Parliamentarians has generated concerns about the implications of this trend for the future of openness, accountability, and understanding of public concerns in nonprofit organizations. Specifically, they note that nonprofit organizations, unlike business corporations, are not subject to market discipline for products and shareholder discipline of their capital; therefore, without membership control of major decisions such as the election of the board, there are few inherent safeguards against abuse.[19][20] A rebuttal to this might be that as nonprofit organizations grow and seek larger donations, the degree of scrutiny increases, including expectations of audited financial statements.[21] A further rebuttal might be that NPOs are constrained, by their choice of legal structure, from financial benefit as far as distribution of profit to members and directors is concerned.

In many countries, nonprofits may apply for tax-exempt status, so that the organization itself may be exempt from income tax and other taxes. In the United States, to be exempt from federal income taxes, the organization must meet the requirements set forth in the Internal Revenue Code (IRC). Granting nonprofit status is done by the state, while granting tax-exempt designation (such as IRC 501(c)) is granted by the federal government via the IRS. This means that not all nonprofits are eligible to be tax-exempt.[22] For example, employees of non-profit organizations pay taxes from their salaries, which they receive according to the laws of the country. NPOs use the model of a double bottom line in that furthering their cause is more important than making a profit, though both are needed to ensure the organization's sustainability.[23][24] An advantage of nonprofits registered in the UK is that they benefit from some reliefs and exemptions. Charities and nonprofits are exempt from Corporation Tax as well as the trustees being exempt from Income Tax.[25] There may also be tax relief available for charitable giving, via Gift Aid, monetary donations, and legacies.[26]

According to the National Center for Charitable Statistics (NCCS), there are more than 1.5 million nonprofit organizations registered in the United States, including public charities, private foundations, and other nonprofit organizations. Private charitable contributions increased for the fourth consecutive year in 2017 (since 2014), at an estimated $410.02 billion. Out of these contributions, religious organizations received 30.9%, education organizations received 14.3%, and human services organizations received 12.1%.[27] Between September 2010 and September 2014, approximately 25.3% of Americans over the age of 16 volunteered for a nonprofit.[28]

In the United States, both nonprofit organizations and not-for-profit organizations are tax-exempt. There are various types of nonprofit exemptions, such as 501(c)(3) organizations that are a religious, charitable, or educational-based organization that does not influence state and federal legislation, and 501(c)(7) organizations that are for pleasure, recreation, or another nonprofit purpose.[29]

There is an important distinction in the US between non-profit and not-for-profit organizations (NFPOs);  while an NFPO does not profit its owners, and money goes into running the organization, it is not required to operate for the public good. An example is a club, whose purpose is its members' enjoyment.[4] Other examples of NFPOs include: credit unions, sports clubs, and advocacy groups. Nonprofit organizations provide services to the community; for example aid and development programs, medical research, education, and health services. It is possible for a nonprofit to be both member-serving and community-serving.[citation needed]

When selecting a domain name, NPOs often use one of the following: .org, the country code top-level domain of their respective country, or the .edu top-level domain (TLD), to differentiate themselves from more commercial entities, which typically use .com.

In the traditional domain noted in RFC 1591, .org is for ""organizations that didn't fit anywhere else"" in the naming system, which implies that it is the proper category for non-commercial organizations if they are not governmental, educational, or one of the other types with a specific TLD. It is not designated specifically for charitable organizations or any specific organizational or tax-law status, but encompasses anything that is not classifiable as another category. Currently, no restrictions are enforced on registration of .com or .org, so one can find organizations of all sorts in either of those domains, as well as other top-level domains including newer, more specific ones which may apply to particular sorts of organization including .museum for museums and .coop for cooperatives. Organizations might also register by the appropriate country code top-level domain for their country.

In 2020, nonprofit organizations began using microvlogging (brief videos with short text formats) on TikTok to reach Gen Z, engage with community stakeholders, and overall build community.[30] TikTok allowed for innovative engagement between nonprofit organizations and younger generations.[31] During COVID-19, TikTok was specifically used to connect rather than inform or fundraise, as its fast-paced, tailored For You Page separates itself from other social media apps such as Facebook and Twitter. 

Some organizations offer new, positive-sounding alternative terminology to describe the sector. The term civil society organization (CSO) has been used by a growing number of organizations, including the Center for the Study of Global Governance.[32] The term citizen sector organization (CSO) has also been advocated to describe the sector – as one of citizens, for citizens – by organizations including Ashoka: Innovators for the Public.[33] Advocates argue that these terms describe the sector in its own terms, without relying on terminology used for the government or business sectors. However, use of terminology by a nonprofit of self-descriptive language that is not legally compliant risks confusing the public about nonprofit abilities, capabilities, and limitations.[34]

Founder's syndrome is an issue organizations experience as they expand. Dynamic founders, who have a strong vision of how to operate the project, try to retain control of the organization, even as new employees or volunteers want to expand the project's scope or change policy.[35]

Financial mismanagement is a particular problem with NPOs because the employees are not accountable to anyone who has a direct stake in the organization. For example, an employee may start a new program without disclosing its complete liabilities. The employee may be rewarded for improving the NPO's reputation, making other employees happy, and attracting new donors. Liabilities promised on the full faith and credit of the organization but not recorded anywhere constitute accounting fraud. But even indirect liabilities negatively affect the financial sustainability of the NPO, and the NPO will have financial problems unless strict controls are instated.[36] Some commenters have argued that the receipt of significant funding from large for-profit corporations can ultimately alter the NPO's functions.[37] A frequent measure of an NPO's efficiency is its expense ratio (i.e. expenditures on things other than its programs, divided by its total expenditures). Tax exempt status of NPOs can result in some cases, such as mismanagement, in negative value for society.[38]

There are reports of major labor shortages in the nonprofit sector today, particularly for management positions.[39][40] While many established NPOs are well-funded and comparative to their public sector competitors, many more are independent and must be creative with which incentives they use to attract and maintain people. The initial interest for many is the remuneration package, though many who have been questioned after leaving an NPO have reported that it was stressful work environments and the workload.[41]

Public- and private-sector employment have, for the most part, been able to offer more to their employees than most nonprofit agencies throughout history. Either in the form of higher wages, more comprehensive benefit packages, or less tedious work, the public and private sectors have enjoyed an advantage over NPOs in attracting employees. Traditionally, the NPO has attracted mission-driven individuals who want to assist their chosen cause. Compounding the issue is that some NPOs do not operate in a manner similar to most businesses, or only seasonally. This leads many young and driven employees to forego NPOs in favor of more stable employment. Today, however, nonprofit organizations are adopting methods used by their competitors and finding new means to retain their employees and attract the best of the newly minted workforce.[42]

One article states that most nonprofits will never be able to match the pay of the private sector[43] and therefore should focus their attention on benefits packages, incentives and implementing pleasurable work environments. A good environment is ranked higher than salary and pressure of work.[40] NPOs are encouraged to pay as much as they are able and offer a low-stress work environment that the employee can associate him or herself positively with. Other incentives that should be implemented are generous vacation allowances or flexible work hours.[44]
"
Arts Services,"

The arts or creative arts are a vast range of human practices of creative expression, storytelling, and cultural participation. The arts encompass diverse and plural modes of thinking, doing, and being in an extensive range of media. Both dynamic and a characteristically constant feature of human life have developed into stylized and intricate forms. This is achieved through sustained and deliberate study, training, or theorizing within a particular tradition, generations, and even between civilizations. The arts are a vehicle through which human beings cultivate distinct social, cultural, and individual identities while transmitting values, impressions, judgements, ideas, visions, spiritual meanings, patterns of life, and experiences across time and space.

Prominent examples of the arts include: visual arts (including architecture, ceramics, drawing, filmmaking, painting, photography, and sculpting), literary arts (including fiction, drama, poetry, and prose), and performing arts (including dance, music, and theatre). They can employ skill and imagination to produce objects and performances, convey insights and experiences, and construct new environments and spaces.

The arts can refer to common, popular, or everyday practices as well as more sophisticated, systematic, or institutionalized ones. They can be discrete and self-contained or combine and interweave with other art forms, such as combining artwork with the written word in comics. They can also develop or contribute to some particular aspect of a more complex art form, as in cinematography. By definition, the arts themselves are open to being continually redefined. The practice of modern art, for example, is a testament to the shifting boundaries, improvisation and experimentation, reflexive nature, and self-criticism or questioning that art and its conditions of production, reception, and possibility can undergo.

As both a means of developing capacities of attention and sensitivity and ends in themselves, the arts can simultaneously be a form of response to the world. It is a way to transform our responses and what we deem worthwhile goals or pursuits. From prehistoric cave paintings to ancient and contemporary forms of ritual to modern-day films, art has served to register, embody, and preserve our ever-shifting relationships with each other and the world.

The arts are considered various practices or objects done by people with skill, creativity, and imagination across cultures and history, viewed as a group.[1] These activities include painting, sculpture, music, theatre, literature, and more.[2] Art refers to the way of doing or applying human creative skills, typically in visual form.[3][4]

In Ancient Greece, art and craft were referred to by the word techne. Ancient Greek art brought the veneration of the animal form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions. Ancient Roman art depicted gods as idealized humans, shown with characteristic distinguishing features, e.g. Zeus' thunderbolt. In Byzantine and Gothic art of the Middle Ages, the dominant church insisted on the expression of Christian themes due to the overlap of church and state.[5] Eastern art has generally worked in style akin to Western medieval art, namely a concentration on surface patterning and local colour (meaning the plain colour of an object, such as basic red for a red robe, rather than the modulations of that colour brought about by light, shade, and reflection). A characteristic of this style is that local colour is defined by an outline (a contemporary equivalent is the cartoon). This is evident, for example, in the art of India, Tibet, and Japan. Islamic art avoids the representation of living beings, particularly humans and other animals, in religious contexts.[6] It instead expresses religious ideas through calligraphy and geometrical designs.[7]

In the Middle Ages, liberal arts were taught in European universities as part of the Trivium, an introductory curriculum involving grammar, rhetoric, and logic,[8] and of the Quadrivium, a curriculum involving the ""mathematical arts"" of arithmetic, geometry, music, and astronomy.[9] In modern academia, the arts can be grouped with, or as a subset of, the humanities.[10]

The arts have been classified as seven: painting, architecture, sculpture, literature, music, performing, and cinema.[11] Some view literature, painting, sculpture, and music as the central four arts, of which the others are derivative; drama is literature with acting, dance is music expressed through motion, and song is music with literature and voice.[12][failed verification] Film is sometimes called the ""eighth"" and comics the ""ninth art"" in Francophone scholarship, adding to the traditional ""Seven Arts"".[13][14] Cultural fields like gastronomy are only sometimes considered as arts.[15]

Architecture is the art and science of designing buildings and structures. A wider definition would include the design of the built environment, from the macro level of urban planning, urban design, and landscape architecture, to the micro level of creating furniture.[16] The word architecture comes from the Latin architectūra, from architectus ""master builder, director of works.""[16][17] Architectural design usually must address feasibility and cost for the builder, as well as function and aesthetics for the user.[18]

In modern usage, architecture is the art and discipline of creating or inferring an implied or apparent plan for a complex object or system.[19] Some types of architecture manipulate space, volume, texture, light, shadow, or abstract elements, to achieve pleasing aesthetics.[20] Architectural works may be seen as cultural and political symbols, or works of art. The role of the architect, though changing, has been central to the design and implementation of pleasingly built environments, in which people live.[21]

Ceramic art is art made from ceramic materials (including clay),[22] which may take forms such as pottery, tile, figurines, sculpture, and tableware. While some ceramic products are considered fine art, others are considered decorative, industrial, or applied art objects. Ceramics may also be considered artefacts in archaeology. Ceramic art can be made by one person or by a group of people. In a pottery or ceramic factory, a group of people design, manufacture, and decorate the pottery. Some pottery is regarded as art pottery.[23] In a one-person pottery studio, ceramists or potters produce studio pottery. Ceramics excludes glass and mosaics made from glass tesserae.[24]

Conceptual art is art wherein the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic and material concerns.[25]
The inception of the term in the 1960s referred to a strict and focused practice of idea-based art that defied traditional visual criteria associated with the visual arts in its presentation as text.[26] Through its association with the Young British Artists and the Turner Prize during the 1990s,[27] its popular usage, particularly in the United Kingdom, developed as a synonym for all contemporary art that does not practice the traditional skills of painting and sculpture.[28]

Drawing is a means of making an image using any of a wide variety of tools and techniques. It generally involves making marks on a surface by applying pressure from a tool, or moving a tool across a surface. Common tools are graphite pencils, pen and ink, inked brushes, wax coloured pencils, crayons, charcoals, pastels, and markers. Digital tools with similar effects are also used. The main techniques used in drawing are line drawing, hatching, cross-hatching, random hatching, scribbling, stippling, and blending. An artist who excels in drawing is referred to as a drafter, draftswoman, or draughtsman.[29] Drawing can be used to create art used in cultural industries such as illustrations, comics, and animation. Comics are often called the ""ninth art"" (le neuvième art) in Francophone scholarship, adding to the traditional ""Seven Arts"".[13]

Painting is considered to be a form of self-expression.[30] Drawing, gesture (as in gestural painting), composition, narration (as in narrative art), or abstraction (as in abstract art), among other aesthetic modes, may serve to manifest the expressive and conceptual intention of the practitioner.[31] Paintings can be a wide variety of topics, such as photographic,[32] abstract,[33] narrative,[34] symbolistic (Symbolist art),[35] emotive (Expressionism),[36] or political in nature (Artivism).[37] Some modern painters incorporate different materials, such as sand, cement, straw, wood, or strands of hair, for their artwork texture. Examples of this are the works of Jean Dubuffet or Anselm Kiefer.[38][39]

Photography as an art form refers to photographs that are created in accordance with the creative vision of the photographer. Art photography stands in contrast to photojournalism, which provides a visual account of news events, and commercial photography, the primary focus of which is to advertise products or services.[40]

Sculpture is the branch of the visual arts that operates in three dimensions. It is one of the plastic arts. Durable sculptural processes originally used carving (the removal of material) and modelling (the addition of material, such as clay), in stone, metal, ceramics, wood, and other materials, but shifts in sculptural processes have led to almost complete freedom of materials and processes following modernism. A wide variety of materials may be worked by removal such as carving, assembled by welding or modelling, or moulded or cast.[41][42][43]

Literature (also known as literary arts or language arts) is literally ""acquaintance with letters"", as in the first sense given in the Oxford English Dictionary. The noun ""literature"" comes from the Latin word littera, meaning ""an individual written character (letter)."" The term has generally come to identify a collection of writings, which in Western culture are mainly prose (both fiction and non-fiction), drama, and poetry. In much, if not all, of the world, artistic linguistic expression can be oral as well and include such genres as epic, legend, myth, ballad, other forms of oral poetry, and folktales. Comics, the combination of drawings or other visual arts with narrating literature, are called the ""ninth art"" (le neuvième art) in Francophone scholarship.[13]

Performing arts comprise dance, music, theatre, opera, mime, and other art forms in which human performance is the principal product. Performing arts are distinguished by this performance element in contrast with disciplines such as visual and literary arts, where the product is an object that does not require a performance to be observed and experienced. Each discipline in the performing arts is temporal in nature, meaning the product is performed over a period of time. Products are broadly categorized as being either repeatable (for example, by script or score) or improvised for each performance.[44] Artists who participate in these arts in front of an audience are called performers, including actors, magicians, comedians, dancers, musicians, and singers. Performing arts are also supported by the services of other artists or essential workers, such as songwriting and stagecraft. Performers adapt their appearance with tools such as costumes and stage makeup.[45]

Dance generally refers to human movement, either used as a form of expression or presented in a social, spiritual, or performance setting.[46][47][a] Choreography is the art of making dances,[52] and the person who does this is called a choreographer.[53] Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic, and moral constraints and range from functional movement (such as folk dance) to codified virtuoso techniques such as ballet. In sports: gymnastics, figure skating, and synchronized swimming are dance disciplines. In martial arts, ""kata"" is compared to dances.[54]

Music is defined as an art form whose medium is a combination of sounds.[55] Though scholars agree that music generally consists of a few core elements, their exact definitions are debated.[56] Commonly identified aspects include pitch (which governs melody and harmony), duration (including rhythm and tempo), intensity (including dynamics), and timbre.[57] Though considered a cultural universal, definitions of music vary wildly throughout the world as they are based on diverse views of nature, the supernatural, and humanity.[58] Music is differentiated into composition and performance, while musical improvisation may be regarded as an intermediary tradition.[59] Music can be divided into genres and subgenres, although the dividing lines and relationships between genres are subtle, open to individual interpretation, and controversial.[60]

Theatre or theater (from Greek theatron (θέατρον); from theasthai, ""behold""[61]) is the branch of the performing arts concerned with acting out stories in front of an audience using combinations of speech, gesture, music, dance, sound, and spectacle.[62] In addition to the standard narrative dialogue style, theatre takes such forms as opera, ballet, mime, kabuki, classical Indian dance, and Chinese opera.[63][64][65][66]

Areas exist in which artistic works incorporate multiple artistic fields, such as film, opera, and performance art. While opera is often categorized as the performing arts of music, the word itself is Italian for ""works"", because opera combines artistic disciplines into a singular artistic experience. In a traditional opera, the work uses the following: the sets, costumes, acting, the libretto, singers and an orchestra.[67]

The composer Richard Wagner recognized the fusion of so many disciplines into a single work of opera, exemplified by his cycle Der Ring des Nibelungen (""The Ring of the Nibelung""). He did not use the term opera for his works, but instead Gesamtkunstwerk (""synthesis of the arts""), sometimes referred to as ""music drama"" in English, emphasizing the literary and theatrical components, which were as important as the music. Classical ballet is another form that emerged in the 17th century in which orchestral music is combined with dance.[68]

Other works in the late 19th, 20th, and 21st centuries have fused other disciplines in creative ways, such as performance art. Performance art is a performance over time that combines any number of instruments, objects, and art within a predefined or less well-defined structure, some of which can be improvised. Performance art may be scripted, unscripted, random, or carefully organized—even audience participation may occur. John Cage is regarded by many as a performance artist rather than a composer, although he preferred the latter term. He did not compose for traditional ensembles. Cage's composition Living Room Music, composed in 1940, is a quartet for unspecified instruments, really non-melodic objects, that can be found in the living room of a typical house, hence the title.[69]

The applied arts are the application of design and decoration to everyday, functional objects to make them aesthetically pleasing.[70] The applied arts include fields such as industrial design, illustration, and commercial art.[71] The term ""applied art"" is used in distinction to the fine arts, where the latter is defined as arts that aim to produce objects that are beautiful or provide intellectual stimulation but have no primary everyday function. In practice, the two often overlap.

Video games are multidisciplinary works that include non-controversially artistic elements such as visuals and sound, as well as an emergent experience from the nature of their interactivity.  Within the video game community, debates surround whether video games should be classified as an art form and whether game developers—AAA or indie—should be classified as artists.[72] Hideo Kojima, a video game designer considered a gaming auteur, argued in 2006 that video games are a type of service rather than an art form.[73][74] In the social sciences, cultural economists show how playing video games is conducive to involvement in more traditional art forms.[75] In 2011, the National Endowment of the Arts included video games in its definition of a ""work of art"",[76] and the Smithsonian American Art Museum presented an exhibit titled The Art of the Video Game in 2012.[77]

Art criticism is the discussion or evaluation of art.[78][79][80] Art critics usually criticize art in the context of aesthetics or the theory of beauty.[79][80] A goal of art criticism is the pursuit of a rational basis for art appreciation[78][79][80] but it is questionable whether such criticism can transcend prevailing sociopolitical circumstances.[81]

The variety of artistic movements has resulted in a division of art criticism into different disciplines, which may each use different criteria for their judgements.[80][82] The most common division in the field of criticism is between historical criticism and evaluation, a form of art history, and contemporary criticism of work by living artists.[78][79][80]

Despite perceptions that criticism is a lower-risk activity than making art, opinions of current art are liable to corrections with the passage of time.[79] Critics of the past can be ridiculed for dismissing artists now venerated (like the early work of the Impressionists).[80][83][84] Some art movements themselves were named disparagingly by critics, with the name later adopted as a badge of honour by the artists of the style with the original negative meaning forgotten, e.g. Impressionism and Cubism.[83][85][86] Artists have had an uneasy relationship with their critics. Artists usually need positive opinions from critics for their work to be viewed and purchased.[79][87]

Many variables determine judgement of art such as aesthetics, cognition or perception. Aesthetic, pragmatic, expressive, formalist, relativist, processional, imitation, ritual, cognition, mimetic, and postmodern theories, are some of the many theories to criticize and appreciate art. Art criticism and appreciation can be subjective based on personal preference toward aesthetics and form, or on the elements and principles of design and by social and cultural acceptance.[88]

Arts in education is a field of educational research and practice informed by investigations into learning through arts experiences. In this context, the arts can include performing arts education (dance, drama, music), literature and poetry, storytelling, visual arts education in film, craft, design, digital art, media and photography.[89]

A strong relationship between the arts and politics, particularly between various kinds of art and power, occurs across history and cultures.[90] As they respond to events and politics, the arts take on political as well as social dimensions, becoming themselves a focus of controversy and a force of political and social change.[91]

One observation is that an artist has a free spirit. For instance Pushkin, a well-regarded writer,[92] attracted the irritation of Russian officialdom and particularly the Tsar, since he ""instead of being a good servant of the state in the rank and file of the administration and extolling conventional virtues in his vocational writings (if write he must), composed extremely arrogant and extremely independent and extremely wicked verse in which dangerous freedom of thought was evident in the novelty of his versification, in the audacity of his sensual fancy, and in his propensity for making fun of major and minor tyrants.""[92]

Artists use their work to express their political views and promote social change, from influencing negatively in the form of hate speech to influencing positively through artivism.[93] Governments use art, or propaganda, to promote their own agendas.[94]
"
Sports Management Services,"Sport management is the field of business dealing with sports and recreation.[1] Sports management involves any combination of skills that correspond with planning, organizing, directing, controlling, budgeting, leading, or evaluating of any organization or business within the sports field.[2] The field of sport management has its origins in physical education departments. The discipline has evolved to incorporate history and sociology.[3] Development of sport management has also extended to esport management growing to a $4.5 billion dollar industry as of 2018.[4] The opportunities in sport management have expanded to include sports marketing, sports media analytics, sports sponsorships and sports facilities management.[5]

Bachelor's and master's degrees in sport management are offered by many colleges and universities. Some research on the impact of sport degrees has focused on sport science.[6] In the United States, the top five universities that offer a degree in sport management are Rice University, University of Michigan, University of Florida, University of Miami and University of Massachusetts Amherst, which has the oldest Sport Management Program in the World.[7]

There are various degrees you can receive for Sport Management. A bachelor of science in Sport Management, bachelor of business administration in Sport Management, an MBA in Sport Management and a Ph.D in Sport Management.[8]

American sport management roles pay an average of $41,645 annually.[9] In America, jobs in sport management include working for professional sports leagues like the NFL, NBA, MLB, NHL, MLS, and other professional or non-professional sport leagues in terms of marketing, health, and promotions.
"
Fitness Coaching,"A professional fitness coach is a professional in the field of fitness and exercise, most often instruction (fitness instructor), including professional sports club's fitness trainers and aerobics and yoga instructors and authors of fitness instruction books or manuals.

Fitness topics may also include nutrition, weight-loss, and self-help. Fitness careers are distinguished from exercise science careers such as athletic training, however the various types of fitness certifications have more and more in common: the, ""distinctions...have become blurred, with more similarities than differences given the common background that all fitness professionals must possess.""[1]

Fitness professionals screen participants for exercise programs, evaluate various fitness components, prescribe exercise to improve these components, and may also help clients with specific or chronic conditions.[1] Fitness professionals help challenge an individual by increasing their performance, as compared to when a person would work out on their own. They also teach new workouts, how to improve their form, performance and help set and achieve goals. The key roles and duties of a fitness professional are to: motivate, assist clients and measure heart rates and body fat levels. Trainers need to be patient, well organized and have time management as well as interpersonal skills.""You are in a helping profession. Although you are not a social worker, psychologist or guidance counselor, neither are you simply a technician with advanced training in exercise science, biomechanics, program design and assessment methodology.""(Jim and Nettie Gavin)[2] Notable fitness professionals or former fitness professionals include Richard Simmons, Susan Powter, John Sitaras and Gov. Arnold Schwarzenegger (Arnold Schwarzenegger's Total Body Workout).

Certified fitness professionals must remain up-to-date on all certifications in order to instruct at particular health clubs and gyms. Often, fitness professionals will have some education in kinesiology, anatomy, and biomechanics to aid in their fitness career.

In Canada, Canadian Fitness Education Services (CFES) provides national fitness leadership program modules to take candidates through the steps in Aquafit, Group Fitness and/or Weight Training Instructor and Personal Trainer national certification.

Personal training, Athletic training, and physical therapy are all technically distinct specialties with different processes and requirements for certification.[3]  In the United States the main certifying agency for personal trainers is ACSM (the American College of Sports Medicine),[4] while the main certifying agency for athletic trainers is NATA (the National Athletic Trainers' Association).  Obtaining certification or licensure as a physical therapist requires that one attend and graduate from a masters or doctoral program in physical therapy.[5]

A coach can help amateur and professional athletes to be successful in a particular sport by teaching them the required skills needed. However, they can coach more than one sport to multiple people. Their role involves identifying athletes strengths and weaknesses as well as those of their opponent.[6] Coaches also improve the physical condition of an athlete to help increase their full performance; improve form, technique, skills and stamina. A coach must be ready to work long and irregular hours including evenings, weekends and holidays. Typically, coaches are required to be a minimum 18 years of age and have a bachelor's degree.[6]

Most national sports teams and professional sportsclubs have professional fitness coaches in order to systematically improve fitness and conditioning.

The median annual pay for a personal trainer in the US as of 2019[update] is $40,390.[7] The highest paid college coach in 2017 was Nick Saban (University of Alabama) who earned $11.1 million.[8] Fitness professionals receive benefits that can include healthcare, paid time off, and a pension. There is also the opportunity to accelerate personal achievements and the option to work full-time or part-time.[9] According to the Bureau of Labor Statistics, ""employment of fitness trainers and instructors is projected to grow 13 percent from 2018 to 2028, much faster than the average for all occupations.”[10]

Fitness professionals need to have at least a high school diploma and in some fields, a bachelor's degree in a related field is required. Additionally, they must be certified in CPR (cardiovascular resuscitation) which is offered by the American Red Cross among many other classes such as First Aid and AED (Automated External Defibrillator). Certification. They also offer lifeguard training, swimming, and water safety.[11]

Other qualifications depending on the specific field can include:
"
Health Promotion Services,"

Health promotion is, as stated in the 1986 World Health Organization (WHO) Ottawa Charter for Health Promotion, the ""process of enabling people to increase control over, and to improve their health.""[1]

The WHO's 1986 Ottawa Charter for Health Promotion and then the 2005 Bangkok Charter for Health Promotion in a Globalized World defines health promotion as ""the process of enabling people to increase control over their health and its determinants, and thereby improve their health"".[2] Health promotion is a multifaceted approach that goes beyond individual behavior change. It encompasses a wide range of social and environmental interventions aimed at addressing health determinants such as income, housing, food security, employment, and quality working conditions.[3][4]

It is important to distinguish between health education and health promotion. Health education refers to structured learning activities aimed at improving health literacy, while health promotion encompasses broader social and environmental interventions designed to support healthy behaviors and lifestyles. The World Health Organization distinguishes between these approaches, emphasizing that health promotion involves not only individual behavior change but also efforts to modify social determinants of health.[5]

Health promotion involves public policy that addresses health determinants such as income, housing, food security, employment, and quality working conditions.[6] More recent work has used the term Health in All Policies (HiAP) to refer to the actions that incorporate health into all public policies. Health promotion is aligned with health equity and can be a focus of non-governmental organizations (NGOs) dedicated to social justice or human rights. Health literacy can be developed in schools, while aspects of health promotion such as breastfeeding promotion can depend on laws and rules of public spaces. One of the Ottawa Charter Health Promotion Action items is infusing prevention into all sectors of society, to that end, it is seen in preventive healthcare rather than a treatment and curative care focused medical model.[citation needed][7]

There is a tendency among some public health officials, governments, and the medical–industrial complex to reduce health promotion to just developing personal skills, also known as health education and social marketing focused on changing behavioral risk factors.[8] However, recent evidence suggests that attitudes about public health policies are less about personal abilities or health messaging than about individuals' philosophical beliefs about morality, politics, and science.[9]

This first publication of health promotion is from the 1974 Lalonde report from the Government of Canada,[10] which contained a health promotion strategy ""aimed at informing, influencing and assisting both individuals and organizations so that they will accept more responsibility and be more active in matters affecting mental and physical health"".[11] Another predecessor of the definition was the 1979 Healthy People report of the Surgeon General of the United States,[10] which noted that health promotion ""seeks the development of community and individual measures which can help... [people] to develop lifestyles that can maintain and enhance the state of well-being"".[12]

At least two publications led to a ""broad empowerment/environmental"" definition of health promotion in the mid-1980s:[10]

The ""American"" definition of health promotion, first promulgated by the American Journal of Health Promotion in the late 1980s, focuses more on the delivery of services with a bio-behavioral approach rather than environmental support using a settings approach. Later the power on the environment over behavior was incorporated. The Health Promotion Glossary 2021 reinforces the international 1986 definition.[citation needed]

The WHO, in collaboration with other organizations, has subsequently co-sponsored international conferences including the 2015 Okanagan Charter on Health Promotion Universities and Colleges.[citation needed]

In November 2019, researchers reported, based on an international study of 27 countries, that caring for families is the main motivator for people worldwide.[16][17]

Health promotion is underpinned by several theoretical frameworks that guide its implementation:

These frameworks provide a foundation for developing effective health promotion strategies and interventions.

Current models of health promotion include the PRECEDE-PROCEED model, which involves planning health promotion interventions based on social and epidemiological assessments, and the Social Cognitive Theory (Bandura, 1986), which emphasizes self-efficacy and the interaction between individuals and their environments. These frameworks enable health professionals to design interventions targeting behavior change at multiple levels—individual, community, and policy.[18]

Health promotion employs various strategies to achieve its goals:

1.Community-based interventions: These involve engaging local communities in identifying health issues and developing solutions.[19]

2.Policy-level approaches: This includes advocating for and implementing policies that support health, such as tobacco control measures or food labeling regulations.[20]

3.Settings-based approach: This strategy focuses on creating health-promoting environments in specific settings like schools, workplaces, and hospitals.[20]

4.Health literacy initiatives: These aim to improve people's ability to access, understand, and use health information to make informed decisions.[20]

5.Social mobilization: This involves bringing together societal and personal influences to raise awareness, deliver resources, and cultivate sustainable community involvement in health promotion.[20]

Global health promotion efforts, such as WHO's ""Health for All"" initiative, highlight the increasing role of digital health interventions. For example, mHealth programs in sub-Saharan Africa have demonstrated success in improving vaccination rates.[21] These technologies allow for real-time health education and preventive care to underserved populations.

Evaluating the effectiveness of health promotion initiatives is crucial for ensuring that resources are used efficiently and that interventions achieve their intended outcomes. However, measuring the impact of health promotion can be challenging due to the complex nature of health determinants and the long-term effects of many interventions.

Health promotion evaluations typically employ a mix of quantitative and qualitative methods:

Several factors complicate the evaluation of health promotion initiatives:

Despite these challenges, numerous studies have demonstrated the effectiveness of health promotion interventions:

While these examples highlight successful interventions, it's important to note that the effectiveness of health promotion initiatives can vary depending on the context, target population, and implementation quality. Ongoing evaluation and adaptation of health promotion strategies remain essential for maximizing their impact on population health.

The WHO's settings approach to health promotion, Healthy Settings, looks at the settings as individual systems that link community participation, equity, empowerment, and partnership to actions that promote health. According to the WHO, a setting is ""the place or social context in which people engage in daily activities in which environmental, organizational, and personal factors interact to affect health and wellbeing.""[27] There are 11 recognized settings in this approach: cities, villages, municipalities and communities, schools, workplaces, markets, homes, islands, hospitals, prisons, and universities.[citation needed][28]

Health promotion in the hospital setting aims to increase health gain by supporting the health of patients, staff, and the community. This is achieved by integrating health promotion concepts, strategies, and values into the culture and organizational structure of the hospital. Specifically, this means setting up a management structure, involving medical and non-medical staff in health promotion communication, devising action plans for health promotion policies and projects, and measuring and measuring health outcomes and impact for staff, patients, and the community.[citation needed]

The International Network of Health Promoting Hospitals and Health Services is the official, international network for the promotion and dissemination of principles, standards, and recommendations for health promotion in the hospital and health services settings.[29]

The process of health promotion works in all settings and sectors where people live, work, play and love. A common setting is the workplace. The focus of health on the work site is that of prevention and the intervention that reduces the health risks of the employee. In 1996, the U.S. Public Health Service issued a report titled ""Physical Activity and Health: A Report of the Surgeon General"" that provided a comprehensive review of the available scientific evidence about the relationship between physical activity and an individual's health status at that time. The report showed that over 60% of Americans were not regularly active and that 25% are not active at all. There is very strong evidence linking physical activity to numerous health improvements. Health promotion can be performed in various locations. Among the settings that have received special attention are the community, health care facilities, schools, and worksites.[30] Worksite health promotion, also known by terms such as ""workplace health promotion"", has been defined as ""the combined efforts of employers, employees and society to improve the health and well-being of people at work"".[31][32] WHO states that the workplace ""has been established as one of the priority settings for health promotion into the 21st century"" because it influences ""physical, mental, economic and social well-being"" and ""offers an ideal setting and infrastructure to support the promotion of health of a large audience"".[33]

Worksite health promotion programs (also called ""workplace health promotion programs"", ""worksite wellness programs"", or ""workplace wellness programs"") include adequate sleep,[34] cooking classes,[35] exercise,[34][36] nutrition,[35] physical activity,[37][38][39] smoking cessation,[34][35][40] stress management,[citation needed][35][41] and, weight loss.[42]

According to the Centers for Disease Control and Prevention (CDC), ""Regular physical activity is one of the most effective disease prevention behaviors.""[43] Physical activity programs reduce feelings of anxiety and depression, reduce obesity (especially when combined with an improved diet), reduce risk of chronic diseases including cardiovascular disease, high blood pressure, and type 2 diabetes; and finally improve stamina, strength, and energy.[citation needed]

Reviews and meta-analyses published between 2005 and 2008 that examined the scientific literature on worksite health promotion programs include the following:

A study conducted by the World Health Organization and the International Labour Organization found that exposure to long working hours is the occupational risk factor with the largest attributable burden of disease, i.e. an estimated 745,000 fatalities from ischemic heart disease and stroke events in 2016.[50] This landmark study established a new global policy argument and agenda for health promotion on psychosocial risk factors (including psychosocial stress) in the workplace setting.
"
Physical Therapy Services,"

Physical therapy (PT), also known as physiotherapy, is a healthcare profession, as well as the care provided by physical therapists who promote, maintain, or restore health through patient education, physical intervention, disease prevention, and health promotion. Physical therapist is the term used for such professionals in the United States, and physiotherapist is the term used in many other countries.

The career has many specialties including musculoskeletal, orthopedics, cardiopulmonary, neurology, endocrinology, sports medicine, geriatrics, pediatrics, women's health, wound care and electromyography. PTs practice in many settings, both public and private.[1]

In addition to clinical practice, other aspects of physical therapy practice include research, education, consultation, and health administration. Physical therapy is provided as a primary care treatment or alongside, or in conjunction with, other medical services. In some jurisdictions, such as the United Kingdom, physical therapists may have the authority to prescribe medication.[2]

Physical therapy addresses the illnesses or injuries that limit a person's abilities to move and perform functional activities in their daily lives.[3] PTs use an individual's history and physical examination to arrive at a diagnosis and establish a management plan and, when necessary, incorporate the results of laboratory and imaging studies like X-rays, CT-scan, or MRI findings. Physical therapists can use sonography to diagnose and manage common musculoskeletal, nerve, and pulmonary conditions.[4][5][6] Electrodiagnostic testing (e.g., electromyograms and nerve conduction velocity testing) may also be used.[7]

PT management commonly includes prescription of or assistance with specific exercises, manual therapy, and manipulation, mechanical devices such as traction, education, electrophysical modalities which include heat, cold, electricity, sound waves, radiation, assistive devices, prostheses, orthoses, and other interventions. In addition, PTs work with individuals to prevent the loss of mobility before it occurs by developing fitness and wellness-oriented programs for healthier and more active lifestyles, providing services to individuals and populations to develop, maintain, and restore maximum movement and functional ability throughout the lifespan. This includes providing treatment in circumstances where movement and function are threatened by aging, injury, disease, or environmental factors. Functional movement is central to what it means to be healthy.[8]

Physical therapy is a professional career that has many specialties including musculoskeletal, orthopedics, cardiopulmonary, neurology, endocrinology, sports medicine, geriatrics, pediatrics, women's health, wound care and electromyography. Neurological rehabilitation is, in particular, a rapidly emerging field. PTs practice in many settings, such as privately-owned physical therapy clinics, outpatient clinics or offices, health and wellness clinics, rehabilitation hospital facilities, skilled nursing facilities, extended care facilities, private homes, education and research centers, schools, hospices, industrial and these workplaces or other occupational environments, fitness centers and sports training facilities.[1]

Physical therapists also practice in non-patient care roles such as health policy,[9][10] health insurance, health care administration and as health care executives. Physical therapists are involved in the medical-legal field serving as experts, performing peer review and independent medical examinations.[11]

Education varies greatly by country. The span of education ranges from some countries having little formal education to others having doctoral degrees and post-doctoral residencies and fellowships.[12]

Regarding its relationship to other healthcare professions, physiotherapy is one of the allied health professions.[13][14][15][16] World Physiotherapy has signed a ""memorandum of understanding"" with the four other members of the World Health Professions Alliance ""to enhance their joint collaboration on protecting and investing in the health workforce to provide safe, quality and equitable care in all settings"".[17]

Physicians like Hippocrates and later Galen are believed to have been the first practitioners of physical therapy, advocating massage, manual therapy techniques and hydrotherapy to treat people in 460 BC.[18] After the development of orthopedics in the eighteenth century, machines like the Gymnasticon were developed to treat gout and similar diseases by systematic exercise of the joints, similar to later developments in physical therapy.[19]

The earliest documented origins of actual physical therapy as a professional group date back to Per Henrik Ling, ""Father of Swedish Gymnastics,"" who founded the Royal Central Institute of Gymnastics (RCIG) in 1813 for manipulation, and exercise. Up until 2014, the Swedish word for a physical therapist was sjukgymnast = someone involved in gymnastics for those who are ill, but the title was then changed to fysioterapeut (physiotherapist), the word used in the other Scandinavian countries.[20] In 1887, PTs were given official registration by Sweden's National Board of Health and Welfare. Other countries soon followed. In 1894, four nurses in Great Britain formed the Chartered Society of Physiotherapy.[21] The School of Physiotherapy at the University of Otago in New Zealand in 1913,[22] and the United States 1914 Reed College in Portland, Oregon, which graduated ""reconstruction aides.""[23] Since the profession's inception, spinal manipulative therapy has been a component of the physical therapist practice.[24]

Modern physical therapy was established towards the end of the 19th century due to events that affected on a global scale, which called for rapid advances in physical therapy. Following this, American orthopedic surgeons began treating children with disabilities and employed women trained in physical education, and remedial exercise. These treatments were further applied and promoted during the Polio outbreak of 1916.[citation needed]

During the First World War, women were recruited to work with and restore physical function to injured soldiers, and the field of physical therapy was institutionalized. In 1918 the term ""Reconstruction Aide"" was used to refer to individuals practicing physical therapy. The first school of physical therapy was established at Walter Reed Army Hospital in Washington, D.C., following the outbreak of World War I.[25] Treatment through the 1940s primarily consisted of exercise, massage, and traction. Manipulative procedures to the spine and extremity joints began to be practiced, especially in the British Commonwealth countries, in the early 1950s.[26][27]

Around the time polio vaccines were developed, physical therapists became a normal occurrence in hospitals throughout North America and Europe.[28] In the late 1950s, physical therapists started to move beyond hospital-based practice to outpatient orthopedic clinics, public schools, colleges/universities health-centres, geriatric settings (skilled nursing facilities), rehabilitation centers and medical centers. Specialization in physical therapy in the U.S. occurred in 1974, with the Orthopaedic Section of the APTA being formed for those physical therapists specializing in orthopedics. In the same year, the International Federation of Orthopaedic Manipulative Physical Therapists was formed,[29] which has ever since played an important role in advancing manual therapy worldwide.

An international organization for the profession is the World Confederation for Physical Therapy (WCPT). It was founded in 1951 and has operated under the brand name World Physiotherapy since 2020.[30][31]

Educational criteria for physical therapy providers vary from state to state, country to country, and among various levels of professional responsibility. Most U.S. states have physical therapy practice acts that recognize both physical therapists (PT) and physical therapist assistants (PTA) and some jurisdictions also recognize physical therapy technicians (PT Techs) or aides. Most countries have licensing bodies that require physical therapists to be member of before they can start practicing as independent professionals.[citation needed]

The Canadian Alliance of Physiotherapy Regulators (CAPR)[32] offers eligible program graduates to apply for the national Physiotherapy Competency Examination (PCE). Passing the PCE is one of the requirements in most provinces and territories to work as a licensed physiotherapist in Canada.[33] CAPR has members which are physiotherapy regulatory organizations recognized in their respective provinces and territories:

Physiotherapy programs are offered at fifteen universities, often through the university's respective college of medicine. Each of Canada's physical therapy schools has transitioned from three-year Bachelor of Science in Physical Therapy (BScPT) programs that required two years of prerequisite university courses (five-year bachelor's degree) to two-year Master's of Physical Therapy (MPT) programs that require prerequisite bachelor's degrees. The last Canadian university to follow suit was the University of Manitoba, which transitioned to the MPT program in 2012, making the MPT credential the new entry to practice standard across Canada. Existing practitioners with BScPT credentials are not required to upgrade their qualifications.

In the province of Quebec, prospective physiotherapists are required to have completed a college diploma in either health sciences, which lasts on average two years, or physical rehabilitation technology, which lasts at least three years, to apply to a physiotherapy program or program in university. Following admission, physical therapy students work on a bachelor of science with a major in physical therapy and rehabilitation. The B.Sc. usually requires three years to complete. Students must then enter graduate school to complete a master's degree in physical therapy, which normally requires one and a half to two years of study. Graduates who obtain their M.Sc. must successfully pass the membership examination to become members of the Ordre Professionnel de la physiothérapie du Québec (PPQ). Physiotherapists can pursue their education in such fields as rehabilitation sciences, sports medicine, kinesiology, and physiology.

In the province of Quebec, physical rehabilitation therapists are health care professionals who are required to complete a four-year college diploma program in physical rehabilitation therapy and be members of the Ordre Professionnel de la physiothérapie du Québec (OPPQ)[45] to practice legally in the country according to specialist De Van Gerard.

Most physical rehabilitation therapists complete their college diploma at Collège Montmorency, Dawson College, or Cégep Marie-Victorin, all situated in and around the Montreal area.

After completing their technical college diploma, graduates have the opportunity to pursue their studies at the university level to perhaps obtain a bachelor's degree in physiotherapy, kinesiology, exercise science, or occupational therapy. The Université de Montréal, the Université Laval and the Université de Sherbrooke are among the Québécois universities that admit physical rehabilitation therapists in their programs of study related to health sciences and rehabilitation to credit courses that were completed in college.

To date, there are no bridging programs available to facilitate upgrading from the BScPT to the MPT credential. However, research Master's of Science (MSc) and Doctor of Philosophy (Ph.D.) programs are available at every university. Aside from academic research, practitioners can upgrade their skills and qualifications through continuing education courses and curriculums. Continuing education is a requirement of the provincial regulatory bodies.

The Canadian Physiotherapy Association offers a curriculum of continuing education courses in orthopedics and manual therapy. The program consists of 5 levels (7 courses) of training with ongoing mentorship and evaluation at each level. The orthopedic curriculum and examinations take a minimum of 4 years to complete. However, upon completion of level 2, physiotherapists can apply to a unique 1-year course-based Master's program in advanced orthopedics and manipulation at the University of Western Ontario to complete their training. This program accepts only 16 physiotherapists annually since 2007. Successful completion of either of these education streams and their respective examinations allows physiotherapists the opportunity to apply to the Canadian Academy of Manipulative Physiotherapy (CAMPT) for fellowship. Fellows of the Canadian Academy of manipulative Physiotherapists (FCAMPT) are considered leaders in the field, having extensive post-graduate education in orthopedics and manual therapy. FCAMPT is an internationally recognized credential, as CAMPT is a member of the International Federation of Manipulative Physiotherapists (IFOMPT), a branch of World Physiotherapy (formerly World Confederation of Physical Therapy (WCPT)) and the World Health Organization (WHO).

Physiotherapy degrees are offered at four universities: Edinburgh Napier University in Edinburgh, Robert Gordon University in Aberdeen, Glasgow Caledonian University in Glasgow, and Queen Margaret University in Edinburgh. Students can qualify as physiotherapists by completing a four-year Bachelor of Science degree or a two-year master's degree (if they already have an undergraduate degree in a related field).

To use the title 'Physiotherapist', a student must register with the Health and Care Professions Council, a UK-wide regulatory body, on qualifying. Many physiotherapists are also members of the Chartered Society of Physiotherapy (CSP),[46] which provides insurance and professional support.

The primary physical therapy practitioner is the Physical Therapist (PT) who is trained and licensed to examine, evaluate, diagnose and treat impairment, functional limitations, and disabilities in patients or clients. Physical therapist education curricula in the United States culminate in a Doctor of Physical Therapy (DPT) degree,[47] with some practicing PTs holding a Master of Physical Therapy degree, and some with a Bachelor's degree. The Master of Physical Therapy and Master of Science in Physical Therapy degrees are no longer offered, and the entry-level degree is the Doctor of Physical Therapy degree, which typically takes 3 years after completing a bachelor's degree.[48] PTs who hold a Masters or bachelors in PT are encouraged to get their DPT because APTA's goal is for all PT's to be on a doctoral level.[49] WCPT recommends physical therapist entry-level educational programs be based on university or university-level studies, of a minimum of four years, independently validated and accredited.[50] Curricula in the United States are accredited by the Commission on Accreditation in Physical Therapy Education (CAPTE). According to CAPTE, as of 2022[update] there are 37,306 students currently enrolled in 294 accredited PT programs in the United States while 10,096 PTA students are currently enrolled in 396 PTA programs in the United States.[51]

The physical therapist professional curriculum includes content in the clinical sciences (e.g., content about the cardiovascular, pulmonary, endocrine, metabolic, gastrointestinal, genitourinary, integumentary, musculoskeletal, and neuromuscular systems and the medical and surgical conditions frequently seen by physical therapists). Current training is specifically aimed to enable physical therapists to appropriately recognize and refer non-musculoskeletal diagnoses that may present similarly to those caused by systems not appropriate for physical therapy intervention, which has resulted in direct access to physical therapists in many states.[52]

Post-doctoral residency and fellowship education prevalence is increasing steadily with 219 residency, and 42 fellowship programs accredited in 2016. Residencies are aimed to train physical therapists in a specialty such as acute care, cardiovascular & pulmonary, clinical electrophysiology, faculty, geriatrics, neurology, orthopaedics, pediatrics, sports, women's health, and wound care, whereas fellowships train specialists in a subspecialty (e.g. critical care, hand therapy, and division 1 sports), similar to the medical model. Residency programs offer eligibility to sit for the specialist certification in their respective area of practice. For example, completion of an orthopedic physical therapy residency, allows its graduates to apply and sit for the clinical specialist examination in orthopedics, achieving the OCS designation upon passing the examination.[53] Board certification of physical therapy specialists is aimed to recognize individuals with advanced clinical knowledge and skill training in their respective area of practice, and exemplifies the trend toward greater education to optimally treat individuals with movement dysfunction.[54]

Physical therapist assistants may deliver treatment and physical interventions for patients and clients under a care plan established by and under the supervision of a physical therapist. Physical therapist assistants in the United States are currently trained under associate of applied sciences curricula specific to the profession, as outlined and accredited by CAPTE. As of December 2022, there were 396 accredited two-year (Associate degree) programs for physical therapist assistants In the United States of America.[55]

Physical therapy–related jobs in North America have shown rapid growth in recent years, but employment rates and average wages may vary significantly between different countries, states, provinces, or regions. A study from 2013 states that 56.4% of physical therapists were globally satisfied with their jobs.[56] Salary, interest in work, and fulfillment in a job are important predictors of job satisfaction.[56] In a Polish study, job burnout among the physical therapists was manifested by increased emotional exhaustion and decreased sense of personal achievement.[57] Emotional exhaustion is significantly higher among physical therapists working with adults and employed in hospitals. Other factors that increased burnout include working in a hospital setting and having seniority from 15 to 19 years.[57]

According to the United States Department of Labor's Bureau of Labor Statistics, there were approximately 210,900 physical therapists employed in the United States in 2014, earning an average of $84,020 annually in 2015, or $40.40 per hour, with 34% growth in employment projected by 2024.[58] The Bureau of Labor Statistics also reports that there were approximately 128,700 Physical Therapist Assistants and Aides employed in the United States in 2014, earning an average $42,980 annually, or $20.66 per hour, with 40% growth in employment projected by 2024. To meet their needs, many healthcare and physical therapy facilities hire ""travel physical therapists"", who work temporary assignments between 8 and 26 weeks for much higher wages; about $113,500 a year.""[59] Bureau of Labor Statistics data on PTAs and techs can be difficult to decipher, due to their tendency to report data on these job fields collectively rather than separately. O-Net reports that in 2015, PTAs in the United States earned a median wage of $55,170 annually or $26.52 hourly and that Aides/Techs earned a median wage of $25,120 annually or $12.08 hourly in 2015.[60][61] The American Physical Therapy Association reports vacancy rates for physical therapists as 11.2% in outpatient private practice, 10% in acute care settings, and 12.1% in skilled nursing facilities. The APTA also reports turnover rates for physical therapists as 10.7% in outpatient private practice, 11.9% in acute care settings, 27.6% in skilled nursing facilities.[62][63][64]

Definitions and licensing requirements in the United States vary among jurisdictions, as each state has enacted its own physical therapy practice act defining the profession within its jurisdiction, but the Federation of State Boards of Physical Therapy [65] has also drafted a model definition to limit this variation. The Commission on Accreditation in Physical Therapy Education [66] (CAPTE) is responsible for accrediting physical therapy education curricula throughout the United States of America.[citation needed]

The title of Physiotherapist is a protected professional title in the United Kingdom. Anyone using this title must be registered with the Health & Care Professions Council[67] (HCPC). Physiotherapists must complete the necessary qualifications, usually an undergraduate physiotherapy degree (at university or as an intern), a master rehabilitation degree, or a doctoral degree in physiotherapy.[68]  This is typically followed by supervised professional experience lasting two to three years. All professionals on the HCPC register must comply with continuing professional development (CPD) and can be audited for this evidence at intervals.[69]

The body of knowledge of physical therapy is large, and therefore physical therapists may specialize in a specific clinical area. While there are many different types of physical therapy, the American Board of Physical Therapy Specialties lists ten current specialist certifications. Most Physical Therapists practicing in a specialty will have undergone further training, such as an accredited residency program, although individuals are currently able to sit for their specialist examination after 2,000 hours of focused practice in their respective specialty population, in addition to requirements set by each respective specialty board.[citation needed]

Cardiovascular and pulmonary rehabilitation respiratory practitioners and physical therapists offer therapy for a wide variety of cardiopulmonary disorders or pre and post cardiac or pulmonary surgery. An example of cardiac surgery is coronary bypass surgery. The primary goals of this specialty include increasing endurance and functional independence. Manual therapy is used in this field to assist in clearing lung secretions experienced with cystic fibrosis. Pulmonary disorders, heart attacks, post coronary bypass surgery, chronic obstructive pulmonary disease, and pulmonary fibrosis, treatments can benefit[70] from cardiovascular and pulmonary specialized physical therapists.[71][verification needed]

This specialty area includes electrotherapy/physical agents, electrophysiological evaluation (EMG/NCV), physical agents, and wound management.

Geriatric physical therapy covers a wide area of issues concerning people as they go through normal adult aging but is usually focused on the older adult. There are many conditions that affect many people as they grow older and include but are not limited to the following: arthritis, osteoporosis, cancer, Alzheimer's disease, hip and joint replacement, balance disorders, incontinence, etc. Geriatric physical therapists specialize in providing therapy for such conditions in older adults.

Physical rehabilitation can prevent deterioration in health and activities of daily living among care home residents. The current evidence suggests benefits to physical health from participating in different types of physical rehabilitation to improve daily living, strength, flexibility, balance, mood, memory, exercise tolerance, fear of falling, injuries, and death.[72] It may be both safe and effective in improving physical and possibly mental state, while reducing disability with few adverse events.[72]

The current body of evidence suggests that physical rehabilitation may be effective for long-term care residents in reducing disability with few adverse events.[72] However, there is insufficient to conclude whether the beneficial effects are sustainable and cost-effective.[72] The findings are based on moderate quality evidence.

Wound management physical therapy includes the treatment of conditions involving the skin and all its related organs. Common conditions managed include wounds and burns. Physical therapists may utilize surgical instruments, wound irrigations, dressings, and topical agents to remove the damaged or contaminated tissue and promote tissue healing.[73] Other commonly used interventions include exercise, edema control, splinting, and compression garments. The work done by physical therapists in the integumentary specialty does work similar to what would be done by medical doctors or nurses in the emergency room or triage.[citation needed]

Neurological physical therapy is a field focused on working with individuals who have a neurological disorder or disease. These can include stroke, chronic back pain, Alzheimer's disease, Charcot-Marie-Tooth disease (CMT), ALS, brain injury, cerebral palsy, multiple sclerosis, Parkinson's disease, facial palsy and spinal cord injury. Common impairments associated with neurologic conditions include impairments of vision, balance, ambulation, activities of daily living, movement, muscle strength and loss of functional independence.[71] The techniques involve in neurological physical therapy are wide-ranging and often require specialized training.[citation needed]

Neurological physiotherapy is also called neurophysiotherapy or neurological rehabilitation. It is recommended for neurophysiotherapists to collaborate with psychologists when providing physical treatment of movement disorders.[74] This is especially important because combining physical therapy and psychotherapy can improve neurological status of the patients.[citation needed]

Orthopedic physical therapists diagnose, manage, and treat disorders and injuries of the musculoskeletal system including rehabilitation after orthopedic surgery, acute trauma such as sprains, strains, injuries of insidious onset such as tendinopathy, bursitis, and deformities like scoliosis. This specialty of physical therapy is most often found in the outpatient clinical setting. Orthopedic therapists are trained in the treatment of post-operative orthopedic procedures, fractures, acute sports injuries, arthritis, sprains, strains, back and neck pain, spinal conditions, and amputations.[citation needed]

Joint and spine mobilization/manipulation, dry needling (similar to acupuncture), therapeutic exercise, neuromuscular techniques, muscle reeducation, hot/cold packs, and electrical muscle stimulation (e.g., cryotherapy, iontophoresis, electrotherapy) are modalities employed to expedite recovery in the orthopedic setting.[75][verification needed] Additionally, an emerging adjunct to diagnosis and treatment is the use of sonography for diagnosis and to guide treatments such as muscle retraining.[76][77][78] Those with injury or disease affecting the muscles, bones, ligaments, or tendons will benefit from assessment by a physical therapist specialized in orthopedics.[79]

Pediatric physical therapy assists in the early detection of health problems and uses a variety of modalities to provide physical therapy for disorders in the pediatric population. These therapists are specialized in the diagnosis, treatment, and management of infants, children, and adolescents with a variety of congenital, developmental, neuromuscular, skeletal, or acquired disorders/diseases. Treatments focus mainly on improving gross and fine motor skills, balance and coordination, strength and endurance as well as cognitive and sensory processing/integration.[citation needed]

Physical therapists are closely involved in the care and wellbeing of athletes including recreational, semi-professional (paid), and professional (full-time employment) participants. This area of practice encompasses athletic injury management under 5 main categories:

Physical therapists who work for professional sports teams often have a specialized sports certification issued through their national registering organization. Most Physical therapists who practice in a sporting environment are also active in collaborative sports medicine programs too (See also: athletic trainers).

Women's health or pelvic floor physical therapy mostly addresses women's issues related to the female reproductive system, child birth, and post-partum. These conditions include lymphedema, osteoporosis, pelvic pain, prenatal and post-partum periods, and urinary incontinence. It also addresses incontinence, pelvic pain, pelvic organ prolapse and other disorders associated with pelvic floor dysfunction. Manual physical therapy has been demonstrated in multiple studies to increase rates of conception in women with infertility.[80][81][82][83]

Physical therapy in the field of oncology and palliative care is a continuously evolving and developing specialty, both in malignant and non-malignant diseases. Physical therapy for both groups of patients is now recognized as an essential part of the clinical pathway, as early diagnoses and new treatments are enabling patients to live longer. it is generally accepted that patients should have access to an appropriate level of rehabilitation, so that they can function at a minimum level of dependency and optimize their quality of life, regardless of their life expectancy.[84]

People with brain injury, musculoskeletal conditions, cardiac conditions, or multiple pathologies benefit from a positive alliance between patient and therapist. Outcomes include the ability to perform activities of daily living, manage pain, complete specific physical function tasks, depression, global assessment of physical health, treatment adherence, and treatment satisfaction.[85]

Studies have explored four themes that may influence patient-therapist interactions: interpersonal and communication skills, practical skills, individualized patient-centered care, and organizational and environmental factors.[86] Physical therapists need to be able to effectively communicate with their patients on a variety of levels. Patients have varying levels of health literacy so physical therapists need to take that into account when discussing the patient's ailments as well as planned treatment. Research has shown that using communication tools tailored to the patient's health literacy leads to improved engagement with their practitioner and their clinical care. In addition, patients reported that shared decision-making will yield a positive relationship.[87] Practical skills such as the ability to educate patients about their conditions, and professional expertise are perceived as valuable factors inpatient care. Patients value the ability of a clinician to provide clear and simple explanations about their problems. Furthermore, patients value when physical therapists possess excellent technical skills that improve the patient effectively.[86]

Environmental factors such as the location, equipment used, and parking are less important to the patient than the physical therapy clinical encounter itself.[88]

Based on the current understanding, the most important factors that contribute to the patient-therapist interactions include that the physical therapist: spends an adequate amount of time with the patient, possesses strong listening and communication skills, treats the patient with respect, provides clear explanations of the treatment, and allows the patient to be involved in the treatment decisions.[88]

Physical therapy has been found to be effective for improving outcomes, both in terms of pain and function, in multiple musculoskeletal conditions. Spinal manipulation by physical therapists is a safe option to improve outcomes for lower back pain.[89] Several studies have suggested that physical therapy, particularly manual therapy techniques focused on the neck and the median nerve, combined with stretching exercises, may be equivalent or even preferable to surgery for carpal tunnel syndrome.[90][91] While spine manipulation and therapeutic massage are effective interventions for neck pain, electroacupuncture, strain-counterstrain, relaxation massage, heat therapy, and ultrasound therapy are not as effective, and thus not recommended.[92]

Studies also show physical therapy is effective for patients with other conditions. Physiotherapy treatment may improve quality of life, promote cardiopulmonary fitness and inspiratory pressure, as well as reduce symptoms and medication use by people with asthma.[93] Physical therapy is sometimes provided to patients in the ICU, as early mobilization can help reduce ICU and hospital length of stay and improve long-term functional ability.[94] Early progressive mobilization for adult, intubated ICU patients on mechanical ventilation is safe and effective.[95]

Psychologically informed physical therapy (PIPT), in which a physical therapist treats patients while other members of a multidisciplinary care team help in preoperative planning for patient management of pain and quality of life, helps improve patient outcomes, especially before and after spine, hip, or knee surgery.[96]

However, in the United States, there are obstacles affecting the effectiveness of physical therapy, such as racial disparities among patients. Studies have shown that patients who identified as black experiences were below standard compared to the white patients. Physical therapy has been experiencing disparities with Hispanic patients like many other medical fields. Whether not receiving a referral for inpatient Hispanic patients to follow-up with their care, despite insurance status. Another being limited access to physical therapy as a reason.[97] Raising awareness of these racial disparities in physical therapy is crucial to improving treatment effectiveness across all demographics.[98]

Telehealth (or telerehabilitation) is a developing form of physical therapy in response to the increasing demand for physical therapy treatment.[99] Telehealth is online communication between the clinician and patient, either live or in pre-recorded sessions with mixed reviews when compared to usual, in-person care.[100] The benefits of telehealth include improved accessibility in remote areas, cost efficiency, and improved convenience for people who are bedridden and home-restricted, or physically disabled.[100] Some considerations for telehealth include: limited evidence to prove effectiveness and compliance more than in-person therapy, licensing and payment policy issues, and compromised privacy.[101] Studies are controversial as to the effectiveness of telehealth in patients with more serious conditions, such as stroke, multiple sclerosis, and lower back pain.[102] The interstate compact, enacted in March 2018, allows patients to participate in Telehealth appointments with medical practices located in different states.[103]

During the COVID-19 pandemic, the need for telehealth came to the fore as patients were less able to safely attend in-person, particularly if they were elderly or had chronic diseases. Telehealth was considered to be a proactive step to prevent decline in individuals that could not attend classes. Physical decline in at risk groups is difficult to address or undo later. The platform licensing or development are found to be the most substantial cost in telehealth. Telehealth does not remove the need for the physical therapist as they still need to oversee the program.[104][105][106]
"
Occupational Health Services,"

Occupational safety and health (OSH) or occupational health and safety (OHS) is a multidisciplinary field concerned with the safety, health, and welfare of people at work (i.e., while performing duties required by one's occupation). OSH is related to the fields of occupational medicine and occupational hygiene[a] and aligns with workplace health promotion initiatives. OSH also protects all the general public who may be affected by the occupational environment.[4]

According to the official estimates of the United Nations, the WHO/ILO Joint Estimate of the Work-related Burden of Disease and Injury, almost 2 million people die each year due to exposure to occupational risk factors.[5] Globally, more than 2.78 million people die annually as a result of workplace-related accidents or diseases, corresponding to one death every fifteen seconds. There are an additional 374 million non-fatal work-related injuries annually. It is estimated that the economic burden of occupational-related injury and death is nearly four per cent of the global gross domestic product each year. The human cost of this adversity is enormous.[6]

In common-law jurisdictions, employers have the common law duty (also called duty of care) to take reasonable care of the safety of their employees.[7] Statute law may, in addition, impose other general duties, introduce specific duties, and create government bodies with powers to regulate occupational safety issues. Details of this vary from jurisdiction to jurisdiction.

Prevention of workplace incidents and occupational diseases is addressed through the implementation of occupational safety and health programs at company level.[8]

The International Labour Organization (ILO) and the World Health Organization (WHO) share a common definition of occupational health.[b] It was first adopted by the Joint ILO/WHO Committee on Occupational Health at its first session in 1950:[10][11]

Occupational health should aim at the promotion and maintenance of the highest degree of physical, mental and social well-being of workers in all occupations; the prevention amongst workers of departures from health caused by their working conditions; the protection of workers in their employment from risks resulting from factors adverse to health; the placing and maintenance of the worker in an occupational environment adapted to his physiological and psychological capabilities and; to summarize: the adaptation of work to man and of each man to his job.
In 1995, a consensus statement was added:[10][11]

The main focus in occupational health is on three different objectives: (i) the maintenance and promotion of workers' health and working capacity; (ii) the improvement of working environment and work to become conducive to safety and health and (iii) development of work organizations and working cultures in a direction which supports health and safety at work and in doing so also promotes a positive social climate and smooth operation and may enhance productivity of the undertakings. The concept of working culture is intended in this context to mean a reflection of the essential value systems adopted by the undertaking concerned. Such a culture is reflected in practice in the managerial systems, personnel policy, principles for participation, training policies and quality management of the undertaking.
An alternative definition for occupational health given by the WHO is: ""occupational health deals with all aspects of health and safety in the workplace and has a strong focus on primary prevention of hazards.""[12]

The expression ""occupational health"", as originally adopted by the WHO and the ILO, refers to both short- and long-term adverse health effects. In more recent times, the expressions ""occupational safety and health"" and ""occupational health and safety"" have come into use (and have also been adopted in works by the ILO),[13] based on the general understanding that occupational health refers to hazards associated to disease and long-term effects, while occupational safety hazards are those associated to work accidents causing injury and sudden severe conditions.[14]

Research and regulation of occupational safety and health are a relatively recent phenomenon. As labor movements arose in response to worker concerns in the wake of the industrial revolution, workers' safety and health entered consideration as a labor-related issue.[15]

Written works on occupational diseases began to appear by the end of the 15th century, when demand for gold and silver was rising due to the increase in trade and iron, copper, and lead were also in demand from the nascent firearms market. Deeper mining became common as a consequence. In 1473, Ulrich Ellenbog [de], a German physician, wrote a short treatise On the Poisonous Wicked Fumes and Smokes, focused on coal, nitric acid, lead, and mercury fumes encountered by metal workers and goldsmiths. In 1587, Paracelsus (1493–1541) published the first work on the mine and smelter workers diseases. In it, he gave accounts of miners' ""lung sickness"". In 1526, Georgius Agricola's (1494–1553) De re metallica, a treaty on metallurgy, described accidents and diseases prevalent among miners and recommended practices to prevent them. Like Paracelsus, Agricola mentioned the dust that ""eats away the lungs, and implants consumption.""[16]

The seeds of state intervention to correct social ills were sown during the reign of Elizabeth I by the Poor Laws, which originated in attempts to alleviate hardship arising from widespread poverty. While they were perhaps more to do with a need to contain unrest than morally motivated, they were significant in transferring responsibility for helping the needy from private hands to the state.[15]

In 1713, Bernardino Ramazzini (1633–1714), often described as the father of occupational medicine and a precursor to occupational health, published his De morbis artificum diatriba (Dissertation on Workers' Diseases), which outlined the health hazards of chemicals, dust, metals, repetitive or violent motions, odd postures, and other disease-causative agents encountered by workers in more than fifty occupations. It was the first broad-ranging presentation of occupational diseases.[16][17][18]

Percivall Pott (1714–1788), an English surgeon, described cancer in chimney sweeps (chimney sweeps' carcinoma), the first recognition of an occupational cancer in history.[16]

The United Kingdom was the first nation to industrialize. Soon shocking evidence emerged of serious physical and moral harm suffered by children and young persons in the cotton textile mills, as a result of exploitation of cheap labor in the factory system. Responding to calls for remedial action from philanthropists and some of the more enlightened employers, in 1802 Sir Robert Peel, himself a mill owner, introduced a bill to Parliament with the aim of improving their conditions. This would engender the Health and Morals of Apprentices Act 1802, generally believed to be the first attempt to regulate conditions of work in the United Kingdom. The act applied only to cotton textile mills and required employers to keep premises clean and healthy by twice yearly washings with quicklime, to ensure there were sufficient windows to admit fresh air, and to supply ""apprentices"" (i.e., pauper and orphan employees) with ""sufficient and suitable"" clothing and accommodation for sleeping.[15] It was the first of the 19th century Factory Acts.

Charles Thackrah (1795–1833), another pioneer of occupational medicine, wrote a report on The State of Children Employed in Cotton Factories, which was sent to the Parliament in 1818. Thackrah recognized issues of inequalities of health in the workplace, with manufacturing in towns causing higher mortality than agriculture.[16]

The Factory Act 1833 created a dedicated professional Factory Inspectorate.[19] The initial remit of the Inspectorate was to police restrictions on the working hours in the textile industry of children and young persons (introduced to prevent chronic overwork, identified as leading directly to ill-health and deformation, and indirectly to a high accident rate).[15]

In 1840 a royal commission published its findings on the state of conditions for the workers of the mining industry that documented the appallingly dangerous environment that they had to work in and the high frequency of accidents. The commission sparked public outrage which resulted in the Mines and Collieries Act 1842. The act set up an inspectorate for mines and collieries which resulted in many prosecutions and safety improvements, and by 1850, inspectors were able to enter and inspect premises at their discretion.[20]

On the urging of the Factory Inspectorate, a further Factories Act 1844 giving similar restrictions on working hours for women in the textile industry introduced a requirement for machinery guarding (but only in the textile industry, and only in areas that might be accessed by women or children).[21] The latter act was the first to take a significant step toward improvement of workers' safety, as the former focused on health aspects alone.[15]

The first decennial British Registrar-General's mortality report was issued in 1851. Deaths were categorized by social classes, with class I corresponding to professionals and executives and class V representing unskilled workers. The report showed that mortality rates increased with the class number.[16]

Otto von Bismarck inaugurated the first social insurance legislation in 1883 and the first worker's compensation law in 1884 – the first of their kind in the Western world. Similar acts followed in other countries, partly in response to labor unrest.[16]

The United States are responsible for the first health program focusing on workplace conditions. This was the Marine Hospital Service, inaugurated in 1798 and providing care for merchant seamen. This was the beginning of what would become the US Public Health Service (USPHS).[16]

The first worker compensation acts in the United States were passed in New York in 1910 and in Washington and Wisconsin in 1911. Later rulings included occupational diseases in the scope of the compensation, which was initially restricted to accidents.[16]

In 1914 the USPHS set up the Office of Industrial Hygiene and Sanitation, the ancestor of the current National Institute for Safety and Health (NIOSH). In the early 20th century, workplace disasters were still common. For example, in 1911 a fire at the Triangle Shirtwaist Company in New York killed 146 workers, mostly women and immigrants. Most died trying to open exits that had been locked. Radium dial painter cancers,""phossy jaw"", mercury and lead poisonings, silicosis, and other pneumoconioses were extremely common.[16]

The enactment of the Federal Coal Mine Health and Safety Act of 1969 was quickly followed by the 1970 Occupational Safety and Health Act, which established the Occupational Safety and Health Administration (OSHA) and NIOSH in their current form`.[16]

A wide array of workplace hazards can damage the health and safety of people at work. These include but are not limited to, ""chemicals, biological agents, physical factors, adverse ergonomic conditions, allergens, a complex network of safety risks,"" as well a broad range of psychosocial risk factors.[23] Personal protective equipment can help protect against many of these hazards.[24] A landmark study conducted by the World Health Organization and the International Labour Organization found that exposure to long working hours is the occupational risk factor with the largest attributable burden of disease, i.e. an estimated 745,000 fatalities from ischemic heart disease and stroke events in 2016.[25] This makes overwork the globally leading occupational health risk factor.[26]

Physical hazards affect many people in the workplace. Occupational hearing loss is the most common work-related injury in the United States, with 22 million workers exposed to hazardous occupational noise levels at work and an estimated $242 million spent annually on worker's compensation for hearing loss disability.[27] Falls are also a common cause of occupational injuries and fatalities, especially in construction, extraction, transportation, healthcare, and building cleaning and maintenance.[28] Machines have moving parts, sharp edges, hot surfaces and other hazards with the potential to crush, burn, cut, shear, stab or otherwise strike or wound workers if used unsafely.[29]

Biological hazards (biohazards) include infectious microorganisms such as viruses, bacteria and toxins produced by those organisms such as anthrax. Biohazards affect workers in many industries; influenza, for example, affects a broad population of workers.[30] Outdoor workers, including farmers, landscapers, and construction workers, risk exposure to numerous biohazards, including animal bites and stings,[31][32][33] urushiol from poisonous plants,[34] and diseases transmitted through animals such as the West Nile virus and Lyme disease.[35][36] Health care workers, including veterinary health workers, risk exposure to blood-borne pathogens and various infectious diseases,[37][38] especially those that are emerging.[39]

Dangerous chemicals can pose a chemical hazard in the workplace. There are many classifications of hazardous chemicals, including neurotoxins, immune agents, dermatologic agents, carcinogens, reproductive toxins, systemic toxins, asthmagens, pneumoconiotic agents, and sensitizers.[40] Authorities such as regulatory agencies set occupational exposure limits to mitigate the risk of chemical hazards.[41] International investigations are ongoing into the health effects of mixtures of chemicals, given that toxins can interact synergistically instead of merely additively. For example, there is some evidence that certain chemicals are harmful at low levels when mixed with one or more other chemicals. Such synergistic effects may be particularly important in causing cancer. Additionally, some substances (such as heavy metals and organohalogens) can accumulate in the body over time, thereby enabling small incremental daily exposures to eventually add up to dangerous levels with little overt warning.[42]

Psychosocial hazards include risks to the mental and emotional well-being of workers, such as feelings of job insecurity, long work hours, and poor work-life balance.[43] Psychological abuse has been found present within the workplace as evidenced by previous research. A study by Gary Namie on workplace emotional abuse found that 31% of women and 21% of men who reported workplace emotional abuse exhibited three key symptoms of post-traumatic stress disorder (hypervigilance, intrusive imagery, and avoidance behaviors).[44] Sexual harassment is a serious hazard that can be found in workplaces.[45]

Specific occupational safety and health risk factors vary depending on the specific sector and industry. Construction workers might be particularly at risk of falls, for instance, whereas fishermen might be particularly at risk of drowning. Similarly psychosocial risks such as workplace violence are more pronounced for certain occupational groups such as health care employees, police, correctional officers and teachers.[46]

Agriculture workers are often at risk of work-related injuries, lung disease, noise-induced hearing loss, skin disease, as well as certain cancers related to chemical use or prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery. The most common cause of fatal agricultural injuries in the United States is tractor rollovers, which can be prevented by the use of roll over protection structures which limit the risk of injury in case a tractor rolls over.[47] Pesticides and other chemicals used in farming can also be hazardous to worker health,[48] and workers exposed to pesticides may experience illnesses or birth defects.[49] As an industry in which families, including children, commonly work alongside their families, agriculture is a common source of occupational injuries and illnesses among younger workers.[50] Common causes of fatal injuries among young farm worker include drowning, machinery and motor vehicle-related accidents.[51]

The 2010 NHIS-OHS found elevated prevalence rates of several occupational exposures in the agriculture, forestry, and fishing sector which may negatively impact health. These workers often worked long hours. The prevalence rate of working more than 48 hours a week among workers employed in these industries was 37%, and 24% worked more than 60 hours a week.[52] Of all workers in these industries, 85% frequently worked outdoors compared to 25% of all US workers. Additionally, 53% were frequently exposed to vapors, gas, dust, or fumes, compared to 25% of all US workers.[53]

The mining industry still has one of the highest rates of fatalities of any industry.[54] There are a range of hazards present in surface and underground mining operations. In surface mining, leading hazards include such issues as geological instability,[55] contact with plant and equipment, rock blasting, thermal environments (heat and cold), respiratory health (black lung), etc.[56] In underground mining, operational hazards include respiratory health, explosions and gas (particularly in coal mine operations), geological instability, electrical equipment, contact with plant and equipment, heat stress, inrush of bodies of water, falls from height, confined spaces, ionising radiation, etc.[57]

According to data from the 2010 NHIS-OHS, workers employed in mining and oil and gas extraction industries had high prevalence rates of exposure to potentially harmful work organization characteristics and hazardous chemicals. Many of these workers worked long hours: 50% worked more than 48 hours a week and 25% worked more than 60 hours a week in 2010. Additionally, 42% worked non-standard shifts (not a regular day shift). These workers also had high prevalence of exposure to physical/chemical hazards. In 2010, 39% had frequent skin contact with chemicals. Among nonsmoking workers, 28% of those in mining and oil and gas extraction industries had frequent exposure to secondhand smoke at work. About two-thirds were frequently exposed to vapors, gas, dust, or fumes at work.[58]

Construction is one of the most dangerous occupations in the world, incurring more occupational fatalities than any other sector in both the United States and in the European Union.[59][60] In 2009, the fatal occupational injury rate among construction workers in the United States was nearly three times that for all workers.[59] Falls are one of the most common causes of fatal and non-fatal injuries among construction workers.[59] Proper safety equipment such as harnesses and guardrails and procedures such as securing ladders and inspecting scaffolding can curtail the risk of occupational injuries in the construction industry.[61] Due to the fact that accidents may have disastrous consequences for employees as well as organizations, it is of utmost importance to ensure health and safety of workers and compliance with HSE construction requirements. Health and safety legislation in the construction industry involves many rules and regulations. For example, the role of the Construction Design Management (CDM) Coordinator as a requirement has been aimed at improving health and safety on-site.[62]

The 2010 National Health Interview Survey Occupational Health Supplement (NHIS-OHS) identified work organization factors and occupational psychosocial and chemical/physical exposures which may increase some health risks. Among all US workers in the construction sector, 44% had non-standard work arrangements (were not regular permanent employees) compared to 19% of all US workers, 15% had temporary employment compared to 7% of all US workers, and 55% experienced job insecurity compared to 32% of all US workers. Prevalence rates for exposure to physical/chemical hazards were especially high for the construction sector. Among nonsmoking workers, 24% of construction workers were exposed to secondhand smoke while only 10% of all US workers were exposed. Other physical/chemical hazards with high prevalence rates in the construction industry were frequently working outdoors (73%) and frequent exposure to vapors, gas, dust, or fumes (51%).[63]

The service sector comprises diverse workplaces. Each type of workplace has its own health risks. While some occupations have become mobile, others still require desk work. As the number of service sector jobs has risen in developed countries, many jobs have turned sedentary, presenting an array of health problems that differ from previous health concerns associated with manufacturing and the primary sector. Contemporary health problems include obesity. Some working conditions, such as occupational stress, workplace bullying, and overwork, have negative consequences for physical and mental health.[64][65]

Tipped wage workers are at a higher risk of negative mental health outcomes like addiction or depression. The higher rates of mental health issues may be attributed to the precarious nature of their employment, characterized by low and unpredictable incomes, inadequate access to benefits, wage exploitation, and minimal control over work schedules and assigned shifts.[66] Close to 70% of tipped wage workers are women.[67] Additionally, ""almost 40 percent of people who work for tips are people of color: 18 percent are Latino, 10 percent are African American, and 9 percent are Asian. Immigrants are also overrepresented in the tipped workforce.""[68] According to data from the 2010 NHIS-OHS, hazardous physical and chemical exposures in the service sector were lower than national averages. However, harmful organizational practices and psychosocial risks were fairly prevalent in this sector. Among all workers in the service industry, 30% experienced job insecurity in 2010, 27% worked non-standard shifts (not a regular day shift), 21% had non-standard work arrangements (were not regular permanent employees).[69]

In addition to these organizational risks, some industries pose significant physical dangers due to the manual labor involved. For instance, on a per employee basis, the US Postal Service, UPS and FedEx are the 4th, 5th and 7th most dangerous companies to work for in the United States, respectively.[70]

In general, healthcare workers are exposed to many hazards that can adversely affect their health and well-being.[71] Long hours, changing shifts, physically demanding tasks, violence, and exposures to infectious diseases and harmful chemicals are examples of hazards that put these workers at risk for illness and injury. Musculoskeletal injury (MSI) is the most common health hazard in for healthcare workers and in workplaces overall.[72] Injuries can be prevented by using proper body mechanics.[73]

According to the Bureau of Labor statistics, US hospitals recorded 253,700 work-related injuries and illnesses in 2011, which is 6.8 work-related injuries and illnesses for every 100 full-time employees.[74] The injury and illness rate in hospitals is higher than the rates in construction and manufacturing – two industries that are traditionally thought to be relatively hazardous.[citation needed]

An estimated 2.90 million work-related deaths occurred in 2019, increased from 2.78 million death from 2015. About, one-third of the total work-related deaths (31%) were due to circulatory diseases, while cancer contributed 29%, respiratory diseases 17%, and occupational injuries contributed 11% (or about 319,000 fatalities). Other diseases such as work-related communicable diseases contributed 6%, while neuropsychiatric conditions contributed 3% and work-related digestive disease and genitourinary diseases contributed 1% each. The contribution of cancers and circulatory diseases to total work-related deaths increased from 2015, while deaths due to occupational injuries decreased. Although work-related injury deaths and non-fatal injuries rates were on a decreasing trend, the total deaths and non-fatal outcomes were on the rise. Cancers represented the most significant cause of mortality in high-income countries. The number of non-fatal occupational injuries for 2019 was estimated to be 402 million.[75]

Mortality rate is unevenly distributed, with male mortality rate (108.3 per 100,000 employed male individuals) being significantly higher than female rate (48.4 per 100,000). 6.7% of all deaths globally are represented by occupational fatalities.[76]

Certain EU member states admit to having lacking quality control in occupational safety services, to situations in which risk analysis takes place without any on-site workplace visits and to insufficient implementation of certain EU OSH directives. Disparities between member states result in different impact of occupational hazards on the economy. In the early 2000s, the total societal costs of work-related health problems and accidents varied from 2.6% to 3.8% of the national GDPs across the member states.[77]

In 2021, in the EU-27 as a whole, 93% of deaths due to injury were of males.[78]

One of the decisions taken by the communist regime under Stalin was to reduce the number of accidents and occupational diseases to zero.[80] The tendency to decline remained in the Russian Federation in the early 21st century. However, as in previous years, data reporting and publication was incomplete and manipulated, so that the actual number of work-related diseases and accidents are unknown.[81] The ILO reports that, according to the information provided by the Russian government, there are 190,000 work-related fatalities each year, of which 15,000 due to occupational accidents.[82]

After the demise of the USSR, enterprises became owned by oligarchs who were not interested in upholding safe and healthy conditions in the workplace. Expenditure on equipment modernization was minimal and the share of harmful workplaces increased.[83] The government did not interfere in this, and sometimes it helped employers.[citation needed] At first, the increase in occupational diseases and accidents was slow, due to the fact that in the 1990s it was compensated by mass deindustrialization.[citation needed] However, in the 2000s deindustrialization slowed and occupational diseases and injuries started to rise in earnest. Therefore, in the 2010s the Ministry of Labor adopted federal law no. 426-FZ. This piece of legislation has been described as ineffective and based on the superficial assumption that the issuance of personal protective equipment to the employee means real improvement of working conditions. Meanwhile, the Ministry of Health made significant changes in the methods of risk assessment in the workplace.[84] However, specialists from the Izmerov Research Institute of Occupational Health found that the post-2014 apparent decrease in the share of employees engaged in hazardous working conditions is due to the change in definitions consequent to the Ministry of Health's decision, but does not reflect actual improvements. This was most clearly shown in the results for the aluminum industry.[85]

Further problems in the accounting of workplace fatalities arise from the fact that multiple Russian federal entities collect and publish records, a practice that should be avoided. In 2008 alone, 2074 accidents at work may have not been reported in official government sources.[86]

In the UK there were 135 fatal injuries at work in financial year 2022–2023, compared with 651 in 1974 (the year when the Health and Safety at Work Act was promulgated). The fatal injury rate declined from 2.1 fatalities per 100,000 workers in 1981 to 0.41 in financial year 2022–2023.[87] Over recent decades reductions in both fatal and non-fatal workplace injuries have been very significant. However, illnesses statistics have not uniformly improved: while musculoskeletal disorders have diminished, the rate of self-reported work-related stress, depression or anxiety has increased, and the rate of mesothelioma deaths has remained broadly flat (due to past asbestos exposures).[88]

The Occupational Safety and Health Statistics (OSHS) program in the Bureau of Labor Statistics of the United States Department of Labor compiles information about workplace fatalities and non-fatal injuries in the United States. The OSHS program produces three annual reports:


The Bureau also uses tools like AgInjuryNews.org to identify and compile additional sources of fatality reports for their datasets.[90][91]
Between 1913 and 2013, workplace fatalities dropped by approximately 80%.[92] In 1970, an estimated 14,000 workers were killed on the job. By 2021, in spite of the workforce having since more than doubled, workplace deaths were down to about 5,190.[93] According to the census of occupational injuries 5,486 people died on the job in 2022, up from the 2021 total of 5,190. The fatal injury rate was 3.7 per 100,000 full-time equivalent workers.[94] The decrease in the mortality rate is only partly (about 10–15%) explained by the deindustrialization of the US in the last 40 years.[95]

About 3.5 million nonfatal workplace injuries and illnesses were reported by private industry employers in 2022, occurring at a rate of 3.0 cases per 100 full-time workers.[96][97]

employees

Companies may adopt a safety and health management system (SMS),[c] either voluntarily or because required by applicable regulations, to deal in a structured and systematic way with safety and health risks in their workplace. An SMS provides a systematic way to assess and improve prevention of workplace accidents and incidents based on structured management of workplace risks and hazards. It must be adaptable to changes in the organization's business and legislative requirements. It is usually based on the Deming cycle, or plan-do-check-act (PDCA) principle.[98] An effective SMS should:

Management standards across a range of business functions such as environment, quality and safety are now being designed so that these traditionally disparate elements can be integrated and managed within a single business management system and not as separate and stand-alone functions. Therefore, some organizations dovetail other management system functions, such as process safety, environmental resource management or quality management together with safety management to meet both regulatory requirements, industry sector requirements and their own internal and discretionary standard requirements.

The ILO published ILO-OSH 2001 on Guidelines on Occupational Safety and Health Management Systems to assist organizations with introducing OSH management systems. These guidelines encouraged continual improvement in employee health and safety, achieved via a constant process of policy; organization; planning and implementation; evaluation; and action for improvement, all supported by constant auditing to determine the success of OSH actions.[99]

From 1999 to 2018, OHSAS 18001 was adopted and widely used internationally. It was developed by a selection of national standards bodies, academic bodies, accreditation bodies, certification bodies and occupational health and safety institutions to address a gap where no third-party certifiable international standard existed.[100] It was designed for integration with ISO 9001 and ISO 14001.[101]

OHSAS 18001 was replaced by ISO 45001, which was published in March 2018 and implemented in March 2021.[citation needed]

National management system standards for occupational health and safety include AS/NZS 4801 for Australia and New Zealand (now superseded by ISO 45001),[102][103] CSA Z1000:14 for Canada (which is due to be discontinued in favor of CSA Z45001:19, the Canadian adoption of ISO 45000)[104] and ANSI/ASSP Z10 for the United States.[105] In Germany, the Bavarian state government, in collaboration with trade associations and private companies, issued their OHRIS standard for occupational health and safety management systems. A new revision was issued in 2018.[106] The Taiwan Occupational Safety and Health Management System (TOSHMS) was issued in 1997 under the auspices of Taiwan's Occupational Safety and Health Administration.[107]

The terminology used in OSH varies between countries, but generally speaking:

""Hazard"", ""risk"", and ""outcome"" are used in other fields to describe e.g., environmental damage or damage to equipment. However, in the context of OSH, ""harm"" generally describes the direct or indirect degradation, temporary or permanent, of the physical, mental, or social well-being of workers. For example, repetitively carrying out manual handling of heavy objects is a hazard. The outcome could be a musculoskeletal disorder (MSD) or an acute back or joint injury. The risk can be expressed numerically (e.g., a 0.5 or 50/50 chance of the outcome occurring during a year), in relative terms (e.g., ""high/medium/low""), or with a multi-dimensional classification scheme (e.g., situation-specific risks).[citation needed]

Hazard identification is an important step in the overall risk assessment and risk management process. It is where individual work hazards are identified, assessed and controlled or eliminated as close to source (location of the hazard) as reasonably practicable. As technology, resources, social expectation or regulatory requirements change, hazard analysis focuses controls more closely toward the source of the hazard. Thus, hazard control is a dynamic program of prevention. Hazard-based programs also have the advantage of not assigning or implying there are ""acceptable risks"" in the workplace.[109] A hazard-based program may not be able to eliminate all risks, but neither does it accept ""satisfactory"" – but still risky – outcomes. And as those who calculate and manage the risk are usually managers, while those exposed to the risks are a different group, a hazard-based approach can bypass conflict inherent in a risk-based approach.[citation needed]

The information that needs to be gathered from sources should apply to the specific type of work from which the hazards can come from. Examples of these sources include interviews with people who have worked in the field of the hazard, history and analysis of past incidents, and official reports of work and the hazards encountered. Of these, the personnel interviews may be the most critical in identifying undocumented practices, events, releases, hazards and other relevant information. Once the information is gathered from a collection of sources, it is recommended for these to be digitally archived (to allow for quick searching) and to have a physical set of the same information in order for it to be more accessible. One innovative way to display the complex historical hazard information is with a historical hazards identification map, which distills the hazard information into an easy-to-use graphical format.[citation needed]

Modern occupational safety and health legislation usually demands that a risk assessment be carried out prior to making an intervention. This assessment should:

The calculation of risk is based on the likelihood or probability of the harm being realized and the severity of the consequences. This can be expressed mathematically as a quantitative assessment (by assigning low, medium and high likelihood and severity with integers and multiplying them to obtain a risk factor), or qualitatively as a description of the circumstances by which the harm could arise.[citation needed]

The assessment should be recorded and reviewed periodically and whenever there is a significant change to work practices. The assessment should include practical recommendations to control the risk. Once recommended controls are implemented, the risk should be re-calculated to determine if it has been lowered to an acceptable level. Generally speaking, newly introduced controls should lower risk by one level, i.e., from high to medium or from medium to low.[110]

Occupational safety and health practice vary among nations with different approaches to legislation, regulation, enforcement, and incentives for compliance. In the EU, for example, some member states promote OSH by providing public monies as subsidies, grants or financing, while others have created tax system incentives for OSH investments. A third group of EU member states has experimented with using workplace accident insurance premium discounts for companies or organizations with strong OSH records.[111][112]

In Australia, four of the six states and both territories have enacted and administer harmonized work health and safety legislation in accordance with the Intergovernmental Agreement for Regulatory and Operational Reform in Occupational Health and Safety.[113] Each of these jurisdictions has enacted work health and safety legislation and regulations based on the Commonwealth Work Health and Safety Act 2011 and common codes of practice developed by Safe Work Australia.[114] Some jurisdictions have also included mine safety under the model approach. However, most have retained separate legislation for the time being. In August 2019, Western Australia committed to join nearly every other state and territory in implementing the harmonized Model WHS Act, Regulations and other subsidiary legislation.[115] Victoria has retained its own regime, although the Model WHS laws themselves drew heavily on the Victorian approach.[citation needed]

In Canada, workers are covered by provincial or federal labor codes depending on the sector in which they work. Workers covered by federal legislation (including those in mining, transportation, and federal employment) are covered by the Canada Labour Code; all other workers are covered by the health and safety legislation of the province in which they work.[116][117] The Canadian Centre for Occupational Health and Safety (CCOHS), an agency of the Government of Canada, was created in 1978 by an act of parliament. CCOHS is mandated to promote safe and healthy workplaces and help prevent work-related injuries and illnesses.[118]

There are significant common elements across relevant provincial OHS legislation. The foundation of each of these legislative frameworks is the belief that all Canadians have ""a fundamental right to a healthy and safe working environment."" In general, provincial workplace safety laws in Canada are designed to promote shared responsibility, prevent accidents, and ensure accountability at all levels of an organization. Employers, supervisors, and workers are expected to work together to minimize risks.  Employers, in particular, are legally obligated to take every reasonable precaution to protect workers. If the workplace has more than a few employees, they are required to develop written health and safety policies and procedures. Employers must also provide and maintain equipment and machinery in a safe working condition. Additionally, employers must inform, instruct, and supervise workers to ensure safe work practices are followed. Employers are also responsible for supplying necessary protective equipment and ensuring it is used correctly, whether it involves machine guards or personal protective equipment (PPE). Supervisors have a duty to ensure that workers use all required safety devices and comply with established procedures. They must also communicate information about existing or potential hazards and provide guidance on how to work safely. Workers also have the right to refuse work if they believe it is unsafe and poses a danger to themselves or others.[119][additional citation(s) needed]

In workplaces with a set minimum number of employees (twenty in the case of workplaces under federal jurisdiction[120]), it is mandatory to have a health and safety committee. This, made up of both worker and management representatives, meets regularly to identify hazards, investigate incidents, and make recommendations to improve workplace safety. These committees are crucial for fostering collaboration and addressing safety concerns in a timely manner.[121]

Law also requires employers to take defined steps to prevent workplace violence and harassment. They must create a workplace violence policy along with a program that identifies risks and outlines procedures for addressing them. A separate workplace harassment policy must explain how complaints should be reported and investigated. Employers are required to train employees on these policies to ensure awareness and compliance. All incidents involving violence, threats, or persistent harassment must be taken seriously and handled appropriately.[122][123]

In severe cases involving serious injury or death due to negligence, organizations and individuals can be prosecuted under the Criminal Code of Canada through the provisions introduced by Bill C-45. In some provinces, like Ontario, this introduces serious criminal consequences for safety violations.[124]

Workplaces are also subject to federal regulations under WHMIS, the Workplace Hazardous Materials Information System. WHMIS governs the labeling, documentation, and communication of hazardous materials. Employers must ensure that all hazardous substances are properly labeled, that material safety data sheets are readily available, and that workers are trained on how to handle these materials safely.[125]

As an example of arrangements at a provincial level, Ontario's primary workplace safety legislation is the Occupational Health and Safety Act (OHSA). This law sets out the responsibilities of employers, supervisors, and workers to promote a safe and healthy work environment. Ontario's occupational health and safety framework is built around the concept known as the ""Internal Responsibility System,"" which means that everyone in the workplace shares responsibility for recognizing and addressing safety concerns. The OHSA is enforced by Ontario’s Ministry of Labour, Immigration, Training and Skills Development. Ministry inspectors have the authority to visit workplaces, investigate complaints, and issue orders. Failure to comply with the law can lead to substantial fines and penalties, and individual supervisors or managers may also be held personally liable.[126][127]

In China, the Ministry of Health is responsible for occupational disease prevention and the State Administration of Work Safety workplace safety issues.[citation needed] The Work Safety Law (安全生产法) was issued on 1 November 2002.[128][129] The Occupational Disease Control Act came into force on 1 May 2002.[130] In 2018, the National Health Commission (NHC) was formally established to formulating national health policies. The NHC formulated the ""National Occupational Disease Prevention and Control Plan (2021–2025)"" in the context of the activities leading to the ""Healthy China 2030"" initiative.[128]

The European Agency for Safety and Health at Work was founded in 1994. In the European Union, member states have enforcing authorities to ensure that the basic legal requirements relating to occupational health and safety are met. In many EU countries, there is strong cooperation between employer and worker organizations (e.g., unions) to ensure good OSH performance, as it is recognized this has benefits for both the worker (through maintenance of health) and the enterprise (through improved productivity and quality).[citation needed]

Member states have all transposed into their national legislation a series of directives that establish minimum standards on occupational health and safety. These directives (of which there are about 20 on a variety of topics) follow a similar structure requiring the employer to assess workplace risks and put in place preventive measures based on a hierarchy of hazard control. This hierarchy starts with elimination of the hazard and ends with personal protective equipment.[citation needed]

per 10,000 full-time employees[131]

In Denmark, occupational safety and health is regulated by the Danish Act on Working Environment and Cooperation at the Workplace.[132] The Danish Working Environment Authority (Arbejdstilsynet) carries out inspections of companies, draws up more detailed rules on health and safety at work and provides information on health and safety at work.[133] The result of each inspection is made public on the web pages of the Danish Working Environment Authority so that the general public, current and prospective employees, customers and other stakeholders can inform themselves about whether a given organization has passed the inspection.[citation needed]

In the Netherlands, the laws for safety and health at work are registered in the Working Conditions Act (Arbeidsomstandighedenwet and Arbeidsomstandighedenbeleid). Apart from the direct laws directed to safety and health in working environments, the private domain has added health and safety rules in Working Conditions Policies (Arbeidsomstandighedenbeleid), which are specified per industry. The Ministry of Social Affairs and Employment (SZW) monitors adherence to the rules through their inspection service. This inspection service investigates industrial accidents and it can suspend work and impose fines when it deems the Working Conditions Act has been violated. Companies can get certified with a VCA certificate for safety, health and environment performance. All employees have to obtain a VCA certificate too, with which they can prove that they know how to work according to the current and applicable safety and environmental regulations.[citation needed]

The main health and safety regulation in Ireland is the Safety, Health and Welfare at Work Act 2005,[134] which replaced earlier legislation from 1989. The Health and Safety Authority, based in Dublin, is responsible for enforcing health and safety at work legislation.[134]

In Spain, occupational safety and health is regulated by the Spanish Act on Prevention of Labor Risks. The Ministry of Labor is the authority responsible for issues relating to labor environment.[citation needed] The National Institute for Safety and Health at Work (Instituto Nacional de Seguridad y Salud en el Trabajo, INSST) is the government's scientific and technical organization specialized in occupational safety and health.[135]

In Sweden, occupational safety and health is regulated by the Work Environment Act.[136] The Swedish Work Environment Authority (Arbetsmiljöverket) is the government agency responsible for issues relating to the working environment. The agency works to disseminate information and furnish advice on OSH, has a mandate to carry out inspections, and a right to issue stipulations and injunctions to any non-compliant employer.[137]

In India, the Ministry of Labour and Employment formulates national policies on occupational safety and health in factories and docks with advice and assistance from its Directorate General Factory Advice Service and Labour Institutes (DGFASLI), and enforces its policies through inspectorates of factories and inspectorates of dock safety. The DGFASLI provides technical support in formulating rules, conducting occupational safety surveys and administering occupational safety training programs.[138]

In Indonesia, the Ministry of Manpower (Kementerian Ketenagakerjaan, or Kemnaker) is responsible to ensure the safety, health and welfare of workers. Important OHS acts include the Occupational Safety Act 1970 and the Occupational Health Act 1992.[139] Sanctions, however, are still low (with a maximum of 15 million rupiahs fine and/or a maximum of one year in prison) and violations are still very frequent.[140]

The Japanese Ministry of Health, Labor and Welfare (MHLW) is the governmental agency overseeing occupational safety and health in Japan. The MHLW is responsible for enforcing Industrial Safety and Health Act of 1972 – the key piece of OSH legislation in Japan –, setting regulations and guidelines, supervising labor inspectors who monitor workplaces for compliance with safety and health standards, investigating accidents, and issuing orders to improve safety conditions. The Labor Standards Bureau is an arm of MHLW tasked with supervising and guiding businesses, inspecting manufacturing facilities for safety and compliance, investigating accidents, collecting statistics, enforcing regulations and administering fines for safety violations, and paying accident compensation for injured workers.[141][142]

The Japan Industrial Safety and Health Association [jp] (JISHA) is a non-profit organization established under the Industrial Safety and Health Act of 1972. It works closely with MHLW, the regulatory body, to promote workplace safety and health. The responsibilities of JISHA include: Providing education and training on occupational safety and health, conducting research and surveys on workplace safety and health issues, offering technical guidance and consultations to businesses, disseminating information and raising awareness about occupational safety and health, and collaborating with international organizations to share best practices and improve global workplace safety standards.[143]

The Japan National Institute of Occupational Safety and Health [jp] (JNIOSH) conducts research to support governmental policies in occupational safety and health. The organization categorizes its research into project studies, cooperative research, fundamental research, and government-requested research. Each category focuses on specific themes, from preventing accidents and ensuring workers' health, to addressing changes in employment structure. The organization sets clear goals, develops road maps, and collaborates with the Ministry of Health, Labor and Welfare to discuss progress and policy contributions.[144]

In Malaysia, the Department of Occupational Safety and Health (DOSH) under the Ministry of Human Resources is responsible to ensure that the safety, health and welfare of workers in both the public and private sector is upheld. DOSH is responsible to enforce the Factories and Machinery Act 1967 and the Occupational Safety and Health Act 1994. Malaysia has a statutory mechanism for worker involvement through elected health and safety representatives and health and safety committees.[145] This followed a similar approach originally adopted in Scandinavia.[citation needed]

In Saudi Arabia, the Ministry of Human Resources and Social Development administrates workers' rights and the labor market as a whole, consistent with human rights rules upheld by the Human Rights Commission of the kingdom.[146]

In Singapore, the Ministry of Manpower (MOM) is the government agency in charge of OHS policies and enforcement. The key piece of legislation regulating aspects of OHS is the Workplace Safety and Health Act.[147] The MOM promotes and manages campaigns against unsafe work practices, such as when working at height, operating cranes and in traffic management. Examples include Operation Cormorant and the Falls Prevention Campaign.[148]

In South Africa the Department of Employment and Labour is responsible for occupational health and safety inspection and enforcement in the commercial and industrial sectors, with the exclusion of mining, where the Department of Mineral Resources is responsible.[149][150] The main statutory legislation on health and safety in the jurisdiction of the Department of Employment and Labour is the OHS Act or OHSA (Act No. 85 of 1993: Occupational Health and Safety Act, as amended by the Occupational Health and Safety Amendment Act, No. 181 of 1993).[149] Regulations implementing the OHS Act include:[151]

In Syria, health and safety is the responsibility of the Ministry of Social Affairs and Labor (Arabic: وزارة الشؤون الاجتماعية والعمل, romanized: Wizārat al-Shuʼūn al-ijtimāʻīyah wa-al-ʻamal).[159]

In Taiwan, the Occupational Safety and Health Administration [zh] of the Ministry of Labor is in charge of occupational safety and health.[160] The matter is governed under the Occupational Safety and Health Act [zh].[161]

In the United Arab Emirates, national OSH legislation is based on the Federal Law on Labor (1980). Order No. 32 of 1982 on Protection from Hazards and Ministerial Decision No. 37/2 of 1982 are also of importance.[162] The competent authority for safety and health at work at the federal level is the Ministry of Human Resources and Emiratisation (MoHRE).[163]

Health and safety legislation in the UK is drawn up and enforced by the Health and Safety Executive and local authorities under the Health and Safety at Work etc. Act 1974 (HASAWA or HSWA).[164][165] HASAWA introduced (section 2) a general duty on an employer to ensure, so far as is reasonably practicable, the health, safety and welfare at work of all his employees, with the intention of giving a legal framework supporting codes of practice not in themselves having legal force but establishing a strong presumption as to what was reasonably practicable (deviations from them could be justified by appropriate risk assessment). The previous reliance on detailed prescriptive rule-setting was seen as having failed to respond rapidly enough to technological change, leaving new technologies potentially unregulated or inappropriately regulated.[166] HSE has continued to make some regulations giving absolute duties (where something must be done with no ""reasonable practicability"" test) but in the UK the regulatory trend is away from prescriptive rules, and toward goal setting and risk assessment. Recent major changes to the laws governing asbestos and fire safety management embrace the concept of risk assessment. The other key aspect of the UK legislation is a statutory mechanism for worker involvement through elected health and safety representatives and health and safety committees. This followed a similar approach in Scandinavia, and that approach has since been adopted in countries such as Australia, Canada, New Zealand and Malaysia.[citation needed]

The Health and Safety Executive service dealing with occupational medicine has been the Employment Medical Advisory Service. In 2014 a new occupational health organization, the Health and Work Service, was created to provide advice and assistance to employers in order to get back to work employees on long-term sick-leave.[167] The service, funded by the government, offers medical assessments and treatment plans, on a voluntary basis, to people on long-term absence from their employer; in return, the government no longer foots the bill for statutory sick pay provided by the employer to the individual.[citation needed]

In the United States, President Richard Nixon signed the Occupational Safety and Health Act into law on 29 December 1970. The act created the three agencies which administer OSH: the Occupational Safety and Health Administration (OSHA), the National Institute for Occupational Safety and Health (NIOSH), and the Occupational Safety and Health Review Commission (OSHRC).[168] The act authorized OSHA to regulate private employers in the 50 states, the District of Columbia, and territories.[169] It includes a general duty clause (29 U.S.C. §654, 5(a)) requiring an employer to comply with the Act and regulations derived from it, and to provide employees with ""employment and a place of employment which are free from recognized hazards that are causing or are likely to cause [them] death or serious physical harm.""[170]

OSHA was established in 1971 under the Department of Labor. It has headquarters in Washington, DC, and ten regional offices, further broken down into districts, each organized into three sections: compliance, training, and assistance. Its stated mission is ""to ensure safe and healthful working conditions for workers by setting and enforcing standards and by providing training, outreach, education and assistance.""[169] The original plan was for OSHA to oversee 50 state plans with OSHA funding 50% of each plan, but this did not work out that way: As of 2023[update] there are 26 approved state plans (with four covering only public employees) and OSHA manages the plan in the states not participating.[93]

OSHA develops safety standards in the Code of Federal Regulations and enforces those safety standards through compliance inspections conducted by Compliance Officers; enforcement resources are focused on high-hazard industries. Worksites may apply to enter OSHA's Voluntary Protection Program (VPP). A successful application leads to an on-site inspection; if this is passed, the site gains VPP status and OSHA no longer inspect it annually nor (normally) visit it unless there is a fatal accident or an employee complaint until VPP revalidation (after three–five years). VPP sites generally have injury and illness rates less than half the average for their industry.[citation needed]

OSHA has a number of specialists in local offices to provide information and training to employers and employees at little or no cost.[4] Similarly OSHA produces a range of publications and funds consultation services available for small businesses.[citation needed]

OSHA has strategic partnership and alliance programs to develop guidelines, assist in compliance, share resources, and educate workers in OHS.[93] OSHA manages Susan B. Harwood grants to non-profit organizations to train workers and employers to recognize, avoid, and prevent safety and health hazards in the workplace.[171] Grants focus on small business, hard-to-reach workers and high-hazard industries.[172]

The National Institute for Occupational Safety and Health (NIOSH), also created under the Occupational Safety and Health Act, is the federal agency responsible for conducting research and making recommendations for the prevention of work-related injury and illness. NIOSH is part of the Centers for Disease Control and Prevention (CDC) within the Department of Health and Human Services.[173]

Those in the field of occupational safety and health come from a wide range of disciplines and professions including medicine, occupational medicine, epidemiology, physiotherapy and rehabilitation, psychology, human factors and ergonomics, and many others. Professionals advise on a broad range of occupational safety and health matters. These include how to avoid particular pre-existing conditions causing a problem in the occupation, correct posture, frequency of rest breaks, preventive actions that can be undertaken, and so forth. The quality of occupational safety is characterized by (1) the indicators reflecting the level of industrial injuries, (2) the average number of days of incapacity for work per employer, (3) employees' satisfaction with their work conditions and (4) employees' motivation to work safely.[174]

The main tasks undertaken by the OSH practitioner include:

OSH specialists examine worksites for environmental or physical factors that could harm employee health, safety, comfort or performance. They then find ways to improve potential risk factors. For example, they may notice potentially hazardous conditions inside a chemical plant and suggest changes to lighting, equipment, materials, or ventilation. OSH technicians assist specialists by collecting data on work environments and implementing the worksite improvements that specialists plan. Technicians also may check to make sure that workers are using required protective gear, such as masks and hardhats. OSH specialists and technicians may develop and conduct employee training programs. These programs cover a range of topics, such as how to use safety equipment correctly and how to respond in an emergency. In the event of a workplace safety incident, specialists and technicians investigate its cause. They then analyze data from the incident, such as the number of people impacted, and look for trends in occurrence. This evaluation helps them to recommend improvements to prevent future incidents.[175]

Given the high demand in society for health and safety provisions at work based on reliable information, OSH professionals should find their roots in evidence-based practice. A new term is ""evidence-informed decision making"". Evidence-based practice can be defined as the use of evidence from literature, and other evidence-based sources, for advice and decisions that favor the health, safety, well-being, and work ability of workers. Therefore, evidence-based information must be integrated with professional expertise and the workers' values. Contextual factors must be considered related to legislation, culture, financial, and technical possibilities. Ethical considerations should be heeded.[176]

The roles and responsibilities of OSH professionals vary regionally but may include evaluating working environments, developing, endorsing and encouraging measures that might prevent injuries and illnesses, providing OSH information to employers, employees, and the public, providing medical examinations, and assessing the success of worker health programs.[citation needed]

In the Netherlands, the required tasks for health and safety staff are only summarily defined and include:[177]

Dutch law influences the job of the safety professional mainly through the requirement on employers to use the services of a certified working-conditions service for advice. A certified service must employ sufficient numbers of four types of certified experts to cover the risks in the organizations which use the service:

In 2004, 14% of health and safety practitioners in the Netherlands had an MSc and 63% had a BSc. 23% had training as an OSH technician.[178]

In Norway, the main required tasks of an occupational health and safety practitioner include:

In 2004, 37% of health and safety practitioners in Norway had an MSc and 44% had a BSc. 19% had training as an OSH technician.[178]

There are multiple levels of training applicable to the field of occupational safety and health. Programs range from individual non-credit certificates and awareness courses focusing on specific areas of concern, to full doctoral programs. The University of Southern California was one of the first schools in the US to offer a PhD program focusing on the field. Further, multiple master's degree programs exist, such as that of the Indiana State University who offer MSc and MA programs. Other masters-level qualifications include the MSc and Master of Research (MRes) degrees offered by the University of Hull in collaboration with the National Examination Board in Occupational Safety and Health (NEBOSH). Graduate programs are designed to train educators, as well as high-level practitioners.[citation needed]

Many OSH generalists focus on undergraduate studies; programs within schools, such as that of the University of North Carolina's online BSc in environmental health and safety, fill a large majority of hygienist needs. However, smaller companies often do not have full-time safety specialists on staff, thus, they appoint a current employee to the responsibility. Individuals finding themselves in positions such as these, or for those enhancing marketability in the job-search and promotion arena, may seek out a credit certificate program. For example, the University of Connecticut's online OSH certificate[179] provides students familiarity with overarching concepts through a 15-credit (5-course) program. Programs such as these are often adequate tools in building a strong educational platform for new safety managers with a minimal outlay of time and money. Further, most hygienists seek certification by organizations that train in specific areas of concentration, focusing on isolated workplace hazards. The American Society of Safety Professionals (ASSP), Board for Global EHS Credentialing (BGC), and American Industrial Hygiene Association (AIHA) offer individual certificates on many different subjects from forklift operation to waste disposal and are the chief facilitators of continuing education in the OSH sector.[citation needed]

In the US, the training of safety professionals is supported by NIOSH through their NIOSH Education and Research Centers.

In the UK, both NEBOSH and the Institution of Occupational Safety and Health (IOSH) develop health and safety qualifications and courses which cater to a mixture of industries and levels of study. Although both organizations are based in the UK, their qualifications are recognized and studied internationally as they are delivered through their own global networks of approved providers. The Health and Safety Executive has also developed health and safety qualifications in collaboration with the NEBOSH.[citation needed]

In Australia, training in OSH is available at the vocational education and training level, and at university undergraduate and postgraduate level. Such university courses may be accredited by an accreditation board of the Safety Institute of Australia. The institute has produced a Body of Knowledge which it considers is required by a generalist safety and health professional and offers a professional qualification.[180] The Australian Institute of Health and Safety has instituted the national Eric Wigglesworth OHS Education Medal to recognize achievement in OSH doctorate education.[181]


Informal or field training may be delivered in the workplace or during off-site training sessions. One form of training delivered in the workplace is known as a toolbox talk. According to the UK's Health and Safety Executive, a toolbox talk is a short presentation to the workforce on a single aspect of health and safety.[182] Such talks are often used, especially in the construction industry, by site supervisors, frontline managers and owners of small construction firms to prepare and deliver advice on matters of health, safety and the environment and to obtain feedback from the workforce.[183]

Virtual reality is a novel tool to deliver safety training in many fields. Some applications have been developed and tested especially for fire and construction safety training.[184][185] Preliminary findings seem to support that virtual reality is more effective than traditional training in knowledge retention.[186]

On an international scale, the World Health Organization (WHO) and the International Labour Organization (ILO) have begun focusing on labor environments in developing nations with projects such as Healthy Cities.[187] Many of these developing countries are stuck in a situation in which their relative lack of resources to invest in OSH leads to increased costs due to work-related illnesses and accidents.[citation needed] The ILO estimates that work-related illness and accidents cost up to 10% of GDP in Latin America, compared with just 2.6% to 3.8% in the EU.[188] There is continued use of asbestos, a notorious hazard, in some developing countries. So asbestos-related disease is expected to continue to be a significant problem well into the future.[citation needed]

There are several broad aspects of artificial intelligence (AI) that may give rise to specific hazards.

Many hazards of AI are psychosocial in nature due to its potential to cause changes in work organization.[189] For example, AI is expected to lead to changes in the skills required of workers, requiring retraining of existing workers, flexibility, and openness to change.[190] Increased monitoring may lead to micromanagement or perception of surveillance, and thus to workplace stress. There is also the risk of people being forced to work at a robot's pace, or to monitor robot performance at nonstandard hours. Additionally, algorithms may show algorithmic bias through being trained on past decisions may mimic undesirable human biases, for example, past discriminatory hiring and firing practices.[191] Some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead.[192]

Physical hazards in the form of human–robot collisions may arise from robots using AI, especially collaborative robots (cobots). Cobots are intended to operate in close proximity to humans, which makes it impossible to implement the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots. Automated guided vehicles are a type of cobot in common use, often as forklifts or pallet jacks in warehouses or factories.[193]

Both applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management. As with all hazards, risk identification is most effective and least costly when done in the design phase.[189] AI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions,[194] as well as information privacy measures.[195] Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues.[195] Workplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate, does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs.[196]

The National Institute of Occupational Safety and Health (NIOSH) National Occupational Research Agenda Manufacturing Council established an externally-lead COVID-19 workgroup to provide exposure control information specific to working in manufacturing environments. The workgroup identified disseminating information most relevant to manufacturing workplaces as a priority, and that would include providing content in Wikipedia. This includes evidence-based practices for infection control plans,[197] and communication tools.

Nanotechnology is an example of a new, relatively unstudied technology. A Swiss survey of 138 companies using or producing nanoparticulate matter in 2006 resulted in forty completed questionnaires. Sixty-five per cent of respondent companies stated they did not have a formal risk assessment process for dealing with nanoparticulate matter.[198] Nanotechnology already presents new issues for OSH professionals that will only become more difficult as nanostructures become more complex. The size of the particles renders most containment and personal protective equipment ineffective. The toxicology values for macro sized industrial substances are rendered inaccurate due to the unique nature of nanoparticulate matter. As nanoparticulate matter decreases in size its relative surface area increases dramatically, increasing any catalytic effect or chemical reactivity substantially versus the known value for the macro substance. This presents a new set of challenges in the near future to rethink contemporary measures to safeguard the health and welfare of employees against a nanoparticulate substance that most conventional controls have not been designed to manage.[199]

Occupational health inequalities refer to differences in occupational injuries and illnesses that are closely linked with demographic, social, cultural, economic, and/or political factors.[200] Although many advances have been made to rectify gaps in occupational health within the past half century, still many persist due to the complex overlapping of occupational health and social factors.[201] There are three main areas of research on occupational health inequities:

Immigrant worker populations often are at greater risk for workplace injuries and fatalities. For example within the United States, immigrant Mexican workers have one of the highest rates of fatal workplace injuries out of all of the working population. Statistics like these are explained through a combination of social, structural, and physical aspects of the workplace. These workers struggle to access safety information and resources in their native languages because of lack of social and political inclusion. In addition to linguistically tailored interventions, it is also critical for the interventions to be culturally appropriate.[205]

Those residing in a country to work without a visa or other formal authorization may also not have access to legal resources and recourse that are designed to protect most workers. Health and Safety organizations that rely on whistleblowers instead of their own independent inspections may be especially at risk of having an incomplete picture of worker health.

Comprehensive Employment and Training Act
"
